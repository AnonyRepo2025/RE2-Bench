diff --git a/sklearn/decomposition/_nmf.py b/sklearn/decomposition/_nmf.py
index 40db8ed..5d47228 100644
--- a/sklearn/decomposition/_nmf.py
+++ b/sklearn/decomposition/_nmf.py
@@ -1,3 +1,196 @@
+import inspect
+def recursive_object_seralizer(obj, visited):
+    seralized_dict = {}
+    keys = list(obj.__dict__)
+    for k in keys:
+        if id(obj.__dict__[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(obj.__dict__[k])
+            continue
+        if isinstance(obj.__dict__[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = obj.__dict__[k]
+        elif isinstance(obj.__dict__[k], tuple):
+            ## handle tuple
+            seralized_dict[k] = recursive_tuple_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], set):
+            ## handle set
+            seralized_dict[k] = recursive_set_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], list):
+            ## handle list
+            seralized_dict[k] = recursive_list_seralizer(obj.__dict__[k], visited)
+        elif hasattr(obj.__dict__[k], '__dict__'):
+            ## handle object
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_object_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], dict):
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_dict_seralizer(obj.__dict__[k], visited)
+        elif callable(obj.__dict__[k]):
+            ## handle function
+            if hasattr(obj.__dict__[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(obj.__dict__[k].__name__)
+        else:
+            seralized_dict[k] = str(obj.__dict__[k])
+    return seralized_dict
+
+def recursive_dict_seralizer(dictionary, visited):
+    seralized_dict = {}
+    keys = list(dictionary)
+    for k in keys:
+        if id(dictionary[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(dictionary[k])
+            continue
+        # if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+        #     pass
+        # else:
+        #     visited.append(id(dictionary[k]))
+        if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = dictionary[k]
+        elif isinstance(dictionary[k], list):
+            seralized_dict[k] = recursive_list_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], tuple):
+            seralized_dict[k] = recursive_tuple_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], set):
+            seralized_dict[k] = recursive_set_seralizer(dictionary[k], visited)        
+        elif hasattr(dictionary[k], '__dict__'):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_object_seralizer(dictionary[k], visited)
+        elif callable(dictionary[k]):
+            if hasattr(dictionary[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(dictionary[k].__name__)
+        elif isinstance(dictionary[k], dict):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_dict_seralizer(dictionary[k], visited)
+        else:
+            seralized_dict[k] =str(dictionary[k])
+    return seralized_dict   
+
+def recursive_set_seralizer(set_data, visited):
+    new_set = set()
+    for s in set_data:
+        if id(s) in visited:
+            continue 
+        if isinstance(s, (float, int, str, bool, type(None))):
+            new_set.add(s)
+        elif isinstance(s, tuple):
+            new_set.add(recursive_tuple_seralizer(s, visited))
+        elif isinstance(s, list):
+            new_set.add(recursive_list_seralizer(s, visited))
+        elif isinstance(s, set):
+            new_set.add(recursive_set_seralizer(s,visited))
+        elif isinstance(s, dict):
+            visited.append(id(s))
+            new_set.add(recursive_dict_seralizer(s, visited))
+        elif hasattr(s, '__dict__'):
+            visited.append(id(s))
+            new_set.add(str(recursive_object_seralizer(s, visited)))
+        elif callable(s):
+            if hasattr(s, '__name__'):
+                new_set.add("<function {}>".format(s.__name__))
+        else:
+            new_set.add(str(s))
+    return new_set
+    
+
+def recursive_tuple_seralizer(tup, visited):
+    new_tup = ()
+    for t in tup:
+        if id(t) in visited:
+           continue
+        if isinstance(t, (float, int, str, bool, type(None))):
+            new_tup = (*new_tup, t)
+        elif isinstance(t, tuple):
+            new_tup = (*new_tup, recursive_tuple_seralizer(t, visited))
+        elif isinstance(t, list):
+            new_tup = (*new_tup, recursive_list_seralizer(t, visited))
+        elif isinstance(t, set):
+            new_tup = (*new_tup, recursive_set_seralizer(t, visited))
+        elif isinstance(t, dict):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_dict_seralizer(t, visited))
+        elif hasattr(t, '__dict__'):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_object_seralizer(t, visited))
+        elif callable(t):
+            if hasattr(t, '__name__'):
+                new_tup = (*new_tup, "<function {}>".format(t.__name__))
+        else:
+            new_tup = (*new_tup, str(t))
+    return new_tup
+
+def recursive_list_seralizer(li, visited):
+    new_list = []
+    for l in li:
+        if id(l) in visited:
+            continue
+        if isinstance(l, (float, int, str, bool, type(None))):
+            new_list.append(l)
+        elif isinstance(l, tuple):
+            new_list.append(recursive_tuple_seralizer(l, visited))
+        elif isinstance(l, list):
+            new_list.append(recursive_list_seralizer(l, visited))
+        elif isinstance(l, set):
+            new_list.append(recursive_set_seralizer(l, visited))
+        elif hasattr(l, '__dict__'):
+            visited.append(id(l))
+            new_list.append(recursive_object_seralizer(l, visited))
+        elif isinstance(l, dict):
+            visited.append(id(l))
+            new_list.append(recursive_dict_seralizer(l, visited))
+        elif callable(l):
+            if hasattr(l, '__name__'):
+                new_list.append("<function {}>".format(l.__name__))
+        else:
+            new_list.append(str(l))       
+
+def inspect_code(func):
+    def wrapper(*args, **kwargs):
+        visited = []
+        filename = "/home/changshu/CODEMIND/scripts/swebench/swebench_playground/obj/scikit-learn__scikit-learn-26634/sklearn/decomposition/_nmf.py"
+        para_dict = {"name": func.__name__}
+        args_names = inspect.getfullargspec(func).args
+        if len(args) > 0 and hasattr(args[0], '__dict__') and args_names[0] == 'self':
+            ## 'self'
+            self_args = args[0]
+            para_dict['self'] = recursive_object_seralizer(self_args, [id(self_args)])
+        else:
+            para_dict['self'] = {}
+        if len(args) > 0 :
+            if args_names[0] == 'self':
+                other_args = {}
+                for m,n in zip(args_names[1:], args[1:]):
+                    other_args[m] = n
+            else:
+                other_args = {}
+                for m,n in zip(args_names, args):
+                    other_args[m] = n
+            para_dict['args'] = recursive_dict_seralizer(other_args, [id(other_args)])
+        else:
+            para_dict['args'] = {}
+        if kwargs:
+            para_dict['kwargs'] = recursive_dict_seralizer(kwargs, [id(kwargs)])
+        else:
+            para_dict['kwargs'] = {}
+            
+        result = func(*args, **kwargs)
+        ## seralize the return value
+        if isinstance(result, tuple):
+            ret = recursive_tuple_seralizer(result, [])
+        elif isinstance(result, (float, int, str)):
+            ret = result
+        elif isinstance(result, list):
+            ret = recursive_list_seralizer(result, [])
+        elif isinstance(result, dict):
+            ret = recursive_dict_seralizer(result, [])
+        elif hasattr(result, '__dict__'):
+            ret = recursive_object_seralizer(result, [])
+        elif callable(result):
+            ret = "<function {}>".format(result.__name__)
+        else:
+            ret = str(result)
+        para_dict["return"] = ret
+        print("@[DATA]@", filename,"[SEP]", para_dict, "[/SEP]")
+        return result
+    return wrapper
 """ Non-negative matrix factorization.
 """
 # Author: Vlad Niculae
@@ -27,6 +220,7 @@ from ..base import (
 from ..exceptions import ConvergenceWarning
 from ..utils import check_array, check_random_state, gen_batches, metadata_routing
 from ..utils._param_validation import (
+    Hidden,
     Interval,
     StrOptions,
     validate_params,
@@ -69,14 +263,19 @@ def trace_dot(X, Y):
 
 def _check_init(A, shape, whom):
     A = check_array(A)
-    if np.shape(A) != shape:
+    if shape[0] != "auto" and A.shape[0] != shape[0]:
+        raise ValueError(
+            f"Array with wrong first dimension passed to {whom}. Expected {shape[0]}, "
+            f"but got {A.shape[0]}."
+        )
+    if shape[1] != "auto" and A.shape[1] != shape[1]:
         raise ValueError(
-            "Array with wrong shape passed to %s. Expected %s, but got %s "
-            % (whom, shape, np.shape(A))
+            f"Array with wrong second dimension passed to {whom}. Expected {shape[1]}, "
+            f"but got {A.shape[1]}."
         )
     check_non_negative(A, whom)
     if np.max(A) == 0:
-        raise ValueError("Array passed to %s is full of zeros." % whom)
+        raise ValueError(f"Array passed to {whom} is full of zeros.")
 
 
 def _beta_divergence(X, W, H, beta, square_root=False):
@@ -899,11 +1098,12 @@ def _fit_multiplicative_update(
     },
     prefer_skip_nested_validation=False,
 )
+@inspect_code
 def non_negative_factorization(
     X,
     W=None,
     H=None,
-    n_components=None,
+    n_components="warn",
     *,
     init=None,
     update_H=True,
@@ -976,9 +1176,14 @@ def non_negative_factorization(
         If `update_H=False`, it is used as a constant, to solve for W only.
         If `None`, uses the initialisation method specified in `init`.
 
-    n_components : int, default=None
+    n_components : int or {'auto'} or None, default=None
         Number of components, if n_components is not set all features
         are kept.
+        If `n_components='auto'`, the number of components is automatically inferred
+        from `W` or `H` shapes.
+
+        .. versionchanged:: 1.4
+            Added `'auto'` value.
 
     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
         Method used to initialize the procedure.
@@ -1133,7 +1338,12 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,
     __metadata_request__inverse_transform = {"W": metadata_routing.UNUSED}
 
     _parameter_constraints: dict = {
-        "n_components": [Interval(Integral, 1, None, closed="left"), None],
+        "n_components": [
+            Interval(Integral, 1, None, closed="left"),
+            None,
+            StrOptions({"auto"}),
+            Hidden(StrOptions({"warn"})),
+        ],
         "init": [
             StrOptions({"random", "nndsvd", "nndsvda", "nndsvdar", "custom"}),
             None,
@@ -1153,7 +1363,7 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,
 
     def __init__(
         self,
-        n_components=None,
+        n_components="warn",
         *,
         init=None,
         beta_loss="frobenius",
@@ -1176,9 +1386,20 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,
         self.l1_ratio = l1_ratio
         self.verbose = verbose
 
+    @inspect_code
     def _check_params(self, X):
         # n_components
         self._n_components = self.n_components
+        if self.n_components == "warn":
+            warnings.warn(
+                (
+                    "The default value of `n_components` will change from `None` to"
+                    " `'auto'` in 1.6. Set the value of `n_components` to `None`"
+                    " explicitly to supress the warning."
+                ),
+                FutureWarning,
+            )
+            self._n_components = None  # Keeping the old default value
         if self._n_components is None:
             self._n_components = X.shape[1]
 
@@ -1188,32 +1409,61 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,
     def _check_w_h(self, X, W, H, update_H):
         """Check W and H, or initialize them."""
         n_samples, n_features = X.shape
+
         if self.init == "custom" and update_H:
             _check_init(H, (self._n_components, n_features), "NMF (input H)")
             _check_init(W, (n_samples, self._n_components), "NMF (input W)")
+            if self._n_components == "auto":
+                self._n_components = H.shape[0]
+
             if H.dtype != X.dtype or W.dtype != X.dtype:
                 raise TypeError(
                     "H and W should have the same dtype as X. Got "
                     "H.dtype = {} and W.dtype = {}.".format(H.dtype, W.dtype)
                 )
+
         elif not update_H:
+            if W is not None:
+                warnings.warn(
+                    "When update_H=False, the provided initial W is not used.",
+                    RuntimeWarning,
+                )
+
             _check_init(H, (self._n_components, n_features), "NMF (input H)")
+            if self._n_components == "auto":
+                self._n_components = H.shape[0]
+
             if H.dtype != X.dtype:
                 raise TypeError(
                     "H should have the same dtype as X. Got H.dtype = {}.".format(
                         H.dtype
                     )
                 )
+
             # 'mu' solver should not be initialized by zeros
             if self.solver == "mu":
                 avg = np.sqrt(X.mean() / self._n_components)
                 W = np.full((n_samples, self._n_components), avg, dtype=X.dtype)
             else:
                 W = np.zeros((n_samples, self._n_components), dtype=X.dtype)
+
         else:
+            if W is not None or H is not None:
+                warnings.warn(
+                    (
+                        "When init!='custom', provided W or H are ignored. Set "
+                        " init='custom' to use them as initialization."
+                    ),
+                    RuntimeWarning,
+                )
+
+            if self._n_components == "auto":
+                self._n_components = X.shape[1]
+
             W, H = _initialize_nmf(
                 X, self._n_components, init=self.init, random_state=self.random_state
             )
+
         return W, H
 
     def _compute_regularization(self, X):
@@ -1352,9 +1602,14 @@ class NMF(_BaseNMF):
 
     Parameters
     ----------
-    n_components : int, default=None
+    n_components : int or {'auto'} or None, default=None
         Number of components, if n_components is not set all features
         are kept.
+        If `n_components='auto'`, the number of components is automatically inferred
+        from W or H shapes.
+
+        .. versionchanged:: 1.4
+            Added `'auto'` value.
 
     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
         Method used to initialize the procedure.
@@ -1517,7 +1772,7 @@ class NMF(_BaseNMF):
 
     def __init__(
         self,
-        n_components=None,
+        n_components="warn",
         *,
         init=None,
         solver="cd",
@@ -1786,9 +2041,14 @@ class MiniBatchNMF(_BaseNMF):
 
     Parameters
     ----------
-    n_components : int, default=None
+    n_components : int or {'auto'} or None, default=None
         Number of components, if `n_components` is not set all features
         are kept.
+        If `n_components='auto'`, the number of components is automatically inferred
+        from W or H shapes.
+
+        .. versionchanged:: 1.4
+            Added `'auto'` value.
 
     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
         Method used to initialize the procedure.
@@ -1953,7 +2213,7 @@ class MiniBatchNMF(_BaseNMF):
 
     def __init__(
         self,
-        n_components=None,
+        n_components="warn",
         *,
         init=None,
         batch_size=1024,
diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py
index 2cd027f..45379f4 100644
--- a/sklearn/decomposition/tests/test_nmf.py
+++ b/sklearn/decomposition/tests/test_nmf.py
@@ -45,9 +45,11 @@ def test_initialize_nn_output():
         assert not ((W < 0).any() or (H < 0).any())
 
 
+# TODO(1.6): remove the warning filter for `n_components`
 @pytest.mark.filterwarnings(
     r"ignore:The multiplicative update \('mu'\) solver cannot update zeros present in"
-    r" the initialization"
+    r" the initialization",
+    "ignore:The default value of `n_components` will change",
 )
 def test_parameter_checking():
     # Here we only check for invalid parameter values that are not already
@@ -267,6 +269,8 @@ def test_nmf_inverse_transform(solver):
     assert_array_almost_equal(A, A_new, decimal=2)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 def test_mbnmf_inverse_transform():
     # Test that MiniBatchNMF.transform followed by MiniBatchNMF.inverse_transform
     # is close to the identity
@@ -344,6 +348,8 @@ def test_nmf_sparse_transform(Estimator, solver):
     assert_allclose(A_fit_tr, A_tr, atol=1e-1)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 @pytest.mark.parametrize("init", ["random", "nndsvd"])
 @pytest.mark.parametrize("solver", ("cd", "mu"))
 @pytest.mark.parametrize("alpha_W", (0.0, 1.0))
@@ -610,6 +616,8 @@ def test_nmf_negative_beta_loss():
         _assert_nmf_no_nan(X_csr, beta_loss)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 @pytest.mark.parametrize("beta_loss", [-0.5, 0.0])
 def test_minibatch_nmf_negative_beta_loss(beta_loss):
     """Check that an error is raised if beta_loss < 0 and X contains zeros."""
@@ -766,6 +774,8 @@ def test_nmf_underflow():
     assert_almost_equal(res, ref)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 @pytest.mark.parametrize(
     "dtype_in, dtype_out",
     [
@@ -784,13 +794,21 @@ def test_nmf_dtype_match(Estimator, solver, dtype_in, dtype_out):
     X = np.random.RandomState(0).randn(20, 15).astype(dtype_in, copy=False)
     np.abs(X, out=X)
 
-    nmf = Estimator(alpha_W=1.0, alpha_H=1.0, tol=1e-2, random_state=0, **solver)
+    nmf = Estimator(
+        alpha_W=1.0,
+        alpha_H=1.0,
+        tol=1e-2,
+        random_state=0,
+        **solver,
+    )
 
     assert nmf.fit(X).transform(X).dtype == dtype_out
     assert nmf.fit_transform(X).dtype == dtype_out
     assert nmf.components_.dtype == dtype_out
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 @pytest.mark.parametrize(
     ["Estimator", "solver"],
     [[NMF, {"solver": "cd"}], [NMF, {"solver": "mu"}], [MiniBatchNMF, {}]],
@@ -807,6 +825,8 @@ def test_nmf_float32_float64_consistency(Estimator, solver):
     assert_allclose(W32, W64, atol=1e-5)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 @pytest.mark.parametrize("Estimator", [NMF, MiniBatchNMF])
 def test_nmf_custom_init_dtype_error(Estimator):
     # Check that an error is raise if custom H and/or W don't have the same
@@ -896,6 +916,8 @@ def test_feature_names_out():
     assert_array_equal([f"nmf{i}" for i in range(3)], names)
 
 
+# TODO(1.6): remove the warning filter
+@pytest.mark.filterwarnings("ignore:The default value of `n_components` will change")
 def test_minibatch_nmf_verbose():
     # Check verbose mode of MiniBatchNMF for better coverage.
     A = np.random.RandomState(0).random_sample((100, 10))
@@ -932,3 +954,106 @@ def test_NMF_inverse_transform_W_deprecation():
 
     with pytest.warns(FutureWarning, match="Input argument `W` was renamed to `Xt`"):
         est.inverse_transform(W=Xt)
+
+
+@pytest.mark.parametrize("Estimator", [NMF, MiniBatchNMF])
+def test_nmf_n_components_auto(Estimator):
+    # Check that n_components is correctly inferred
+    # from the provided custom initialization.
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    W = rng.random_sample((6, 2))
+    H = rng.random_sample((2, 5))
+    est = Estimator(
+        n_components="auto",
+        init="custom",
+        random_state=0,
+        tol=1e-6,
+    )
+    est.fit_transform(X, W=W, H=H)
+    assert est._n_components == H.shape[0]
+
+
+def test_nmf_non_negative_factorization_n_components_auto():
+    # Check that n_components is correctly inferred from the provided
+    # custom initialization.
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    W_init = rng.random_sample((6, 2))
+    H_init = rng.random_sample((2, 5))
+    W, H, _ = non_negative_factorization(
+        X, W=W_init, H=H_init, init="custom", n_components="auto"
+    )
+    assert H.shape == H_init.shape
+    assert W.shape == W_init.shape
+
+
+# TODO(1.6): remove
+def test_nmf_n_components_default_value_warning():
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    H = rng.random_sample((2, 5))
+    with pytest.warns(
+        FutureWarning, match="The default value of `n_components` will change from"
+    ):
+        non_negative_factorization(X, H=H)
+
+
+def test_nmf_n_components_auto_no_h_update():
+    # Tests that non_negative_factorization does not fail when setting
+    # n_components="auto" also tests that the inferred n_component
+    # value is the right one.
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    H_true = rng.random_sample((2, 5))
+    W, H, _ = non_negative_factorization(
+        X, H=H_true, n_components="auto", update_H=False
+    )  # should not fail
+    assert_allclose(H, H_true)
+    assert W.shape == (X.shape[0], H_true.shape[0])
+
+
+def test_nmf_w_h_not_used_warning():
+    # Check that warnings are raised if user provided W and H are not used
+    # and initialization overrides value of W or H
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    W_init = rng.random_sample((6, 2))
+    H_init = rng.random_sample((2, 5))
+    with pytest.warns(
+        RuntimeWarning,
+        match="When init!='custom', provided W or H are ignored",
+    ):
+        non_negative_factorization(X, H=H_init, update_H=True, n_components="auto")
+
+    with pytest.warns(
+        RuntimeWarning,
+        match="When init!='custom', provided W or H are ignored",
+    ):
+        non_negative_factorization(
+            X, W=W_init, H=H_init, update_H=True, n_components="auto"
+        )
+
+    with pytest.warns(
+        RuntimeWarning, match="When update_H=False, the provided initial W is not used."
+    ):
+        # When update_H is False, W is ignored regardless of init
+        # TODO: use the provided W when init="custom".
+        non_negative_factorization(
+            X, W=W_init, H=H_init, update_H=False, n_components="auto"
+        )
+
+
+def test_nmf_custom_init_shape_error():
+    # Check that an informative error is raised when custom initialization does not
+    # have the right shape
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((6, 5))
+    H = rng.random_sample((2, 5))
+    nmf = NMF(n_components=2, init="custom", random_state=0)
+
+    with pytest.raises(ValueError, match="Array with wrong first dimension passed"):
+        nmf.fit(X, H=H, W=rng.random_sample((5, 2)))
+
+    with pytest.raises(ValueError, match="Array with wrong second dimension passed"):
+        nmf.fit(X, H=H, W=rng.random_sample((6, 3)))
diff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py
index d395b6c..f2235d5 100644
--- a/sklearn/tests/test_docstring_parameters.py
+++ b/sklearn/tests/test_docstring_parameters.py
@@ -260,6 +260,10 @@ def test_fit_docstring_attributes(name, Estimator):
     ):
         est.set_params(force_alpha=True)
 
+    # TODO(1.6): remove (avoid FutureWarning)
+    if Estimator.__name__ in ("NMF", "MiniBatchNMF"):
+        est.set_params(n_components="auto")
+
     if Estimator.__name__ == "QuantileRegressor":
         solver = "highs" if sp_version >= parse_version("1.6.0") else "interior-point"
         est.set_params(solver=solver)
