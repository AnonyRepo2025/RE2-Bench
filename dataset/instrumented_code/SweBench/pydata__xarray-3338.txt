diff --git a/xarray/core/groupby.py b/xarray/core/groupby.py
index 3399b27..bc9d94d 100644
--- a/xarray/core/groupby.py
+++ b/xarray/core/groupby.py
@@ -1,3 +1,196 @@
+import inspect
+def recursive_object_seralizer(obj, visited):
+    seralized_dict = {}
+    keys = list(obj.__dict__)
+    for k in keys:
+        if id(obj.__dict__[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(obj.__dict__[k])
+            continue
+        if isinstance(obj.__dict__[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = obj.__dict__[k]
+        elif isinstance(obj.__dict__[k], tuple):
+            ## handle tuple
+            seralized_dict[k] = recursive_tuple_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], set):
+            ## handle set
+            seralized_dict[k] = recursive_set_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], list):
+            ## handle list
+            seralized_dict[k] = recursive_list_seralizer(obj.__dict__[k], visited)
+        elif hasattr(obj.__dict__[k], '__dict__'):
+            ## handle object
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_object_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], dict):
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_dict_seralizer(obj.__dict__[k], visited)
+        elif callable(obj.__dict__[k]):
+            ## handle function
+            if hasattr(obj.__dict__[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(obj.__dict__[k].__name__)
+        else:
+            seralized_dict[k] = str(obj.__dict__[k])
+    return seralized_dict
+
+def recursive_dict_seralizer(dictionary, visited):
+    seralized_dict = {}
+    keys = list(dictionary)
+    for k in keys:
+        if id(dictionary[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(dictionary[k])
+            continue
+        # if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+        #     pass
+        # else:
+        #     visited.append(id(dictionary[k]))
+        if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = dictionary[k]
+        elif isinstance(dictionary[k], list):
+            seralized_dict[k] = recursive_list_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], tuple):
+            seralized_dict[k] = recursive_tuple_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], set):
+            seralized_dict[k] = recursive_set_seralizer(dictionary[k], visited)        
+        elif hasattr(dictionary[k], '__dict__'):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_object_seralizer(dictionary[k], visited)
+        elif callable(dictionary[k]):
+            if hasattr(dictionary[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(dictionary[k].__name__)
+        elif isinstance(dictionary[k], dict):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_dict_seralizer(dictionary[k], visited)
+        else:
+            seralized_dict[k] =str(dictionary[k])
+    return seralized_dict   
+
+def recursive_set_seralizer(set_data, visited):
+    new_set = set()
+    for s in set_data:
+        if id(s) in visited:
+            continue 
+        if isinstance(s, (float, int, str, bool, type(None))):
+            new_set.add(s)
+        elif isinstance(s, tuple):
+            new_set.add(recursive_tuple_seralizer(s, visited))
+        elif isinstance(s, list):
+            new_set.add(recursive_list_seralizer(s, visited))
+        elif isinstance(s, set):
+            new_set.add(recursive_set_seralizer(s,visited))
+        elif isinstance(s, dict):
+            visited.append(id(s))
+            new_set.add(recursive_dict_seralizer(s, visited))
+        elif hasattr(s, '__dict__'):
+            visited.append(id(s))
+            new_set.add(str(recursive_object_seralizer(s, visited)))
+        elif callable(s):
+            if hasattr(s, '__name__'):
+                new_set.add("<function {}>".format(s.__name__))
+        else:
+            new_set.add(str(s))
+    return new_set
+    
+
+def recursive_tuple_seralizer(tup, visited):
+    new_tup = ()
+    for t in tup:
+        if id(t) in visited:
+           continue
+        if isinstance(t, (float, int, str, bool, type(None))):
+            new_tup = (*new_tup, t)
+        elif isinstance(t, tuple):
+            new_tup = (*new_tup, recursive_tuple_seralizer(t, visited))
+        elif isinstance(t, list):
+            new_tup = (*new_tup, recursive_list_seralizer(t, visited))
+        elif isinstance(t, set):
+            new_tup = (*new_tup, recursive_set_seralizer(t, visited))
+        elif isinstance(t, dict):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_dict_seralizer(t, visited))
+        elif hasattr(t, '__dict__'):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_object_seralizer(t, visited))
+        elif callable(t):
+            if hasattr(t, '__name__'):
+                new_tup = (*new_tup, "<function {}>".format(t.__name__))
+        else:
+            new_tup = (*new_tup, str(t))
+    return new_tup
+
+def recursive_list_seralizer(li, visited):
+    new_list = []
+    for l in li:
+        if id(l) in visited:
+            continue
+        if isinstance(l, (float, int, str, bool, type(None))):
+            new_list.append(l)
+        elif isinstance(l, tuple):
+            new_list.append(recursive_tuple_seralizer(l, visited))
+        elif isinstance(l, list):
+            new_list.append(recursive_list_seralizer(l, visited))
+        elif isinstance(l, set):
+            new_list.append(recursive_set_seralizer(l, visited))
+        elif hasattr(l, '__dict__'):
+            visited.append(id(l))
+            new_list.append(recursive_object_seralizer(l, visited))
+        elif isinstance(l, dict):
+            visited.append(id(l))
+            new_list.append(recursive_dict_seralizer(l, visited))
+        elif callable(l):
+            if hasattr(l, '__name__'):
+                new_list.append("<function {}>".format(l.__name__))
+        else:
+            new_list.append(str(l))       
+
+def inspect_code(func):
+    def wrapper(*args, **kwargs):
+        visited = []
+        filename = "/home/changshu/CODEMIND/scripts/swebench/swebench_playground/obj/pydata__xarray-3338/xarray/core/groupby.py"
+        para_dict = {"name": func.__name__}
+        args_names = inspect.getfullargspec(func).args
+        if len(args) > 0 and hasattr(args[0], '__dict__') and args_names[0] == 'self':
+            ## 'self'
+            self_args = args[0]
+            para_dict['self'] = recursive_object_seralizer(self_args, [id(self_args)])
+        else:
+            para_dict['self'] = {}
+        if len(args) > 0 :
+            if args_names[0] == 'self':
+                other_args = {}
+                for m,n in zip(args_names[1:], args[1:]):
+                    other_args[m] = n
+            else:
+                other_args = {}
+                for m,n in zip(args_names, args):
+                    other_args[m] = n
+            para_dict['args'] = recursive_dict_seralizer(other_args, [id(other_args)])
+        else:
+            para_dict['args'] = {}
+        if kwargs:
+            para_dict['kwargs'] = recursive_dict_seralizer(kwargs, [id(kwargs)])
+        else:
+            para_dict['kwargs'] = {}
+            
+        result = func(*args, **kwargs)
+        ## seralize the return value
+        if isinstance(result, tuple):
+            ret = recursive_tuple_seralizer(result, [])
+        elif isinstance(result, (float, int, str)):
+            ret = result
+        elif isinstance(result, list):
+            ret = recursive_list_seralizer(result, [])
+        elif isinstance(result, dict):
+            ret = recursive_dict_seralizer(result, [])
+        elif hasattr(result, '__dict__'):
+            ret = recursive_object_seralizer(result, [])
+        elif callable(result):
+            ret = "<function {}>".format(result.__name__)
+        else:
+            ret = str(result)
+        para_dict["return"] = ret
+        print("@[DATA]@", filename,"[SEP]", para_dict, "[/SEP]")
+        return result
+    return wrapper
 import datetime
 import functools
 import warnings
@@ -7,7 +200,7 @@ import pandas as pd
 
 from . import dtypes, duck_array_ops, nputils, ops
 from .arithmetic import SupportsArithmetic
-from .common import ImplementsArrayReduce, ImplementsDatasetReduce
+from .common import ALL_DIMS, ImplementsArrayReduce, ImplementsDatasetReduce
 from .concat import concat
 from .formatting import format_array_flat
 from .options import _get_keep_attrs
@@ -248,8 +441,10 @@ class GroupBy(SupportsArithmetic):
         "_restore_coord_dims",
         "_stacked_dim",
         "_unique_coord",
+        "_dims",
     )
 
+    @inspect_code
     def __init__(
         self,
         obj,
@@ -320,6 +515,8 @@ class GroupBy(SupportsArithmetic):
         full_index = None
 
         if bins is not None:
+            if np.isnan(bins).all():
+                raise ValueError("All bin edges are NaN.")
             binned = pd.cut(group.values, bins, **cut_kwargs)
             new_dim_name = group.name + "_bins"
             group = DataArray(binned, group.coords, name=new_dim_name)
@@ -351,6 +548,16 @@ class GroupBy(SupportsArithmetic):
             )
             unique_coord = IndexVariable(group.name, unique_values)
 
+        if len(group_indices) == 0:
+            if bins is not None:
+                raise ValueError(
+                    "None of the data falls within bins with edges %r" % bins
+                )
+            else:
+                raise ValueError(
+                    "Failed to group data. Are you grouping by a variable that is all NaN?"
+                )
+
         if (
             isinstance(obj, DataArray)
             and restore_coord_dims is None
@@ -379,6 +586,16 @@ class GroupBy(SupportsArithmetic):
 
         # cached attributes
         self._groups = None
+        self._dims = None
+
+    @property
+    def dims(self):
+        if self._dims is None:
+            self._dims = self._obj.isel(
+                **{self._group_dim: self._group_indices[0]}
+            ).dims
+
+        return self._dims
 
     @property
     def groups(self):
@@ -393,8 +610,9 @@ class GroupBy(SupportsArithmetic):
     def __iter__(self):
         return zip(self._unique_coord.values, self._iter_grouped())
 
+    @inspect_code
     def __repr__(self):
-        return "%s, grouped over %r \n%r groups with labels %s" % (
+        return "%s, grouped over %r \n%r groups with labels %s." % (
             self.__class__.__name__,
             self._unique_coord.name,
             self._unique_coord.size,
@@ -680,6 +898,7 @@ class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):
         combined = self._maybe_unstack(combined)
         return combined
 
+    @inspect_code
     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
         """Compute the qth quantile over each array in the groups and
         concatenate them together into a new array.
@@ -689,7 +908,7 @@ class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):
         q : float in range of [0,1] (or sequence of floats)
             Quantile to compute, which must be between 0 and 1
             inclusive.
-        dim : str or sequence of str, optional
+        dim : xarray.ALL_DIMS, str or sequence of str, optional
             Dimension(s) over which to apply quantile.
             Defaults to the grouped dimension.
         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
@@ -734,6 +953,7 @@ class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):
             out = out.drop("quantile")
         return out
 
+    @inspect_code
     def reduce(
         self, func, dim=None, axis=None, keep_attrs=None, shortcut=True, **kwargs
     ):
@@ -746,7 +966,7 @@ class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):
             Function which can be called in the form
             `func(x, axis=axis, **kwargs)` to return the result of collapsing
             an np.ndarray over an integer valued axis.
-        dim : str or sequence of str, optional
+        dim : xarray.ALL_DIMS, str or sequence of str, optional
             Dimension(s) over which to apply `func`.
         axis : int or sequence of int, optional
             Axis(es) over which to apply `func`. Only one of the 'dimension'
@@ -765,9 +985,18 @@ class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):
             Array with summarized data and the indicated dimension(s)
             removed.
         """
+        if dim is None:
+            dim = self._group_dim
+
         if keep_attrs is None:
             keep_attrs = _get_keep_attrs(default=False)
 
+        if dim is not ALL_DIMS and dim not in self.dims:
+            raise ValueError(
+                "cannot reduce over dimension %r. expected either xarray.ALL_DIMS to reduce over all dimensions or one or more of %r."
+                % (dim, self.dims)
+            )
+
         def reduce_array(ar):
             return ar.reduce(func, dim, axis, keep_attrs=keep_attrs, **kwargs)
 
@@ -835,7 +1064,7 @@ class DatasetGroupBy(GroupBy, ImplementsDatasetReduce):
             Function which can be called in the form
             `func(x, axis=axis, **kwargs)` to return the result of collapsing
             an np.ndarray over an integer valued axis.
-        dim : str or sequence of str, optional
+        dim : xarray.ALL_DIMS, str or sequence of str, optional
             Dimension(s) over which to apply `func`.
         axis : int or sequence of int, optional
             Axis(es) over which to apply `func`. Only one of the 'dimension'
@@ -863,6 +1092,12 @@ class DatasetGroupBy(GroupBy, ImplementsDatasetReduce):
         def reduce_dataset(ds):
             return ds.reduce(func, dim, keep_attrs, **kwargs)
 
+        if dim is not ALL_DIMS and dim not in self.dims:
+            raise ValueError(
+                "cannot reduce over dimension %r. expected either xarray.ALL_DIMS to reduce over all dimensions or one or more of %r."
+                % (dim, self.dims)
+            )
+
         return self.apply(reduce_dataset)
 
     def assign(self, **kwargs):
diff --git a/xarray/tests/test_dataarray.py b/xarray/tests/test_dataarray.py
index 4bae0d8..520174d 100644
--- a/xarray/tests/test_dataarray.py
+++ b/xarray/tests/test_dataarray.py
@@ -2579,6 +2579,15 @@ class TestDataArray:
         expected = change_metadata(expected)
         assert_equal(expected, actual)
 
+    def test_groupby_reduce_dimension_error(self):
+        array = self.make_groupby_example_array()
+        grouped = array.groupby("y")
+        with raises_regex(ValueError, "cannot reduce over dimension 'y'"):
+            grouped.mean()
+
+        grouped = array.groupby("y", squeeze=False)
+        assert_identical(array, grouped.mean())
+
     def test_groupby_math(self):
         array = self.make_groupby_example_array()
         for squeeze in [True, False]:
diff --git a/xarray/tests/test_groupby.py b/xarray/tests/test_groupby.py
index 463a6a3..957d480 100644
--- a/xarray/tests/test_groupby.py
+++ b/xarray/tests/test_groupby.py
@@ -5,7 +5,7 @@ import pytest
 import xarray as xr
 from xarray.core.groupby import _consolidate_slices
 
-from . import assert_identical
+from . import assert_identical, raises_regex
 
 
 def test_consolidate_slices():
@@ -21,6 +21,19 @@ def test_consolidate_slices():
         _consolidate_slices([slice(3), 4])
 
 
+def test_groupby_dims_property():
+    ds = xr.Dataset(
+        {"foo": (("x", "y", "z"), np.random.randn(3, 4, 2))},
+        {"x": ["a", "bcd", "c"], "y": [1, 2, 3, 4], "z": [1, 2]},
+    )
+
+    assert ds.groupby("x").dims == ds.isel(x=1).dims
+    assert ds.groupby("y").dims == ds.isel(y=1).dims
+
+    stacked = ds.stack({"xy": ("x", "y")})
+    assert stacked.groupby("xy").dims == stacked.isel(xy=0).dims
+
+
 def test_multi_index_groupby_apply():
     # regression test for GH873
     ds = xr.Dataset(
@@ -222,13 +235,13 @@ def test_groupby_repr(obj, dim):
     expected += ", grouped over %r " % dim
     expected += "\n%r groups with labels " % (len(np.unique(obj[dim])))
     if dim == "x":
-        expected += "1, 2, 3, 4, 5"
+        expected += "1, 2, 3, 4, 5."
     elif dim == "y":
-        expected += "0, 1, 2, 3, 4, 5, ..., 15, 16, 17, 18, 19"
+        expected += "0, 1, 2, 3, 4, 5, ..., 15, 16, 17, 18, 19."
     elif dim == "z":
-        expected += "'a', 'b', 'c'"
+        expected += "'a', 'b', 'c'."
     elif dim == "month":
-        expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12"
+        expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12."
     assert actual == expected
 
 
@@ -238,8 +251,29 @@ def test_groupby_repr_datetime(obj):
     expected = "%sGroupBy" % obj.__class__.__name__
     expected += ", grouped over 'month' "
     expected += "\n%r groups with labels " % (len(np.unique(obj.t.dt.month)))
-    expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12"
+    expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12."
     assert actual == expected
 
 
+def test_groupby_grouping_errors():
+    dataset = xr.Dataset({"foo": ("x", [1, 1, 1])}, {"x": [1, 2, 3]})
+    with raises_regex(ValueError, "None of the data falls within bins with edges"):
+        dataset.groupby_bins("x", bins=[0.1, 0.2, 0.3])
+
+    with raises_regex(ValueError, "None of the data falls within bins with edges"):
+        dataset.to_array().groupby_bins("x", bins=[0.1, 0.2, 0.3])
+
+    with raises_regex(ValueError, "All bin edges are NaN."):
+        dataset.groupby_bins("x", bins=[np.nan, np.nan, np.nan])
+
+    with raises_regex(ValueError, "All bin edges are NaN."):
+        dataset.to_array().groupby_bins("x", bins=[np.nan, np.nan, np.nan])
+
+    with raises_regex(ValueError, "Failed to group data."):
+        dataset.groupby(dataset.foo * np.nan)
+
+    with raises_regex(ValueError, "Failed to group data."):
+        dataset.to_array().groupby(dataset.foo * np.nan)
+
+
 # TODO: move other groupby tests from test_dataset and test_dataarray over here
