diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
index 413cc8a..fa75db5 100644
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -1,3 +1,196 @@
+import inspect
+def recursive_object_seralizer(obj, visited):
+    seralized_dict = {}
+    keys = list(obj.__dict__)
+    for k in keys:
+        if id(obj.__dict__[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(obj.__dict__[k])
+            continue
+        if isinstance(obj.__dict__[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = obj.__dict__[k]
+        elif isinstance(obj.__dict__[k], tuple):
+            ## handle tuple
+            seralized_dict[k] = recursive_tuple_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], set):
+            ## handle set
+            seralized_dict[k] = recursive_set_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], list):
+            ## handle list
+            seralized_dict[k] = recursive_list_seralizer(obj.__dict__[k], visited)
+        elif hasattr(obj.__dict__[k], '__dict__'):
+            ## handle object
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_object_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], dict):
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_dict_seralizer(obj.__dict__[k], visited)
+        elif callable(obj.__dict__[k]):
+            ## handle function
+            if hasattr(obj.__dict__[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(obj.__dict__[k].__name__)
+        else:
+            seralized_dict[k] = str(obj.__dict__[k])
+    return seralized_dict
+
+def recursive_dict_seralizer(dictionary, visited):
+    seralized_dict = {}
+    keys = list(dictionary)
+    for k in keys:
+        if id(dictionary[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(dictionary[k])
+            continue
+        # if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+        #     pass
+        # else:
+        #     visited.append(id(dictionary[k]))
+        if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = dictionary[k]
+        elif isinstance(dictionary[k], list):
+            seralized_dict[k] = recursive_list_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], tuple):
+            seralized_dict[k] = recursive_tuple_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], set):
+            seralized_dict[k] = recursive_set_seralizer(dictionary[k], visited)        
+        elif hasattr(dictionary[k], '__dict__'):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_object_seralizer(dictionary[k], visited)
+        elif callable(dictionary[k]):
+            if hasattr(dictionary[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(dictionary[k].__name__)
+        elif isinstance(dictionary[k], dict):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_dict_seralizer(dictionary[k], visited)
+        else:
+            seralized_dict[k] =str(dictionary[k])
+    return seralized_dict   
+
+def recursive_set_seralizer(set_data, visited):
+    new_set = set()
+    for s in set_data:
+        if id(s) in visited:
+            continue 
+        if isinstance(s, (float, int, str, bool, type(None))):
+            new_set.add(s)
+        elif isinstance(s, tuple):
+            new_set.add(recursive_tuple_seralizer(s, visited))
+        elif isinstance(s, list):
+            new_set.add(recursive_list_seralizer(s, visited))
+        elif isinstance(s, set):
+            new_set.add(recursive_set_seralizer(s,visited))
+        elif isinstance(s, dict):
+            visited.append(id(s))
+            new_set.add(recursive_dict_seralizer(s, visited))
+        elif hasattr(s, '__dict__'):
+            visited.append(id(s))
+            new_set.add(str(recursive_object_seralizer(s, visited)))
+        elif callable(s):
+            if hasattr(s, '__name__'):
+                new_set.add("<function {}>".format(s.__name__))
+        else:
+            new_set.add(str(s))
+    return new_set
+    
+
+def recursive_tuple_seralizer(tup, visited):
+    new_tup = ()
+    for t in tup:
+        if id(t) in visited:
+           continue
+        if isinstance(t, (float, int, str, bool, type(None))):
+            new_tup = (*new_tup, t)
+        elif isinstance(t, tuple):
+            new_tup = (*new_tup, recursive_tuple_seralizer(t, visited))
+        elif isinstance(t, list):
+            new_tup = (*new_tup, recursive_list_seralizer(t, visited))
+        elif isinstance(t, set):
+            new_tup = (*new_tup, recursive_set_seralizer(t, visited))
+        elif isinstance(t, dict):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_dict_seralizer(t, visited))
+        elif hasattr(t, '__dict__'):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_object_seralizer(t, visited))
+        elif callable(t):
+            if hasattr(t, '__name__'):
+                new_tup = (*new_tup, "<function {}>".format(t.__name__))
+        else:
+            new_tup = (*new_tup, str(t))
+    return new_tup
+
+def recursive_list_seralizer(li, visited):
+    new_list = []
+    for l in li:
+        if id(l) in visited:
+            continue
+        if isinstance(l, (float, int, str, bool, type(None))):
+            new_list.append(l)
+        elif isinstance(l, tuple):
+            new_list.append(recursive_tuple_seralizer(l, visited))
+        elif isinstance(l, list):
+            new_list.append(recursive_list_seralizer(l, visited))
+        elif isinstance(l, set):
+            new_list.append(recursive_set_seralizer(l, visited))
+        elif hasattr(l, '__dict__'):
+            visited.append(id(l))
+            new_list.append(recursive_object_seralizer(l, visited))
+        elif isinstance(l, dict):
+            visited.append(id(l))
+            new_list.append(recursive_dict_seralizer(l, visited))
+        elif callable(l):
+            if hasattr(l, '__name__'):
+                new_list.append("<function {}>".format(l.__name__))
+        else:
+            new_list.append(str(l))       
+
+def inspect_code(func):
+    def wrapper(*args, **kwargs):
+        visited = []
+        filename = "/home/changshu/CODEMIND/scripts/swebench/swebench_playground/obj/scikit-learn__scikit-learn-12983/sklearn/ensemble/gradient_boosting.py"
+        para_dict = {"name": func.__name__}
+        args_names = inspect.getfullargspec(func).args
+        if len(args) > 0 and hasattr(args[0], '__dict__') and args_names[0] == 'self':
+            ## 'self'
+            self_args = args[0]
+            para_dict['self'] = recursive_object_seralizer(self_args, [id(self_args)])
+        else:
+            para_dict['self'] = {}
+        if len(args) > 0 :
+            if args_names[0] == 'self':
+                other_args = {}
+                for m,n in zip(args_names[1:], args[1:]):
+                    other_args[m] = n
+            else:
+                other_args = {}
+                for m,n in zip(args_names, args):
+                    other_args[m] = n
+            para_dict['args'] = recursive_dict_seralizer(other_args, [id(other_args)])
+        else:
+            para_dict['args'] = {}
+        if kwargs:
+            para_dict['kwargs'] = recursive_dict_seralizer(kwargs, [id(kwargs)])
+        else:
+            para_dict['kwargs'] = {}
+            
+        result = func(*args, **kwargs)
+        ## seralize the return value
+        if isinstance(result, tuple):
+            ret = recursive_tuple_seralizer(result, [])
+        elif isinstance(result, (float, int, str)):
+            ret = result
+        elif isinstance(result, list):
+            ret = recursive_list_seralizer(result, [])
+        elif isinstance(result, dict):
+            ret = recursive_dict_seralizer(result, [])
+        elif hasattr(result, '__dict__'):
+            ret = recursive_object_seralizer(result, [])
+        elif callable(result):
+            ret = "<function {}>".format(result.__name__)
+        else:
+            ret = str(result)
+        para_dict["return"] = ret
+        print("@[DATA]@", filename,"[SEP]", para_dict, "[/SEP]")
+        return result
+    return wrapper
 """Gradient Boosted Regression Trees
 
 This module contains methods for fitting gradient boosted regression trees for
@@ -26,6 +219,8 @@ from abc import abstractmethod
 from .base import BaseEnsemble
 from ..base import ClassifierMixin
 from ..base import RegressorMixin
+from ..base import BaseEstimator
+from ..base import is_classifier
 
 from ._gradient_boosting import predict_stages
 from ._gradient_boosting import predict_stage
@@ -44,6 +239,7 @@ from ..model_selection import train_test_split
 from ..tree.tree import DecisionTreeRegressor
 from ..tree._tree import DTYPE
 from ..tree._tree import TREE_LEAF
+from . import _gb_losses
 
 from ..utils import check_random_state
 from ..utils import check_array
@@ -58,6 +254,14 @@ from ..utils.multiclass import check_classification_targets
 from ..exceptions import NotFittedError
 
 
+# FIXME: 0.23
+# All the losses and corresponding init estimators have been moved to the
+# _losses module in 0.21. We deprecate them and keep them here for now in case
+# someone has imported them. None of these losses can be used as a parameter
+# to a GBDT estimator anyway (loss param only accepts strings).
+
+@deprecated("QuantileEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class QuantileEstimator:
     """An estimator predicting the alpha-quantile of the training targets.
 
@@ -111,6 +315,8 @@ class QuantileEstimator:
         return y
 
 
+@deprecated("MeanEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class MeanEstimator:
     """An estimator predicting the mean of the training targets."""
     def fit(self, X, y, sample_weight=None):
@@ -152,6 +358,8 @@ class MeanEstimator:
         return y
 
 
+@deprecated("LogOddsEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LogOddsEstimator:
     """An estimator predicting the log odds ratio."""
     scale = 1.0
@@ -202,11 +410,15 @@ class LogOddsEstimator:
         return y
 
 
+@deprecated("ScaledLogOddsEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ScaledLogOddsEstimator(LogOddsEstimator):
     """Log odds ratio scaled by 0.5 -- for exponential loss. """
     scale = 0.5
 
 
+@deprecated("PriorProbablityEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class PriorProbabilityEstimator:
     """An estimator predicting the probability of each
     class in the training data.
@@ -250,8 +462,16 @@ class PriorProbabilityEstimator:
         return y
 
 
+@deprecated("Using ZeroEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ZeroEstimator:
-    """An estimator that simply predicts zero. """
+    """An estimator that simply predicts zero.
+
+    .. deprecated:: 0.21
+        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version
+        0.21 and will be removed in version 0.23.
+
+    """
 
     def fit(self, X, y, sample_weight=None):
         """Fit the estimator.
@@ -295,7 +515,13 @@ class ZeroEstimator:
         y.fill(0.0)
         return y
 
+    def predict_proba(self, X):
+        return self.predict(X)
+
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LossFunction(metaclass=ABCMeta):
     """Abstract base class for various loss functions.
 
@@ -403,6 +629,9 @@ class LossFunction(metaclass=ABCMeta):
         """Template method for updating terminal regions (=leaves). """
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
     """Base class for regression loss functions.
 
@@ -418,6 +647,9 @@ class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
         super().__init__(n_classes)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LeastSquaresError(RegressionLossFunction):
     """Loss function for least squares (LS) estimation.
     Terminal regions need not to be updated for least squares.
@@ -501,6 +733,9 @@ class LeastSquaresError(RegressionLossFunction):
         pass
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LeastAbsoluteError(RegressionLossFunction):
     """Loss function for least absolute deviation (LAD) regression.
 
@@ -557,6 +792,9 @@ class LeastAbsoluteError(RegressionLossFunction):
         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class HuberLossFunction(RegressionLossFunction):
     """Huber loss function for robust regression.
 
@@ -660,6 +898,9 @@ class HuberLossFunction(RegressionLossFunction):
             np.minimum(np.abs(diff_minus_median), gamma))
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class QuantileLossFunction(RegressionLossFunction):
     """Loss function for quantile regression.
 
@@ -737,6 +978,9 @@ class QuantileLossFunction(RegressionLossFunction):
         tree.value[leaf, 0] = val
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
     """Base class for classification loss functions. """
 
@@ -755,6 +999,9 @@ class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
         """
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class BinomialDeviance(ClassificationLossFunction):
     """Binomial deviance loss function for binary classification.
 
@@ -846,6 +1093,9 @@ class BinomialDeviance(ClassificationLossFunction):
         return np.argmax(proba, axis=1)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class MultinomialDeviance(ClassificationLossFunction):
     """Multinomial deviance loss function for multi-class classification.
 
@@ -941,6 +1191,9 @@ class MultinomialDeviance(ClassificationLossFunction):
         return np.argmax(proba, axis=1)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ExponentialLoss(ClassificationLossFunction):
     """Exponential loss function for binary classification.
 
@@ -1028,19 +1281,7 @@ class ExponentialLoss(ClassificationLossFunction):
         return (score.ravel() >= 0.0).astype(np.int)
 
 
-LOSS_FUNCTIONS = {'ls': LeastSquaresError,
-                  'lad': LeastAbsoluteError,
-                  'huber': HuberLossFunction,
-                  'quantile': QuantileLossFunction,
-                  'deviance': None,    # for both, multinomial and binomial
-                  'exponential': ExponentialLoss,
-                  }
-
-
-INIT_ESTIMATORS = {'zero': ZeroEstimator}
-
-
-class VerboseReporter:
+class VerboseReporter(object):
     """Reports verbose output to stdout.
 
     Parameters
@@ -1151,7 +1392,8 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
         self.n_iter_no_change = n_iter_no_change
         self.tol = tol
 
-    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
+    @inspect_code
+    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
         """Fit another stage of ``n_classes_`` trees to the boosting model. """
 
@@ -1159,17 +1401,17 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
         loss = self.loss_
         original_y = y
 
-        # Need to pass a copy of y_pred to negative_gradient() because y_pred
-        # is partially updated at the end of the loop in
-        # update_terminal_regions(), and gradients need to be evaluated at
+        # Need to pass a copy of raw_predictions to negative_gradient()
+        # because raw_predictions is partially updated at the end of the loop
+        # in update_terminal_regions(), and gradients need to be evaluated at
         # iteration i - 1.
-        y_pred_copy = y_pred.copy()
+        raw_predictions_copy = raw_predictions.copy()
 
         for k in range(loss.K):
             if loss.is_multi_class:
                 y = np.array(original_y == k, dtype=np.float64)
 
-            residual = loss.negative_gradient(y, y_pred_copy, k=k,
+            residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
                                               sample_weight=sample_weight)
 
             # induce regression tree on residuals
@@ -1196,15 +1438,16 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                      check_input=False, X_idx_sorted=X_idx_sorted)
 
             # update tree leaves
-            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
-                                         sample_weight, sample_mask,
-                                         learning_rate=self.learning_rate, k=k)
+            loss.update_terminal_regions(
+                tree.tree_, X, y, residual, raw_predictions, sample_weight,
+                sample_mask, learning_rate=self.learning_rate, k=k)
 
             # add tree to ensemble
             self.estimators_[i, k] = tree
 
-        return y_pred
+        return raw_predictions
 
+    @inspect_code
     def _check_params(self):
         """Check validity of parameters and raise ValueError if not valid. """
         if self.n_estimators <= 0:
@@ -1216,15 +1459,15 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                              "was %r" % self.learning_rate)
 
         if (self.loss not in self._SUPPORTED_LOSS
-                or self.loss not in LOSS_FUNCTIONS):
+                or self.loss not in _gb_losses.LOSS_FUNCTIONS):
             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
 
         if self.loss == 'deviance':
-            loss_class = (MultinomialDeviance
+            loss_class = (_gb_losses.MultinomialDeviance
                           if len(self.classes_) > 2
-                          else BinomialDeviance)
+                          else _gb_losses.BinomialDeviance)
         else:
-            loss_class = LOSS_FUNCTIONS[self.loss]
+            loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
 
         if self.loss in ('huber', 'quantile'):
             self.loss_ = loss_class(self.n_classes_, self.alpha)
@@ -1236,15 +1479,14 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                              "was %r" % self.subsample)
 
         if self.init is not None:
-            if isinstance(self.init, str):
-                if self.init not in INIT_ESTIMATORS:
-                    raise ValueError('init="%s" is not supported' % self.init)
-            else:
-                if (not hasattr(self.init, 'fit')
-                        or not hasattr(self.init, 'predict')):
-                    raise ValueError("init=%r must be valid BaseEstimator "
-                                     "and support both fit and "
-                                     "predict" % self.init)
+            # init must be an estimator or 'zero'
+            if isinstance(self.init, BaseEstimator):
+                self.loss_.check_init_estimator(self.init)
+            elif not (isinstance(self.init, str) and self.init == 'zero'):
+                raise ValueError(
+                    "The init parameter must be an estimator or 'zero'. "
+                    "Got init={}".format(self.init)
+                )
 
         if not (0.0 < self.alpha < 1.0):
             raise ValueError("alpha must be in (0.0, 1.0) but "
@@ -1290,15 +1532,13 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
             raise ValueError("'presort' should be in {}. Got {!r} instead."
                              .format(allowed_presort, self.presort))
 
+    @inspect_code
     def _init_state(self):
         """Initialize model state and allocate model state data structures. """
 
-        if self.init is None:
+        self.init_ = self.init
+        if self.init_ is None:
             self.init_ = self.loss_.init_estimator()
-        elif isinstance(self.init, str):
-            self.init_ = INIT_ESTIMATORS[self.init]()
-        else:
-            self.init_ = self.init
 
         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
                                     dtype=np.object)
@@ -1354,6 +1594,7 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
     def n_features(self):
         return self.n_features_
 
+    @inspect_code
     def fit(self, X, y, sample_weight=None, monitor=None):
         """Fit the gradient boosting model.
 
@@ -1396,10 +1637,13 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
         # Check input
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
         n_samples, self.n_features_ = X.shape
-        if sample_weight is None:
+
+        sample_weight_is_none = sample_weight is None
+        if sample_weight_is_none:
             sample_weight = np.ones(n_samples, dtype=np.float32)
         else:
             sample_weight = column_or_1d(sample_weight, warn=True)
+            sample_weight_is_none = False
 
         check_consistent_length(X, y, sample_weight)
 
@@ -1410,6 +1654,17 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                 train_test_split(X, y, sample_weight,
                                  random_state=self.random_state,
                                  test_size=self.validation_fraction))
+            if is_classifier(self):
+                if self.n_classes_ != np.unique(y).shape[0]:
+                    # We choose to error here. The problem is that the init
+                    # estimator would be trained on y, which has some missing
+                    # classes now, so its predictions would not have the
+                    # correct shape.
+                    raise ValueError(
+                        'The training data after the early stopping split '
+                        'is missing some classes. Try using another random '
+                        'seed.'
+                    )
         else:
             X_val = y_val = sample_weight_val = None
 
@@ -1419,11 +1674,25 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
             # init state
             self._init_state()
 
-            # fit initial model - FIXME make sample_weight optional
-            self.init_.fit(X, y, sample_weight)
+            # fit initial model and initialize raw predictions
+            if self.init_ == 'zero':
+                raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
+                                           dtype=np.float64)
+            else:
+                try:
+                    self.init_.fit(X, y, sample_weight=sample_weight)
+                except TypeError:
+                    if sample_weight_is_none:
+                        self.init_.fit(X, y)
+                    else:
+                        raise ValueError(
+                            "The initial estimator {} does not support sample "
+                            "weights.".format(self.init_.__class__.__name__))
+
+                raw_predictions = \
+                    self.loss_.get_init_raw_predictions(X, self.init_)
+
 
-            # init predictions
-            y_pred = self.init_.predict(X)
             begin_at_stage = 0
 
             # The rng state must be preserved if warm_start is True
@@ -1443,7 +1712,7 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
             # below) are more constrained than fit. It accepts only CSR
             # matrices.
             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
-            y_pred = self._decision_function(X)
+            raw_predictions = self._raw_predict(X)
             self._resize_state()
 
         if self.presort is True and issparse(X):
@@ -1462,9 +1731,9 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                                              dtype=np.int32)
 
         # fit the boosting stages
-        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
-                                    X_val, y_val, sample_weight_val,
-                                    begin_at_stage, monitor, X_idx_sorted)
+        n_stages = self._fit_stages(
+            X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
+            sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
 
         # change shape of arrays after fit (early-stopping or additional ests)
         if n_stages != self.estimators_.shape[0]:
@@ -1476,7 +1745,8 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
         self.n_estimators_ = n_stages
         return self
 
-    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
+    @inspect_code
+    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
                     X_val, y_val, sample_weight_val,
                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
         """Iteratively fits the stages.
@@ -1510,7 +1780,7 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
             loss_history = np.full(self.n_iter_no_change, np.inf)
             # We create a generator to get the predictions for X_val after
             # the addition of each successive stage
-            y_val_pred_iter = self._staged_decision_function(X_val)
+            y_val_pred_iter = self._staged_raw_predict(X_val)
 
         # perform boosting iterations
         i = begin_at_stage
@@ -1522,26 +1792,26 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
                                                   random_state)
                 # OOB score before adding this stage
                 old_oob_score = loss_(y[~sample_mask],
-                                      y_pred[~sample_mask],
+                                      raw_predictions[~sample_mask],
                                       sample_weight[~sample_mask])
 
             # fit next stage of trees
-            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
-                                     sample_mask, random_state, X_idx_sorted,
-                                     X_csc, X_csr)
+            raw_predictions = self._fit_stage(
+                i, X, y, raw_predictions, sample_weight, sample_mask,
+                random_state, X_idx_sorted, X_csc, X_csr)
 
             # track deviance (= loss)
             if do_oob:
                 self.train_score_[i] = loss_(y[sample_mask],
-                                             y_pred[sample_mask],
+                                             raw_predictions[sample_mask],
                                              sample_weight[sample_mask])
                 self.oob_improvement_[i] = (
                     old_oob_score - loss_(y[~sample_mask],
-                                          y_pred[~sample_mask],
+                                          raw_predictions[~sample_mask],
                                           sample_weight[~sample_mask]))
             else:
                 # no need to fancy index w/ no subsampling
-                self.train_score_[i] = loss_(y, y_pred, sample_weight)
+                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
 
             if self.verbose > 0:
                 verbose_reporter.update(i, self)
@@ -1572,26 +1842,30 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
         # we don't need _make_estimator
         raise NotImplementedError()
 
-    def _init_decision_function(self, X):
-        """Check input and compute prediction of ``init``. """
+    def _raw_predict_init(self, X):
+        """Check input and compute raw predictions of the init estimtor."""
         self._check_initialized()
         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
         if X.shape[1] != self.n_features_:
             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
                 self.n_features_, X.shape[1]))
-        score = self.init_.predict(X).astype(np.float64)
-        return score
-
-    def _decision_function(self, X):
-        # for use in inner loop, not raveling the output in single-class case,
-        # not doing input validation.
-        score = self._init_decision_function(X)
-        predict_stages(self.estimators_, X, self.learning_rate, score)
-        return score
+        if self.init_ == 'zero':
+            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
+                                       dtype=np.float64)
+        else:
+            raw_predictions = self.loss_.get_init_raw_predictions(
+                X, self.init_).astype(np.float64)
+        return raw_predictions
 
+    def _raw_predict(self, X):
+        """Return the sum of the trees raw predictions (+ init estimator)."""
+        raw_predictions = self._raw_predict_init(X)
+        predict_stages(self.estimators_, X, self.learning_rate,
+                       raw_predictions)
+        return raw_predictions
 
-    def _staged_decision_function(self, X):
-        """Compute decision function of ``X`` for each iteration.
+    def _staged_raw_predict(self, X):
+        """Compute raw predictions of ``X`` for each iteration.
 
         This method allows monitoring (i.e. determine error on testing set)
         after each stage.
@@ -1605,17 +1879,18 @@ class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
 
         Returns
         -------
-        score : generator of array, shape (n_samples, k)
-            The decision function of the input samples. The order of the
+        raw_predictions : generator of array, shape (n_samples, k)
+            The raw predictions of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
             ``k == 1``, otherwise ``k==n_classes``.
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        score = self._init_decision_function(X)
+        raw_predictions = self._raw_predict_init(X)
         for i in range(self.estimators_.shape[0]):
-            predict_stage(self.estimators_, i, X, self.learning_rate, score)
-            yield score.copy()
+            predict_stage(self.estimators_, i, X, self.learning_rate,
+                          raw_predictions)
+            yield raw_predictions.copy()
 
     @property
     def feature_importances_(self):
@@ -1793,10 +2068,11 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
 
-    init : estimator, optional
-        An estimator object that is used to compute the initial
-        predictions. ``init`` has to provide ``fit`` and ``predict``.
-        If None it uses ``loss.init_estimator``.
+    init : estimator or 'zero', optional (default=None)
+        An estimator object that is used to compute the initial predictions.
+        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the
+        initial raw predictions are set to zero. By default, a
+        ``DummyEstimator`` predicting the classes priors is used.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1984,16 +2260,17 @@ shape (n_estimators, ``loss_.K``)
         Returns
         -------
         score : array, shape (n_samples, n_classes) or (n_samples,)
-            The decision function of the input samples. The order of the
-            classes corresponds to that in the attribute `classes_`.
-            Regression and binary classification produce an array of shape
-            [n_samples].
+            The decision function of the input samples, which corresponds to
+            the raw values predicted from the trees of the ensemble . The
+            order of the classes corresponds to that in the attribute
+            `classes_`. Regression and binary classification produce an
+            array of shape [n_samples].
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        score = self._decision_function(X)
-        if score.shape[1] == 1:
-            return score.ravel()
-        return score
+        raw_predictions = self._raw_predict(X)
+        if raw_predictions.shape[1] == 1:
+            return raw_predictions.ravel()
+        return raw_predictions
 
     def staged_decision_function(self, X):
         """Compute decision function of ``X`` for each iteration.
@@ -2011,13 +2288,15 @@ shape (n_estimators, ``loss_.K``)
         Returns
         -------
         score : generator of array, shape (n_samples, k)
-            The decision function of the input samples. The order of the
+            The decision function of the input samples, which corresponds to
+            the raw values predicted from the trees of the ensemble . The
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
             ``k == 1``, otherwise ``k==n_classes``.
         """
-        yield from self._staged_decision_function(X)
+        yield from self._staged_raw_predict(X)
 
+    @inspect_code
     def predict(self, X):
         """Predict class for X.
 
@@ -2033,10 +2312,12 @@ shape (n_estimators, ``loss_.K``)
         y : array, shape (n_samples,)
             The predicted values.
         """
-        score = self.decision_function(X)
-        decisions = self.loss_._score_to_decision(score)
-        return self.classes_.take(decisions, axis=0)
+        raw_predictions = self.decision_function(X)
+        encoded_labels = \
+            self.loss_._raw_prediction_to_decision(raw_predictions)
+        return self.classes_.take(encoded_labels, axis=0)
 
+    @inspect_code
     def staged_predict(self, X):
         """Predict class at each stage for X.
 
@@ -2055,9 +2336,10 @@ shape (n_estimators, ``loss_.K``)
         y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
-        for score in self._staged_decision_function(X):
-            decisions = self.loss_._score_to_decision(score)
-            yield self.classes_.take(decisions, axis=0)
+        for raw_predictions in self._staged_raw_predict(X):
+            encoded_labels = \
+                self.loss_._raw_prediction_to_decision(raw_predictions)
+            yield self.classes_.take(encoded_labels, axis=0)
 
     def predict_proba(self, X):
         """Predict class probabilities for X.
@@ -2080,9 +2362,9 @@ shape (n_estimators, ``loss_.K``)
             The class probabilities of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
         """
-        score = self.decision_function(X)
+        raw_predictions = self.decision_function(X)
         try:
-            return self.loss_._score_to_proba(score)
+            return self.loss_._raw_prediction_to_proba(raw_predictions)
         except NotFittedError:
             raise
         except AttributeError:
@@ -2113,6 +2395,7 @@ shape (n_estimators, ``loss_.K``)
         proba = self.predict_proba(X)
         return np.log(proba)
 
+    @inspect_code
     def staged_predict_proba(self, X):
         """Predict class probabilities at each stage for X.
 
@@ -2132,8 +2415,8 @@ shape (n_estimators, ``loss_.K``)
             The predicted value of the input samples.
         """
         try:
-            for score in self._staged_decision_function(X):
-                yield self.loss_._score_to_proba(score)
+            for raw_predictions in self._staged_raw_predict(X):
+                yield self.loss_._raw_prediction_to_proba(raw_predictions)
         except NotFittedError:
             raise
         except AttributeError:
@@ -2251,10 +2534,12 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
 
-    init : estimator, optional (default=None)
-        An estimator object that is used to compute the initial
-        predictions. ``init`` has to provide ``fit`` and ``predict``.
-        If None it uses ``loss.init_estimator``.
+    init : estimator or 'zero', optional (default=None)
+        An estimator object that is used to compute the initial predictions.
+        ``init`` has to provide `fit` and `predict`. If 'zero', the initial
+        raw predictions are set to zero. By default a ``DummyEstimator`` is
+        used, predicting either the average target value (for loss='ls'), or
+        a quantile for the other losses.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -2410,6 +2695,7 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
             presort=presort, validation_fraction=validation_fraction,
             n_iter_no_change=n_iter_no_change, tol=tol)
 
+    @inspect_code
     def predict(self, X):
         """Predict regression target for X.
 
@@ -2426,8 +2712,10 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
             The predicted values.
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        return self._decision_function(X).ravel()
+        # In regression we can directly return the raw value from the trees.
+        return self._raw_predict(X).ravel()
 
+    @inspect_code
     def staged_predict(self, X):
         """Predict regression target at each stage for X.
 
@@ -2446,8 +2734,8 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
         y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
-        for y in self._staged_decision_function(X):
-            yield y.ravel()
+        for raw_predictions in self._staged_raw_predict(X):
+            yield raw_predictions.ravel()
 
     def apply(self, X):
         """Apply trees in the ensemble to X, return leaf indices.
diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py
index 0bcb036..dc9ec0c 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting.py
@@ -13,11 +13,15 @@ import pytest
 
 from sklearn import datasets
 from sklearn.base import clone
-from sklearn.datasets import make_classification, fetch_california_housing
+from sklearn.base import BaseEstimator
+from sklearn.datasets import (make_classification, fetch_california_housing,
+                              make_regression)
 from sklearn.ensemble import GradientBoostingClassifier
 from sklearn.ensemble import GradientBoostingRegressor
 from sklearn.ensemble.gradient_boosting import ZeroEstimator
 from sklearn.ensemble._gradient_boosting import predict_stages
+from sklearn.preprocessing import OneHotEncoder
+from sklearn.svm import LinearSVC
 from sklearn.metrics import mean_squared_error
 from sklearn.model_selection import train_test_split
 from sklearn.utils import check_random_state, tosequence
@@ -34,6 +38,8 @@ from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import skip_if_32bit
 from sklearn.exceptions import DataConversionWarning
 from sklearn.exceptions import NotFittedError
+from sklearn.dummy import DummyClassifier, DummyRegressor
+
 
 GRADIENT_BOOSTING_ESTIMATORS = [GradientBoostingClassifier,
                                 GradientBoostingRegressor]
@@ -1046,13 +1052,7 @@ def test_complete_regression():
 
 
 def test_zero_estimator_reg():
-    # Test if ZeroEstimator works for regression.
-    est = GradientBoostingRegressor(n_estimators=20, max_depth=1,
-                                    random_state=1, init=ZeroEstimator())
-    est.fit(boston.data, boston.target)
-    y_pred = est.predict(boston.data)
-    mse = mean_squared_error(boston.target, y_pred)
-    assert_almost_equal(mse, 33.0, decimal=0)
+    # Test if init='zero' works for regression.
 
     est = GradientBoostingRegressor(n_estimators=20, max_depth=1,
                                     random_state=1, init='zero')
@@ -1067,14 +1067,9 @@ def test_zero_estimator_reg():
 
 
 def test_zero_estimator_clf():
-    # Test if ZeroEstimator works for classification.
+    # Test if init='zero' works for classification.
     X = iris.data
     y = np.array(iris.target)
-    est = GradientBoostingClassifier(n_estimators=20, max_depth=1,
-                                     random_state=1, init=ZeroEstimator())
-    est.fit(X, y)
-
-    assert_greater(est.score(X, y), 0.96)
 
     est = GradientBoostingClassifier(n_estimators=20, max_depth=1,
                                      random_state=1, init='zero')
@@ -1324,3 +1319,81 @@ def test_gradient_boosting_validation_fraction():
     gbr3.fit(X_train, y_train)
     assert gbr.n_estimators_ < gbr3.n_estimators_
     assert gbc.n_estimators_ < gbc3.n_estimators_
+
+
+class _NoSampleWeightWrapper(BaseEstimator):
+    def __init__(self, est):
+        self.est = est
+
+    def fit(self, X, y):
+        self.est.fit(X, y)
+
+    def predict(self, X):
+        return self.est.predict(X)
+
+    def predict_proba(self, X):
+        return self.est.predict_proba(X)
+
+
+def _make_multiclass():
+    return make_classification(n_classes=3, n_clusters_per_class=1)
+
+
+@pytest.mark.parametrize(
+    "gb, dataset_maker, init_estimator",
+    [(GradientBoostingClassifier, make_classification, DummyClassifier),
+     (GradientBoostingClassifier, _make_multiclass, DummyClassifier),
+     (GradientBoostingRegressor, make_regression, DummyRegressor)],
+    ids=["binary classification", "multiclass classification", "regression"])
+def test_gradient_boosting_with_init(gb, dataset_maker, init_estimator):
+    # Check that GradientBoostingRegressor works when init is a sklearn
+    # estimator.
+    # Check that an error is raised if trying to fit with sample weight but
+    # inital estimator does not support sample weight
+
+    X, y = dataset_maker()
+    sample_weight = np.random.RandomState(42).rand(100)
+
+    # init supports sample weights
+    init_est = init_estimator()
+    gb(init=init_est).fit(X, y, sample_weight=sample_weight)
+
+    # init does not support sample weights
+    init_est = _NoSampleWeightWrapper(init_estimator())
+    gb(init=init_est).fit(X, y)  # ok no sample weights
+    with pytest.raises(ValueError,
+                       match="estimator.*does not support sample weights"):
+        gb(init=init_est).fit(X, y, sample_weight=sample_weight)
+
+
+@pytest.mark.parametrize('estimator, missing_method', [
+    (GradientBoostingClassifier(init=LinearSVC()), 'predict_proba'),
+    (GradientBoostingRegressor(init=OneHotEncoder()), 'predict')
+])
+def test_gradient_boosting_init_wrong_methods(estimator, missing_method):
+    # Make sure error is raised if init estimators don't have the required
+    # methods (fit, predict, predict_proba)
+
+    message = ("The init parameter must be a valid estimator and support "
+               "both fit and " + missing_method)
+    with pytest.raises(ValueError, match=message):
+        estimator.fit(X, y)
+
+
+def test_early_stopping_n_classes():
+    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))
+    # there might be classes in y that are missing in y_train. As the init
+    # estimator will be trained on y_train, we need to raise an error if this
+    # happens.
+
+    X = [[1, 2], [2, 3], [3, 4], [4, 5]]
+    y = [0, 1, 1, 1]
+    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)
+    with pytest.raises(
+                ValueError,
+                match='The training data after the early stopping split'):
+        gb.fit(X, y)
+
+    # No error with another random seed
+    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)
+    gb.fit(X, y)
diff --git a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
index 2448721..32c65aa 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
@@ -3,19 +3,21 @@ Testing for the gradient boosting loss functions and initial estimators.
 """
 
 import numpy as np
-from numpy.testing import assert_array_equal
 from numpy.testing import assert_almost_equal
+from numpy.testing import assert_allclose
 from numpy.testing import assert_equal
 
 from sklearn.utils import check_random_state
-from sklearn.utils.testing import assert_raises
-from sklearn.ensemble.gradient_boosting import BinomialDeviance
-from sklearn.ensemble.gradient_boosting import LogOddsEstimator
-from sklearn.ensemble.gradient_boosting import LeastSquaresError
-from sklearn.ensemble.gradient_boosting import RegressionLossFunction
-from sklearn.ensemble.gradient_boosting import LOSS_FUNCTIONS
-from sklearn.ensemble.gradient_boosting import _weighted_percentile
-from sklearn.ensemble.gradient_boosting import QuantileLossFunction
+from sklearn.utils.stats import _weighted_percentile
+from sklearn.ensemble._gb_losses import RegressionLossFunction
+from sklearn.ensemble._gb_losses import LeastSquaresError
+from sklearn.ensemble._gb_losses import LeastAbsoluteError
+from sklearn.ensemble._gb_losses import HuberLossFunction
+from sklearn.ensemble._gb_losses import QuantileLossFunction
+from sklearn.ensemble._gb_losses import BinomialDeviance
+from sklearn.ensemble._gb_losses import MultinomialDeviance
+from sklearn.ensemble._gb_losses import ExponentialLoss
+from sklearn.ensemble._gb_losses import LOSS_FUNCTIONS
 
 
 def test_binomial_deviance():
@@ -52,17 +54,6 @@ def test_binomial_deviance():
         assert_almost_equal(bd.negative_gradient(*datum), alt_ng(*datum))
 
 
-def test_log_odds_estimator():
-    # Check log odds estimator.
-    est = LogOddsEstimator()
-    assert_raises(ValueError, est.fit, None, np.array([1]))
-
-    est.fit(None, np.array([1.0, 0.0]))
-    assert_equal(est.prior, 0.0)
-    assert_array_equal(est.predict(np.array([[1.0], [1.0]])),
-                       np.array([[0.0], [0.0]]))
-
-
 def test_sample_weight_smoke():
     rng = check_random_state(13)
     y = rng.rand(100)
@@ -100,16 +91,16 @@ def test_sample_weight_init_estimators():
         loss = Loss(k)
         init_est = loss.init_estimator()
         init_est.fit(X, y)
-        out = init_est.predict(X)
+        out = loss.get_init_raw_predictions(X, init_est)
         assert_equal(out.shape, (y.shape[0], 1))
 
         sw_init_est = loss.init_estimator()
         sw_init_est.fit(X, y, sample_weight=sample_weight)
-        sw_out = init_est.predict(X)
+        sw_out = loss.get_init_raw_predictions(X, sw_init_est)
         assert_equal(sw_out.shape, (y.shape[0], 1))
 
         # check if predictions match
-        assert_array_equal(out, sw_out)
+        assert_allclose(out, sw_out, rtol=1e-2)
 
 
 def test_weighted_percentile():
@@ -155,7 +146,6 @@ def test_quantile_loss_function():
 def test_sample_weight_deviance():
     # Test if deviance supports sample weights.
     rng = check_random_state(13)
-    X = rng.rand(100, 2)
     sample_weight = np.ones(100)
     reg_y = rng.rand(100)
     clf_y = rng.randint(0, 2, size=100)
@@ -184,3 +174,102 @@ def test_sample_weight_deviance():
         deviance_w_w = loss(y, p, sample_weight)
         deviance_wo_w = loss(y, p)
         assert deviance_wo_w == deviance_w_w
+
+
+def test_init_raw_predictions_shapes():
+    # Make sure get_init_raw_predictions returns float64 arrays with shape
+    # (n_samples, K) where K is 1 for binary classification and regression, and
+    # K = n_classes for multiclass classification
+    rng = np.random.RandomState(0)
+
+    n_samples = 100
+    X = rng.normal(size=(n_samples, 5))
+    y = rng.normal(size=n_samples)
+    for loss in (LeastSquaresError(n_classes=1),
+                 LeastAbsoluteError(n_classes=1),
+                 QuantileLossFunction(n_classes=1),
+                 HuberLossFunction(n_classes=1)):
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        assert raw_predictions.shape == (n_samples, 1)
+        assert raw_predictions.dtype == np.float64
+
+    y = rng.randint(0, 2, size=n_samples)
+    for loss in (BinomialDeviance(n_classes=2),
+                 ExponentialLoss(n_classes=2)):
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        assert raw_predictions.shape == (n_samples, 1)
+        assert raw_predictions.dtype == np.float64
+
+    for n_classes in range(3, 5):
+        y = rng.randint(0, n_classes, size=n_samples)
+        loss = MultinomialDeviance(n_classes=n_classes)
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        assert raw_predictions.shape == (n_samples, n_classes)
+        assert raw_predictions.dtype == np.float64
+
+
+def test_init_raw_predictions_values():
+    # Make sure the get_init_raw_predictions() returns the expected values for
+    # each loss.
+    rng = np.random.RandomState(0)
+
+    n_samples = 100
+    X = rng.normal(size=(n_samples, 5))
+    y = rng.normal(size=n_samples)
+
+    # Least squares loss
+    loss = LeastSquaresError(n_classes=1)
+    init_estimator = loss.init_estimator().fit(X, y)
+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+    # Make sure baseline prediction is the mean of all targets
+    assert_almost_equal(raw_predictions, y.mean())
+
+    # Least absolute and huber loss
+    for Loss in (LeastAbsoluteError, HuberLossFunction):
+        loss = Loss(n_classes=1)
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        # Make sure baseline prediction is the median of all targets
+        assert_almost_equal(raw_predictions, np.median(y))
+
+    # Quantile loss
+    for alpha in (.1, .5, .9):
+        loss = QuantileLossFunction(n_classes=1, alpha=alpha)
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        # Make sure baseline prediction is the alpha-quantile of all targets
+        assert_almost_equal(raw_predictions, np.percentile(y, alpha * 100))
+
+    y = rng.randint(0, 2, size=n_samples)
+
+    # Binomial deviance
+    loss = BinomialDeviance(n_classes=2)
+    init_estimator = loss.init_estimator().fit(X, y)
+    # Make sure baseline prediction is equal to link_function(p), where p
+    # is the proba of the positive class. We want predict_proba() to return p,
+    # and by definition
+    # p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)
+    # So we want raw_prediction = link_function(p) = log(p / (1 - p))
+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+    p = y.mean()
+    assert_almost_equal(raw_predictions, np.log(p / (1 - p)))
+
+    # Exponential loss
+    loss = ExponentialLoss(n_classes=2)
+    init_estimator = loss.init_estimator().fit(X, y)
+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+    p = y.mean()
+    assert_almost_equal(raw_predictions, .5 * np.log(p / (1 - p)))
+
+    # Multinomial deviance loss
+    for n_classes in range(3, 5):
+        y = rng.randint(0, n_classes, size=n_samples)
+        loss = MultinomialDeviance(n_classes=n_classes)
+        init_estimator = loss.init_estimator().fit(X, y)
+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)
+        for k in range(n_classes):
+            p = (y == k).mean()
+        assert_almost_equal(raw_predictions[:, k], np.log(p))
