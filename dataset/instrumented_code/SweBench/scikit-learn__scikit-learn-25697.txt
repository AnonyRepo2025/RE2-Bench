diff --git a/sklearn/linear_model/_bayes.py b/sklearn/linear_model/_bayes.py
index 7f712b1..d716452 100644
--- a/sklearn/linear_model/_bayes.py
+++ b/sklearn/linear_model/_bayes.py
@@ -1,3 +1,196 @@
+import inspect
+def recursive_object_seralizer(obj, visited):
+    seralized_dict = {}
+    keys = list(obj.__dict__)
+    for k in keys:
+        if id(obj.__dict__[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(obj.__dict__[k])
+            continue
+        if isinstance(obj.__dict__[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = obj.__dict__[k]
+        elif isinstance(obj.__dict__[k], tuple):
+            ## handle tuple
+            seralized_dict[k] = recursive_tuple_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], set):
+            ## handle set
+            seralized_dict[k] = recursive_set_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], list):
+            ## handle list
+            seralized_dict[k] = recursive_list_seralizer(obj.__dict__[k], visited)
+        elif hasattr(obj.__dict__[k], '__dict__'):
+            ## handle object
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_object_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], dict):
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_dict_seralizer(obj.__dict__[k], visited)
+        elif callable(obj.__dict__[k]):
+            ## handle function
+            if hasattr(obj.__dict__[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(obj.__dict__[k].__name__)
+        else:
+            seralized_dict[k] = str(obj.__dict__[k])
+    return seralized_dict
+
+def recursive_dict_seralizer(dictionary, visited):
+    seralized_dict = {}
+    keys = list(dictionary)
+    for k in keys:
+        if id(dictionary[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(dictionary[k])
+            continue
+        # if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+        #     pass
+        # else:
+        #     visited.append(id(dictionary[k]))
+        if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = dictionary[k]
+        elif isinstance(dictionary[k], list):
+            seralized_dict[k] = recursive_list_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], tuple):
+            seralized_dict[k] = recursive_tuple_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], set):
+            seralized_dict[k] = recursive_set_seralizer(dictionary[k], visited)        
+        elif hasattr(dictionary[k], '__dict__'):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_object_seralizer(dictionary[k], visited)
+        elif callable(dictionary[k]):
+            if hasattr(dictionary[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(dictionary[k].__name__)
+        elif isinstance(dictionary[k], dict):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_dict_seralizer(dictionary[k], visited)
+        else:
+            seralized_dict[k] =str(dictionary[k])
+    return seralized_dict   
+
+def recursive_set_seralizer(set_data, visited):
+    new_set = set()
+    for s in set_data:
+        if id(s) in visited:
+            continue 
+        if isinstance(s, (float, int, str, bool, type(None))):
+            new_set.add(s)
+        elif isinstance(s, tuple):
+            new_set.add(recursive_tuple_seralizer(s, visited))
+        elif isinstance(s, list):
+            new_set.add(recursive_list_seralizer(s, visited))
+        elif isinstance(s, set):
+            new_set.add(recursive_set_seralizer(s,visited))
+        elif isinstance(s, dict):
+            visited.append(id(s))
+            new_set.add(recursive_dict_seralizer(s, visited))
+        elif hasattr(s, '__dict__'):
+            visited.append(id(s))
+            new_set.add(str(recursive_object_seralizer(s, visited)))
+        elif callable(s):
+            if hasattr(s, '__name__'):
+                new_set.add("<function {}>".format(s.__name__))
+        else:
+            new_set.add(str(s))
+    return new_set
+    
+
+def recursive_tuple_seralizer(tup, visited):
+    new_tup = ()
+    for t in tup:
+        if id(t) in visited:
+           continue
+        if isinstance(t, (float, int, str, bool, type(None))):
+            new_tup = (*new_tup, t)
+        elif isinstance(t, tuple):
+            new_tup = (*new_tup, recursive_tuple_seralizer(t, visited))
+        elif isinstance(t, list):
+            new_tup = (*new_tup, recursive_list_seralizer(t, visited))
+        elif isinstance(t, set):
+            new_tup = (*new_tup, recursive_set_seralizer(t, visited))
+        elif isinstance(t, dict):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_dict_seralizer(t, visited))
+        elif hasattr(t, '__dict__'):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_object_seralizer(t, visited))
+        elif callable(t):
+            if hasattr(t, '__name__'):
+                new_tup = (*new_tup, "<function {}>".format(t.__name__))
+        else:
+            new_tup = (*new_tup, str(t))
+    return new_tup
+
+def recursive_list_seralizer(li, visited):
+    new_list = []
+    for l in li:
+        if id(l) in visited:
+            continue
+        if isinstance(l, (float, int, str, bool, type(None))):
+            new_list.append(l)
+        elif isinstance(l, tuple):
+            new_list.append(recursive_tuple_seralizer(l, visited))
+        elif isinstance(l, list):
+            new_list.append(recursive_list_seralizer(l, visited))
+        elif isinstance(l, set):
+            new_list.append(recursive_set_seralizer(l, visited))
+        elif hasattr(l, '__dict__'):
+            visited.append(id(l))
+            new_list.append(recursive_object_seralizer(l, visited))
+        elif isinstance(l, dict):
+            visited.append(id(l))
+            new_list.append(recursive_dict_seralizer(l, visited))
+        elif callable(l):
+            if hasattr(l, '__name__'):
+                new_list.append("<function {}>".format(l.__name__))
+        else:
+            new_list.append(str(l))       
+
+def inspect_code(func):
+    def wrapper(*args, **kwargs):
+        visited = []
+        filename = "/home/changshu/CODEMIND/scripts/swebench/swebench_playground/obj/scikit-learn__scikit-learn-25697/sklearn/linear_model/_bayes.py"
+        para_dict = {"name": func.__name__}
+        args_names = inspect.getfullargspec(func).args
+        if len(args) > 0 and hasattr(args[0], '__dict__') and args_names[0] == 'self':
+            ## 'self'
+            self_args = args[0]
+            para_dict['self'] = recursive_object_seralizer(self_args, [id(self_args)])
+        else:
+            para_dict['self'] = {}
+        if len(args) > 0 :
+            if args_names[0] == 'self':
+                other_args = {}
+                for m,n in zip(args_names[1:], args[1:]):
+                    other_args[m] = n
+            else:
+                other_args = {}
+                for m,n in zip(args_names, args):
+                    other_args[m] = n
+            para_dict['args'] = recursive_dict_seralizer(other_args, [id(other_args)])
+        else:
+            para_dict['args'] = {}
+        if kwargs:
+            para_dict['kwargs'] = recursive_dict_seralizer(kwargs, [id(kwargs)])
+        else:
+            para_dict['kwargs'] = {}
+            
+        result = func(*args, **kwargs)
+        ## seralize the return value
+        if isinstance(result, tuple):
+            ret = recursive_tuple_seralizer(result, [])
+        elif isinstance(result, (float, int, str)):
+            ret = result
+        elif isinstance(result, list):
+            ret = recursive_list_seralizer(result, [])
+        elif isinstance(result, dict):
+            ret = recursive_dict_seralizer(result, [])
+        elif hasattr(result, '__dict__'):
+            ret = recursive_object_seralizer(result, [])
+        elif callable(result):
+            ret = "<function {}>".format(result.__name__)
+        else:
+            ret = str(result)
+        para_dict["return"] = ret
+        print("@[DATA]@", filename,"[SEP]", para_dict, "[/SEP]")
+        return result
+    return wrapper
 """
 Various bayesian regression
 """
@@ -5,6 +198,7 @@ Various bayesian regression
 # Authors: V. Michel, F. Pedregosa, A. Gramfort
 # License: BSD 3 clause
 
+import warnings
 from math import log
 from numbers import Integral, Real
 import numpy as np
@@ -15,7 +209,49 @@ from ..base import RegressorMixin
 from ..utils.extmath import fast_logdet
 from scipy.linalg import pinvh
 from ..utils.validation import _check_sample_weight
-from ..utils._param_validation import Interval
+from ..utils._param_validation import Interval, Hidden, StrOptions
+
+
+# TODO(1.5) Remove
+def _deprecate_n_iter(n_iter, max_iter):
+    """Deprecates n_iter in favour of max_iter. Checks if the n_iter has been
+    used instead of max_iter and generates a deprecation warning if True.
+
+    Parameters
+    ----------
+    n_iter : int,
+        Value of n_iter attribute passed by the estimator.
+
+    max_iter : int, default=None
+        Value of max_iter attribute passed by the estimator.
+        If `None`, it corresponds to `max_iter=300`.
+
+    Returns
+    -------
+    max_iter : int,
+        Value of max_iter which shall further be used by the estimator.
+
+    Notes
+    -----
+    This function should be completely removed in 1.5.
+    """
+    if n_iter != "deprecated":
+        if max_iter is not None:
+            raise ValueError(
+                "Both `n_iter` and `max_iter` attributes were set. Attribute"
+                " `n_iter` was deprecated in version 1.3 and will be removed in"
+                " 1.5. To avoid this error, only set the `max_iter` attribute."
+            )
+        warnings.warn(
+            "'n_iter' was renamed to 'max_iter' in version 1.3 and "
+            "will be removed in 1.5",
+            FutureWarning,
+        )
+        max_iter = n_iter
+    elif max_iter is None:
+        max_iter = 300
+    return max_iter
+
 
 ###############################################################################
 # BayesianRidge regression
@@ -32,8 +268,12 @@ class BayesianRidge(RegressorMixin, LinearModel):
 
     Parameters
     ----------
-    n_iter : int, default=300
-        Maximum number of iterations. Should be greater than or equal to 1.
+    max_iter : int, default=None
+        Maximum number of iterations over the complete dataset before
+        stopping independently of any early stopping criterion. If `None`, it
+        corresponds to `max_iter=300`.
+
+        .. versionchanged:: 1.3
 
     tol : float, default=1e-3
         Stop the algorithm if w has converged.
@@ -83,6 +323,13 @@ class BayesianRidge(RegressorMixin, LinearModel):
     verbose : bool, default=False
         Verbose mode when fitting the model.
 
+    n_iter : int
+        Maximum number of iterations. Should be greater than or equal to 1.
+
+        .. deprecated:: 1.3
+           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
+           `max_iter` instead.
+
     Attributes
     ----------
     coef_ : array-like of shape (n_features,)
@@ -90,7 +337,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
 
     intercept_ : float
         Independent term in decision function. Set to 0.0 if
-        ``fit_intercept = False``.
+        `fit_intercept = False`.
 
     alpha_ : float
        Estimated precision of the noise.
@@ -162,7 +409,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
     """
 
     _parameter_constraints: dict = {
-        "n_iter": [Interval(Integral, 1, None, closed="left")],
+        "max_iter": [Interval(Integral, 1, None, closed="left"), None],
         "tol": [Interval(Real, 0, None, closed="neither")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
@@ -174,12 +421,17 @@ class BayesianRidge(RegressorMixin, LinearModel):
         "fit_intercept": ["boolean"],
         "copy_X": ["boolean"],
         "verbose": ["verbose"],
+        "n_iter": [
+            Interval(Integral, 1, None, closed="left"),
+            Hidden(StrOptions({"deprecated"})),
+        ],
     }
 
+    @inspect_code
     def __init__(
         self,
         *,
-        n_iter=300,
+        max_iter=None,  # TODO(1.5): Set to 300
         tol=1.0e-3,
         alpha_1=1.0e-6,
         alpha_2=1.0e-6,
@@ -191,8 +443,9 @@ class BayesianRidge(RegressorMixin, LinearModel):
         fit_intercept=True,
         copy_X=True,
         verbose=False,
+        n_iter="deprecated",  # TODO(1.5): Remove
     ):
-        self.n_iter = n_iter
+        self.max_iter = max_iter
         self.tol = tol
         self.alpha_1 = alpha_1
         self.alpha_2 = alpha_2
@@ -204,7 +457,9 @@ class BayesianRidge(RegressorMixin, LinearModel):
         self.fit_intercept = fit_intercept
         self.copy_X = copy_X
         self.verbose = verbose
+        self.n_iter = n_iter
 
+    @inspect_code
     def fit(self, X, y, sample_weight=None):
         """Fit the model.
 
@@ -228,6 +483,8 @@ class BayesianRidge(RegressorMixin, LinearModel):
         """
         self._validate_params()
 
+        max_iter = _deprecate_n_iter(self.n_iter, self.max_iter)
+
         X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
 
         if sample_weight is not None:
@@ -274,7 +531,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
         eigen_vals_ = S**2
 
         # Convergence loop of the bayesian ridge regression
-        for iter_ in range(self.n_iter):
+        for iter_ in range(max_iter):
 
             # update posterior mean coef_ based on alpha_ and lambda_ and
             # compute corresponding rmse
@@ -430,8 +687,10 @@ class ARDRegression(RegressorMixin, LinearModel):
 
     Parameters
     ----------
-    n_iter : int, default=300
-        Maximum number of iterations.
+    max_iter : int, default=None
+        Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.
+
+        .. versionchanged:: 1.3
 
     tol : float, default=1e-3
         Stop the algorithm if w has converged.
@@ -470,6 +729,13 @@ class ARDRegression(RegressorMixin, LinearModel):
     verbose : bool, default=False
         Verbose mode when fitting the model.
 
+    n_iter : int
+        Maximum number of iterations.
+
+        .. deprecated:: 1.3
+           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
+           `max_iter` instead.
+
     Attributes
     ----------
     coef_ : array-like of shape (n_features,)
@@ -487,6 +753,11 @@ class ARDRegression(RegressorMixin, LinearModel):
     scores_ : float
         if computed, value of the objective function (to be maximized)
 
+    n_iter_ : int
+        The actual number of iterations to reach the stopping criterion.
+
+        .. versionadded:: 1.3
+
     intercept_ : float
         Independent term in decision function. Set to 0.0 if
         ``fit_intercept = False``.
@@ -542,7 +813,7 @@ class ARDRegression(RegressorMixin, LinearModel):
     """
 
     _parameter_constraints: dict = {
-        "n_iter": [Interval(Integral, 1, None, closed="left")],
+        "max_iter": [Interval(Integral, 1, None, closed="left"), None],
         "tol": [Interval(Real, 0, None, closed="left")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
@@ -553,12 +824,16 @@ class ARDRegression(RegressorMixin, LinearModel):
         "fit_intercept": ["boolean"],
         "copy_X": ["boolean"],
         "verbose": ["verbose"],
+        "n_iter": [
+            Interval(Integral, 1, None, closed="left"),
+            Hidden(StrOptions({"deprecated"})),
+        ],
     }
 
     def __init__(
         self,
         *,
-        n_iter=300,
+        max_iter=None,  # TODO(1.5): Set to 300
         tol=1.0e-3,
         alpha_1=1.0e-6,
         alpha_2=1.0e-6,
@@ -569,8 +844,9 @@ class ARDRegression(RegressorMixin, LinearModel):
         fit_intercept=True,
         copy_X=True,
         verbose=False,
+        n_iter="deprecated",  # TODO(1.5): Remove
     ):
-        self.n_iter = n_iter
+        self.max_iter = max_iter
         self.tol = tol
         self.fit_intercept = fit_intercept
         self.alpha_1 = alpha_1
@@ -581,6 +857,7 @@ class ARDRegression(RegressorMixin, LinearModel):
         self.threshold_lambda = threshold_lambda
         self.copy_X = copy_X
         self.verbose = verbose
+        self.n_iter = n_iter
 
     def fit(self, X, y):
         """Fit the model according to the given training data and parameters.
@@ -603,6 +880,8 @@ class ARDRegression(RegressorMixin, LinearModel):
 
         self._validate_params()
 
+        max_iter = _deprecate_n_iter(self.n_iter, self.max_iter)
+
         X, y = self._validate_data(
             X, y, dtype=[np.float64, np.float32], y_numeric=True, ensure_min_samples=2
         )
@@ -648,7 +927,7 @@ class ARDRegression(RegressorMixin, LinearModel):
             else self._update_sigma_woodbury
         )
         # Iterative procedure of ARDRegression
-        for iter_ in range(self.n_iter):
+        for iter_ in range(max_iter):
             sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)
             coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)
 
@@ -688,6 +967,8 @@ class ARDRegression(RegressorMixin, LinearModel):
             if not keep_lambda.any():
                 break
 
+        self.n_iter_ = iter_ + 1
+
         if keep_lambda.any():
             # update sigma and mu using updated params from the last iteration
             sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)
diff --git a/sklearn/linear_model/tests/test_bayes.py b/sklearn/linear_model/tests/test_bayes.py
index 5bb6ae2..b33e656 100644
--- a/sklearn/linear_model/tests/test_bayes.py
+++ b/sklearn/linear_model/tests/test_bayes.py
@@ -73,7 +73,7 @@ def test_bayesian_ridge_score_values():
         alpha_2=alpha_2,
         lambda_1=lambda_1,
         lambda_2=lambda_2,
-        n_iter=1,
+        max_iter=1,
         fit_intercept=False,
         compute_score=True,
     )
@@ -174,7 +174,7 @@ def test_update_of_sigma_in_ard():
     # of the ARDRegression algorithm. See issue #10128.
     X = np.array([[1, 0], [0, 0]])
     y = np.array([0, 0])
-    clf = ARDRegression(n_iter=1)
+    clf = ARDRegression(max_iter=1)
     clf.fit(X, y)
     # With the inputs above, ARDRegression prunes both of the two coefficients
     # in the first iteration. Hence, the expected shape of `sigma_` is (0, 0).
@@ -292,3 +292,33 @@ def test_dtype_correctness(Estimator):
     coef_32 = model.fit(X.astype(np.float32), y).coef_
     coef_64 = model.fit(X.astype(np.float64), y).coef_
     np.testing.assert_allclose(coef_32, coef_64, rtol=1e-4)
+
+
+# TODO(1.5) remove
+@pytest.mark.parametrize("Estimator", [BayesianRidge, ARDRegression])
+def test_bayesian_ridge_ard_n_iter_deprecated(Estimator):
+    """Check the deprecation warning of `n_iter`."""
+    depr_msg = (
+        "'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5"
+    )
+    X, y = diabetes.data, diabetes.target
+    model = Estimator(n_iter=5)
+
+    with pytest.warns(FutureWarning, match=depr_msg):
+        model.fit(X, y)
+
+
+# TODO(1.5) remove
+@pytest.mark.parametrize("Estimator", [BayesianRidge, ARDRegression])
+def test_bayesian_ridge_ard_max_iter_and_n_iter_both_set(Estimator):
+    """Check that a ValueError is raised when both `max_iter` and `n_iter` are set."""
+    err_msg = (
+        "Both `n_iter` and `max_iter` attributes were set. Attribute"
+        " `n_iter` was deprecated in version 1.3 and will be removed in"
+        " 1.5. To avoid this error, only set the `max_iter` attribute."
+    )
+    X, y = diabetes.data, diabetes.target
+    model = Estimator(n_iter=5, max_iter=5)
+
+    with pytest.raises(ValueError, match=err_msg):
+        model.fit(X, y)
