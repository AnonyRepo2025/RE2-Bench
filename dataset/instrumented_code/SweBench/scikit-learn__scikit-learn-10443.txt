diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index ff4dd6b..b517f84 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -1,6 +1,9 @@
 from __future__ import unicode_literals
 import warnings
 
+import pytest
+from scipy import sparse
+
 from sklearn.feature_extraction.text import strip_tags
 from sklearn.feature_extraction.text import strip_accents_unicode
 from sklearn.feature_extraction.text import strip_accents_ascii
@@ -28,15 +31,14 @@ from sklearn.utils.testing import (assert_equal, assert_false, assert_true,
                                    assert_in, assert_less, assert_greater,
                                    assert_warns_message, assert_raise_message,
                                    clean_warning_registry, ignore_warnings,
-                                   SkipTest, assert_raises)
+                                   SkipTest, assert_raises,
+                                   assert_allclose_dense_sparse)
 
 from collections import defaultdict, Mapping
 from functools import partial
 import pickle
 from io import StringIO
 
-import pytest
-
 JUNK_FOOD_DOCS = (
     "the pizza pizza beer copyright",
     "the pizza burger beer copyright",
@@ -1042,6 +1044,42 @@ def test_vectorizer_string_object_as_input():
             ValueError, message, vec.transform, "hello world!")
 
 
+@pytest.mark.parametrize("X_dtype", [np.float32, np.float64])
+def test_tfidf_transformer_type(X_dtype):
+    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)
+    X_trans = TfidfTransformer().fit_transform(X)
+    assert X_trans.dtype == X.dtype
+
+
+def test_tfidf_transformer_sparse():
+    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)
+    X_csc = sparse.csc_matrix(X)
+    X_csr = sparse.csr_matrix(X)
+
+    X_trans_csc = TfidfTransformer().fit_transform(X_csc)
+    X_trans_csr = TfidfTransformer().fit_transform(X_csr)
+    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)
+    assert X_trans_csc.format == X_trans_csr.format
+
+
+@pytest.mark.parametrize(
+    "vectorizer_dtype, output_dtype, expected_warning, msg_warning",
+    [(np.int32, np.float64, UserWarning, "'dtype' should be used."),
+     (np.int64, np.float64, UserWarning, "'dtype' should be used."),
+     (np.float32, np.float32, None, None),
+     (np.float64, np.float64, None, None)]
+)
+def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,
+                               expected_warning, msg_warning):
+    X = np.array(["numpy", "scipy", "sklearn"])
+    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)
+    with pytest.warns(expected_warning, match=msg_warning) as record:
+            X_idf = vectorizer.fit_transform(X)
+    if expected_warning is None:
+        assert len(record) == 0
+    assert X_idf.dtype == output_dtype
+
+
 @pytest.mark.parametrize("vec", [
         HashingVectorizer(ngram_range=(2, 1)),
         CountVectorizer(ngram_range=(2, 1)),
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index df0582d..19c8bd8 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -11,7 +11,200 @@
 The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to
 build feature vectors from text documents.
 """
-from __future__ import unicode_literals
+from __future__ import unicode_literals, division
+import inspect
+def recursive_object_seralizer(obj, visited):
+    seralized_dict = {}
+    keys = list(obj.__dict__)
+    for k in keys:
+        if id(obj.__dict__[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(obj.__dict__[k])
+            continue
+        if isinstance(obj.__dict__[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = obj.__dict__[k]
+        elif isinstance(obj.__dict__[k], tuple):
+            ## handle tuple
+            seralized_dict[k] = recursive_tuple_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], set):
+            ## handle set
+            seralized_dict[k] = recursive_set_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], list):
+            ## handle list
+            seralized_dict[k] = recursive_list_seralizer(obj.__dict__[k], visited)
+        elif hasattr(obj.__dict__[k], '__dict__'):
+            ## handle object
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_object_seralizer(obj.__dict__[k], visited)
+        elif isinstance(obj.__dict__[k], dict):
+            visited.append(id(obj.__dict__[k]))
+            seralized_dict[k] = recursive_dict_seralizer(obj.__dict__[k], visited)
+        elif callable(obj.__dict__[k]):
+            ## handle function
+            if hasattr(obj.__dict__[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(obj.__dict__[k].__name__)
+        else:
+            seralized_dict[k] = str(obj.__dict__[k])
+    return seralized_dict
+
+def recursive_dict_seralizer(dictionary, visited):
+    seralized_dict = {}
+    keys = list(dictionary)
+    for k in keys:
+        if id(dictionary[k]) in visited:
+            seralized_dict[k] = "<RECURSIVE {}>".format(dictionary[k])
+            continue
+        # if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+        #     pass
+        # else:
+        #     visited.append(id(dictionary[k]))
+        if isinstance(dictionary[k], (float, int, str, bool, type(None))):
+            seralized_dict[k] = dictionary[k]
+        elif isinstance(dictionary[k], list):
+            seralized_dict[k] = recursive_list_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], tuple):
+            seralized_dict[k] = recursive_tuple_seralizer(dictionary[k], visited)
+        elif isinstance(dictionary[k], set):
+            seralized_dict[k] = recursive_set_seralizer(dictionary[k], visited)        
+        elif hasattr(dictionary[k], '__dict__'):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_object_seralizer(dictionary[k], visited)
+        elif callable(dictionary[k]):
+            if hasattr(dictionary[k], '__name__'):
+                seralized_dict[k] = "<function {}>".format(dictionary[k].__name__)
+        elif isinstance(dictionary[k], dict):
+            visited.append(id(dictionary[k]))
+            seralized_dict[k] = recursive_dict_seralizer(dictionary[k], visited)
+        else:
+            seralized_dict[k] =str(dictionary[k])
+    return seralized_dict   
+
+def recursive_set_seralizer(set_data, visited):
+    new_set = set()
+    for s in set_data:
+        if id(s) in visited:
+            continue 
+        if isinstance(s, (float, int, str, bool, type(None))):
+            new_set.add(s)
+        elif isinstance(s, tuple):
+            new_set.add(recursive_tuple_seralizer(s, visited))
+        elif isinstance(s, list):
+            new_set.add(recursive_list_seralizer(s, visited))
+        elif isinstance(s, set):
+            new_set.add(recursive_set_seralizer(s,visited))
+        elif isinstance(s, dict):
+            visited.append(id(s))
+            new_set.add(recursive_dict_seralizer(s, visited))
+        elif hasattr(s, '__dict__'):
+            visited.append(id(s))
+            new_set.add(str(recursive_object_seralizer(s, visited)))
+        elif callable(s):
+            if hasattr(s, '__name__'):
+                new_set.add("<function {}>".format(s.__name__))
+        else:
+            new_set.add(str(s))
+    return new_set
+    
+
+def recursive_tuple_seralizer(tup, visited):
+    new_tup = ()
+    for t in tup:
+        if id(t) in visited:
+           continue
+        if isinstance(t, (float, int, str, bool, type(None))):
+            new_tup = (*new_tup, t)
+        elif isinstance(t, tuple):
+            new_tup = (*new_tup, recursive_tuple_seralizer(t, visited))
+        elif isinstance(t, list):
+            new_tup = (*new_tup, recursive_list_seralizer(t, visited))
+        elif isinstance(t, set):
+            new_tup = (*new_tup, recursive_set_seralizer(t, visited))
+        elif isinstance(t, dict):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_dict_seralizer(t, visited))
+        elif hasattr(t, '__dict__'):
+            visited.append(id(t))
+            new_tup = (*new_tup, recursive_object_seralizer(t, visited))
+        elif callable(t):
+            if hasattr(t, '__name__'):
+                new_tup = (*new_tup, "<function {}>".format(t.__name__))
+        else:
+            new_tup = (*new_tup, str(t))
+    return new_tup
+
+def recursive_list_seralizer(li, visited):
+    new_list = []
+    for l in li:
+        if id(l) in visited:
+            continue
+        if isinstance(l, (float, int, str, bool, type(None))):
+            new_list.append(l)
+        elif isinstance(l, tuple):
+            new_list.append(recursive_tuple_seralizer(l, visited))
+        elif isinstance(l, list):
+            new_list.append(recursive_list_seralizer(l, visited))
+        elif isinstance(l, set):
+            new_list.append(recursive_set_seralizer(l, visited))
+        elif hasattr(l, '__dict__'):
+            visited.append(id(l))
+            new_list.append(recursive_object_seralizer(l, visited))
+        elif isinstance(l, dict):
+            visited.append(id(l))
+            new_list.append(recursive_dict_seralizer(l, visited))
+        elif callable(l):
+            if hasattr(l, '__name__'):
+                new_list.append("<function {}>".format(l.__name__))
+        else:
+            new_list.append(str(l))       
+
+def inspect_code(func):
+    def wrapper(*args, **kwargs):
+        visited = []
+        filename = "/home/changshu/CODEMIND/scripts/swebench/swebench_playground/obj/scikit-learn__scikit-learn-10443/sklearn/feature_extraction/text.py"
+        para_dict = {"name": func.__name__}
+        args_names = inspect.getfullargspec(func).args
+        if len(args) > 0 and hasattr(args[0], '__dict__') and args_names[0] == 'self':
+            ## 'self'
+            self_args = args[0]
+            para_dict['self'] = recursive_object_seralizer(self_args, [id(self_args)])
+        else:
+            para_dict['self'] = {}
+        if len(args) > 0 :
+            if args_names[0] == 'self':
+                other_args = {}
+                for m,n in zip(args_names[1:], args[1:]):
+                    other_args[m] = n
+            else:
+                other_args = {}
+                for m,n in zip(args_names, args):
+                    other_args[m] = n
+            para_dict['args'] = recursive_dict_seralizer(other_args, [id(other_args)])
+        else:
+            para_dict['args'] = {}
+        if kwargs:
+            para_dict['kwargs'] = recursive_dict_seralizer(kwargs, [id(kwargs)])
+        else:
+            para_dict['kwargs'] = {}
+            
+        result = func(*args, **kwargs)
+        ## seralize the return value
+        if isinstance(result, tuple):
+            ret = recursive_tuple_seralizer(result, [])
+        elif isinstance(result, (float, int, str)):
+            ret = result
+        elif isinstance(result, list):
+            ret = recursive_list_seralizer(result, [])
+        elif isinstance(result, dict):
+            ret = recursive_dict_seralizer(result, [])
+        elif hasattr(result, '__dict__'):
+            ret = recursive_object_seralizer(result, [])
+        elif callable(result):
+            ret = "<function {}>".format(result.__name__)
+        else:
+            ret = str(result)
+        para_dict["return"] = ret
+        print("@[DATA]@", filename,"[SEP]", para_dict, "[/SEP]")
+        return result
+    return wrapper
 
 import array
 from collections import Mapping, defaultdict
@@ -19,6 +212,7 @@ import numbers
 from operator import itemgetter
 import re
 import unicodedata
+import warnings
 
 import numpy as np
 import scipy.sparse as sp
@@ -29,7 +223,7 @@ from ..externals.six.moves import xrange
 from ..preprocessing import normalize
 from .hashing import FeatureHasher
 from .stop_words import ENGLISH_STOP_WORDS
-from ..utils.validation import check_is_fitted
+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES
 from ..utils.fixes import sp_version
 
 __all__ = ['CountVectorizer',
@@ -568,12 +762,13 @@ class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
                              non_negative=self.non_negative)
 
 
+@inspect_code
 def _document_frequency(X):
     """Count the number of non-zero values for each feature in sparse X."""
     if sp.isspmatrix_csr(X):
         return np.bincount(X.indices, minlength=X.shape[1])
     else:
-        return np.diff(sp.csc_matrix(X, copy=False).indptr)
+        return np.diff(X.indptr)
 
 
 class CountVectorizer(BaseEstimator, VectorizerMixin):
@@ -1109,6 +1304,7 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
         self.smooth_idf = smooth_idf
         self.sublinear_tf = sublinear_tf
 
+    @inspect_code
     def fit(self, X, y=None):
         """Learn the idf vector (global term weights)
 
@@ -1117,11 +1313,14 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
         X : sparse matrix, [n_samples, n_features]
             a matrix of term/token counts
         """
+        X = check_array(X, accept_sparse=('csr', 'csc'))
         if not sp.issparse(X):
-            X = sp.csc_matrix(X)
+            X = sp.csr_matrix(X)
+        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64
+
         if self.use_idf:
             n_samples, n_features = X.shape
-            df = _document_frequency(X)
+            df = _document_frequency(X).astype(dtype)
 
             # perform idf smoothing if required
             df += int(self.smooth_idf)
@@ -1129,12 +1328,15 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
 
             # log+1 instead of log makes sure terms with zero idf don't get
             # suppressed entirely.
-            idf = np.log(float(n_samples) / df) + 1.0
-            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,
-                                        n=n_features, format='csr')
+            idf = np.log(n_samples / df) + 1
+            self._idf_diag = sp.diags(idf, offsets=0,
+                                      shape=(n_features, n_features),
+                                      format='csr',
+                                      dtype=dtype)
 
         return self
 
+    @inspect_code
     def transform(self, X, copy=True):
         """Transform a count matrix to a tf or tf-idf representation
 
@@ -1151,12 +1353,9 @@ class TfidfTransformer(BaseEstimator, TransformerMixin):
         -------
         vectors : sparse matrix, [n_samples, n_features]
         """
-        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):
-            # preserve float family dtype
-            X = sp.csr_matrix(X, copy=copy)
-        else:
-            # convert counts or binary occurrences to floats
-            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)
+        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
+        if not sp.issparse(X):
+            X = sp.csr_matrix(X, dtype=np.float64)
 
         n_samples, n_features = X.shape
 
@@ -1361,13 +1560,14 @@ class TfidfVectorizer(CountVectorizer):
     be safely removed using delattr or set to None before pickling.
     """
 
+    @inspect_code
     def __init__(self, input='content', encoding='utf-8',
                  decode_error='strict', strip_accents=None, lowercase=True,
                  preprocessor=None, tokenizer=None, analyzer='word',
                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
                  ngram_range=(1, 1), max_df=1.0, min_df=1,
                  max_features=None, vocabulary=None, binary=False,
-                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,
+                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,
                  sublinear_tf=False):
 
         super(TfidfVectorizer, self).__init__(
@@ -1432,6 +1632,15 @@ class TfidfVectorizer(CountVectorizer):
                                  (len(value), len(self.vocabulary)))
         self._tfidf.idf_ = value
 
+    @inspect_code
+    def _check_params(self):
+        if self.dtype not in FLOAT_DTYPES:
+            warnings.warn("Only {} 'dtype' should be used. {} 'dtype' will "
+                          "be converted to np.float64."
+                          .format(FLOAT_DTYPES, self.dtype),
+                          UserWarning)
+
+    @inspect_code
     def fit(self, raw_documents, y=None):
         """Learn vocabulary and idf from training set.
 
@@ -1444,6 +1653,7 @@ class TfidfVectorizer(CountVectorizer):
         -------
         self : TfidfVectorizer
         """
+        self._check_params()
         X = super(TfidfVectorizer, self).fit_transform(raw_documents)
         self._tfidf.fit(X)
         return self
@@ -1464,6 +1674,7 @@ class TfidfVectorizer(CountVectorizer):
         X : sparse matrix, [n_samples, n_features]
             Tf-idf-weighted document-term matrix.
         """
+        self._check_params()
         X = super(TfidfVectorizer, self).fit_transform(raw_documents)
         self._tfidf.fit(X)
         # X is already a transformed view of raw_documents so
