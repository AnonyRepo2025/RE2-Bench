{
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.__init__": "def __init__(self, func=None, inverse_func=None, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None):\n    self.func = func\n    self.inverse_func = inverse_func\n    self.validate = validate\n    self.accept_sparse = accept_sparse\n    self.check_inverse = check_inverse\n    self.kw_args = kw_args\n    self.inv_kw_args = inv_kw_args",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.fit": "def fit(self, X, y=None):\n    X = self._check_input(X)\n    if self.check_inverse and (not (self.func is None or self.inverse_func is None)):\n        self._check_inverse_transform(X)\n    return self",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer._check_input": "def _check_input(self, X):\n    if self.validate:\n        return check_array(X, accept_sparse=self.accept_sparse)\n    return X",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer._check_inverse_transform": "def _check_inverse_transform(self, X):\n    idx_selected = slice(None, None, max(1, X.shape[0] // 100))\n    X_round_trip = self.inverse_transform(self.transform(X[idx_selected]))\n    if not _allclose_dense_sparse(X[idx_selected], X_round_trip):\n        warnings.warn(\"The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.\", UserWarning)",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.transform": "def transform(self, X):\n    return self._transform(X, func=self.func, kw_args=self.kw_args)",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer._transform": "def _transform(self, X, func=None, kw_args=None):\n    X = self._check_input(X)\n    if func is None:\n        func = _identity\n    return func(X, **kw_args if kw_args else {})",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.inverse_transform": "def inverse_transform(self, X):\n    return self._transform(X, func=self.inverse_func, kw_args=self.inv_kw_args)",
    ".sklearn.utils.validation.py@@_allclose_dense_sparse": "def _allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-09):\n    if sp.issparse(x) and sp.issparse(y):\n        x = x.tocsr()\n        y = y.tocsr()\n        x.sum_duplicates()\n        y.sum_duplicates()\n        return np.array_equal(x.indices, y.indices) and np.array_equal(x.indptr, y.indptr) and np.allclose(x.data, y.data, rtol=rtol, atol=atol)\n    elif not sp.issparse(x) and (not sp.issparse(y)):\n        return np.allclose(x, y, rtol=rtol, atol=atol)\n    raise ValueError('Can only compare two sparse matrices, not a sparse matrix and an array')",
    ".sklearn.utils.__init__.py@@safe_indexing": "def safe_indexing(X, indices, axis=0):\n    if indices is None:\n        return X\n    if axis not in (0, 1):\n        raise ValueError(\"'axis' should be either 0 (to index rows) or 1 (to index  column). Got {} instead.\".format(axis))\n    indices_dtype = _determine_key_type(indices)\n    if axis == 0 and indices_dtype == 'str':\n        raise ValueError(\"String indexing is not supported with 'axis=0'\")\n    if axis == 1 and X.ndim != 2:\n        raise ValueError(\"'X' should be a 2D NumPy array, 2D sparse matrix or pandas dataframe when indexing the columns (i.e. 'axis=1'). Got {} instead with {} dimension(s).\".format(type(X), X.ndim))\n    if axis == 1 and indices_dtype == 'str' and (not hasattr(X, 'loc')):\n        raise ValueError('Specifying the columns using strings is only supported for pandas DataFrames')\n    if hasattr(X, 'iloc'):\n        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n    elif hasattr(X, 'shape'):\n        return _array_indexing(X, indices, indices_dtype, axis=axis)\n    else:\n        return _list_indexing(X, indices, indices_dtype)",
    ".sklearn.utils.__init__.py@@_determine_key_type": "def _determine_key_type(key):\n    err_msg = 'No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed'\n    dtype_to_str = {int: 'int', str: 'str', bool: 'bool', np.bool_: 'bool'}\n    array_dtype_to_str = {'i': 'int', 'u': 'int', 'b': 'bool', 'O': 'str', 'U': 'str', 'S': 'str'}\n    if key is None:\n        return None\n    if isinstance(key, tuple(dtype_to_str.keys())):\n        try:\n            return dtype_to_str[type(key)]\n        except KeyError:\n            raise ValueError(err_msg)\n    if isinstance(key, slice):\n        if key.start is None and key.stop is None:\n            return None\n        key_start_type = _determine_key_type(key.start)\n        key_stop_type = _determine_key_type(key.stop)\n        if key_start_type is not None and key_stop_type is not None:\n            if key_start_type != key_stop_type:\n                raise ValueError(err_msg)\n        if key_start_type is not None:\n            return key_start_type\n        return key_stop_type\n    if isinstance(key, list):\n        unique_key = set(key)\n        key_type = {_determine_key_type(elt) for elt in unique_key}\n        if not key_type:\n            return None\n        if len(key_type) != 1:\n            raise ValueError(err_msg)\n        return key_type.pop()\n    if hasattr(key, 'dtype'):\n        try:\n            return array_dtype_to_str[key.dtype.kind]\n        except KeyError:\n            raise ValueError(err_msg)\n    raise ValueError(err_msg)",
    ".sklearn.utils.__init__.py@@_array_indexing": "def _array_indexing(array, key, key_dtype, axis):\n    if np_version < (1, 12) or issparse(array):\n        if key_dtype == 'bool':\n            key = np.asarray(key)\n    return array[key] if axis == 0 else array[:, key]",
    ".sklearn.base.py@@clone": "def clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\" % (repr(estimator), type(estimator)))\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError('Cannot clone object %s, as the constructor either does not set or modifies parameter %s' % (estimator, name))\n    return new_object",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        try:\n            value = getattr(self, key)\n        except AttributeError:\n            warnings.warn('From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.', FutureWarning)\n            value = None\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.linear_model.base.py@@LinearRegression.__init__": "def __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None):\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.copy_X = copy_X\n    self.n_jobs = n_jobs",
    ".sklearn.linear_model.base.py@@LinearRegression.fit": "def fit(self, X, y, sample_weight=None):\n    n_jobs_ = self.n_jobs\n    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], y_numeric=True, multi_output=True)\n    if sample_weight is not None and np.atleast_1d(sample_weight).ndim > 1:\n        raise ValueError('Sample weights must be 1D array or scalar')\n    X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, fit_intercept=self.fit_intercept, normalize=self.normalize, copy=self.copy_X, sample_weight=sample_weight, return_mean=True)\n    if sample_weight is not None:\n        X, y = _rescale_data(X, y, sample_weight)\n    if sp.issparse(X):\n        X_offset_scale = X_offset / X_scale\n\n        def matvec(b):\n            return X.dot(b) - b.dot(X_offset_scale)\n\n        def rmatvec(b):\n            return X.T.dot(b) - X_offset_scale * np.sum(b)\n        X_centered = sparse.linalg.LinearOperator(shape=X.shape, matvec=matvec, rmatvec=rmatvec)\n        if y.ndim < 2:\n            out = sparse_lsqr(X_centered, y)\n            self.coef_ = out[0]\n            self._residues = out[3]\n        else:\n            outs = Parallel(n_jobs=n_jobs_)((delayed(sparse_lsqr)(X_centered, y[:, j].ravel()) for j in range(y.shape[1])))\n            self.coef_ = np.vstack([out[0] for out in outs])\n            self._residues = np.vstack([out[3] for out in outs])\n    else:\n        self.coef_, self._residues, self.rank_, self.singular_ = linalg.lstsq(X, y)\n        self.coef_ = self.coef_.T\n    if y.ndim == 1:\n        self.coef_ = np.ravel(self.coef_)\n    self._set_intercept(X_offset, y_offset, X_scale)\n    return self",
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.linear_model.base.py@@_preprocess_data": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order='K')\n    y = np.asarray(y, dtype=X.dtype)\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n            if normalize:\n                X_var *= X.shape[0]\n                X_scale = np.sqrt(X_var, X_var)\n                del X_var\n                X_scale[X_scale == 0] = 1\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        else:\n            X_offset = np.average(X, axis=0, weights=sample_weight)\n            X -= X_offset\n            if normalize:\n                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n    return (X, y, X_offset, y_offset, X_scale)",
    ".sklearn.linear_model.base.py@@LinearModel._set_intercept": "def _set_intercept(self, X_offset, y_offset, X_scale):\n    if self.fit_intercept:\n        self.coef_ = self.coef_ / X_scale\n        self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n    else:\n        self.intercept_ = 0.0",
    ".sklearn.preprocessing.data.py@@StandardScaler.__init__": "def __init__(self, copy=True, with_mean=True, with_std=True):\n    self.with_mean = with_mean\n    self.with_std = with_std\n    self.copy = copy",
    ".sklearn.preprocessing.data.py@@StandardScaler.fit": "def fit(self, X, y=None):\n    self._reset()\n    return self.partial_fit(X, y)",
    ".sklearn.preprocessing.data.py@@StandardScaler._reset": "def _reset(self):\n    if hasattr(self, 'scale_'):\n        del self.scale_\n        del self.n_samples_seen_\n        del self.mean_\n        del self.var_",
    ".sklearn.preprocessing.data.py@@StandardScaler.partial_fit": "def partial_fit(self, X, y=None):\n    X = check_array(X, accept_sparse=('csr', 'csc'), estimator=self, dtype=FLOAT_DTYPES, force_all_finite='allow-nan')\n    if hasattr(self, 'n_samples_seen_') and isinstance(self.n_samples_seen_, numbers.Integral):\n        self.n_samples_seen_ = np.repeat(self.n_samples_seen_, X.shape[1]).astype(np.int64, copy=False)\n    if sparse.issparse(X):\n        if self.with_mean:\n            raise ValueError('Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.')\n        sparse_constructor = sparse.csr_matrix if X.format == 'csr' else sparse.csc_matrix\n        counts_nan = sparse_constructor((np.isnan(X.data), X.indices, X.indptr), shape=X.shape).sum(axis=0).A.ravel()\n        if not hasattr(self, 'n_samples_seen_'):\n            self.n_samples_seen_ = (X.shape[0] - counts_nan).astype(np.int64, copy=False)\n        if self.with_std:\n            if not hasattr(self, 'scale_'):\n                self.mean_, self.var_ = mean_variance_axis(X, axis=0)\n            else:\n                self.mean_, self.var_, self.n_samples_seen_ = incr_mean_variance_axis(X, axis=0, last_mean=self.mean_, last_var=self.var_, last_n=self.n_samples_seen_)\n        else:\n            self.mean_ = None\n            self.var_ = None\n            if hasattr(self, 'scale_'):\n                self.n_samples_seen_ += X.shape[0] - counts_nan\n    else:\n        if not hasattr(self, 'n_samples_seen_'):\n            self.n_samples_seen_ = np.zeros(X.shape[1], dtype=np.int64)\n        if not hasattr(self, 'scale_'):\n            self.mean_ = 0.0\n            if self.with_std:\n                self.var_ = 0.0\n            else:\n                self.var_ = None\n        if not self.with_mean and (not self.with_std):\n            self.mean_ = None\n            self.var_ = None\n            self.n_samples_seen_ += X.shape[0] - np.isnan(X).sum(axis=0)\n        else:\n            self.mean_, self.var_, self.n_samples_seen_ = _incremental_mean_and_var(X, self.mean_, self.var_, self.n_samples_seen_)\n    if np.ptp(self.n_samples_seen_) == 0:\n        self.n_samples_seen_ = self.n_samples_seen_[0]\n    if self.with_std:\n        self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n    else:\n        self.scale_ = None\n    return self",
    ".sklearn.utils.extmath.py@@_incremental_mean_and_var": "def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):\n    last_sum = last_mean * last_sample_count\n    new_sum = _safe_accumulator_op(np.nansum, X, axis=0)\n    new_sample_count = np.sum(~np.isnan(X), axis=0)\n    updated_sample_count = last_sample_count + new_sample_count\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n    if last_variance is None:\n        updated_variance = None\n    else:\n        new_unnormalized_variance = _safe_accumulator_op(np.nanvar, X, axis=0) * new_sample_count\n        last_unnormalized_variance = last_variance * last_sample_count\n        with np.errstate(divide='ignore', invalid='ignore'):\n            last_over_new_count = last_sample_count / new_sample_count\n            updated_unnormalized_variance = last_unnormalized_variance + new_unnormalized_variance + last_over_new_count / updated_sample_count * (last_sum / last_over_new_count - new_sum) ** 2\n        zeros = last_sample_count == 0\n        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n    return (updated_mean, updated_variance, updated_sample_count)",
    ".sklearn.preprocessing.data.py@@_handle_zeros_in_scale": "def _handle_zeros_in_scale(scale, copy=True):\n    if np.isscalar(scale):\n        if scale == 0.0:\n            scale = 1.0\n        return scale\n    elif isinstance(scale, np.ndarray):\n        if copy:\n            scale = scale.copy()\n        scale[scale == 0.0] = 1.0\n        return scale",
    ".sklearn.preprocessing.data.py@@StandardScaler.transform": "def transform(self, X, copy=None):\n    check_is_fitted(self)\n    copy = copy if copy is not None else self.copy\n    X = check_array(X, accept_sparse='csr', copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite='allow-nan')\n    if sparse.issparse(X):\n        if self.with_mean:\n            raise ValueError('Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.')\n        if self.scale_ is not None:\n            inplace_column_scale(X, 1 / self.scale_)\n    else:\n        if self.with_mean:\n            X -= self.mean_\n        if self.with_std:\n            X /= self.scale_\n    return X",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes='deprecated', msg=None, all_or_any='deprecated'):\n    if attributes != 'deprecated':\n        warnings.warn('Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.', DeprecationWarning)\n    if all_or_any != 'deprecated':\n        warnings.warn('Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.', DeprecationWarning)\n    if isclass(estimator):\n        raise TypeError('{} is a class, not an instance.'.format(estimator))\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    attrs = [v for v in vars(estimator) if (v.endswith('_') or v.startswith('_')) and (not v.startswith('__'))]\n    if not attrs:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.preprocessing.data.py@@StandardScaler.inverse_transform": "def inverse_transform(self, X, copy=None):\n    check_is_fitted(self)\n    copy = copy if copy is not None else self.copy\n    if sparse.issparse(X):\n        if self.with_mean:\n            raise ValueError('Cannot uncenter sparse matrices: pass `with_mean=False` instead See docstring for motivation and alternatives.')\n        if not sparse.isspmatrix_csr(X):\n            X = X.tocsr()\n            copy = False\n        if copy:\n            X = X.copy()\n        if self.scale_ is not None:\n            inplace_column_scale(X, self.scale_)\n    else:\n        X = np.asarray(X)\n        if copy:\n            X = X.copy()\n        if self.with_std:\n            X *= self.scale_\n        if self.with_mean:\n            X += self.mean_\n    return X",
    ".sklearn.dummy.py@@DummyRegressor.__init__": "def __init__(self, strategy='mean', constant=None, quantile=None):\n    self.strategy = strategy\n    self.constant = constant\n    self.quantile = quantile",
    ".sklearn.dummy.py@@DummyRegressor.fit": "def fit(self, X, y, sample_weight=None):\n    allowed_strategies = ('mean', 'median', 'quantile', 'constant')\n    if self.strategy not in allowed_strategies:\n        raise ValueError('Unknown strategy type: %s, expected one of %s.' % (self.strategy, allowed_strategies))\n    y = check_array(y, ensure_2d=False)\n    if len(y) == 0:\n        raise ValueError('y must not be empty.')\n    self.output_2d_ = y.ndim == 2 and y.shape[1] > 1\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n    self.n_outputs_ = y.shape[1]\n    check_consistent_length(X, y, sample_weight)\n    if self.strategy == 'mean':\n        self.constant_ = np.average(y, axis=0, weights=sample_weight)\n    elif self.strategy == 'median':\n        if sample_weight is None:\n            self.constant_ = np.median(y, axis=0)\n        else:\n            self.constant_ = [_weighted_percentile(y[:, k], sample_weight, percentile=50.0) for k in range(self.n_outputs_)]\n    elif self.strategy == 'quantile':\n        if self.quantile is None or not np.isscalar(self.quantile):\n            raise ValueError('Quantile must be a scalar in the range [0.0, 1.0], but got %s.' % self.quantile)\n        percentile = self.quantile * 100.0\n        if sample_weight is None:\n            self.constant_ = np.percentile(y, axis=0, q=percentile)\n        else:\n            self.constant_ = [_weighted_percentile(y[:, k], sample_weight, percentile=percentile) for k in range(self.n_outputs_)]\n    elif self.strategy == 'constant':\n        if self.constant is None:\n            raise TypeError('Constant target value has to be specified when the constant strategy is used.')\n        self.constant = check_array(self.constant, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False, ensure_min_samples=0)\n        if self.output_2d_ and self.constant.shape[0] != y.shape[1]:\n            raise ValueError('Constant target value should have shape (%d, 1).' % y.shape[1])\n        self.constant_ = self.constant\n    self.constant_ = np.reshape(self.constant_, (1, -1))\n    return self"
}