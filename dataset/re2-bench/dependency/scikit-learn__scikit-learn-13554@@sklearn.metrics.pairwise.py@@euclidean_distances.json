{
    ".sklearn.metrics.pairwise.py@@check_pairwise_arrays": "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\n    X, Y, dtype_float = _return_float_dtype(X, Y)\n    estimator = 'check_pairwise_arrays'\n    if dtype is None:\n        dtype = dtype_float\n    if Y is X or Y is None:\n        X = Y = check_array(X, accept_sparse='csr', dtype=dtype, estimator=estimator)\n    else:\n        X = check_array(X, accept_sparse='csr', dtype=dtype, estimator=estimator)\n        Y = check_array(Y, accept_sparse='csr', dtype=dtype, estimator=estimator)\n    if precomputed:\n        if X.shape[1] != Y.shape[0]:\n            raise ValueError('Precomputed metric requires shape (n_queries, n_indexed). Got (%d, %d) for %d indexed.' % (X.shape[0], X.shape[1], Y.shape[0]))\n    elif X.shape[1] != Y.shape[1]:\n        raise ValueError('Incompatible dimension for X and Y matrices: X.shape[1] == %d while Y.shape[1] == %d' % (X.shape[1], Y.shape[1]))\n    return (X, Y)",
    ".sklearn.metrics.pairwise.py@@_return_float_dtype": "def _return_float_dtype(X, Y):\n    if not issparse(X) and (not isinstance(X, np.ndarray)):\n        X = np.asarray(X)\n    if Y is None:\n        Y_dtype = X.dtype\n    elif not issparse(Y) and (not isinstance(Y, np.ndarray)):\n        Y = np.asarray(Y)\n        Y_dtype = Y.dtype\n    else:\n        Y_dtype = Y.dtype\n    if X.dtype == Y_dtype == np.float32:\n        dtype = np.float32\n    else:\n        dtype = np.float\n    return (X, Y, dtype)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if sparse.issparse(a) or sparse.issparse(b):\n        ret = a * b\n        if dense_output and hasattr(ret, 'toarray'):\n            ret = ret.toarray()\n        return ret\n    else:\n        return np.dot(a, b)",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.metrics.pairwise.py@@rbf_kernel": "def rbf_kernel(X, Y=None, gamma=None):\n    X, Y = check_pairwise_arrays(X, Y)\n    if gamma is None:\n        gamma = 1.0 / X.shape[1]\n    K = euclidean_distances(X, Y, squared=True)\n    K *= -gamma\n    np.exp(K, K)\n    return K",
    ".sklearn.metrics.pairwise.py@@_euclidean_distances_upcast": "def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):\n    n_samples_X = X.shape[0]\n    n_samples_Y = Y.shape[0]\n    n_features = X.shape[1]\n    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)\n    x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1\n    y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1\n    maxmem = max(((x_density * n_samples_X + y_density * n_samples_Y) * n_features + x_density * n_samples_X * y_density * n_samples_Y) / 10, 10 * 2 ** 17)\n    tmp = (x_density + y_density) * n_features\n    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) / 2\n    batch_size = max(int(batch_size), 1)\n    x_batches = gen_batches(X.shape[0], batch_size)\n    y_batches = gen_batches(Y.shape[0], batch_size)\n    for i, x_slice in enumerate(x_batches):\n        X_chunk = X[x_slice].astype(np.float64)\n        if XX is None:\n            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]\n        else:\n            XX_chunk = XX[x_slice]\n        for j, y_slice in enumerate(y_batches):\n            if X is Y and j < i:\n                d = distances[y_slice, x_slice].T\n            else:\n                Y_chunk = Y[y_slice].astype(np.float64)\n                if YY is None:\n                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]\n                else:\n                    YY_chunk = YY[:, y_slice]\n                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)\n                d += XX_chunk\n                d += YY_chunk\n            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)\n    return distances",
    ".sklearn.utils.__init__.py@@gen_batches": "def gen_batches(n, batch_size, min_batch_size=0):\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)"
}