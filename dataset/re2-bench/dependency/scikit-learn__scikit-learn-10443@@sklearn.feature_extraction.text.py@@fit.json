{
    ".sklearn.feature_extraction.text.py@@CountVectorizer.fit_transform": "def fit_transform(self, raw_documents, y=None):\n    if isinstance(raw_documents, six.string_types):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    self._validate_vocabulary()\n    max_df = self.max_df\n    min_df = self.min_df\n    max_features = self.max_features\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n    if self.binary:\n        X.data.fill(1)\n    if not self.fixed_vocabulary_:\n        X = self._sort_features(X, vocabulary)\n        n_doc = X.shape[0]\n        max_doc_count = max_df if isinstance(max_df, numbers.Integral) else max_df * n_doc\n        min_doc_count = min_df if isinstance(min_df, numbers.Integral) else min_df * n_doc\n        if max_doc_count < min_doc_count:\n            raise ValueError('max_df corresponds to < documents than min_df')\n        X, self.stop_words_ = self._limit_features(X, vocabulary, max_doc_count, min_doc_count, max_features)\n        self.vocabulary_ = vocabulary\n    return X",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._validate_params": "def _validate_params(self):\n    min_n, max_m = self.ngram_range\n    if min_n > max_m:\n        raise ValueError('Invalid value for ngram_range=%s lower boundary larger than the upper boundary.' % str(self.ngram_range))",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._validate_vocabulary": "def _validate_vocabulary(self):\n    vocabulary = self.vocabulary\n    if vocabulary is not None:\n        if isinstance(vocabulary, set):\n            vocabulary = sorted(vocabulary)\n        if not isinstance(vocabulary, Mapping):\n            vocab = {}\n            for i, t in enumerate(vocabulary):\n                if vocab.setdefault(t, i) != i:\n                    msg = 'Duplicate term in vocabulary: %r' % t\n                    raise ValueError(msg)\n            vocabulary = vocab\n        else:\n            indices = set(six.itervalues(vocabulary))\n            if len(indices) != len(vocabulary):\n                raise ValueError('Vocabulary contains repeated indices.')\n            for i in xrange(len(vocabulary)):\n                if i not in indices:\n                    msg = \"Vocabulary of size %d doesn't contain index %d.\" % (len(vocabulary), i)\n                    raise ValueError(msg)\n        if not vocabulary:\n            raise ValueError('empty vocabulary passed to fit')\n        self.fixed_vocabulary_ = True\n        self.vocabulary_ = dict(vocabulary)\n    else:\n        self.fixed_vocabulary_ = False",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._count_vocab": "def _count_vocab(self, raw_documents, fixed_vocab):\n    if fixed_vocab:\n        vocabulary = self.vocabulary_\n    else:\n        vocabulary = defaultdict()\n        vocabulary.default_factory = vocabulary.__len__\n    analyze = self.build_analyzer()\n    j_indices = []\n    indptr = []\n    values = _make_int_array()\n    indptr.append(0)\n    for doc in raw_documents:\n        feature_counter = {}\n        for feature in analyze(doc):\n            try:\n                feature_idx = vocabulary[feature]\n                if feature_idx not in feature_counter:\n                    feature_counter[feature_idx] = 1\n                else:\n                    feature_counter[feature_idx] += 1\n            except KeyError:\n                continue\n        j_indices.extend(feature_counter.keys())\n        values.extend(feature_counter.values())\n        indptr.append(len(j_indices))\n    if not fixed_vocab:\n        vocabulary = dict(vocabulary)\n        if not vocabulary:\n            raise ValueError('empty vocabulary; perhaps the documents only contain stop words')\n    if indptr[-1] > 2147483648:\n        if sp_version >= (0, 14):\n            indices_dtype = np.int64\n        else:\n            raise ValueError('sparse CSR array has {} non-zero elements and requires 64 bit indexing,  which is unsupported with scipy {}. Please upgrade to scipy >=0.14'.format(indptr[-1], '.'.join(sp_version)))\n    else:\n        indices_dtype = np.int32\n    j_indices = np.asarray(j_indices, dtype=indices_dtype)\n    indptr = np.asarray(indptr, dtype=indices_dtype)\n    values = np.frombuffer(values, dtype=np.intc)\n    X = sp.csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, len(vocabulary)), dtype=self.dtype)\n    X.sort_indices()\n    return (vocabulary, X)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_analyzer": "def build_analyzer(self):\n    if callable(self.analyzer):\n        return self.analyzer\n    preprocess = self.build_preprocessor()\n    if self.analyzer == 'char':\n        return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\n    elif self.analyzer == 'char_wb':\n        return lambda doc: self._char_wb_ngrams(preprocess(self.decode(doc)))\n    elif self.analyzer == 'word':\n        stop_words = self.get_stop_words()\n        tokenize = self.build_tokenizer()\n        return lambda doc: self._word_ngrams(tokenize(preprocess(self.decode(doc))), stop_words)\n    else:\n        raise ValueError('%s is not a valid tokenization scheme/analyzer' % self.analyzer)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_preprocessor": "def build_preprocessor(self):\n    if self.preprocessor is not None:\n        return self.preprocessor\n    noop = lambda x: x\n    if not self.strip_accents:\n        strip_accents = noop\n    elif callable(self.strip_accents):\n        strip_accents = self.strip_accents\n    elif self.strip_accents == 'ascii':\n        strip_accents = strip_accents_ascii\n    elif self.strip_accents == 'unicode':\n        strip_accents = strip_accents_unicode\n    else:\n        raise ValueError('Invalid value for \"strip_accents\": %s' % self.strip_accents)\n    if self.lowercase:\n        return lambda x: strip_accents(x.lower())\n    else:\n        return strip_accents",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.get_stop_words": "def get_stop_words(self):\n    return _check_stop_list(self.stop_words)",
    ".sklearn.feature_extraction.text.py@@_check_stop_list": "def _check_stop_list(stop):\n    if stop == 'english':\n        return ENGLISH_STOP_WORDS\n    elif isinstance(stop, six.string_types):\n        raise ValueError('not a built-in stop list: %s' % stop)\n    elif stop is None:\n        return None\n    else:\n        return frozenset(stop)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_tokenizer": "def build_tokenizer(self):\n    if self.tokenizer is not None:\n        return self.tokenizer\n    token_pattern = re.compile(self.token_pattern)\n    return lambda doc: token_pattern.findall(doc)",
    ".sklearn.feature_extraction.text.py@@_make_int_array": "def _make_int_array():\n    return array.array(str('i'))",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.decode": "def decode(self, doc):\n    if self.input == 'filename':\n        with open(doc, 'rb') as fh:\n            doc = fh.read()\n    elif self.input == 'file':\n        doc = doc.read()\n    if isinstance(doc, bytes):\n        doc = doc.decode(self.encoding, self.decode_error)\n    if doc is np.nan:\n        raise ValueError('np.nan is an invalid document, expected byte or unicode string.')\n    return doc",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._word_ngrams": "def _word_ngrams(self, tokens, stop_words=None):\n    if stop_words is not None:\n        tokens = [w for w in tokens if w not in stop_words]\n    min_n, max_n = self.ngram_range\n    if max_n != 1:\n        original_tokens = tokens\n        if min_n == 1:\n            tokens = list(original_tokens)\n            min_n += 1\n        else:\n            tokens = []\n        n_original_tokens = len(original_tokens)\n        tokens_append = tokens.append\n        space_join = ' '.join\n        for n in xrange(min_n, min(max_n + 1, n_original_tokens + 1)):\n            for i in xrange(n_original_tokens - n + 1):\n                tokens_append(space_join(original_tokens[i:i + n]))\n    return tokens",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._sort_features": "def _sort_features(self, X, vocabulary):\n    sorted_features = sorted(six.iteritems(vocabulary))\n    map_index = np.empty(len(sorted_features), dtype=np.int32)\n    for new_val, (term, old_val) in enumerate(sorted_features):\n        vocabulary[term] = new_val\n        map_index[old_val] = new_val\n    X.indices = map_index.take(X.indices, mode='clip')\n    return X",
    ".sklearn.externals.six.py@@iteritems": "def iteritems(d, **kw):\n    return iter(getattr(d, _iteritems)(**kw))",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._limit_features": "def _limit_features(self, X, vocabulary, high=None, low=None, limit=None):\n    if high is None and low is None and (limit is None):\n        return (X, set())\n    dfs = _document_frequency(X)\n    tfs = np.asarray(X.sum(axis=0)).ravel()\n    mask = np.ones(len(dfs), dtype=bool)\n    if high is not None:\n        mask &= dfs <= high\n    if low is not None:\n        mask &= dfs >= low\n    if limit is not None and mask.sum() > limit:\n        mask_inds = (-tfs[mask]).argsort()[:limit]\n        new_mask = np.zeros(len(dfs), dtype=bool)\n        new_mask[np.where(mask)[0][mask_inds]] = True\n        mask = new_mask\n    new_indices = np.cumsum(mask) - 1\n    removed_terms = set()\n    for term, old_index in list(six.iteritems(vocabulary)):\n        if mask[old_index]:\n            vocabulary[term] = new_indices[old_index]\n        else:\n            del vocabulary[term]\n            removed_terms.add(term)\n    kept_indices = np.where(mask)[0]\n    if len(kept_indices) == 0:\n        raise ValueError('After pruning, no terms remain. Try a lower min_df or a higher max_df.')\n    return (X[:, kept_indices], removed_terms)",
    ".sklearn.feature_extraction.text.py@@_document_frequency": "def _document_frequency(X):\n    if sp.isspmatrix_csr(X):\n        return np.bincount(X.indices, minlength=X.shape[1])\n    else:\n        return np.diff(X.indptr)",
    ".sklearn.feature_extraction.text.py@@TfidfTransformer.fit": "def fit(self, X, y=None):\n    X = check_array(X, accept_sparse=('csr', 'csc'))\n    if not sp.issparse(X):\n        X = sp.csr_matrix(X)\n    dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n    if self.use_idf:\n        n_samples, n_features = X.shape\n        df = _document_frequency(X).astype(dtype)\n        df += int(self.smooth_idf)\n        n_samples += int(self.smooth_idf)\n        idf = np.log(n_samples / df) + 1\n        self._idf_diag = sp.diags(idf, offsets=0, shape=(n_features, n_features), format='csr', dtype=dtype)\n    return self",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    array_orig = array\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of strings will be interpreted as decimal numbers if parameter 'dtype' is 'numeric'. It is recommended that you convert the array to type np.float64 before passing it to check_array.\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, six.string_types):\n        accept_sparse = [accept_sparse]\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(X.sum()):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.validation.py@@_shape_repr": "def _shape_repr(shape):\n    if len(shape) == 0:\n        return '()'\n    joined = ', '.join(('%d' % e for e in shape))\n    if len(shape) == 1:\n        joined += ','\n    return '(%s)' % joined",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        return x.shape[0]\n    else:\n        return len(x)",
    ".sklearn.utils.testing.py@@assert_raise_message": "def assert_raise_message(exceptions, message, function, *args, **kwargs):\n    try:\n        function(*args, **kwargs)\n    except exceptions as e:\n        error_message = str(e)\n        if message not in error_message:\n            raise AssertionError('Error message does not include the expected string: %r. Observed error message: %r' % (message, error_message))\n    else:\n        if isinstance(exceptions, tuple):\n            names = ' or '.join((e.__name__ for e in exceptions))\n        else:\n            names = exceptions.__name__\n        raise AssertionError('%s not raised by %s' % (names, function.__name__))",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.feature_extraction.text.py@@CountVectorizer.transform": "def transform(self, raw_documents):\n    if isinstance(raw_documents, six.string_types):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    if not hasattr(self, 'vocabulary_'):\n        self._validate_vocabulary()\n    self._check_vocabulary()\n    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n    if self.binary:\n        X.data.fill(1)\n    return X",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.fit_transform": "def fit_transform(self, X, y=None):\n    return self.fit(X, y).transform(X)",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.fit": "def fit(self, X, y=None):\n    if isinstance(X, six.string_types):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    self._get_hasher().fit(X, y=y)\n    return self",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.transform": "def transform(self, X):\n    if isinstance(X, six.string_types):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    analyzer = self.build_analyzer()\n    X = self._get_hasher().transform((analyzer(doc) for doc in X))\n    if self.binary:\n        X.data.fill(1)\n    if self.norm is not None:\n        X = normalize(X, norm=self.norm, copy=False)\n    return X",
    ".sklearn.feature_extraction.text.py@@TfidfTransformer.__init__": "def __init__(self, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False):\n    self.norm = norm\n    self.use_idf = use_idf\n    self.smooth_idf = smooth_idf\n    self.sublinear_tf = sublinear_tf",
    ".sklearn.base.py@@TransformerMixin.fit_transform": "def fit_transform(self, X, y=None, **fit_params):\n    if y is None:\n        return self.fit(X, **fit_params).transform(X)\n    else:\n        return self.fit(X, y, **fit_params).transform(X)"
}