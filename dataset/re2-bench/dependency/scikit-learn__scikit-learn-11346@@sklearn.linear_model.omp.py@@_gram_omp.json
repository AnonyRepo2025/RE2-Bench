{
    ".sklearn.externals.joblib._parallel_backends.py@@SafeFunction.__call__": "def __call__(self, *args, **kwargs):\n    try:\n        return self.func(*args, **kwargs)\n    except KeyboardInterrupt:\n        raise WorkerInterrupt()\n    except:\n        e_type, e_value, e_tb = sys.exc_info()\n        text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)\n        raise TransportableException(text, e_type)",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__call__": "def __call__(self):\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]",
    ".sklearn.decomposition.dict_learning.py@@_sparse_encode": "def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars', regularization=None, copy_cov=True, init=None, max_iter=1000, check_input=True, verbose=0, positive=False):\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n    if dictionary.shape[1] != X.shape[1]:\n        raise ValueError('Dictionary and X have different numbers of features:dictionary.shape: {} X.shape{}'.format(dictionary.shape, X.shape))\n    if cov is None and algorithm != 'lasso_cd':\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n    if algorithm == 'lasso_lars':\n        alpha = float(regularization) / n_features\n        try:\n            err_mgt = np.seterr(all='ignore')\n            lasso_lars = LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False, positive=positive)\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\n            new_code = lasso_lars.coef_\n        finally:\n            np.seterr(**err_mgt)\n    elif algorithm == 'lasso_cd':\n        alpha = float(regularization) / n_features\n        clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False, precompute=gram, max_iter=max_iter, warm_start=True, positive=positive)\n        if init is not None:\n            clf.coef_ = init\n        clf.fit(dictionary.T, X.T, check_input=check_input)\n        new_code = clf.coef_\n    elif algorithm == 'lars':\n        try:\n            err_mgt = np.seterr(all='ignore')\n            lars = Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False, positive=positive)\n            lars.fit(dictionary.T, X.T, Xy=cov)\n            new_code = lars.coef_\n        finally:\n            np.seterr(**err_mgt)\n    elif algorithm == 'threshold':\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\n        if positive:\n            new_code[new_code < 0] = 0\n    elif algorithm == 'omp':\n        if positive:\n            raise ValueError('Positive constraint not supported for \"omp\" coding method.')\n        new_code = orthogonal_mp_gram(Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization), tol=None, norms_squared=row_norms(X, squared=True), copy_Xy=copy_cov).T\n    else:\n        raise ValueError('Sparse coding method must be \"lasso_lars\" \"lasso_cd\",  \"lasso\", \"threshold\" or \"omp\", got %s.' % algorithm)\n    if new_code.ndim != 2:\n        return new_code.reshape(n_samples, n_components)\n    return new_code",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(X.sum()):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.linear_model.omp.py@@orthogonal_mp_gram": "def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False):\n    Gram = check_array(Gram, order='F', copy=copy_Gram)\n    Xy = np.asarray(Xy)\n    if Xy.ndim > 1 and Xy.shape[1] > 1:\n        copy_Gram = True\n    if Xy.ndim == 1:\n        Xy = Xy[:, np.newaxis]\n        if tol is not None:\n            norms_squared = [norms_squared]\n    if copy_Xy or not Xy.flags.writeable:\n        Xy = Xy.copy()\n    if n_nonzero_coefs is None and tol is None:\n        n_nonzero_coefs = int(0.1 * len(Gram))\n    if tol is not None and norms_squared is None:\n        raise ValueError('Gram OMP needs the precomputed norms in order to evaluate the error sum of squares.')\n    if tol is not None and tol < 0:\n        raise ValueError('Epsilon cannot be negative')\n    if tol is None and n_nonzero_coefs <= 0:\n        raise ValueError('The number of atoms must be positive')\n    if tol is None and n_nonzero_coefs > len(Gram):\n        raise ValueError('The number of atoms cannot be more than the number of features')\n    if return_path:\n        coef = np.zeros((len(Gram), Xy.shape[1], len(Gram)))\n    else:\n        coef = np.zeros((len(Gram), Xy.shape[1]))\n    n_iters = []\n    for k in range(Xy.shape[1]):\n        out = _gram_omp(Gram, Xy[:, k], n_nonzero_coefs, norms_squared[k] if tol is not None else None, tol, copy_Gram=copy_Gram, copy_Xy=False, return_path=return_path)\n        if return_path:\n            _, idx, coefs, n_iter = out\n            coef = coef[:, :, :len(idx)]\n            for n_active, x in enumerate(coefs.T):\n                coef[idx[:n_active + 1], k, n_active] = x[:n_active + 1]\n        else:\n            x, idx, n_iter = out\n            coef[idx, k] = x\n        n_iters.append(n_iter)\n    if Xy.shape[1] == 1:\n        n_iters = n_iters[0]\n    if return_n_iter:\n        return (np.squeeze(coef), n_iters)\n    else:\n        return np.squeeze(coef)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    array_orig = array\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of strings will be interpreted as decimal numbers if parameter 'dtype' is 'numeric'. It is recommended that you convert the array to type np.float64 before passing it to check_array.\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    return array",
    ".sklearn.utils.validation.py@@_shape_repr": "def _shape_repr(shape):\n    if len(shape) == 0:\n        return '()'\n    joined = ', '.join(('%d' % e for e in shape))\n    if len(shape) == 1:\n        joined += ','\n    return '(%s)' % joined",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        return x.shape[0]\n    else:\n        return len(x)",
    ".sklearn.externals.joblib.pool.py@@CustomizablePickler.__init__": "def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):\n    Pickler.__init__(self, writer, protocol=protocol)\n    if reducers is None:\n        reducers = {}\n    if hasattr(Pickler, 'dispatch'):\n        self.dispatch = Pickler.dispatch.copy()\n    else:\n        self.dispatch_table = copyreg.dispatch_table.copy()\n    for type, reduce_func in reducers.items():\n        self.register(type, reduce_func)",
    ".sklearn.externals.joblib.pool.py@@CustomizablePickler.register": "def register(self, type, reduce_func):\n    if hasattr(Pickler, 'dispatch'):\n\n        def dispatcher(self, obj):\n            reduced = reduce_func(obj)\n            self.save_reduce(*reduced, obj=obj)\n        self.dispatch[type] = dispatcher\n    else:\n        self.dispatch_table[type] = reduce_func",
    ".sklearn.externals.joblib.pool.py@@_get_backing_memmap": "def _get_backing_memmap(a):\n    b = getattr(a, 'base', None)\n    if b is None:\n        return None\n    elif isinstance(b, mmap):\n        return a\n    else:\n        return _get_backing_memmap(b)",
    ".sklearn.externals.joblib.pool.py@@CustomizablePicklingQueue.get": "def get():\n    racquire()\n    try:\n        return recv()\n    finally:\n        rrelease()",
    ".sklearn.externals.joblib.pool.py@@CustomizablePicklingQueue.put": "def put(obj):\n    wlock_acquire()\n    try:\n        return send(obj)\n    finally:\n        wlock_release()",
    ".sklearn.externals.joblib.pool.py@@CustomizablePicklingQueue.send": "def send(obj):\n    buffer = BytesIO()\n    CustomizablePickler(buffer, self._reducers).dump(obj)\n    self._writer.send_bytes(buffer.getvalue())",
    ".sklearn.externals.joblib.pool.py@@ArrayMemmapReducer.__call__": "def __call__(self, a):\n    m = _get_backing_memmap(a)\n    if m is not None:\n        return _reduce_memmap_backed(a, m)\n    if not a.dtype.hasobject and self._max_nbytes is not None and (a.nbytes > self._max_nbytes):\n        try:\n            os.makedirs(self._temp_folder)\n            os.chmod(self._temp_folder, FOLDER_PERMISSIONS)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise e\n        basename = '%d-%d-%s.pkl' % (os.getpid(), id(threading.current_thread()), hash(a))\n        filename = os.path.join(self._temp_folder, basename)\n        if not os.path.exists(filename):\n            if self.verbose > 0:\n                print('Memmaping (shape=%r, dtype=%s) to new file %s' % (a.shape, a.dtype, filename))\n            for dumped_filename in dump(a, filename):\n                os.chmod(dumped_filename, FILE_PERMISSIONS)\n            if self._prewarm:\n                load(filename, mmap_mode=self._mmap_mode).max()\n        elif self.verbose > 1:\n            print('Memmaping (shape=%s, dtype=%s) to old file %s' % (a.shape, a.dtype, filename))\n        return (load, (filename, self._mmap_mode))\n    else:\n        if self.verbose > 1:\n            print('Pickling array (shape=%r, dtype=%s).' % (a.shape, a.dtype))\n        return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))",
    ".sklearn.externals.joblib.parallel.py@@BatchCompletionCallBack.__call__": "def __call__(self, out):\n    self.parallel.n_completed_tasks += self.batch_size\n    this_batch_duration = time.time() - self.dispatch_timestamp\n    self.parallel._backend.batch_completed(self.batch_size, this_batch_duration)\n    self.parallel.print_progress()\n    if self.parallel._original_iterator is not None:\n        self.parallel.dispatch_next()",
    ".sklearn.externals.joblib._parallel_backends.py@@AutoBatchingMixin.batch_completed": "def batch_completed(self, batch_size, duration):\n    if batch_size == self._effective_batch_size:\n        old_duration = self._smoothed_batch_duration\n        if old_duration == 0:\n            new_duration = duration\n        else:\n            new_duration = 0.8 * old_duration + 0.2 * duration\n        self._smoothed_batch_duration = new_duration",
    ".sklearn.externals.joblib.parallel.py@@Parallel.print_progress": "def print_progress(self):\n    if not self.verbose:\n        return\n    elapsed_time = time.time() - self._start_time\n    if self._original_iterator is not None:\n        if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n            return\n        self._print('Done %3i tasks      | elapsed: %s', (self.n_completed_tasks, short_format_time(elapsed_time)))\n    else:\n        index = self.n_completed_tasks\n        total_tasks = self.n_dispatched_tasks\n        if not index == 0:\n            cursor = total_tasks - index + 1 - self._pre_dispatch_amount\n            frequency = total_tasks // self.verbose + 1\n            is_last_item = index + 1 == total_tasks\n            if is_last_item or cursor % frequency:\n                return\n        remaining_time = elapsed_time / index * (self.n_dispatched_tasks - index * 1.0)\n        self._print('Done %3i out of %3i | elapsed: %s remaining: %s', (index, total_tasks, short_format_time(elapsed_time), short_format_time(remaining_time)))",
    ".sklearn.externals.joblib.numpy_pickle.py@@load": "def load(filename, mmap_mode=None):\n    if Path is not None and isinstance(filename, Path):\n        filename = str(filename)\n    if hasattr(filename, 'read'):\n        fobj = filename\n        filename = getattr(fobj, 'name', '')\n        with _read_fileobject(fobj, filename, mmap_mode) as fobj:\n            obj = _unpickle(fobj)\n    else:\n        with open(filename, 'rb') as f:\n            with _read_fileobject(f, filename, mmap_mode) as fobj:\n                if isinstance(fobj, _basestring):\n                    return load_compatibility(fobj)\n                obj = _unpickle(fobj, filename, mmap_mode)\n    return obj",
    ".sklearn.externals.joblib.numpy_pickle_utils.py@@_read_fileobject": "def _read_fileobject(fileobj, filename, mmap_mode=None):\n    compressor = _detect_compressor(fileobj)\n    if compressor == 'compat':\n        warnings.warn(\"The file '%s' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\" % filename, DeprecationWarning, stacklevel=2)\n        yield filename\n    else:\n        if compressor == 'zlib':\n            fileobj = _buffered_read_file(BinaryZlibFile(fileobj, 'rb'))\n        elif compressor == 'gzip':\n            fileobj = _buffered_read_file(BinaryGzipFile(fileobj, 'rb'))\n        elif compressor == 'bz2' and bz2 is not None:\n            if PY3_OR_LATER:\n                fileobj = _buffered_read_file(bz2.BZ2File(fileobj, 'rb'))\n            else:\n                fileobj = _buffered_read_file(bz2.BZ2File(fileobj.name, 'rb'))\n        elif compressor == 'lzma' or compressor == 'xz':\n            if PY3_OR_LATER and lzma is not None:\n                fileobj = _buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))\n            else:\n                raise NotImplementedError('Lzma decompression is not supported for this version of python ({}.{})'.format(sys.version_info[0], sys.version_info[1]))\n        if mmap_mode is not None:\n            if isinstance(fileobj, io.BytesIO):\n                warnings.warn('In memory persistence is not compatible with mmap_mode \"%(mmap_mode)s\" flag passed. mmap_mode option will be ignored.' % locals(), stacklevel=2)\n            elif compressor != 'not-compressed':\n                warnings.warn('mmap_mode \"%(mmap_mode)s\" is not compatible with compressed file %(filename)s. \"%(mmap_mode)s\" flag will be ignored.' % locals(), stacklevel=2)\n            elif not _is_raw_file(fileobj):\n                warnings.warn('\"%(fileobj)r\" is not a raw file, mmap_mode \"%(mmap_mode)s\" flag will be ignored.' % locals(), stacklevel=2)\n        yield fileobj",
    ".sklearn.externals.joblib.numpy_pickle_utils.py@@_detect_compressor": "def _detect_compressor(fileobj):\n    if hasattr(fileobj, 'peek'):\n        first_bytes = fileobj.peek(_MAX_PREFIX_LEN)\n    else:\n        first_bytes = fileobj.read(_MAX_PREFIX_LEN)\n        fileobj.seek(0)\n    if first_bytes.startswith(_ZLIB_PREFIX):\n        return 'zlib'\n    elif first_bytes.startswith(_GZIP_PREFIX):\n        return 'gzip'\n    elif first_bytes.startswith(_BZ2_PREFIX):\n        return 'bz2'\n    elif first_bytes.startswith(_LZMA_PREFIX):\n        return 'lzma'\n    elif first_bytes.startswith(_XZ_PREFIX):\n        return 'xz'\n    elif first_bytes.startswith(_ZFILE_PREFIX):\n        return 'compat'\n    return 'not-compressed'",
    ".sklearn.externals.joblib.numpy_pickle_utils.py@@_is_raw_file": "def _is_raw_file(fileobj):\n    if PY3_OR_LATER:\n        fileobj = getattr(fileobj, 'raw', fileobj)\n        return isinstance(fileobj, io.FileIO)\n    else:\n        return isinstance(fileobj, file)",
    ".sklearn.externals.joblib.numpy_pickle.py@@_unpickle": "def _unpickle(fobj, filename='', mmap_mode=None):\n    unpickler = NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)\n    obj = None\n    try:\n        obj = unpickler.load()\n        if unpickler.compat_mode:\n            warnings.warn(\"The file '%s' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\" % filename, DeprecationWarning, stacklevel=3)\n    except UnicodeDecodeError as exc:\n        if PY3_OR_LATER:\n            new_exc = ValueError('You may be trying to read with python 3 a joblib pickle generated with python 2. This feature is not supported by joblib.')\n            new_exc.__cause__ = exc\n            raise new_exc\n        raise\n    return obj",
    ".sklearn.externals.joblib.numpy_pickle.py@@NumpyUnpickler.__init__": "def __init__(self, filename, file_handle, mmap_mode=None):\n    self._dirname = os.path.dirname(filename)\n    self.mmap_mode = mmap_mode\n    self.file_handle = file_handle\n    self.filename = filename\n    self.compat_mode = False\n    Unpickler.__init__(self, self.file_handle)\n    try:\n        import numpy as np\n    except ImportError:\n        np = None\n    self.np = np",
    ".sklearn.externals.joblib.numpy_pickle.py@@NumpyUnpickler.load_build": "def load_build(self):\n    Unpickler.load_build(self)\n    if isinstance(self.stack[-1], (NDArrayWrapper, NumpyArrayWrapper)):\n        if self.np is None:\n            raise ImportError(\"Trying to unpickle an ndarray, but numpy didn't import correctly\")\n        array_wrapper = self.stack.pop()\n        if isinstance(array_wrapper, NDArrayWrapper):\n            self.compat_mode = True\n        self.stack.append(array_wrapper.read(self))",
    ".sklearn.externals.joblib.numpy_pickle.py@@NumpyArrayWrapper.read": "def read(self, unpickler):\n    if unpickler.mmap_mode is not None and self.allow_mmap:\n        array = self.read_mmap(unpickler)\n    else:\n        array = self.read_array(unpickler)\n    if hasattr(array, '__array_prepare__') and self.subclass not in (unpickler.np.ndarray, unpickler.np.memmap):\n        new_array = unpickler.np.core.multiarray._reconstruct(self.subclass, (0,), 'b')\n        return new_array.__array_prepare__(array)\n    else:\n        return array",
    ".sklearn.externals.joblib.numpy_pickle.py@@NumpyArrayWrapper.read_mmap": "def read_mmap(self, unpickler):\n    offset = unpickler.file_handle.tell()\n    if unpickler.mmap_mode == 'w+':\n        unpickler.mmap_mode = 'r+'\n    marray = make_memmap(unpickler.filename, dtype=self.dtype, shape=self.shape, order=self.order, mode=unpickler.mmap_mode, offset=offset)\n    unpickler.file_handle.seek(offset + marray.nbytes)\n    return marray",
    ".sklearn.externals.joblib.backports.py@@make_memmap": "def make_memmap(filename, dtype='uint8', mode='r+', offset=0, shape=None, order='C'):\n    raise NotImplementedError(\"'joblib.backports.make_memmap' should not be used if numpy is not installed.\")",
    ".sklearn.externals.joblib.parallel.py@@Parallel.dispatch_next": "def dispatch_next(self):\n    if not self.dispatch_one_batch(self._original_iterator):\n        self._iterating = False\n        self._original_iterator = None",
    ".sklearn.externals.joblib.parallel.py@@Parallel.dispatch_one_batch": "def dispatch_one_batch(self, iterator):\n    if self.batch_size == 'auto':\n        batch_size = self._backend.compute_batch_size()\n    else:\n        batch_size = self.batch_size\n    with self._lock:\n        tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n        if len(tasks) == 0:\n            return False\n        else:\n            self._dispatch(tasks)\n            return True",
    ".sklearn.externals.joblib._parallel_backends.py@@AutoBatchingMixin.compute_batch_size": "def compute_batch_size(self):\n    old_batch_size = self._effective_batch_size\n    batch_duration = self._smoothed_batch_duration\n    if batch_duration > 0 and batch_duration < self.MIN_IDEAL_BATCH_DURATION:\n        ideal_batch_size = int(old_batch_size * self.MIN_IDEAL_BATCH_DURATION / batch_duration)\n        batch_size = max(2 * ideal_batch_size, 1)\n        self._effective_batch_size = batch_size\n        if self.parallel.verbose >= 10:\n            self.parallel._print('Batch computation too fast (%.4fs.) Setting batch_size=%d.', (batch_duration, batch_size))\n    elif batch_duration > self.MAX_IDEAL_BATCH_DURATION and old_batch_size >= 2:\n        batch_size = old_batch_size // 2\n        self._effective_batch_size = batch_size\n        if self.parallel.verbose >= 10:\n            self.parallel._print('Batch computation too slow (%.4fs.) Setting batch_size=%d.', (batch_duration, batch_size))\n    else:\n        batch_size = old_batch_size\n    if batch_size != old_batch_size:\n        self._smoothed_batch_duration = 0\n    return batch_size",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__init__": "def __init__(self, iterator_slice):\n    self.items = list(iterator_slice)\n    self._size = len(self.items)",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__len__": "def __len__(self):\n    return self._size"
}