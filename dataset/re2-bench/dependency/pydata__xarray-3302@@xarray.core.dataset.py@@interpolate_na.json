{
    ".xarray.core.missing.py@@_apply_over_vars_with_dim": "def _apply_over_vars_with_dim(func, self, dim=None, **kwargs):\n    ds = type(self)(coords=self.coords, attrs=self.attrs)\n    for name, var in self.data_vars.items():\n        if dim in var.dims:\n            ds[name] = func(var, dim=dim, **kwargs)\n        else:\n            ds[name] = var\n    return ds",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._data = dataset",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '{!r} object has no attribute {!r}'.format(type(self).__name__, name):\n            raise\n        raise AttributeError(\"cannot set attribute %r on a %r object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\" % (name, type(self).__name__)) from e",
    ".xarray.core.coordinates.py@@Coordinates.__iter__": "def __iter__(self) -> Iterator['Hashable']:\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen({k: v for k, v in self._data.variables.items() if k in self._names})",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return self._data._coord_names",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data, coords, compat='broadcast_equals', join='outer'):\n    objects = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(_extract_indexes_from_coords(coords))\n    return merge_core(objects, compat, join, explicit_coords=explicit_coords, indexes=indexes)",
    ".xarray.core.merge.py@@_extract_indexes_from_coords": "def _extract_indexes_from_coords(coords):\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield (name, variable.to_index())",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objects: Iterable['CoercibleMapping'], compat: str='broadcast_equals', join: str='outer', priority_arg: Optional[int]=None, explicit_coords: Optional[Sequence]=None, indexes: Optional[Mapping[Hashable, pd.Index]]=None, fill_value: object=dtypes.NA) -> _MergeResult:\n    from .dataset import calculate_dimensions\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    dims = calculate_dimensions(variables)\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError('unable to determine if these variables should be coordinates or not in the merged result: %s' % ambiguous_coords)\n    return _MergeResult(variables, coord_names, dims, out_indexes)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError('compat={!r} invalid: must be {}'.format(compat, set(_VALID_COMPAT)))",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects: Iterable['CoercibleMapping']) -> List['DatasetLike']:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables: 'DatasetLike' = obj\n        else:\n            variables = {}\n            if isinstance(obj, PANDAS_TYPES):\n                obj = dict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.alignment.py@@align": "def align(*objects, join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA):\n    if indexes is None:\n        indexes = {}\n    if not indexes and len(objects) == 1:\n        obj, = objects\n        return (obj.copy(deep=copy),)\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n    if join == 'override':\n        objects = _override_indexes(objects, all_indexes, exclude)\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if any((not index.equals(other) for other in matching_indexes)) or dim in unlabeled_dim_sizes:\n                joined_indexes[dim] = index\n        elif any((not matching_indexes[0].equals(other) for other in matching_indexes[1:])) or dim in unlabeled_dim_sizes:\n            if join == 'exact':\n                raise ValueError(f'indexes along dimension {dim!r} are not equal')\n            index = joiner(matching_indexes)\n            joined_indexes[dim] = index\n        else:\n            index = matching_indexes[0]\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension size(s) %r than the size of the aligned dimension labels: %r' % (dim, unlabeled_sizes, labeled_size))\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension sizes: %r' % (dim, sizes))\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n        if not valid_indexers:\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, **valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n    return tuple(result)",
    ".xarray.core.alignment.py@@_get_joiner": "def _get_joiner(join):\n    if join == 'outer':\n        return functools.partial(functools.reduce, operator.or_)\n    elif join == 'inner':\n        return functools.partial(functools.reduce, operator.and_)\n    elif join == 'left':\n        return operator.itemgetter(0)\n    elif join == 'right':\n        return operator.itemgetter(-1)\n    elif join == 'exact':\n        return None\n    elif join == 'override':\n        return operator.itemgetter(0)\n    else:\n        raise ValueError('invalid value for join: %s' % join)",
    ".xarray.core.merge.py@@collect_variables_and_indexes": "def collect_variables_and_indexes(list_of_mappings: 'List[DatasetLike]') -> Dict[Hashable, List[MergeElement]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n\n    def append(name, variable, index):\n        values = grouped.setdefault(name, [])\n        values.append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping.indexes)\n            continue\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()\n                indexes = dict(variable.indexes)\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n            variable = as_variable(variable, name=name)\n            if variable.dims == (name,):\n                variable = variable.to_index_variable()\n                index = variable.to_index()\n            else:\n                index = None\n            append(name, variable, index)\n    return grouped",
    ".xarray.core.merge.py@@_get_priority_vars_and_indexes": "def _get_priority_vars_and_indexes(objects: List['DatasetLike'], priority_arg: Optional[int], compat: str='equals') -> Dict[Hashable, MergeElement]:\n    if priority_arg is None:\n        return {}\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped: Dict[Hashable, MergeElement] = {}\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: Dict[Hashable, List[MergeElement]], prioritized: Mapping[Hashable, MergeElement]=None, compat: str='minimal') -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    if prioritized is None:\n        prioritized = {}\n    _assert_compat_valid(compat)\n    merged_vars: Dict[Hashable, Variable] = {}\n    merged_indexes: Dict[Hashable, pd.Index] = {}\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for _, other_index in indexed_elements[1:]:\n                    if not index.equals(other_index):\n                        raise MergeError('conflicting values for index %r on objects to be combined:\\nfirst value: %r\\nsecond value: %r' % (name, index, other_index))\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError('conflicting attribute values on combined variable %r:\\nfirst value: %r\\nsecond value: %r' % (name, variable.attrs, other_variable.attrs))\n                merged_vars[name] = variable\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n    return (merged_vars, merged_indexes)",
    ".xarray.core.variable.py@@assert_unique_multiindex_level_names": "def assert_unique_multiindex_level_names(variables):\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append(f'{n!r} ({var_name})')\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n    for k, v in level_names.items():\n        if k in variables:\n            v.append('(%s)' % k)\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = '\\n'.join([', '.join(v) for v in duplicate_names])\n        raise ValueError('conflicting MultiIndex level name(s):\\n%s' % conflict_str)\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError('conflicting level / dimension names. {} already exists as a level name.'.format(d))",
    ".xarray.core.dataset.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:\n    dims: Dict[Hashable, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError('dimension %r already exists as a scalar variable' % dim)\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError('conflicting sizes for dimension %r: length %s on %r and length %s on %r' % (dim, size, k, dims[dim], last_used[dim]))\n    return dims",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_mappings: Iterable['DatasetLike']) -> Tuple[Set[Hashable], Set[Hashable]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names: Set[Hashable] = set()\n    noncoord_names: Set[Hashable] = set()\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError('coordinate %s shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.' % coord_name)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core.dataset.py@@DataVariables.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    if key not in self._dataset._coord_names:\n        return cast('DataArray', self._dataset[key])\n    raise KeyError(key)",
    ".xarray.core.utils.py@@hashable": "def hashable(v: Any) -> bool:\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None]=None, dims: Union[Hashable, Sequence[Hashable], None]=None, name: Hashable=None, attrs: Mapping=None, encoding=None, indexes: Dict[Hashable, pd.Index]=None, fastpath: bool=False):\n    if encoding is not None:\n        warnings.warn('The `encoding` argument to `DataArray` is deprecated, and . will be removed in 0.15. Instead, specify the encoding when writing to disk or set the `encoding` attribute directly.', FutureWarning, stacklevel=2)\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n        assert encoding is None\n    else:\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n            elif isinstance(data, pdcompat.Panel):\n                coords = [data.items, data.major_axis, data.minor_axis]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None and (not isinstance(data, PANDAS_TYPES)):\n            attrs = getattr(data, 'attrs', None)\n        if encoding is None:\n            encoding = getattr(data, 'encoding', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, encoding, fastpath=True)\n    self._variable = variable\n    assert isinstance(coords, dict)\n    self._coords = coords\n    self._name = name\n    self._accessors = None\n    self._indexes = indexes\n    self._file_obj = None",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.missing.py@@interp_na": "def interp_na(self, dim: Hashable=None, use_coordinate: Union[bool, str]=True, method: str='linear', limit: int=None, max_gap: Union[int, float, str, pd.Timedelta, np.timedelta64]=None, **kwargs):\n    if dim is None:\n        raise NotImplementedError('dim is a required argument')\n    if limit is not None:\n        valids = _get_valid_fill_mask(self, dim, limit)\n    if max_gap is not None:\n        max_type = type(max_gap).__name__\n        if not is_scalar(max_gap):\n            raise ValueError('max_gap must be a scalar.')\n        if dim in self.indexes and isinstance(self.indexes[dim], pd.DatetimeIndex) and use_coordinate:\n            if not isinstance(max_gap, (np.timedelta64, pd.Timedelta, str)):\n                raise TypeError(f'Underlying index is DatetimeIndex. Expected max_gap of type str, pandas.Timedelta or numpy.timedelta64 but received {max_type}')\n            if isinstance(max_gap, str):\n                try:\n                    max_gap = pd.to_timedelta(max_gap)\n                except ValueError:\n                    raise ValueError(f'Could not convert {max_gap!r} to timedelta64 using pandas.to_timedelta')\n            if isinstance(max_gap, pd.Timedelta):\n                max_gap = np.timedelta64(max_gap.value, 'ns')\n            max_gap = np.timedelta64(max_gap, 'ns').astype(np.float64)\n        if not use_coordinate:\n            if not isinstance(max_gap, (Number, np.number)):\n                raise TypeError(f'Expected integer or floating point max_gap since use_coordinate=False. Received {max_type}.')\n    index = get_clean_interp_index(self, dim, use_coordinate=use_coordinate)\n    interp_class, kwargs = _get_interpolator(method, **kwargs)\n    interpolator = partial(func_interpolate_na, interp_class, **kwargs)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'overflow', RuntimeWarning)\n        warnings.filterwarnings('ignore', 'invalid value', RuntimeWarning)\n        arr = apply_ufunc(interpolator, index, self, input_core_dims=[[dim], [dim]], output_core_dims=[[dim]], output_dtypes=[self.dtype], dask='parallelized', vectorize=True, keep_attrs=True).transpose(*self.dims)\n    if limit is not None:\n        arr = arr.where(valids)\n    if max_gap is not None:\n        if dim not in self.coords:\n            raise NotImplementedError('max_gap not implemented for unlabeled coordinates yet.')\n        nan_block_lengths = _get_nan_block_lengths(self, dim, index)\n        arr = arr.where(nan_block_lengths <= max_gap)\n    return arr",
    ".xarray.core.missing.py@@get_clean_interp_index": "def get_clean_interp_index(arr, dim: Hashable, use_coordinate: Union[str, bool]=True):\n    if use_coordinate:\n        if use_coordinate is True:\n            index = arr.get_index(dim)\n        else:\n            index = arr.coords[use_coordinate]\n            if index.ndim != 1:\n                raise ValueError(f'Coordinates used for interpolation must be 1D, {use_coordinate} is {index.ndim}D.')\n            index = index.to_index()\n        if isinstance(index, pd.MultiIndex):\n            index.name = dim\n        if not index.is_monotonic:\n            raise ValueError(f'Index {index.name!r} must be monotonically increasing')\n        if not index.is_unique:\n            raise ValueError(f'Index {index.name!r} has duplicate values')\n        try:\n            index = index.values.astype(np.float64)\n        except (TypeError, ValueError):\n            raise TypeError(f'Index {index.name!r} must be castable to float64 to support interpolation, got {type(index).__name__}.')\n    else:\n        axis = arr.get_axis_num(dim)\n        index = np.arange(arr.shape[axis], dtype=np.float64)\n    return index",
    ".xarray.core.common.py@@DataWithCoords.get_index": "def get_index(self, key: Hashable) -> pd.Index:\n    if key not in self.dims:\n        raise KeyError(key)\n    try:\n        return self.indexes[key]\n    except KeyError:\n        return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)",
    ".xarray.core.dataarray.py@@DataArray.indexes": "def indexes(self) -> Indexes:\n    if self._indexes is None:\n        self._indexes = default_indexes(self._coords, self.dims)\n    return Indexes(self._indexes)",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes):\n    self._indexes = indexes",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key):\n    return self._indexes[key]",
    ".xarray.core.common.py@@AbstractArray.sizes": "def sizes(self: Any) -> Mapping[Hashable, int]:\n    return Frozen(dict(zip(self.dims, self.shape)))",
    ".xarray.core.dataarray.py@@DataArray.shape": "def shape(self) -> Tuple[int, ...]:\n    return self.variable.shape",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.missing.py@@_get_interpolator": "def _get_interpolator(method, vectorizeable_only=False, **kwargs):\n    interp1d_methods = ['linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial']\n    valid_methods = interp1d_methods + ['barycentric', 'krog', 'pchip', 'spline', 'akima']\n    has_scipy = True\n    try:\n        from scipy import interpolate\n    except ImportError:\n        has_scipy = False\n    if method == 'linear' and (not kwargs.get('fill_value', None) == 'extrapolate') and (not vectorizeable_only):\n        kwargs.update(method=method)\n        interp_class = NumpyInterpolator\n    elif method in valid_methods:\n        if not has_scipy:\n            raise ImportError('Interpolation with method `%s` requires scipy' % method)\n        if method in interp1d_methods:\n            kwargs.update(method=method)\n            interp_class = ScipyInterpolator\n        elif vectorizeable_only:\n            raise ValueError('{} is not a vectorizeable interpolator. Available methods are {}'.format(method, interp1d_methods))\n        elif method == 'barycentric':\n            interp_class = interpolate.BarycentricInterpolator\n        elif method == 'krog':\n            interp_class = interpolate.KroghInterpolator\n        elif method == 'pchip':\n            interp_class = interpolate.PchipInterpolator\n        elif method == 'spline':\n            kwargs.update(method=method)\n            interp_class = SplineInterpolator\n        elif method == 'akima':\n            interp_class = interpolate.Akima1DInterpolator\n        else:\n            raise ValueError('%s is not a valid scipy interpolator' % method)\n    else:\n        raise ValueError('%s is not a valid interpolator' % method)\n    return (interp_class, kwargs)",
    ".xarray.core.dataarray.py@@DataArray.dtype": "def dtype(self) -> np.dtype:\n    return self.variable.dtype",
    ".xarray.core.variable.py@@Variable.dtype": "def dtype(self):\n    return self._data.dtype",
    ".xarray.core.computation.py@@apply_ufunc": "def apply_ufunc(func: Callable, *args: Any, input_core_dims: Sequence[Sequence]=None, output_core_dims: Optional[Sequence[Sequence]]=((),), exclude_dims: AbstractSet=frozenset(), vectorize: bool=False, join: str='exact', dataset_join: str='exact', dataset_fill_value: object=_NO_FILL_VALUE, keep_attrs: bool=False, kwargs: Mapping=None, dask: str='forbidden', output_dtypes: Sequence=None, output_sizes: Mapping[Any, int]=None) -> Any:\n    from .groupby import GroupBy\n    from .dataarray import DataArray\n    from .variable import Variable\n    if input_core_dims is None:\n        input_core_dims = ((),) * len(args)\n    elif len(input_core_dims) != len(args):\n        raise ValueError('input_core_dims must be None or a tuple with the length same to the number of arguments. Given input_core_dims: {}, number of args: {}.'.format(input_core_dims, len(args)))\n    if kwargs is None:\n        kwargs = {}\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n    if exclude_dims and (not exclude_dims <= signature.all_core_dims):\n        raise ValueError('each dimension in `exclude_dims` must also be a core dimension in the function signature')\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n    if vectorize:\n        if signature.all_core_dims:\n            func = np.vectorize(func, otypes=output_dtypes, signature=signature.to_gufunc_string())\n        else:\n            func = np.vectorize(func, otypes=output_dtypes)\n    variables_vfunc = functools.partial(apply_variable_ufunc, func, signature=signature, exclude_dims=exclude_dims, keep_attrs=keep_attrs, dask=dask, output_dtypes=output_dtypes, output_sizes=output_sizes)\n    if any((isinstance(a, GroupBy) for a in args)):\n        this_apply = functools.partial(apply_ufunc, func, input_core_dims=input_core_dims, output_core_dims=output_core_dims, exclude_dims=exclude_dims, join=join, dataset_join=dataset_join, dataset_fill_value=dataset_fill_value, keep_attrs=keep_attrs, dask=dask)\n        return apply_groupby_func(this_apply, *args)\n    elif any((is_dict_like(a) for a in args)):\n        return apply_dataset_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, dataset_join=dataset_join, fill_value=dataset_fill_value, keep_attrs=keep_attrs)\n    elif any((isinstance(a, DataArray) for a in args)):\n        return apply_dataarray_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, keep_attrs=keep_attrs)\n    elif any((isinstance(a, Variable) for a in args)):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)",
    ".xarray.core.computation.py@@_UFuncSignature.__init__": "def __init__(self, input_core_dims, output_core_dims=((),)):\n    self.input_core_dims = tuple((tuple(a) for a in input_core_dims))\n    self.output_core_dims = tuple((tuple(a) for a in output_core_dims))\n    self._all_input_core_dims = None\n    self._all_output_core_dims = None\n    self._all_core_dims = None",
    ".xarray.core.computation.py@@_UFuncSignature.all_core_dims": "def all_core_dims(self):\n    if self._all_core_dims is None:\n        self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n    return self._all_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_input_core_dims": "def all_input_core_dims(self):\n    if self._all_input_core_dims is None:\n        self._all_input_core_dims = frozenset((dim for dims in self.input_core_dims for dim in dims))\n    return self._all_input_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_output_core_dims": "def all_output_core_dims(self):\n    if self._all_output_core_dims is None:\n        self._all_output_core_dims = frozenset((dim for dims in self.output_core_dims for dim in dims))\n    return self._all_output_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.to_gufunc_string": "def to_gufunc_string(self):\n    all_dims = self.all_core_dims\n    dims_map = dict(zip(sorted(all_dims), range(len(all_dims))))\n    input_core_dims = [['dim%d' % dims_map[dim] for dim in core_dims] for core_dims in self.input_core_dims]\n    output_core_dims = [['dim%d' % dims_map[dim] for dim in core_dims] for core_dims in self.output_core_dims]\n    alt_signature = type(self)(input_core_dims, output_core_dims)\n    return str(alt_signature)",
    ".xarray.core.computation.py@@_UFuncSignature.__str__": "def __str__(self):\n    lhs = ','.join(('({})'.format(','.join(dims)) for dims in self.input_core_dims))\n    rhs = ','.join(('({})'.format(','.join(dims)) for dims in self.output_core_dims))\n    return f'{lhs}->{rhs}'",
    ".xarray.core.common.py@@AttrAccessMixin.__getattr__": "def __getattr__(self, name: str) -> Any:\n    if name not in {'__dict__', '__setstate__'}:\n        for source in self._attr_sources:\n            with suppress(KeyError):\n                return source[name]\n    raise AttributeError('{!r} object has no attribute {!r}'.format(type(self).__name__, name))",
    ".xarray.core.dataarray.py@@DataArray._attr_sources": "def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n    return self._item_sources + [self.attrs]",
    ".xarray.core.dataarray.py@@DataArray._item_sources": "def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n    return [self.coords, {d: self.coords[d] for d in self.dims}, LevelCoordinatesSource(self)]",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: 'DataArray'):\n    self._data = dataarray",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    return self._data._getitem_coord(key)",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self, key):\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, self._level_coords, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray._level_coords": "def _level_coords(self) -> Dict[Hashable, Hashable]:\n    level_coords: Dict[Hashable, Hashable] = {}\n    for cname, var in self._coords.items():\n        if var.ndim == 1 and isinstance(var, IndexVariable):\n            level_names = var.level_names\n            if level_names is not None:\n                dim, = var.dims\n                level_coords.update({lname: dim for lname in level_names})\n    return level_coords",
    ".xarray.core.dataset.py@@_get_virtual_variable": "def _get_virtual_variable(variables, key: Hashable, level_vars: Mapping=None, dim_sizes: Mapping=None) -> Tuple[Hashable, Hashable, Variable]:\n    if level_vars is None:\n        level_vars = {}\n    if dim_sizes is None:\n        dim_sizes = {}\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return (key, key, variable)\n    if not isinstance(key, str):\n        raise KeyError(key)\n    split_key = key.split('.', 1)\n    var_name: Optional[str]\n    if len(split_key) == 2:\n        ref_name, var_name = split_key\n    elif len(split_key) == 1:\n        ref_name, var_name = (key, None)\n    else:\n        raise KeyError(key)\n    if ref_name in level_vars:\n        dim_var = variables[level_vars[ref_name]]\n        ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n    else:\n        ref_var = variables[ref_name]\n    if var_name is None:\n        virtual_var = ref_var\n        var_name = key\n    else:\n        if _contains_datetime_like_objects(ref_var):\n            ref_var = xr.DataArray(ref_var)\n            data = getattr(ref_var.dt, var_name).data\n        else:\n            data = getattr(ref_var, var_name).data\n        virtual_var = Variable(ref_var.dims, data)\n    return (ref_name, var_name, virtual_var)",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError('%s objects must be 1-dimensional' % type(self).__name__)\n    if not isinstance(self._data, PandasIndexAdapter):\n        self._data = PandasIndexAdapter(self._data)",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, Variable):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    data = getattr(data, 'values', data)\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray):\n        if hasattr(data, '__array_function__'):\n            if IS_NEP18_ACTIVE:\n                return data\n            else:\n                raise TypeError('Got an NumPy-like array type providing the __array_function__ protocol but NEP18 is not enabled. Check that numpy >= v1.16 and that the environment variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to \"1\"')\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'M':\n            data = np.asarray(data, 'datetime64[ns]')\n        elif data.dtype.kind == 'm':\n            data = np.asarray(data, 'timedelta64[ns]')\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__init__": "def __init__(self, array: Any, dtype: DTypeLike=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        if isinstance(array, pd.PeriodIndex):\n            dtype = np.dtype('O')\n        elif hasattr(array, 'categories'):\n            dtype = array.categories.dtype\n        elif not utils.is_valid_numpy_dtype(array.dtype):\n            dtype = np.dtype('O')\n        else:\n            dtype = array.dtype\n    else:\n        dtype = np.dtype(dtype)\n    self._dtype = dtype",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError('dimensions %s must have the same length as the number of data dimensions, ndim=%s' % (dims, self.ndim))\n    return dims",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.shape": "def shape(self) -> Tuple[int]:\n    return (len(self.array),)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self, variable: Variable, name: Union[Hashable, None, Default]=_default) -> 'DataArray':\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = {k: v for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))}\n    else:\n        allowed_dims = set(variable.dims)\n        coords = {k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims}\n    return self._replace(variable, coords, name)",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self, variable: Variable=None, coords=None, name: Union[Hashable, None, Default]=_default, indexes=None) -> 'DataArray':\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if name is _default:\n        name = self.name\n    return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)",
    ".xarray.core.coordinates.py@@LevelCoordinatesSource.__init__": "def __init__(self, data_object: 'Union[DataArray, Dataset]'):\n    self._data = data_object",
    ".xarray.core.dataarray.py@@DataArray.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    return self.variable.attrs",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.coordinates.py@@LevelCoordinatesSource.__getitem__": "def __getitem__(self, key):\n    raise KeyError()",
    ".xarray.core.computation.py@@apply_dataarray_vfunc": "def apply_dataarray_vfunc(func, *args, signature, join='inner', exclude_dims=frozenset(), keep_attrs=False):\n    from .dataarray import DataArray\n    if len(args) > 1:\n        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)\n    if keep_attrs and hasattr(args[0], 'name'):\n        name = args[0].name\n    else:\n        name = result_name(args)\n    result_coords = build_output_coords(args, signature, exclude_dims)\n    data_vars = [getattr(a, 'variable', a) for a in args]\n    result_var = func(*data_vars)\n    if signature.num_outputs > 1:\n        out = tuple((DataArray(variable, coords, name=name, fastpath=True) for variable, coords in zip(result_var, result_coords)))\n    else:\n        coords, = result_coords\n        out = DataArray(result_var, coords, name=name, fastpath=True)\n    return out",
    ".xarray.core.dataarray.py@@DataArray.copy": "def copy(self, deep: bool=True, data: Any=None) -> 'DataArray':\n    variable = self.variable.copy(deep=deep, data=data)\n    coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n    return self._replace(variable, coords)",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            if hasattr(data, '__array_function__') or isinstance(data, dask_array_type):\n                data = data.copy()\n            elif not isinstance(data, PandasIndexAdapter):\n                data = np.array(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self):\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Optional[Hashable]:\n    return self._name",
    ".xarray.core.computation.py@@result_name": "def result_name(objects: list) -> Any:\n    names = {getattr(obj, 'name', _DEFAULT_NAME) for obj in objects}\n    names.discard(_DEFAULT_NAME)\n    if len(names) == 1:\n        name, = names\n    else:\n        name = None\n    return name",
    ".xarray.core.utils.py@@ReprObject.__hash__": "def __hash__(self) -> int:\n    return hash((ReprObject, self._value))",
    ".xarray.core.computation.py@@build_output_coords": "def build_output_coords(args: list, signature: _UFuncSignature, exclude_dims: AbstractSet=frozenset()) -> 'List[Dict[Any, Variable]]':\n    coords_list = _get_coords_list(args)\n    if len(coords_list) == 1 and (not exclude_dims):\n        unpacked_coords, = coords_list\n        merged_vars = dict(unpacked_coords.variables)\n    else:\n        merged_vars, unused_indexes = merge_coordinates_without_align(coords_list, exclude_dims=exclude_dims)\n    output_coords = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered = {k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)}\n        else:\n            filtered = merged_vars\n        output_coords.append(filtered)\n    return output_coords",
    ".xarray.core.computation.py@@_get_coords_list": "def _get_coords_list(args) -> List['Coordinates']:\n    coords_list = []\n    for arg in args:\n        try:\n            coords = arg.coords\n        except AttributeError:\n            pass\n        else:\n            coords_list.append(coords)\n    return coords_list",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.computation.py@@apply_variable_ufunc": "def apply_variable_ufunc(func, *args, signature, exclude_dims=frozenset(), dask='forbidden', output_dtypes=None, output_sizes=None, keep_attrs=False):\n    from .variable import Variable, as_compatible_data\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')), exclude_dims=exclude_dims)\n    broadcast_dims = tuple((dim for dim in dim_sizes if dim not in signature.all_core_dims))\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims) if isinstance(arg, Variable) else arg for arg, core_dims in zip(args, signature.input_core_dims)]\n    if any((isinstance(array, dask_array_type) for array in input_data)):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an argument, but handling for dask arrays has not been enabled. Either set the ``dask`` argument or load your data into memory first with ``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            input_dims = [broadcast_dims + dims for dims in signature.input_core_dims]\n            numpy_func = func\n\n            def func(*arrays):\n                return _apply_blockwise(numpy_func, arrays, input_dims, output_dims, signature, output_dtypes, output_sizes)\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in apply_ufunc: {}'.format(dask))\n    result_data = func(*input_data)\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs:\n        raise ValueError('applied function does not have the number of outputs specified in the ufunc signature. Result is not a tuple of {} elements: {!r}'.format(signature.num_outputs, result_data))\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError('applied function returned data with unexpected number of dimensions: {} vs {}, for dimensions {}'.format(data.ndim, len(dims), dims))\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError('size of dimension {!r} on inputs was unexpectedly changed by applied function from {} to {}. Only dimensions specified in ``exclude_dims`` with xarray.apply_ufunc are allowed to change size.'.format(dim, dim_sizes[dim], new_size))\n        if keep_attrs and isinstance(args[0], Variable):\n            var.attrs.update(args[0].attrs)\n        output.append(var)\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)",
    ".xarray.core.computation.py@@unified_dim_sizes": "def unified_dim_sizes(variables: Iterable[Variable], exclude_dims: AbstractSet=frozenset()) -> Dict[Hashable, int]:\n    dim_sizes: Dict[Hashable, int] = {}\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError('broadcasting cannot handle duplicate dimensions on a variable: %r' % list(var.dims))\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError('operands cannot be broadcast together with mismatched lengths for dimension %r: %s vs %s' % (dim, dim_sizes[dim], size))\n    return dim_sizes",
    ".xarray.core.computation.py@@broadcast_compat_data": "def broadcast_compat_data(variable: Variable, broadcast_dims: Tuple[Hashable, ...], core_dims: Tuple[Hashable, ...]) -> Any:\n    data = variable.data\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n    if new_dims == old_dims:\n        return data\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError('operand to apply_ufunc has required core dimensions {}, but some of these dimensions are absent on an input variable: {}'.format(list(core_dims), missing_core_dims))\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError('operand to apply_ufunc encountered unexpected dimensions %r on an input variable: these are core dimensions on other input or output variables' % unexpected_dims)\n    old_broadcast_dims = tuple((d for d in broadcast_dims if d in set_old_dims))\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple((old_dims.index(d) for d in reordered_dims))\n        data = duck_array_ops.transpose(data, order)\n    if new_dims != reordered_dims:\n        key_parts = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n    return data",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if hasattr(self._data, '__array_function__') or isinstance(self._data, dask_array_type):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.missing.py@@func_interpolate_na": "def func_interpolate_na(interpolator, x, y, **kwargs):\n    out = y.copy()\n    nans = pd.isnull(y)\n    nonans = ~nans\n    n_nans = nans.sum()\n    if n_nans == 0 or n_nans == len(y):\n        return y\n    f = interpolator(x[nonans], y[nonans], **kwargs)\n    out[nans] = f(x[nans])\n    return out",
    ".xarray.core.missing.py@@NumpyInterpolator.__init__": "def __init__(self, xi, yi, method='linear', fill_value=None, period=None):\n    if method != 'linear':\n        raise ValueError('only method `linear` is valid for the NumpyInterpolator')\n    self.method = method\n    self.f = np.interp\n    self.cons_kwargs = {}\n    self.call_kwargs = {'period': period}\n    self._xi = xi\n    self._yi = yi\n    if fill_value is None:\n        self._left = np.nan\n        self._right = np.nan\n    elif isinstance(fill_value, Sequence) and len(fill_value) == 2:\n        self._left = fill_value[0]\n        self._right = fill_value[1]\n    elif is_scalar(fill_value):\n        self._left = fill_value\n        self._right = fill_value\n    else:\n        raise ValueError('%s is not a valid fill_value' % fill_value)",
    ".xarray.core.missing.py@@NumpyInterpolator.__call__": "def __call__(self, x):\n    return self.f(x, self._xi, self._yi, left=self._left, right=self._right, **self.call_kwargs)",
    ".xarray.core.computation.py@@_UFuncSignature.num_outputs": "def num_outputs(self):\n    return len(self.output_core_dims)",
    ".xarray.core.dataarray.py@@DataArray.transpose": "def transpose(self, *dims: Hashable, transpose_coords: bool=None) -> 'DataArray':\n    if dims:\n        dims = tuple(utils.infix_dims(dims, self.dims))\n    variable = self.variable.transpose(*dims)\n    if transpose_coords:\n        coords: Dict[Hashable, Variable] = {}\n        for name, coord in self.coords.items():\n            coord_dims = tuple((dim for dim in dims if dim in coord.dims))\n            coords[name] = coord.variable.transpose(*coord_dims)\n        return self._replace(variable, coords)\n    else:\n        if transpose_coords is None and any((self[c].ndim > 1 for c in self.coords)):\n            warnings.warn('This DataArray contains multi-dimensional coordinates. In the future, these coordinates will be transposed as well unless you specify transpose_coords=False.', FutureWarning, stacklevel=2)\n        return self._replace(variable)",
    ".xarray.core.utils.py@@infix_dims": "def infix_dims(dims_supplied: Collection, dims_all: Collection) -> Iterator:\n    if ... in dims_supplied:\n        if len(set(dims_all)) != len(dims_all):\n            raise ValueError('Cannot use ellipsis with repeated dims')\n        if len([d for d in dims_supplied if d == ...]) > 1:\n            raise ValueError('More than one ellipsis supplied')\n        other_dims = [d for d in dims_all if d not in dims_supplied]\n        for d in dims_supplied:\n            if d == ...:\n                yield from other_dims\n            else:\n                yield d\n    else:\n        if set(dims_supplied) ^ set(dims_all):\n            raise ValueError(f'{dims_supplied} must be a permuted list of {dims_all}, unless `...` is included')\n        yield from dims_supplied",
    ".xarray.core.variable.py@@Variable.transpose": "def transpose(self, *dims) -> 'Variable':\n    if len(dims) == 0:\n        dims = self.dims[::-1]\n    dims = tuple(infix_dims(dims, self.dims))\n    axes = self.get_axis_num(dims)\n    if len(dims) < 2 or dims == self.dims:\n        return self.copy(deep=False)\n    data = as_indexable(self._data).transpose(axes)\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Union[Hashable, Iterable[Hashable]]) -> Union[int, Tuple[int, ...]]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError(f'{dim!r} not found in array dimensions {self.dims!r}')",
    ".xarray.core.utils.py@@_check_inplace": "def _check_inplace(inplace: Optional[bool]) -> None:\n    if inplace is not None:\n        raise TypeError(\"The `inplace` argument has been removed from xarray. You can achieve an identical effect with python's standard assignment.\")",
    ".xarray.core.merge.py@@dataset_update_method": "def dataset_update_method(dataset: 'Dataset', other: 'CoercibleMapping') -> _MergeResult:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                coord_names = [c for c in value.coords if c not in value.dims and c in dataset.coords]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n    return merge_core([dataset, other], priority_arg=1, indexes=dataset.indexes)",
    ".xarray.core.indexes.py@@Indexes.__contains__": "def __contains__(self, key):\n    return key in self._indexes",
    ".xarray.core.indexes.py@@Indexes.__len__": "def __len__(self):\n    return len(self._indexes)",
    ".xarray.core.utils.py@@SortedKeysDict.__init__": "def __init__(self, mapping: MutableMapping[K, V]=None):\n    self.mapping = {} if mapping is None else mapping",
    ".xarray.core.utils.py@@SortedKeysDict.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(sorted(self.mapping))",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, Variable], dims: Iterable) -> Dict[Hashable, pd.Index]:\n    return {key: coords[key].to_index() for key in dims if key in coords}",
    ".xarray.core.dataset.py@@Dataset.encoding": "def encoding(self) -> Dict:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.dataarray.py@@DataArray.encoding": "def encoding(self) -> Dict[Hashable, Any]:\n    return self.variable.encoding",
    ".xarray.core.merge.py@@append_all": "def append_all(variables, indexes):\n    for name, variable in variables.items():\n        append(name, variable, indexes.get(name))",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self):\n    return iter(self._indexes)",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> 'Union[Variable, IndexVariable]':\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError('variable {!r} has invalid type {!r}'.format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError('cannot set variable %r with %r-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.' % (name, data.ndim))\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError('unable to convert object into a variable without an explicit list of dimensions: %r' % obj)\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError('%r has more than 1-dimension and the same name as one of its dimensions %r. xarray disallows such variables because they conflict with the coordinates used to label dimensions.' % (name, obj.dims))\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.merge.py@@append": "def append(name, variable, index):\n    values = grouped.setdefault(name, [])\n    values.append((variable, index))",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name: Hashable, variables: List[Variable], compat: str='broadcast_equals', equals: bool=None) -> Variable:\n    out = variables[0]\n    if len(variables) == 1 or compat == 'override':\n        return out\n    combine_method = None\n    if compat == 'minimal':\n        compat = 'broadcast_equals'\n    if compat == 'broadcast_equals':\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n    if compat == 'no_conflicts':\n        combine_method = 'fillna'\n    if equals is None:\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n            if equals is not True:\n                break\n        if equals is None:\n            out = out.compute()\n            for var in variables[1:]:\n                equals = getattr(out, compat)(var)\n                if not equals:\n                    break\n    if not equals:\n        raise MergeError(f\"conflicting values for variable {name!r} on objects to be combined. You can skip this check by specifying compat='override'.\")\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n    return out",
    ".xarray.core.utils.py@@SortedKeysDict.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    if key in self._data.data_vars:\n        raise KeyError(key)\n    return cast('DataArray', self._data[key])",
    ".xarray.core.dataset.py@@DataVariables.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._dataset._variables and key not in self._dataset._coord_names",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data.copy(deep=deep)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.copy": "def copy(self, deep: bool=True) -> 'PandasIndexAdapter':\n    array = self.array.copy(deep=True) if deep else self.array\n    return PandasIndexAdapter(array, self._dtype)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i) for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self):\n    return self.dims[0]",
    ".xarray.core.variable.py@@IndexVariable.level_names": "def level_names(self):\n    index = self.to_index()\n    if isinstance(index, pd.MultiIndex):\n        return index.names\n    else:\n        return None",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> bool:\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return set(self._data._coords)",
    ".xarray.core.dataarray.py@@DataArray.__getitem__": "def __getitem__(self, key: Any) -> 'DataArray':\n    if isinstance(key, str):\n        return self._getitem_coord(key)\n    else:\n        return self.isel(indexers=self._item_key_to_dict(key))",
    ".xarray.core.dataarray.py@@DataArray.ndim": "def ndim(self) -> int:\n    return self.variable.ndim",
    ".xarray.core.coordinates.py@@Coordinates.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._names",
    ".xarray.core.missing.py@@_get_nan_block_lengths": "def _get_nan_block_lengths(obj, dim: Hashable, index: Variable):\n    index = Variable([dim], index)\n    arange = ones_like(obj) * index\n    valid = obj.notnull()\n    valid_arange = arange.where(valid)\n    cumulative_nans = valid_arange.ffill(dim=dim).fillna(index[0])\n    nan_block_lengths = cumulative_nans.diff(dim=dim, label='upper').reindex({dim: obj[dim]}).where(valid).bfill(dim=dim).where(~valid, 0).fillna(index[-1] - valid_arange.max())\n    return nan_block_lengths",
    ".xarray.core.common.py@@ones_like": "def ones_like(other, dtype: DTypeLike=None):\n    return full_like(other, 1, dtype)",
    ".xarray.core.common.py@@full_like": "def full_like(other, fill_value, dtype: DTypeLike=None):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    from .variable import Variable\n    if isinstance(other, Dataset):\n        data_vars = {k: _full_like_variable(v, fill_value, dtype) for k, v in other.data_vars.items()}\n        return Dataset(data_vars, coords=other.coords, attrs=other.attrs)\n    elif isinstance(other, DataArray):\n        return DataArray(_full_like_variable(other.variable, fill_value, dtype), dims=other.dims, coords=other.coords, attrs=other.attrs, name=other.name)\n    elif isinstance(other, Variable):\n        return _full_like_variable(other, fill_value, dtype)\n    else:\n        raise TypeError('Expected DataArray, Dataset, or Variable')",
    ".xarray.core.common.py@@_full_like_variable": "def _full_like_variable(other, fill_value, dtype: DTypeLike=None):\n    from .variable import Variable\n    if isinstance(other.data, dask_array_type):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype, chunks=other.data.chunks)\n    else:\n        data = np.full_like(other, fill_value, dtype=dtype)\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)",
    ".xarray.core.common.py@@AbstractArray.__array__": "def __array__(self: Any, dtype: DTypeLike=None) -> np.ndarray:\n    return np.asarray(self.values, dtype=dtype)",
    ".xarray.core.variable.py@@Variable.values": "def values(self):\n    return _as_array_or_item(self._data)",
    ".xarray.core.variable.py@@_as_array_or_item": "def _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data",
    ".xarray.core.dataarray.py@@_check_data_shape": "def _check_data_shape(data, coords, dims):\n    if data is dtypes.NA:\n        data = np.nan\n    if coords is not None and utils.is_scalar(data, include_0d=False):\n        if utils.is_dict_like(coords):\n            if dims is None:\n                return data\n            else:\n                data_shape = tuple((as_variable(coords[k], k).size if k in coords.keys() else 1 for k in dims))\n        else:\n            data_shape = tuple((as_variable(coord, 'foo').size for coord in coords))\n        data = np.full(data_shape, data)\n    return data",
    ".xarray.core.dataarray.py@@_infer_coords_and_dims": "def _infer_coords_and_dims(shape, coords, dims) -> 'Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]':\n    if coords is not None and (not utils.is_dict_like(coords)) and (len(coords) != len(shape)):\n        raise ValueError('coords is not dict-like, but it has %s items, which does not match the %s dimensions of the data' % (len(coords), len(shape)))\n    if isinstance(dims, str):\n        dims = (dims,)\n    if dims is None:\n        dims = ['dim_%s' % n for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            if utils.is_dict_like(coords):\n                raise ValueError('inferring DataArray dimensions from dictionary like ``coords`` is no longer supported. Use an explicit list of ``dims`` instead.')\n            for n, (dim, coord) in enumerate(zip(dims, coords)):\n                coord = as_variable(coord, name=dims[n]).to_index_variable()\n                dims[n] = coord.name\n        dims = tuple(dims)\n    elif len(dims) != len(shape):\n        raise ValueError('different number of dimensions on data and dims: %s vs %s' % (len(shape), len(dims)))\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError('dimension %s is not a string' % d)\n    new_coords: Dict[Any, Variable] = {}\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var.to_index_variable()\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any((d not in dims for d in v.dims)):\n            raise ValueError('coordinate %s has dimensions %s, but these are not a subset of the DataArray dimensions %s' % (k, v.dims, dims))\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError('conflicting sizes for dimension %r: length %s on the data but length %s on coordinate %r' % (d, sizes[d], s, k))\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError('coordinate %r is a DataArray dimension, but it has shape %r rather than expected shape %r matching the dimension size' % (k, v.shape, (sizes[k],)))\n    assert_unique_multiindex_level_names(new_coords)\n    return (new_coords, dims)",
    ".xarray.core.dataarray.py@@DataArray.func": "def func(self, other):\n    if isinstance(other, groupby.GroupBy):\n        raise TypeError('in-place operations between a DataArray and a grouped object are not permitted')\n    other_coords = getattr(other, 'coords', None)\n    other_variable = getattr(other, 'variable', other)\n    with self.coords._merge_inplace(other_coords):\n        f(self.variable, other_variable)\n    return self",
    ".xarray.core.variable.py@@Variable.func": "def func(self, other):\n    if isinstance(other, xr.Dataset):\n        raise TypeError('cannot add a Dataset to a Variable in-place')\n    self_data, other_data, dims = _broadcast_compat_data(self, other)\n    if dims != self.dims:\n        raise ValueError('dimensions cannot change for in-place operations')\n    with np.errstate(all='ignore'):\n        self.values = f(self_data, other_data)\n    return self",
    ".xarray.core.variable.py@@_broadcast_compat_data": "def _broadcast_compat_data(self, other):\n    if all((hasattr(other, attr) for attr in ['dims', 'data', 'shape', 'encoding'])):\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return (self_data, other_data, dims)",
    ".xarray.core.variable.py@@_broadcast_compat_variables": "def _broadcast_compat_variables(*variables):\n    dims = tuple(_unified_dims(variables))\n    return tuple((var.set_dims(dims) if var.dims != dims else var for var in variables))",
    ".xarray.core.variable.py@@_unified_dims": "def _unified_dims(variables):\n    all_dims = {}\n    for var in variables:\n        var_dims = var.dims\n        if len(set(var_dims)) < len(var_dims):\n            raise ValueError('broadcasting cannot handle duplicate dimensions: %r' % list(var_dims))\n        for d, s in zip(var_dims, var.shape):\n            if d not in all_dims:\n                all_dims[d] = s\n            elif all_dims[d] != s:\n                raise ValueError('operands cannot be broadcast together with mismatched lengths for dimension %r: %s' % (d, (all_dims[d], s)))\n    return all_dims",
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    global_choice = OPTIONS['keep_attrs']\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(\"The global option keep_attrs must be one of True, False or 'default'.\")",
    ".xarray.core.coordinates.py@@Coordinates._merge_raw": "def _merge_raw(self, other):\n    if other is None:\n        variables = dict(self.variables)\n        indexes = dict(self.indexes)\n    else:\n        variables, indexes = merge_coordinates_without_align([self, other])\n    return (variables, indexes)",
    ".xarray.core.coordinates.py@@Coordinates.indexes": "def indexes(self) -> Indexes:\n    return self._data.indexes",
    ".xarray.core.dataarray.py@@DataArray._result_name": "def _result_name(self, other: Any=None) -> Optional[Hashable]:\n    other_name = getattr(other, 'name', _default)\n    if other_name is _default or other_name == self.name:\n        return self.name\n    else:\n        return None",
    ".xarray.core.ops.py@@func": "def func(self, *args, **kwargs):\n    try:\n        return getattr(self, name)(*args, **kwargs)\n    except AttributeError:\n        return f(self, *args, **kwargs)",
    ".xarray.core.duck_array_ops.py@@notnull": "def notnull(data):\n    return ~isnull(data)",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, (np.ndarray, dask_array_type)):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data):\n    return data if isinstance(data, dask_array_type) or hasattr(data, '__array_function__') else np.asarray(data)",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError(f'`out` is not valid for {name}')\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        func = _dask_or_eager_func(name)\n    try:\n        return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if isinstance(values, dask_array_type):\n            try:\n                return func(values, axis=axis, dtype=values.dtype, **kwargs)\n            except (AttributeError, TypeError):\n                msg = '%s is not yet implemented on dask arrays' % name\n        else:\n            msg = '%s is not available with skipna=False with the installed version of numpy; upgrade to numpy 1.12 or newer to use skipna=True or skipna=None' % name\n        raise NotImplementedError(msg)",
    ".xarray.core.dataarray.py@@DataArray.__array_wrap__": "def __array_wrap__(self, obj, context=None) -> 'DataArray':\n    new_var = self.variable.__array_wrap__(obj, context)\n    return self._replace(new_var)",
    ".xarray.core.variable.py@@Variable.__array_wrap__": "def __array_wrap__(self, obj, context=None):\n    return Variable(self.dims, obj)",
    ".xarray.core.common.py@@DataWithCoords.where": "def where(self, cond, other=dtypes.NA, drop: bool=False):\n    from .alignment import align\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if drop:\n        if other is not dtypes.NA:\n            raise ValueError('cannot set `other` if drop=True')\n        if not isinstance(cond, (Dataset, DataArray)):\n            raise TypeError('cond argument is %r but must be a %r or %r' % (cond, Dataset, DataArray))\n        self, cond = align(self, cond)\n        if isinstance(cond, Dataset):\n            clipcond = cond.to_array().any('variable')\n        else:\n            clipcond = cond\n        nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n        indexers = {k: np.unique(v) for k, v in nonzeros}\n        self = self.isel(**indexers)\n        cond = cond.isel(**indexers)\n    return ops.where_method(self, cond, other)",
    ".xarray.core.ops.py@@where_method": "def where_method(self, cond, other=dtypes.NA):\n    from .computation import apply_ufunc\n    join = 'inner' if other is dtypes.NA else 'exact'\n    return apply_ufunc(duck_array_ops.where_method, self, cond, other, join=join, dataset_join=join, dask='allowed', keep_attrs=True)",
    ".xarray.core.merge.py@@merge_coordinates_without_align": "def merge_coordinates_without_align(objects: 'List[Coordinates]', prioritized: Mapping[Hashable, MergeElement]=None, exclude_dims: AbstractSet=frozenset()) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    collected = collect_from_coordinates(objects)\n    if exclude_dims:\n        filtered: Dict[Hashable, List[MergeElement]] = {}\n        for name, elements in collected.items():\n            new_elements = [(variable, index) for variable, index in elements if exclude_dims.isdisjoint(variable.dims)]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n    return merge_collected(filtered, prioritized)",
    ".xarray.core.merge.py@@collect_from_coordinates": "def collect_from_coordinates(list_of_coords: 'List[Coordinates]') -> Dict[Hashable, List[MergeElement]]:\n    grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.indexes\n        for name, variable in variables.items():\n            value = grouped.setdefault(name, [])\n            value.append((variable, indexes.get(name)))\n    return grouped",
    ".xarray.core.duck_array_ops.py@@where_method": "def where_method(data, cond, other=dtypes.NA):\n    if other is dtypes.NA:\n        other = dtypes.get_fill_value(data.dtype)\n    return where(cond, data, other)",
    ".xarray.core.dtypes.py@@get_fill_value": "def get_fill_value(dtype):\n    _, fill_value = maybe_promote(dtype)\n    return fill_value",
    ".xarray.core.dtypes.py@@maybe_promote": "def maybe_promote(dtype):\n    if np.issubdtype(dtype, np.floating):\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.timedelta64):\n        fill_value = np.timedelta64('NaT')\n    elif np.issubdtype(dtype, np.integer):\n        if dtype.itemsize <= 2:\n            dtype = np.float32\n        else:\n            dtype = np.float64\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.complexfloating):\n        fill_value = np.nan + np.nan * 1j\n    elif np.issubdtype(dtype, np.datetime64):\n        fill_value = np.datetime64('NaT')\n    else:\n        dtype = object\n        fill_value = np.nan\n    return (np.dtype(dtype), fill_value)",
    ".xarray.core.duck_array_ops.py@@where": "def where(condition, x, y):\n    return _where(condition, *as_shared_dtype([x, y]))",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)",
    ".xarray.core.dataarray.py@@DataArray.ffill": "def ffill(self, dim: Hashable, limit: int=None) -> 'DataArray':\n    from .missing import ffill\n    return ffill(self, dim, limit=limit)",
    ".xarray.core.missing.py@@ffill": "def ffill(arr, dim=None, limit=None):\n    import bottleneck as bn\n    axis = arr.get_axis_num(dim)\n    _limit = limit if limit is not None else arr.shape[axis]\n    return apply_ufunc(bn.push, arr, dask='parallelized', keep_attrs=True, output_dtypes=[arr.dtype], kwargs=dict(n=_limit, axis=axis)).transpose(*arr.dims)",
    ".xarray.core.variable.py@@Variable.__getitem__": "def __getitem__(self: VariableType, key) -> VariableType:\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    data = as_indexable(self._data)[indexer]\n    if new_order:\n        data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes": "def _broadcast_indexes(self, key):\n    key = self._item_key_to_tuple(key)\n    key = indexing.expanded_indexer(key, self.ndim)\n    key = tuple((k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key))\n    key = tuple((k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key))\n    if all((isinstance(k, BASIC_INDEXING_TYPES) for k in key)):\n        return self._broadcast_indexes_basic(key)\n    self._validate_indexers(key)\n    if all((not isinstance(k, Variable) for k in key)):\n        return self._broadcast_indexes_outer(key)\n    dims = []\n    for k, d in zip(key, self.dims):\n        if isinstance(k, Variable):\n            if len(k.dims) > 1:\n                return self._broadcast_indexes_vectorized(key)\n            dims.append(k.dims[0])\n        elif not isinstance(k, integer_types):\n            dims.append(d)\n    if len(set(dims)) == len(dims):\n        return self._broadcast_indexes_outer(key)\n    return self._broadcast_indexes_vectorized(key)",
    ".xarray.core.variable.py@@Variable._item_key_to_tuple": "def _item_key_to_tuple(self, key):\n    if utils.is_dict_like(key):\n        return tuple((key.get(dim, slice(None)) for dim in self.dims))\n    else:\n        return key",
    ".xarray.core.indexing.py@@expanded_indexer": "def expanded_indexer(key, ndim):\n    if not isinstance(key, tuple):\n        key = (key,)\n    new_key = []\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_basic": "def _broadcast_indexes_basic(self, key):\n    dims = tuple((dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    return (dims, BasicIndexer(key), None)",
    ".xarray.core.indexing.py@@BasicIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.__init__": "def __init__(self, key):\n    if type(self) is ExplicitIndexer:\n        raise TypeError('cannot instantiate base ExplicitIndexer objects')\n    self._key = tuple(key)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, '__array_function__'):\n        return NdArrayLikeIndexingAdapter(array)\n    raise TypeError('Invalid array type: {}'.format(type(array)))",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    array, key = self._indexing_array_and_key(key)\n    return array[key]",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter._indexing_array_and_key": "def _indexing_array_and_key(self, key):\n    if isinstance(key, OuterIndexer):\n        array = self.array\n        key = _outer_to_numpy_indexer(key, self.array.shape)\n    elif isinstance(key, VectorizedIndexer):\n        array = nputils.NumpyVIndexAdapter(self.array)\n        key = key.tuple\n    elif isinstance(key, BasicIndexer):\n        array = self.array\n        key = key.tuple + (Ellipsis,)\n    else:\n        raise TypeError('unexpected key type: {}'.format(type(key)))\n    return (array, key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.tuple": "def tuple(self):\n    return self._key",
    ".xarray.core.variable.py@@Variable._finalize_indexing_result": "def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.dataarray.py@@DataArray.fillna": "def fillna(self, value: Any) -> 'DataArray':\n    if utils.is_dict_like(value):\n        raise TypeError('cannot provide fill value as a dictionary with fillna on a DataArray')\n    out = ops.fillna(self, value)\n    return out",
    ".xarray.core.ops.py@@fillna": "def fillna(data, other, join='left', dataset_join='left'):\n    from .computation import apply_ufunc\n    return apply_ufunc(duck_array_ops.fillna, data, other, join=join, dask='allowed', dataset_join=dataset_join, dataset_fill_value=np.nan, keep_attrs=True)",
    ".xarray.core.duck_array_ops.py@@fillna": "def fillna(data, other):\n    return where(isnull(data), other, data)",
    ".xarray.core.dataarray.py@@DataArray.diff": "def diff(self, dim: Hashable, n: int=1, label: Hashable='upper') -> 'DataArray':\n    ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataarray.py@@DataArray._to_temp_dataset": "def _to_temp_dataset(self) -> Dataset:\n    return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)",
    ".xarray.core.dataarray.py@@DataArray._to_dataset_whole": "def _to_dataset_whole(self, name: Hashable=None, shallow_copy: bool=True) -> Dataset:\n    if name is None:\n        name = self.name\n    if name is None:\n        raise ValueError('unable to convert unnamed DataArray to a Dataset without providing an explicit name')\n    if name in self.coords:\n        raise ValueError('cannot create a Dataset from a DataArray with the same name as one of its coordinates')\n    variables = self._coords.copy()\n    variables[name] = self.variable\n    if shallow_copy:\n        for k in variables:\n            variables[k] = variables[k].copy(deep=False)\n    coord_names = set(self._coords)\n    dataset = Dataset._from_vars_and_coord_names(variables, coord_names)\n    return dataset",
    ".xarray.core.variable.py@@Variable.isel": "def isel(self: VariableType, indexers: Mapping[Hashable, Any]=None, **indexers_kwargs: Any) -> VariableType:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    invalid = indexers.keys() - set(self.dims)\n    if invalid:\n        raise ValueError('dimensions %r do not exist' % invalid)\n    key = tuple((indexers.get(dim, slice(None)) for dim in self.dims))\n    return self[key]",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Optional[Mapping[Hashable, T]], kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .%s must be a dictionary' % func_name)\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional arguments to .%s' % func_name)\n        return pos_kwargs\n    else:\n        return cast(Mapping[Hashable, T], kw_kwargs)",
    ".xarray.core.indexing.py@@as_integer_slice": "def as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)",
    ".xarray.core.indexing.py@@as_integer_or_none": "def as_integer_or_none(value):\n    return None if value is None else operator.index(value)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__getitem__": "def __getitem__(self, indexer) -> Union[NumpyIndexingAdapter, np.ndarray, np.datetime64, np.timedelta64]:\n    key = indexer.tuple\n    if isinstance(key, tuple) and len(key) == 1:\n        key, = key\n    if getattr(key, 'ndim', 0) > 1:\n        return NumpyIndexingAdapter(self.array.values)[indexer]\n    result = self.array[key]\n    if isinstance(result, pd.Index):\n        result = PandasIndexAdapter(result, dtype=self.dtype)\n    else:\n        if result is pd.NaT:\n            result = np.datetime64('NaT', 'ns')\n        elif isinstance(result, timedelta):\n            result = np.timedelta64(getattr(result, 'value', result), 'ns')\n        elif isinstance(result, pd.Timestamp):\n            result = np.asarray(result.to_datetime64())\n        elif self.dtype != object:\n            result = np.asarray(result, dtype=self.dtype)\n        result = utils.to_0d_array(result)\n    return result",
    ".xarray.core.indexing.py@@PandasIndexAdapter.dtype": "def dtype(self) -> np.dtype:\n    return self._dtype",
    ".xarray.core.variable.py@@IndexVariable._finalize_indexing_result": "def _finalize_indexing_result(self, dims, data):\n    if getattr(data, 'ndim', 0) != 1:\n        return Variable(dims, data, self._attrs, self._encoding)\n    else:\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.dataarray.py@@DataArray._from_temp_dataset": "def _from_temp_dataset(self, dataset: Dataset, name: Hashable=_default) -> 'DataArray':\n    variable = dataset._variables.pop(_THIS_ARRAY)\n    coords = dataset._variables\n    indexes = dataset._indexes\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.reindex": "def reindex(self, indexers: Mapping[Hashable, Any]=None, method: str=None, tolerance=None, copy: bool=True, fill_value=dtypes.NA, **indexers_kwargs: Any) -> 'DataArray':\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'reindex')\n    ds = self._to_temp_dataset().reindex(indexers=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.utils.py@@SortedKeysDict.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.alignment.py@@reindex_variables": "def reindex_variables(variables: Mapping[Any, Variable], sizes: Mapping[Any, int], indexes: Mapping[Any, pd.Index], indexers: Mapping, method: Optional[str]=None, tolerance: Any=None, copy: bool=True, fill_value: Optional[Any]=dtypes.NA) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    from .dataarray import DataArray\n    reindexed: Dict[Hashable, Variable] = {}\n    int_indexers = {}\n    new_indexes = dict(indexes)\n    masked_dims = set()\n    unchanged_dims = set()\n    for dim, indexer in indexers.items():\n        if isinstance(indexer, DataArray) and indexer.dims != (dim,):\n            raise ValueError('Indexer has dimensions {:s} that are different from that to be indexed along {:s}'.format(str(indexer.dims), dim))\n        target = new_indexes[dim] = utils.safe_cast_to_index(indexers[dim])\n        if dim in indexes:\n            index = indexes[dim]\n            if not index.is_unique:\n                raise ValueError('cannot reindex or align along dimension %r because the index has duplicate values' % dim)\n            int_indexer = get_indexer_nd(index, target, method, tolerance)\n            if (int_indexer < 0).any():\n                masked_dims.add(dim)\n            elif np.array_equal(int_indexer, np.arange(len(index))):\n                unchanged_dims.add(dim)\n            int_indexers[dim] = int_indexer\n        if dim in variables:\n            var = variables[dim]\n            args: tuple = (var.attrs, var.encoding)\n        else:\n            args = ()\n        reindexed[dim] = IndexVariable((dim,), target, *args)\n    for dim in sizes:\n        if dim not in indexes and dim in indexers:\n            existing_size = sizes[dim]\n            new_size = indexers[dim].size\n            if existing_size != new_size:\n                raise ValueError('cannot reindex or align along dimension %r without an index because its size %r is different from the size of the new index %r' % (dim, existing_size, new_size))\n    for name, var in variables.items():\n        if name not in indexers:\n            key = tuple((slice(None) if d in unchanged_dims else int_indexers.get(d, slice(None)) for d in var.dims))\n            needs_masking = any((d in masked_dims for d in var.dims))\n            if needs_masking:\n                new_var = var._getitem_with_mask(key, fill_value=fill_value)\n            elif all((is_full_slice(k) for k in key)):\n                new_var = var.copy(deep=copy)\n            else:\n                new_var = var[key]\n            reindexed[name] = new_var\n    return (reindexed, new_indexes)",
    ".xarray.core.dataarray.py@@DataArray.to_index": "def to_index(self) -> pd.Index:\n    return self.variable.to_index()",
    ".xarray.core.indexing.py@@get_indexer_nd": "def get_indexer_nd(index, labels, method=None, tolerance=None):\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer",
    ".xarray.core.variable.py@@Variable._getitem_with_mask": "def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(self.dtype)\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    if self.size:\n        if isinstance(self._data, dask_array_type):\n            actual_indexer = indexing.posify_mask_indexer(indexer)\n        else:\n            actual_indexer = indexer\n        data = as_indexable(self._data)[actual_indexer]\n        mask = indexing.create_mask(indexer, self.shape, data)\n        data = duck_array_ops.where(mask, fill_value, data)\n    else:\n        mask = indexing.create_mask(indexer, self.shape)\n        data = np.broadcast_to(fill_value, getattr(mask, 'shape', ()))\n    if new_order:\n        data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.variable.py@@Variable._validate_indexers": "def _validate_indexers(self, key):\n    for dim, k in zip(self.dims, key):\n        if isinstance(k, BASIC_INDEXING_TYPES):\n            pass\n        else:\n            if not isinstance(k, Variable):\n                k = np.asarray(k)\n                if k.ndim > 1:\n                    raise IndexError('Unlabeled multi-dimensional array cannot be used for indexing: {}'.format(k))\n            if k.dtype.kind == 'b':\n                if self.shape[self.get_axis_num(dim)] != len(k):\n                    raise IndexError('Boolean array size {:d} is used to index array with shape {:s}.'.format(len(k), str(self.shape)))\n                if k.ndim > 1:\n                    raise IndexError('{}-dimensional boolean indexing is not supported. '.format(k.ndim))\n                if getattr(k, 'dims', (dim,)) != (dim,):\n                    raise IndexError('Boolean indexer should be unlabeled or on the same dimension to the indexed array. Indexer is on {:s} but the target dimension is {:s}.'.format(str(k.dims), dim))",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_outer": "def _broadcast_indexes_outer(self, key):\n    dims = tuple((k.dims[0] if isinstance(k, Variable) else dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    new_key = []\n    for k in key:\n        if isinstance(k, Variable):\n            k = k.data\n        if not isinstance(k, BASIC_INDEXING_TYPES):\n            k = np.asarray(k)\n            if k.dtype.kind == 'b':\n                k, = np.nonzero(k)\n        new_key.append(k)\n    return (dims, OuterIndexer(tuple(new_key)), None)",
    ".xarray.core.indexing.py@@OuterIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if k.ndim != 1:\n                raise TypeError(f'invalid indexer array for {type(self).__name__}; must have exactly 1 dimension: {k!r}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.utils.py@@NdimSizeLenMixin.size": "def size(self: Any) -> int:\n    return int(np.prod(self.shape))",
    ".xarray.core.indexing.py@@_outer_to_numpy_indexer": "def _outer_to_numpy_indexer(key, shape):\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple",
    ".xarray.core.indexing.py@@create_mask": "def create_mask(indexer, shape, data=None):\n    if isinstance(indexer, OuterIndexer):\n        key = _outer_to_vectorized_indexer(indexer, shape).tuple\n        assert not any((isinstance(k, slice) for k in key))\n        mask = _masked_result_drop_slice(key, data)\n    elif isinstance(indexer, VectorizedIndexer):\n        key = indexer.tuple\n        base_mask = _masked_result_drop_slice(key, data)\n        slice_shape = tuple((np.arange(*k.indices(size)).size for k, size in zip(key, shape) if isinstance(k, slice)))\n        expanded_mask = base_mask[(Ellipsis,) + (np.newaxis,) * len(slice_shape)]\n        mask = duck_array_ops.broadcast_to(expanded_mask, base_mask.shape + slice_shape)\n    elif isinstance(indexer, BasicIndexer):\n        mask = any((k == -1 for k in indexer.tuple))\n    else:\n        raise TypeError('unexpected key type: {}'.format(type(indexer)))\n    return mask",
    ".xarray.core.indexing.py@@_outer_to_vectorized_indexer": "def _outer_to_vectorized_indexer(key, shape):\n    key = key.tuple\n    n_dim = len([k for k in key if not isinstance(k, integer_types)])\n    i_dim = 0\n    new_key = []\n    for k, size in zip(key, shape):\n        if isinstance(k, integer_types):\n            new_key.append(np.array(k).reshape((1,) * n_dim))\n        else:\n            if isinstance(k, slice):\n                k = np.arange(*k.indices(size))\n            assert k.dtype.kind in {'i', 'u'}\n            shape = [(1,) * i_dim + (k.size,) + (1,) * (n_dim - i_dim - 1)]\n            new_key.append(k.reshape(*shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))",
    ".xarray.core.indexing.py@@VectorizedIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    ndim = None\n    for k in key:\n        if isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if ndim is None:\n                ndim = k.ndim\n            elif ndim != k.ndim:\n                ndims = [k.ndim for k in key if isinstance(k, np.ndarray)]\n                raise ValueError(f'invalid indexer key: ndarray arguments have different numbers of dimensions: {ndims}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@_masked_result_drop_slice": "def _masked_result_drop_slice(key, data=None):\n    key = (k for k in key if not isinstance(k, slice))\n    chunks_hint = getattr(data, 'chunks', None)\n    new_keys = []\n    for k in key:\n        if isinstance(k, np.ndarray):\n            if isinstance(data, dask_array_type):\n                new_keys.append(_dask_array_with_chunks_hint(k, chunks_hint))\n            elif isinstance(data, sparse_array_type):\n                import sparse\n                new_keys.append(sparse.COO.from_numpy(k))\n            else:\n                new_keys.append(k)\n        else:\n            new_keys.append(k)\n    mask = _logical_any((k == -1 for k in new_keys))\n    return mask",
    ".xarray.core.indexing.py@@_logical_any": "def _logical_any(args):\n    return functools.reduce(operator.or_, args)",
    ".xarray.core.dataarray.py@@DataArray.bfill": "def bfill(self, dim: Hashable, limit: int=None) -> 'DataArray':\n    from .missing import bfill\n    return bfill(self, dim, limit=limit)",
    ".xarray.core.missing.py@@bfill": "def bfill(arr, dim=None, limit=None):\n    axis = arr.get_axis_num(dim)\n    _limit = limit if limit is not None else arr.shape[axis]\n    return apply_ufunc(_bfill, arr, dask='parallelized', keep_attrs=True, output_dtypes=[arr.dtype], kwargs=dict(n=_limit, axis=axis)).transpose(*arr.dims)",
    ".xarray.core.missing.py@@_bfill": "def _bfill(arr, n=None, axis=-1):\n    import bottleneck as bn\n    arr = np.flip(arr, axis=axis)\n    arr = bn.push(arr, axis=axis, n=n)\n    return np.flip(arr, axis=axis)",
    ".xarray.core.common.py@@ImplementsArrayReduce.wrapped_func": "def wrapped_func(self, dim=None, axis=None, **kwargs):\n    return self.reduce(func, dim, axis, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reduce": "def reduce(self, func: Callable[..., Any], dim: Union[None, Hashable, Sequence[Hashable]]=None, axis: Union[None, int, Sequence[int]]=None, keep_attrs: bool=None, keepdims: bool=False, **kwargs: Any) -> 'DataArray':\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n    return self._replace_maybe_drop_dims(var)",
    ".xarray.core.variable.py@@Variable.reduce": "def reduce(self, func, dim=None, axis=None, keep_attrs=None, keepdims=False, allow_lazy=None, **kwargs):\n    if dim == ...:\n        dim = None\n    if dim is not None and axis is not None:\n        raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n    if dim is not None:\n        axis = self.get_axis_num(dim)\n    if allow_lazy is not None:\n        warnings.warn('allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.', DeprecationWarning)\n    else:\n        allow_lazy = True\n    input_data = self.data if allow_lazy else self.values\n    if axis is not None:\n        data = func(input_data, axis=axis, **kwargs)\n    else:\n        data = func(input_data, **kwargs)\n    if getattr(data, 'shape', ()) == self.shape:\n        dims = self.dims\n    else:\n        removed_axes = range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n        if keepdims:\n            slices = tuple((np.newaxis if i in removed_axes else slice(None, None) for i in range(self.ndim)))\n            if getattr(data, 'shape', None) is None:\n                data = np.asanyarray(data)[slices]\n            else:\n                data = data[slices]\n            dims = self.dims\n        else:\n            dims = [adim for n, adim in enumerate(self.dims) if n not in removed_axes]\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    return Variable(dims, data, attrs=attrs)",
    ".xarray.core.nanops.py@@nanmax": "def nanmax(a, axis=None, out=None):\n    if a.dtype.kind == 'O':\n        return _nan_minmax_object('max', dtypes.get_neg_infinity(a.dtype), a, axis)\n    module = dask_array if isinstance(a, dask_array_type) else nputils\n    return module.nanmax(a, axis=axis)",
    ".xarray.core.nputils.py@@f": "def f(values, axis=None, **kwargs):\n    dtype = kwargs.get('dtype', None)\n    bn_func = getattr(bn, name, None)\n    if _USE_BOTTLENECK and isinstance(values, np.ndarray) and (bn_func is not None) and (not isinstance(axis, tuple)) and (values.dtype.kind in 'uifc') and values.dtype.isnative and (dtype is None or np.dtype(dtype) == values.dtype):\n        kwargs.pop('dtype', None)\n        result = bn_func(values, axis=axis, **kwargs)\n    else:\n        result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n    return result"
}