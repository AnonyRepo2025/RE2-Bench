{
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=''):\n    if isinstance(array, np.matrix):\n        raise TypeError('np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html')\n    xp, is_array_api = get_namespace(array)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    pandas_requires_conversion = False\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if not hasattr(array, 'sparse') and array.dtypes.apply(is_sparse).any():\n                warnings.warn('pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.')\n        dtypes_orig = list(array.dtypes)\n        pandas_requires_conversion = any((_pandas_dtype_needs_early_conversion(i) for i in dtypes_orig))\n        if all((isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig)):\n            dtype_orig = np.result_type(*dtypes_orig)\n    elif hasattr(array, 'iloc') and hasattr(array, 'dtype'):\n        pandas_requires_conversion = _pandas_dtype_needs_early_conversion(array.dtype)\n        if isinstance(array.dtype, np.dtype):\n            dtype_orig = array.dtype\n        else:\n            dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = xp.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if pandas_requires_conversion:\n        new_dtype = dtype_orig if dtype is None else dtype\n        array = array.astype(new_dtype)\n        dtype = None\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    estimator_name = _check_estimator_name(estimator)\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if hasattr(array, 'sparse') and array.ndim > 1:\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if array.dtypes.apply(is_sparse).all():\n                array = array.sparse.to_coo()\n                if array.dtype == np.dtype('object'):\n                    unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])\n                    if len(unique_dtypes) > 1:\n                        raise ValueError('Pandas DataFrame with mixed sparse extension arrays generated a sparse matrix with object dtype which can not be converted to a scipy sparse matrix.Sparse extension arrays should all have the same numeric type.')\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse, estimator_name=estimator_name, input_name=input_name)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = _asarray_with_order(array, order=order, xp=xp)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype, estimator_name=estimator_name, input_name=input_name)\n                    array = xp.astype(array, dtype, copy=False)\n                else:\n                    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            except ComplexWarning as complex_warning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array)) from complex_warning\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and array.dtype.kind in 'USV':\n            raise ValueError(\"dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.\")\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, input_name=input_name, estimator_name=estimator_name, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if copy:\n        if xp.__name__ in {'numpy', 'numpy.array_api'}:\n            if np.may_share_memory(array, array_orig):\n                array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n        else:\n            array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n    return array",
    ".sklearn.utils._array_api.py@@get_namespace": "def get_namespace(*arrays):\n    if not get_config()['array_api_dispatch']:\n        return (_NumPyApiWrapper(), False)\n    namespaces = {x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in arrays if not isinstance(x, (bool, int, float, complex))}\n    if not namespaces:\n        raise ValueError('Unrecognized array input')\n    if len(namespaces) != 1:\n        raise ValueError(f'Multiple namespaces for array inputs: {namespaces}')\n    xp, = namespaces\n    if xp is None:\n        return (_NumPyApiWrapper(), False)\n    return (_ArrayAPIWrapper(xp), True)",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _get_threadlocal_config().copy()",
    ".sklearn._config.py@@_get_threadlocal_config": "def _get_threadlocal_config():\n    if not hasattr(_threadlocal, 'global_config'):\n        _threadlocal.global_config = _global_config.copy()\n    return _threadlocal.global_config",
    ".sklearn.utils.validation.py@@_check_estimator_name": "def _check_estimator_name(estimator):\n    if estimator is not None:\n        if isinstance(estimator, str):\n            return estimator\n        else:\n            return estimator.__class__.__name__\n    return None",
    ".sklearn.utils._array_api.py@@_asarray_with_order": "def _asarray_with_order(array, dtype=None, order=None, copy=None, xp=None):\n    if xp is None:\n        xp, _ = get_namespace(array)\n    if xp.__name__ in {'numpy', 'numpy.array_api'}:\n        array = numpy.asarray(array, order=order, dtype=dtype)\n        return xp.asarray(array, copy=copy)\n    else:\n        return xp.asarray(array, dtype=dtype, copy=copy)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.__getattr__": "def __getattr__(self, name):\n    return getattr(numpy, name)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.asarray": "def asarray(self, x, *, dtype=None, device=None, copy=None):\n    if copy is True:\n        return numpy.array(x, copy=True, dtype=dtype)\n    else:\n        return numpy.asarray(x, dtype=dtype)",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None, estimator_name=None, input_name=''):\n    xp, _ = get_namespace(X)\n    if _get_config()['assume_finite']:\n        return\n    X = xp.asarray(X)\n    if X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')\n    if X.dtype.kind not in 'fc':\n        return\n    with np.errstate(over='ignore'):\n        first_pass_isfinite = xp.isfinite(xp.sum(X))\n    if first_pass_isfinite:\n        return\n    use_cython = xp is np and X.data.contiguous and (X.dtype.type in {np.float32, np.float64})\n    if use_cython:\n        out = cy_isfinite(X.reshape(-1), allow_nan=allow_nan)\n        has_nan_error = False if allow_nan else out == FiniteStatus.has_nan\n        has_inf = out == FiniteStatus.has_infinite\n    else:\n        has_inf = xp.any(xp.isinf(X))\n        has_nan_error = False if allow_nan else xp.any(xp.isnan(X))\n    if has_inf or has_nan_error:\n        if has_nan_error:\n            type_err = 'NaN'\n        else:\n            msg_dtype = msg_dtype if msg_dtype is not None else X.dtype\n            type_err = f'infinity or a value too large for {msg_dtype!r}'\n        padded_input_name = input_name + ' ' if input_name else ''\n        msg_err = f'Input {padded_input_name}contains {type_err}.'\n        if estimator_name and input_name == 'X' and has_nan_error:\n            msg_err += f'\\n{estimator_name} does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values'\n        raise ValueError(msg_err)",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.preprocessing._label.py@@LabelBinarizer.__init__": "def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):\n    self.neg_label = neg_label\n    self.pos_label = pos_label\n    self.sparse_output = sparse_output",
    ".sklearn.preprocessing._label.py@@LabelBinarizer.fit": "def fit(self, y):\n    self._validate_params()\n    if self.neg_label >= self.pos_label:\n        raise ValueError(f'neg_label={self.neg_label} must be strictly less than pos_label={self.pos_label}.')\n    if self.sparse_output and (self.pos_label == 0 or self.neg_label != 0):\n        raise ValueError(f'Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={self.pos_label} and neg_label={self.neg_label}')\n    self.y_type_ = type_of_target(y, input_name='y')\n    if 'multioutput' in self.y_type_:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    self.sparse_input_ = sp.issparse(y)\n    self.classes_ = unique_labels(y)\n    return self",
    ".sklearn.base.py@@BaseEstimator._validate_params": "def _validate_params(self):\n    validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key)\n        if deep and hasattr(value, 'get_params') and (not isinstance(value, type)):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.utils._param_validation.py@@validate_parameter_constraints": "def validate_parameter_constraints(parameter_constraints, params, caller_name):\n    for param_name, param_val in params.items():\n        if param_name not in parameter_constraints:\n            continue\n        constraints = parameter_constraints[param_name]\n        if constraints == 'no_validation':\n            continue\n        constraints = [make_constraint(constraint) for constraint in constraints]\n        for constraint in constraints:\n            if constraint.is_satisfied_by(param_val):\n                break\n        else:\n            constraints = [constraint for constraint in constraints if not constraint.hidden]\n            if len(constraints) == 1:\n                constraints_str = f'{constraints[0]}'\n            else:\n                constraints_str = f'{', '.join([str(c) for c in constraints[:-1]])} or {constraints[-1]}'\n            raise InvalidParameterError(f'The {param_name!r} parameter of {caller_name} must be {constraints_str}. Got {param_val!r} instead.')",
    ".sklearn.utils._param_validation.py@@make_constraint": "def make_constraint(constraint):\n    if isinstance(constraint, str) and constraint == 'array-like':\n        return _ArrayLikes()\n    if isinstance(constraint, str) and constraint == 'sparse matrix':\n        return _SparseMatrices()\n    if isinstance(constraint, str) and constraint == 'random_state':\n        return _RandomStates()\n    if constraint is callable:\n        return _Callables()\n    if constraint is None:\n        return _NoneConstraint()\n    if isinstance(constraint, type):\n        return _InstancesOf(constraint)\n    if isinstance(constraint, (Interval, StrOptions, Options, HasMethods)):\n        return constraint\n    if isinstance(constraint, str) and constraint == 'boolean':\n        return _Booleans()\n    if isinstance(constraint, str) and constraint == 'verbose':\n        return _VerboseHelper()\n    if isinstance(constraint, str) and constraint == 'missing_values':\n        return _MissingValues()\n    if isinstance(constraint, str) and constraint == 'cv_object':\n        return _CVObjects()\n    if isinstance(constraint, Hidden):\n        constraint = make_constraint(constraint.constraint)\n        constraint.hidden = True\n        return constraint\n    raise ValueError(f'Unknown constraint type: {constraint}')",
    ".sklearn.utils._param_validation.py@@_InstancesOf.__init__": "def __init__(self, type):\n    super().__init__()\n    self.type = type",
    ".sklearn.utils._param_validation.py@@_Constraint.__init__": "def __init__(self):\n    self.hidden = False",
    ".sklearn.utils._param_validation.py@@_InstancesOf.is_satisfied_by": "def is_satisfied_by(self, val):\n    return isinstance(val, self.type)",
    ".sklearn.utils._param_validation.py@@_Booleans.__init__": "def __init__(self):\n    super().__init__()\n    self._constraints = [_InstancesOf(bool), _InstancesOf(np.bool_), _InstancesOf(Integral)]",
    ".sklearn.utils._param_validation.py@@_Booleans.is_satisfied_by": "def is_satisfied_by(self, val):\n    if isinstance(val, Integral) and (not isinstance(val, bool)):\n        warnings.warn(\"Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\", FutureWarning)\n    return any((c.is_satisfied_by(val) for c in self._constraints))",
    ".sklearn.utils.multiclass.py@@type_of_target": "def type_of_target(y, input_name=''):\n    xp, is_array_api = get_namespace(y)\n    valid = (isinstance(y, Sequence) or issparse(y) or hasattr(y, '__array__')) and (not isinstance(y, str)) or is_array_api\n    if not valid:\n        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)\n    sparse_pandas = y.__class__.__name__ in ['SparseSeries', 'SparseArray']\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n    if is_multilabel(y):\n        return 'multilabel-indicator'\n    with warnings.catch_warnings():\n        warnings.simplefilter('error', np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                y = xp.asarray(y, dtype=object)\n    try:\n        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):\n            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.')\n    except IndexError:\n        pass\n    if y.ndim not in (1, 2):\n        return 'unknown'\n    if not min(y.shape):\n        if y.ndim == 1:\n            return 'binary'\n        return 'unknown'\n    if not issparse(y) and y.dtype == object and (not isinstance(y.flat[0], str)):\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = '-multioutput'\n    else:\n        suffix = ''\n    if y.dtype.kind == 'f':\n        data = y.data if issparse(y) else y\n        if xp.any(data != data.astype(int)):\n            _assert_all_finite(data, input_name=input_name)\n            return 'continuous' + suffix\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        return 'multiclass' + suffix\n    else:\n        return 'binary'",
    ".sklearn.utils.multiclass.py@@is_multilabel": "def is_multilabel(y):\n    xp, is_array_api = get_namespace(y)\n    if hasattr(y, '__array__') or isinstance(y, Sequence) or is_array_api:\n        with warnings.catch_warnings():\n            warnings.simplefilter('error', np.VisibleDeprecationWarning)\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                y = xp.asarray(y, dtype=object)\n    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):\n        return False\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        labels = xp.unique_values(y.data)\n        return len(y.data) == 0 or ((labels.size == 1 or (labels.size == 2 and 0 in labels)) and (y.dtype.kind in 'biu' or _is_integral_float(labels)))\n    else:\n        labels = xp.unique_values(y)\n        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.unique_values": "def unique_values(self, x):\n    return numpy.unique(x)",
    ".sklearn.utils.multiclass.py@@unique_labels": "def unique_labels(*ys):\n    xp, is_array_api = get_namespace(*ys)\n    if not ys:\n        raise ValueError('No argument has been passed.')\n    ys_types = set((type_of_target(x) for x in ys))\n    if ys_types == {'binary', 'multiclass'}:\n        ys_types = {'multiclass'}\n    if len(ys_types) > 1:\n        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)\n    label_type = ys_types.pop()\n    if label_type == 'multilabel-indicator' and len(set((check_array(y, accept_sparse=['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:\n        raise ValueError('Multi-label binary indicator input with different numbers of labels')\n    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n    if not _unique_labels:\n        raise ValueError('Unknown label type: %s' % repr(ys))\n    if is_array_api:\n        unique_ys = xp.concat([_unique_labels(y) for y in ys])\n        return xp.unique_values(unique_ys)\n    ys_labels = set(chain.from_iterable(((i for i in _unique_labels(y)) for y in ys)))\n    if len(set((isinstance(label, str) for label in ys_labels))) > 1:\n        raise ValueError('Mix of label input types (string and number)')\n    return xp.asarray(sorted(ys_labels))",
    ".sklearn.utils.multiclass.py@@_unique_multiclass": "def _unique_multiclass(y):\n    xp, is_array_api = get_namespace(y)\n    if hasattr(y, '__array__') or is_array_api:\n        return xp.unique_values(xp.asarray(y))\n    else:\n        return set(y)",
    ".sklearn.utils._set_output.py@@wrapped": "def wrapped(self, X, *args, **kwargs):\n    data_to_wrap = f(self, X, *args, **kwargs)\n    if isinstance(data_to_wrap, tuple):\n        return (_wrap_data_with_container(method, data_to_wrap[0], X, self), *data_to_wrap[1:])\n    return _wrap_data_with_container(method, data_to_wrap, X, self)",
    ".sklearn.preprocessing._label.py@@LabelBinarizer.transform": "def transform(self, y):\n    check_is_fitted(self)\n    y_is_multilabel = type_of_target(y).startswith('multilabel')\n    if y_is_multilabel and (not self.y_type_.startswith('multilabel')):\n        raise ValueError('The object was not fitted with multilabel input.')\n    return label_binarize(y, classes=self.classes_, pos_label=self.pos_label, neg_label=self.neg_label, sparse_output=self.sparse_output)",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    if isclass(estimator):\n        raise TypeError('{} is a class, not an instance.'.format(estimator))\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if attributes is not None:\n        if not isinstance(attributes, (list, tuple)):\n            attributes = [attributes]\n        fitted = all_or_any([hasattr(estimator, attr) for attr in attributes])\n    elif hasattr(estimator, '__sklearn_is_fitted__'):\n        fitted = estimator.__sklearn_is_fitted__()\n    else:\n        fitted = [v for v in vars(estimator) if v.endswith('_') and (not v.startswith('__'))]\n    if not fitted:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.preprocessing._label.py@@label_binarize": "def label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False):\n    if not isinstance(y, list):\n        y = check_array(y, input_name='y', accept_sparse='csr', ensure_2d=False, dtype=None)\n    elif _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    if neg_label >= pos_label:\n        raise ValueError('neg_label={0} must be strictly less than pos_label={1}.'.format(neg_label, pos_label))\n    if sparse_output and (pos_label == 0 or neg_label != 0):\n        raise ValueError('Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={0} and neg_label={1}'.format(pos_label, neg_label))\n    pos_switch = pos_label == 0\n    if pos_switch:\n        pos_label = -neg_label\n    y_type = type_of_target(y)\n    if 'multioutput' in y_type:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if y_type == 'unknown':\n        raise ValueError('The type of target data is not known')\n    n_samples = y.shape[0] if sp.issparse(y) else len(y)\n    n_classes = len(classes)\n    classes = np.asarray(classes)\n    if y_type == 'binary':\n        if n_classes == 1:\n            if sparse_output:\n                return sp.csr_matrix((n_samples, 1), dtype=int)\n            else:\n                Y = np.zeros((len(y), 1), dtype=int)\n                Y += neg_label\n                return Y\n        elif len(classes) >= 3:\n            y_type = 'multiclass'\n    sorted_class = np.sort(classes)\n    if y_type == 'multilabel-indicator':\n        y_n_classes = y.shape[1] if hasattr(y, 'shape') else len(y[0])\n        if classes.size != y_n_classes:\n            raise ValueError('classes {0} mismatch with the labels {1} found in the data'.format(classes, unique_labels(y)))\n    if y_type in ('binary', 'multiclass'):\n        y = column_or_1d(y)\n        y_in_classes = np.in1d(y, classes)\n        y_seen = y[y_in_classes]\n        indices = np.searchsorted(sorted_class, y_seen)\n        indptr = np.hstack((0, np.cumsum(y_in_classes)))\n        data = np.empty_like(indices)\n        data.fill(pos_label)\n        Y = sp.csr_matrix((data, indices, indptr), shape=(n_samples, n_classes))\n    elif y_type == 'multilabel-indicator':\n        Y = sp.csr_matrix(y)\n        if pos_label != 1:\n            data = np.empty_like(Y.data)\n            data.fill(pos_label)\n            Y.data = data\n    else:\n        raise ValueError('%s target data is not supported with label binarization' % y_type)\n    if not sparse_output:\n        Y = Y.toarray()\n        Y = Y.astype(int, copy=False)\n        if neg_label != 0:\n            Y[Y == 0] = neg_label\n        if pos_switch:\n            Y[Y == pos_label] = 0\n    else:\n        Y.data = Y.data.astype(int, copy=False)\n    if np.any(classes != sorted_class):\n        indices = np.searchsorted(sorted_class, classes)\n        Y = Y[:, indices]\n    if y_type == 'binary':\n        if sparse_output:\n            Y = Y.getcol(-1)\n        else:\n            Y = Y[:, -1].reshape((-1, 1))\n    return Y",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, *, dtype=None, warn=False):\n    xp, _ = get_namespace(y)\n    y = check_array(y, ensure_2d=False, dtype=dtype, input_name='y', force_all_finite=False, ensure_min_samples=0)\n    shape = y.shape\n    if len(shape) == 1:\n        return _asarray_with_order(xp.reshape(y, -1), order='C', xp=xp)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return _asarray_with_order(xp.reshape(y, -1), order='C', xp=xp)\n    raise ValueError('y should be a 1d array, got an array of shape {} instead.'.format(shape))",
    ".sklearn.utils._set_output.py@@_wrap_data_with_container": "def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n    output_config = _get_output_config(method, estimator)\n    if output_config['dense'] == 'default' or not _auto_wrap_is_configured(estimator):\n        return data_to_wrap\n    return _wrap_in_pandas_container(data_to_wrap=data_to_wrap, index=getattr(original_input, 'index', None), columns=estimator.get_feature_names_out)",
    ".sklearn.utils._set_output.py@@_get_output_config": "def _get_output_config(method, estimator=None):\n    est_sklearn_output_config = getattr(estimator, '_sklearn_output_config', {})\n    if method in est_sklearn_output_config:\n        dense_config = est_sklearn_output_config[method]\n    else:\n        dense_config = get_config()[f'{method}_output']\n    if dense_config not in {'default', 'pandas'}:\n        raise ValueError(f\"output config must be 'default' or 'pandas' got {dense_config}\")\n    return {'dense': dense_config}",
    ".sklearn.utils._mocking.py@@MockDataFrame.__array__": "def __array__(self, dtype=None):\n    return self.array",
    ".sklearn.utils.validation.py@@_pandas_dtype_needs_early_conversion": "def _pandas_dtype_needs_early_conversion(pd_dtype):\n    from pandas.api.types import is_bool_dtype, is_sparse, is_float_dtype, is_integer_dtype\n    if is_bool_dtype(pd_dtype):\n        return True\n    if is_sparse(pd_dtype):\n        return False\n    try:\n        from pandas.api.types import is_extension_array_dtype\n    except ImportError:\n        return False\n    if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n        return False\n    elif is_float_dtype(pd_dtype):\n        return True\n    elif is_integer_dtype(pd_dtype):\n        return True\n    return False",
    ".sklearn.utils.fixes.py@@_object_dtype_isnan": "def _object_dtype_isnan(X):\n    return X != X"
}