{
    ".sklearn.base.py@@BaseEstimator._validate_params": "def _validate_params(self):\n    validate_parameter_constraints(self._parameter_constraints, self.get_params(deep=False), caller_name=self.__class__.__name__)",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key)\n        if deep and hasattr(value, 'get_params') and (not isinstance(value, type)):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.utils._param_validation.py@@validate_parameter_constraints": "def validate_parameter_constraints(parameter_constraints, params, caller_name):\n    for param_name, param_val in params.items():\n        if param_name not in parameter_constraints:\n            continue\n        constraints = parameter_constraints[param_name]\n        if constraints == 'no_validation':\n            continue\n        constraints = [make_constraint(constraint) for constraint in constraints]\n        for constraint in constraints:\n            if constraint.is_satisfied_by(param_val):\n                break\n        else:\n            constraints = [constraint for constraint in constraints if not constraint.hidden]\n            if len(constraints) == 1:\n                constraints_str = f'{constraints[0]}'\n            else:\n                constraints_str = f'{', '.join([str(c) for c in constraints[:-1]])} or {constraints[-1]}'\n            raise InvalidParameterError(f'The {param_name!r} parameter of {caller_name} must be {constraints_str}. Got {param_val!r} instead.')",
    ".sklearn.utils._param_validation.py@@make_constraint": "def make_constraint(constraint):\n    if isinstance(constraint, str) and constraint == 'array-like':\n        return _ArrayLikes()\n    if isinstance(constraint, str) and constraint == 'sparse matrix':\n        return _SparseMatrices()\n    if isinstance(constraint, str) and constraint == 'random_state':\n        return _RandomStates()\n    if constraint is callable:\n        return _Callables()\n    if constraint is None:\n        return _NoneConstraint()\n    if isinstance(constraint, type):\n        return _InstancesOf(constraint)\n    if isinstance(constraint, (Interval, StrOptions, Options, HasMethods)):\n        return constraint\n    if isinstance(constraint, str) and constraint == 'boolean':\n        return _Booleans()\n    if isinstance(constraint, str) and constraint == 'verbose':\n        return _VerboseHelper()\n    if isinstance(constraint, str) and constraint == 'missing_values':\n        return _MissingValues()\n    if isinstance(constraint, str) and constraint == 'cv_object':\n        return _CVObjects()\n    if isinstance(constraint, Hidden):\n        constraint = make_constraint(constraint.constraint)\n        constraint.hidden = True\n        return constraint\n    raise ValueError(f'Unknown constraint type: {constraint}')",
    ".sklearn.utils._param_validation.py@@Interval.is_satisfied_by": "def is_satisfied_by(self, val):\n    if not isinstance(val, self.type):\n        return False\n    return val in self",
    ".sklearn.utils._param_validation.py@@Interval.__contains__": "def __contains__(self, val):\n    if np.isnan(val):\n        return False\n    left_cmp = operator.lt if self.closed in ('left', 'both') else operator.le\n    right_cmp = operator.gt if self.closed in ('right', 'both') else operator.ge\n    left = -np.inf if self.left is None else self.left\n    right = np.inf if self.right is None else self.right\n    if left_cmp(val, left):\n        return False\n    if right_cmp(val, right):\n        return False\n    return True",
    ".sklearn.utils._param_validation.py@@_Booleans.__init__": "def __init__(self):\n    super().__init__()\n    self._constraints = [_InstancesOf(bool), _InstancesOf(np.bool_), _InstancesOf(Integral)]",
    ".sklearn.utils._param_validation.py@@_Constraint.__init__": "def __init__(self):\n    self.hidden = False",
    ".sklearn.utils._param_validation.py@@_InstancesOf.__init__": "def __init__(self, type):\n    super().__init__()\n    self.type = type",
    ".sklearn.utils._param_validation.py@@_Booleans.is_satisfied_by": "def is_satisfied_by(self, val):\n    if isinstance(val, Integral) and (not isinstance(val, bool)):\n        warnings.warn(\"Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\", FutureWarning)\n    return any((c.is_satisfied_by(val) for c in self._constraints))",
    ".sklearn.utils._param_validation.py@@_InstancesOf.is_satisfied_by": "def is_satisfied_by(self, val):\n    return isinstance(val, self.type)",
    ".sklearn.utils._param_validation.py@@Options.is_satisfied_by": "def is_satisfied_by(self, val):\n    return isinstance(val, self.type) and val in self.options",
    ".sklearn.utils._param_validation.py@@_NoneConstraint.is_satisfied_by": "def is_satisfied_by(self, val):\n    return val is None",
    ".sklearn.utils._param_validation.py@@_RandomStates.__init__": "def __init__(self):\n    super().__init__()\n    self._constraints = [Interval(Integral, 0, 2 ** 32 - 1, closed='both'), _InstancesOf(np.random.RandomState), _NoneConstraint()]",
    ".sklearn.utils._param_validation.py@@Interval.__init__": "def __init__(self, type, left, right, *, closed):\n    super().__init__()\n    self.type = type\n    self.left = left\n    self.right = right\n    self.closed = closed\n    self._check_params()",
    ".sklearn.utils._param_validation.py@@Interval._check_params": "def _check_params(self):\n    if self.type not in (Integral, Real, RealNotInt):\n        raise ValueError(f'type must be either numbers.Integral, numbers.Real or RealNotInt. Got {self.type} instead.')\n    if self.closed not in ('left', 'right', 'both', 'neither'):\n        raise ValueError(f\"closed must be either 'left', 'right', 'both' or 'neither'. Got {self.closed} instead.\")\n    if self.type is Integral:\n        suffix = 'for an interval over the integers.'\n        if self.left is not None and (not isinstance(self.left, Integral)):\n            raise TypeError(f'Expecting left to be an int {suffix}')\n        if self.right is not None and (not isinstance(self.right, Integral)):\n            raise TypeError(f'Expecting right to be an int {suffix}')\n        if self.left is None and self.closed in ('left', 'both'):\n            raise ValueError(f\"left can't be None when closed == {self.closed} {suffix}\")\n        if self.right is None and self.closed in ('right', 'both'):\n            raise ValueError(f\"right can't be None when closed == {self.closed} {suffix}\")\n    else:\n        if self.left is not None and (not isinstance(self.left, Real)):\n            raise TypeError('Expecting left to be a real number.')\n        if self.right is not None and (not isinstance(self.right, Real)):\n            raise TypeError('Expecting right to be a real number.')\n    if self.right is not None and self.left is not None and (self.right <= self.left):\n        raise ValueError(f\"right can't be less than left. Got left={self.left} and right={self.right}\")",
    ".sklearn.utils._param_validation.py@@_RandomStates.is_satisfied_by": "def is_satisfied_by(self, val):\n    return any((c.is_satisfied_by(val) for c in self._constraints))",
    ".sklearn.utils._param_validation.py@@_VerboseHelper.__init__": "def __init__(self):\n    super().__init__()\n    self._constraints = [Interval(Integral, 0, None, closed='left'), _InstancesOf(bool), _InstancesOf(np.bool_)]",
    ".sklearn.utils._param_validation.py@@_VerboseHelper.is_satisfied_by": "def is_satisfied_by(self, val):\n    return any((c.is_satisfied_by(val) for c in self._constraints))",
    ".sklearn.base.py@@BaseEstimator._validate_data": "def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):\n    self._check_feature_names(X, reset=reset)\n    if y is None and self._get_tags()['requires_y']:\n        raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')\n    no_val_X = isinstance(X, str) and X == 'no_validation'\n    no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')\n    default_check_params = {'estimator': self}\n    check_params = {**default_check_params, **check_params}\n    if no_val_X and no_val_y:\n        raise ValueError('Validation should be done on X, y or both.')\n    elif not no_val_X and no_val_y:\n        if cast_to_ndarray:\n            X = check_array(X, input_name='X', **check_params)\n        out = X\n    elif no_val_X and (not no_val_y):\n        if cast_to_ndarray:\n            y = _check_y(y, **check_params) if cast_to_ndarray else y\n        out = y\n    else:\n        if validate_separately and cast_to_ndarray:\n            check_X_params, check_y_params = validate_separately\n            if 'estimator' not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name='X', **check_X_params)\n            if 'estimator' not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name='y', **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = (X, y)\n    if not no_val_X and check_params.get('ensure_2d', True):\n        self._check_n_features(X, reset=reset)\n    return out",
    ".sklearn.base.py@@BaseEstimator._check_feature_names": "def _check_feature_names(self, X, *, reset):\n    if reset:\n        feature_names_in = _get_feature_names(X)\n        if feature_names_in is not None:\n            self.feature_names_in_ = feature_names_in\n        elif hasattr(self, 'feature_names_in_'):\n            delattr(self, 'feature_names_in_')\n        return\n    fitted_feature_names = getattr(self, 'feature_names_in_', None)\n    X_feature_names = _get_feature_names(X)\n    if fitted_feature_names is None and X_feature_names is None:\n        return\n    if X_feature_names is not None and fitted_feature_names is None:\n        warnings.warn(f'X has feature names, but {self.__class__.__name__} was fitted without feature names')\n        return\n    if X_feature_names is None and fitted_feature_names is not None:\n        warnings.warn(f'X does not have valid feature names, but {self.__class__.__name__} was fitted with feature names')\n        return\n    if len(fitted_feature_names) != len(X_feature_names) or np.any(fitted_feature_names != X_feature_names):\n        message = 'The feature names should match those that were passed during fit.\\n'\n        fitted_feature_names_set = set(fitted_feature_names)\n        X_feature_names_set = set(X_feature_names)\n        unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)\n        missing_names = sorted(fitted_feature_names_set - X_feature_names_set)\n\n        def add_names(names):\n            output = ''\n            max_n_names = 5\n            for i, name in enumerate(names):\n                if i >= max_n_names:\n                    output += '- ...\\n'\n                    break\n                output += f'- {name}\\n'\n            return output\n        if unexpected_names:\n            message += 'Feature names unseen at fit time:\\n'\n            message += add_names(unexpected_names)\n        if missing_names:\n            message += 'Feature names seen at fit time, yet now missing:\\n'\n            message += add_names(missing_names)\n        if not missing_names and (not unexpected_names):\n            message += 'Feature names must be in the same order as they were in fit.\\n'\n        raise ValueError(message)",
    ".sklearn.utils.validation.py@@_get_feature_names": "def _get_feature_names(X):\n    feature_names = None\n    if hasattr(X, 'columns'):\n        feature_names = np.asarray(X.columns, dtype=object)\n    if feature_names is None or len(feature_names) == 0:\n        return\n    types = sorted((t.__qualname__ for t in set((type(v) for v in feature_names))))\n    if len(types) > 1 and 'str' in types:\n        raise TypeError(f'Feature names are only supported if all input features have string names, but your input has {types} as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.')\n    if len(types) == 1 and types[0] == 'str':\n        return feature_names",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=''):\n    if isinstance(array, np.matrix):\n        raise TypeError('np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html')\n    xp, is_array_api = get_namespace(array)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    pandas_requires_conversion = False\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if not hasattr(array, 'sparse') and array.dtypes.apply(is_sparse).any():\n                warnings.warn('pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.')\n        dtypes_orig = list(array.dtypes)\n        pandas_requires_conversion = any((_pandas_dtype_needs_early_conversion(i) for i in dtypes_orig))\n        if all((isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig)):\n            dtype_orig = np.result_type(*dtypes_orig)\n    elif hasattr(array, 'iloc') and hasattr(array, 'dtype'):\n        pandas_requires_conversion = _pandas_dtype_needs_early_conversion(array.dtype)\n        if isinstance(array.dtype, np.dtype):\n            dtype_orig = array.dtype\n        else:\n            dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = xp.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if pandas_requires_conversion:\n        new_dtype = dtype_orig if dtype is None else dtype\n        array = array.astype(new_dtype)\n        dtype = None\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    estimator_name = _check_estimator_name(estimator)\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if hasattr(array, 'sparse') and array.ndim > 1:\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if array.dtypes.apply(is_sparse).all():\n                array = array.sparse.to_coo()\n                if array.dtype == np.dtype('object'):\n                    unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])\n                    if len(unique_dtypes) > 1:\n                        raise ValueError('Pandas DataFrame with mixed sparse extension arrays generated a sparse matrix with object dtype which can not be converted to a scipy sparse matrix.Sparse extension arrays should all have the same numeric type.')\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse, estimator_name=estimator_name, input_name=input_name)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = _asarray_with_order(array, order=order, xp=xp)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype, estimator_name=estimator_name, input_name=input_name)\n                    array = xp.astype(array, dtype, copy=False)\n                else:\n                    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            except ComplexWarning as complex_warning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array)) from complex_warning\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and array.dtype.kind in 'USV':\n            raise ValueError(\"dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.\")\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, input_name=input_name, estimator_name=estimator_name, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if copy:\n        if xp.__name__ in {'numpy', 'numpy.array_api'}:\n            if np.may_share_memory(array, array_orig):\n                array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n        else:\n            array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n    return array",
    ".sklearn.utils._array_api.py@@get_namespace": "def get_namespace(*arrays):\n    if not get_config()['array_api_dispatch']:\n        return (_NumPyApiWrapper(), False)\n    namespaces = {x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in arrays if not isinstance(x, (bool, int, float, complex))}\n    if not namespaces:\n        raise ValueError('Unrecognized array input')\n    if len(namespaces) != 1:\n        raise ValueError(f'Multiple namespaces for array inputs: {namespaces}')\n    xp, = namespaces\n    if xp is None:\n        return (_NumPyApiWrapper(), False)\n    return (_ArrayAPIWrapper(xp), True)",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _get_threadlocal_config().copy()",
    ".sklearn._config.py@@_get_threadlocal_config": "def _get_threadlocal_config():\n    if not hasattr(_threadlocal, 'global_config'):\n        _threadlocal.global_config = _global_config.copy()\n    return _threadlocal.global_config",
    ".sklearn.utils.validation.py@@_check_estimator_name": "def _check_estimator_name(estimator):\n    if estimator is not None:\n        if isinstance(estimator, str):\n            return estimator\n        else:\n            return estimator.__class__.__name__\n    return None",
    ".sklearn.utils._array_api.py@@_asarray_with_order": "def _asarray_with_order(array, dtype=None, order=None, copy=None, xp=None):\n    if xp is None:\n        xp, _ = get_namespace(array)\n    if xp.__name__ in {'numpy', 'numpy.array_api'}:\n        array = numpy.asarray(array, order=order, dtype=dtype)\n        return xp.asarray(array, copy=copy)\n    else:\n        return xp.asarray(array, dtype=dtype, copy=copy)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.__getattr__": "def __getattr__(self, name):\n    return getattr(numpy, name)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.asarray": "def asarray(self, x, *, dtype=None, device=None, copy=None):\n    if copy is True:\n        return numpy.array(x, copy=True, dtype=dtype)\n    else:\n        return numpy.asarray(x, dtype=dtype)",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None, estimator_name=None, input_name=''):\n    xp, _ = get_namespace(X)\n    if _get_config()['assume_finite']:\n        return\n    X = xp.asarray(X)\n    if X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')\n    if X.dtype.kind not in 'fc':\n        return\n    with np.errstate(over='ignore'):\n        first_pass_isfinite = xp.isfinite(xp.sum(X))\n    if first_pass_isfinite:\n        return\n    use_cython = xp is np and X.data.contiguous and (X.dtype.type in {np.float32, np.float64})\n    if use_cython:\n        out = cy_isfinite(X.reshape(-1), allow_nan=allow_nan)\n        has_nan_error = False if allow_nan else out == FiniteStatus.has_nan\n        has_inf = out == FiniteStatus.has_infinite\n    else:\n        has_inf = xp.any(xp.isinf(X))\n        has_nan_error = False if allow_nan else xp.any(xp.isnan(X))\n    if has_inf or has_nan_error:\n        if has_nan_error:\n            type_err = 'NaN'\n        else:\n            msg_dtype = msg_dtype if msg_dtype is not None else X.dtype\n            type_err = f'infinity or a value too large for {msg_dtype!r}'\n        padded_input_name = input_name + ' ' if input_name else ''\n        msg_err = f'Input {padded_input_name}contains {type_err}.'\n        if estimator_name and input_name == 'X' and has_nan_error:\n            msg_err += f'\\n{estimator_name} does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values'\n        raise ValueError(msg_err)",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error",
    ".sklearn.base.py@@BaseEstimator._check_n_features": "def _check_n_features(self, X, reset):\n    try:\n        n_features = _num_features(X)\n    except TypeError as e:\n        if not reset and hasattr(self, 'n_features_in_'):\n            raise ValueError(f'X does not contain any features, but {self.__class__.__name__} is expecting {self.n_features_in_} features') from e\n        return\n    if reset:\n        self.n_features_in_ = n_features\n        return\n    if not hasattr(self, 'n_features_in_'):\n        return\n    if n_features != self.n_features_in_:\n        raise ValueError(f'X has {n_features} features, but {self.__class__.__name__} is expecting {self.n_features_in_} features as input.')",
    ".sklearn.utils.validation.py@@_num_features": "def _num_features(X):\n    type_ = type(X)\n    if type_.__module__ == 'builtins':\n        type_name = type_.__qualname__\n    else:\n        type_name = f'{type_.__module__}.{type_.__qualname__}'\n    message = f'Unable to find the number of features from X of type {type_name}'\n    if not hasattr(X, '__len__') and (not hasattr(X, 'shape')):\n        if not hasattr(X, '__array__'):\n            raise TypeError(message)\n        X = np.asarray(X)\n    if hasattr(X, 'shape'):\n        if not hasattr(X.shape, '__len__') or len(X.shape) <= 1:\n            message += f' with shape {X.shape}'\n            raise TypeError(message)\n        return X.shape[1]\n    first_sample = X[0]\n    if isinstance(first_sample, (str, bytes, dict)):\n        message += f' where the samples are of type {type(first_sample).__qualname__}'\n        raise TypeError(message)\n    try:\n        return len(first_sample)\n    except Exception as err:\n        raise TypeError(message) from err",
    ".sklearn.cluster._kmeans.py@@MiniBatchKMeans._check_params_vs_input": "def _check_params_vs_input(self, X):\n    super()._check_params_vs_input(X, default_n_init=3)\n    self._batch_size = min(self.batch_size, X.shape[0])\n    self._init_size = self.init_size\n    if self._init_size is None:\n        self._init_size = 3 * self._batch_size\n        if self._init_size < self.n_clusters:\n            self._init_size = 3 * self.n_clusters\n    elif self._init_size < self.n_clusters:\n        warnings.warn(f'init_size={self._init_size} should be larger than n_clusters={self.n_clusters}. Setting it to min(3*n_clusters, n_samples)', RuntimeWarning, stacklevel=2)\n        self._init_size = 3 * self.n_clusters\n    self._init_size = min(self._init_size, X.shape[0])\n    if self.reassignment_ratio < 0:\n        raise ValueError(f'reassignment_ratio should be >= 0, got {self.reassignment_ratio} instead.')",
    ".sklearn.cluster._kmeans.py@@_BaseKMeans._check_params_vs_input": "def _check_params_vs_input(self, X, default_n_init=None):\n    if X.shape[0] < self.n_clusters:\n        raise ValueError(f'n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.')\n    self._tol = _tolerance(X, self.tol)\n    self._n_init = self.n_init\n    if self._n_init == 'warn':\n        warnings.warn(f\"The default value of `n_init` will change from {default_n_init} to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\", FutureWarning)\n        self._n_init = default_n_init\n    if self._n_init == 'auto':\n        if self.init == 'k-means++':\n            self._n_init = 1\n        else:\n            self._n_init = default_n_init\n    if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n        warnings.warn(f'Explicit initial center position passed: performing only one init in {self.__class__.__name__} instead of n_init={self._n_init}.', RuntimeWarning, stacklevel=2)\n        self._n_init = 1",
    ".sklearn.cluster._kmeans.py@@_tolerance": "def _tolerance(X, tol):\n    if tol == 0:\n        return 0\n    if sp.issparse(X):\n        variances = mean_variance_axis(X, axis=0)[1]\n    else:\n        variances = np.var(X, axis=0)\n    return np.mean(variances) * tol",
    ".sklearn.utils.validation.py@@_is_arraylike_not_scalar": "def _is_arraylike_not_scalar(array):\n    return _is_arraylike(array) and (not np.isscalar(array))",
    ".sklearn.utils.validation.py@@_is_arraylike": "def _is_arraylike(x):\n    return hasattr(x, '__len__') or hasattr(x, 'shape') or hasattr(x, '__array__')",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.utils.validation.py@@_check_sample_weight": "def _check_sample_weight(sample_weight, X, dtype=None, copy=False, only_non_negative=False):\n    n_samples = _num_samples(X)\n    if dtype is not None and dtype not in [np.float32, np.float64]:\n        dtype = np.float64\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=dtype)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)\n    else:\n        if dtype is None:\n            dtype = [np.float64, np.float32]\n        sample_weight = check_array(sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype, order='C', copy=copy, input_name='sample_weight')\n        if sample_weight.ndim != 1:\n            raise ValueError('Sample weights must be 1D array or scalar')\n        if sample_weight.shape != (n_samples,):\n            raise ValueError('sample_weight.shape == {}, expected {}!'.format(sample_weight.shape, (n_samples,)))\n    if only_non_negative:\n        check_non_negative(sample_weight, '`sample_weight`')\n    return sample_weight",
    ".sklearn.cluster._kmeans.py@@_BaseKMeans._check_mkl_vcomp": "def _check_mkl_vcomp(self, X, n_samples):\n    if sp.issparse(X):\n        return\n    n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n    if n_active_threads < self._n_threads:\n        modules = threadpool_info()\n        has_vcomp = 'vcomp' in [module['prefix'] for module in modules]\n        has_mkl = ('mkl', 'intel') in [(module['internal_api'], module.get('threading_layer', None)) for module in modules]\n        if has_vcomp and has_mkl:\n            self._warn_mkl_vcomp(n_active_threads)",
    ".sklearn.utils.fixes.py@@threadpool_info": "def threadpool_info():\n    controller = _get_threadpool_controller()\n    if controller is not None:\n        return controller.info()\n    else:\n        return threadpoolctl.threadpool_info()",
    ".sklearn.utils.fixes.py@@_get_threadpool_controller": "def _get_threadpool_controller():\n    if not hasattr(threadpoolctl, 'ThreadpoolController'):\n        return None\n    if not hasattr(sklearn, '_sklearn_threadpool_controller'):\n        sklearn._sklearn_threadpool_controller = threadpoolctl.ThreadpoolController()\n    return sklearn._sklearn_threadpool_controller",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.cluster._kmeans.py@@_BaseKMeans._init_centroids": "def _init_centroids(self, X, x_squared_norms, init, random_state, init_size=None, n_centroids=None, sample_weight=None):\n    n_samples = X.shape[0]\n    n_clusters = self.n_clusters if n_centroids is None else n_centroids\n    if init_size is not None and init_size < n_samples:\n        init_indices = random_state.randint(0, n_samples, init_size)\n        X = X[init_indices]\n        x_squared_norms = x_squared_norms[init_indices]\n        n_samples = X.shape[0]\n        sample_weight = sample_weight[init_indices]\n    if isinstance(init, str) and init == 'k-means++':\n        centers, _ = _kmeans_plusplus(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms, sample_weight=sample_weight)\n    elif isinstance(init, str) and init == 'random':\n        seeds = random_state.choice(n_samples, size=n_clusters, replace=False, p=sample_weight / sample_weight.sum())\n        centers = X[seeds]\n    elif _is_arraylike_not_scalar(self.init):\n        centers = init\n    elif callable(init):\n        centers = init(X, n_clusters, random_state=random_state)\n        centers = check_array(centers, dtype=X.dtype, copy=False, order='C')\n        self._validate_center_shape(X, centers)\n    if sp.issparse(centers):\n        centers = centers.toarray()\n    return centers",
    ".sklearn.cluster._kmeans.py@@_kmeans_plusplus": "def _kmeans_plusplus(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials=None):\n    n_samples, n_features = X.shape\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n    center_id = random_state.choice(n_samples, p=sample_weight / sample_weight.sum())\n    indices = np.full(n_clusters, -1, dtype=int)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n    indices[0] = center_id\n    closest_dist_sq = _euclidean_distances(centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq @ sample_weight\n    for c in range(1, n_clusters):\n        rand_vals = random_state.uniform(size=n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(sample_weight * closest_dist_sq), rand_vals)\n        np.clip(candidate_ids, None, closest_dist_sq.size - 1, out=candidate_ids)\n        distance_to_candidates = _euclidean_distances(X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n        np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n        candidates_pot = distance_to_candidates @ sample_weight.reshape(-1, 1)\n        best_candidate = np.argmin(candidates_pot)\n        current_pot = candidates_pot[best_candidate]\n        closest_dist_sq = distance_to_candidates[best_candidate]\n        best_candidate = candidate_ids[best_candidate]\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        indices[c] = best_candidate\n    return (centers, indices)",
    ".sklearn.metrics.pairwise.py@@_euclidean_distances": "def _euclidean_distances(X, Y, X_norm_squared=None, Y_norm_squared=None, squared=False):\n    if X_norm_squared is not None:\n        if X_norm_squared.dtype == np.float32:\n            XX = None\n        else:\n            XX = X_norm_squared.reshape(-1, 1)\n    elif X.dtype == np.float32:\n        XX = None\n    else:\n        XX = row_norms(X, squared=True)[:, np.newaxis]\n    if Y is X:\n        YY = None if XX is None else XX.T\n    elif Y_norm_squared is not None:\n        if Y_norm_squared.dtype == np.float32:\n            YY = None\n        else:\n            YY = Y_norm_squared.reshape(1, -1)\n    elif Y.dtype == np.float32:\n        YY = None\n    else:\n        YY = row_norms(Y, squared=True)[np.newaxis, :]\n    if X.dtype == np.float32:\n        distances = _euclidean_distances_upcast(X, XX, Y, YY)\n    else:\n        distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n        distances += XX\n        distances += YY\n    np.maximum(distances, 0, out=distances)\n    if X is Y:\n        np.fill_diagonal(distances, 0)\n    return distances if squared else np.sqrt(distances, out=distances)",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, *, dense_output=False):\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n    if sparse.issparse(a) and sparse.issparse(b) and dense_output and hasattr(ret, 'toarray'):\n        return ret.toarray()\n    return ret",
    ".sklearn.utils.extmath.py@@stable_cumsum": "def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):\n    out = np.cumsum(arr, axis=axis, dtype=np.float64)\n    expected = np.sum(arr, axis=axis, dtype=np.float64)\n    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=True)):\n        warnings.warn('cumsum was found to be unstable: its last element does not correspond to sum', RuntimeWarning)\n    return out",
    ".sklearn.cluster._kmeans.py@@_labels_inertia_threadpool_limit": "def _labels_inertia_threadpool_limit(X, sample_weight, centers, n_threads=1, return_inertia=True):\n    with threadpool_limits(limits=1, user_api='blas'):\n        result = _labels_inertia(X, sample_weight, centers, n_threads, return_inertia)\n    return result",
    ".sklearn.utils.fixes.py@@threadpool_limits": "def threadpool_limits(limits=None, user_api=None):\n    controller = _get_threadpool_controller()\n    if controller is not None:\n        return controller.limit(limits=limits, user_api=user_api)\n    else:\n        return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)",
    ".sklearn.cluster._kmeans.py@@_labels_inertia": "def _labels_inertia(X, sample_weight, centers, n_threads=1, return_inertia=True):\n    n_samples = X.shape[0]\n    n_clusters = centers.shape[0]\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    center_shift = np.zeros(n_clusters, dtype=centers.dtype)\n    if sp.issparse(X):\n        _labels = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        _labels = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n    _labels(X, sample_weight, centers, centers_new=None, weight_in_clusters=None, labels=labels, center_shift=center_shift, n_threads=n_threads, update_centers=False)\n    if return_inertia:\n        inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n        return (labels, inertia)\n    return labels",
    ".sklearn.cluster._kmeans.py@@MiniBatchKMeans._random_reassign": "def _random_reassign(self):\n    self._n_since_last_reassign += self._batch_size\n    if (self._counts == 0).any() or self._n_since_last_reassign >= 10 * self.n_clusters:\n        self._n_since_last_reassign = 0\n        return True\n    return False",
    ".sklearn.cluster._kmeans.py@@_mini_batch_step": "def _mini_batch_step(X, sample_weight, centers, centers_new, weight_sums, random_state, random_reassign=False, reassignment_ratio=0.01, verbose=False, n_threads=1):\n    labels, inertia = _labels_inertia(X, sample_weight, centers, n_threads=n_threads)\n    if sp.issparse(X):\n        _minibatch_update_sparse(X, sample_weight, centers, centers_new, weight_sums, labels, n_threads)\n    else:\n        _minibatch_update_dense(X, sample_weight, centers, centers_new, weight_sums, labels, n_threads)\n    if random_reassign and reassignment_ratio > 0:\n        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()\n        if to_reassign.sum() > 0.5 * X.shape[0]:\n            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]):]\n            to_reassign[indices_dont_reassign] = False\n        n_reassigns = to_reassign.sum()\n        if n_reassigns:\n            new_centers = random_state.choice(X.shape[0], replace=False, size=n_reassigns)\n            if verbose:\n                print(f'[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.')\n            if sp.issparse(X):\n                assign_rows_csr(X, new_centers.astype(np.intp, copy=False), np.where(to_reassign)[0].astype(np.intp, copy=False), centers_new)\n            else:\n                centers_new[to_reassign] = X[new_centers]\n        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])\n    return inertia",
    ".sklearn.cluster._kmeans.py@@MiniBatchKMeans._mini_batch_convergence": "def _mini_batch_convergence(self, step, n_steps, n_samples, centers_squared_diff, batch_inertia):\n    batch_inertia /= self._batch_size\n    step = step + 1\n    if step == 1:\n        if self.verbose:\n            print(f'Minibatch step {step}/{n_steps}: mean batch inertia: {batch_inertia}')\n        return False\n    if self._ewa_inertia is None:\n        self._ewa_inertia = batch_inertia\n    else:\n        alpha = self._batch_size * 2.0 / (n_samples + 1)\n        alpha = min(alpha, 1)\n        self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n    if self.verbose:\n        print(f'Minibatch step {step}/{n_steps}: mean batch inertia: {batch_inertia}, ewa inertia: {self._ewa_inertia}')\n    if self._tol > 0.0 and centers_squared_diff <= self._tol:\n        if self.verbose:\n            print(f'Converged (small centers change) at step {step}/{n_steps}')\n        return True\n    if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n        self._no_improvement = 0\n        self._ewa_inertia_min = self._ewa_inertia\n    else:\n        self._no_improvement += 1\n    if self.max_no_improvement is not None and self._no_improvement >= self.max_no_improvement:\n        if self.verbose:\n            print(f'Converged (lack of improvement in inertia) at step {step}/{n_steps}')\n        return True\n    return False",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name=None, input_name=''):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan', estimator_name=estimator_name, input_name=input_name)\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.utils._param_validation.py@@_Callables.is_satisfied_by": "def is_satisfied_by(self, val):\n    return callable(val)",
    ".sklearn.utils._param_validation.py@@_ArrayLikes.is_satisfied_by": "def is_satisfied_by(self, val):\n    return _is_arraylike_not_scalar(val)",
    ".sklearn.cluster._kmeans.py@@_BaseKMeans._validate_center_shape": "def _validate_center_shape(self, X, centers):\n    if centers.shape[0] != self.n_clusters:\n        raise ValueError(f'The shape of the initial centers {centers.shape} does not match the number of clusters {self.n_clusters}.')\n    if centers.shape[1] != X.shape[1]:\n        raise ValueError(f'The shape of the initial centers {centers.shape} does not match the number of features of the data {X.shape[1]}.')",
    ".sklearn.metrics.pairwise.py@@_euclidean_distances_upcast": "def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):\n    n_samples_X = X.shape[0]\n    n_samples_Y = Y.shape[0]\n    n_features = X.shape[1]\n    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)\n    if batch_size is None:\n        x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1\n        y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1\n        maxmem = max(((x_density * n_samples_X + y_density * n_samples_Y) * n_features + x_density * n_samples_X * y_density * n_samples_Y) / 10, 10 * 2 ** 17)\n        tmp = (x_density + y_density) * n_features\n        batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) / 2\n        batch_size = max(int(batch_size), 1)\n    x_batches = gen_batches(n_samples_X, batch_size)\n    for i, x_slice in enumerate(x_batches):\n        X_chunk = X[x_slice].astype(np.float64)\n        if XX is None:\n            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]\n        else:\n            XX_chunk = XX[x_slice]\n        y_batches = gen_batches(n_samples_Y, batch_size)\n        for j, y_slice in enumerate(y_batches):\n            if X is Y and j < i:\n                d = distances[y_slice, x_slice].T\n            else:\n                Y_chunk = Y[y_slice].astype(np.float64)\n                if YY is None:\n                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]\n                else:\n                    YY_chunk = YY[:, y_slice]\n                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)\n                d += XX_chunk\n                d += YY_chunk\n            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)\n    return distances",
    ".sklearn.utils._param_validation.py@@wrapper": "def wrapper(*args, **kwargs):\n    func_sig = signature(func)\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n    to_ignore = [p.name for p in func_sig.parameters.values() if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)]\n    to_ignore += ['self', 'cls']\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n    validate_parameter_constraints(parameter_constraints, params, caller_name=func.__qualname__)\n    try:\n        return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        msg = re.sub('parameter of \\\\w+ must be', f'parameter of {func.__qualname__} must be', str(e))\n        raise InvalidParameterError(msg) from e",
    ".sklearn.utils.__init__.py@@gen_batches": "def gen_batches(n, batch_size, *, min_batch_size=0):\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    ".sklearn.conftest.py@@pytest_runtest_setup": "def pytest_runtest_setup(item):\n    xdist_worker_count = environ.get('PYTEST_XDIST_WORKER_COUNT')\n    if xdist_worker_count is None:\n        return\n    else:\n        xdist_worker_count = int(xdist_worker_count)\n    openmp_threads = _openmp_effective_n_threads()\n    threads_per_worker = max(openmp_threads // xdist_worker_count, 1)\n    threadpool_limits(threads_per_worker, user_api='openmp')",
    ".sklearn.cluster._kmeans.py@@kmeans_plusplus": "def kmeans_plusplus(X, n_clusters, *, sample_weight=None, x_squared_norms=None, random_state=None, n_local_trials=None):\n    check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32])\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n    if X.shape[0] < n_clusters:\n        raise ValueError(f'n_samples={X.shape[0]} should be >= n_clusters={n_clusters}.')\n    if x_squared_norms is None:\n        x_squared_norms = row_norms(X, squared=True)\n    else:\n        x_squared_norms = check_array(x_squared_norms, dtype=X.dtype, ensure_2d=False)\n    if x_squared_norms.shape[0] != X.shape[0]:\n        raise ValueError(f'The length of x_squared_norms {x_squared_norms.shape[0]} should be equal to the length of n_samples {X.shape[0]}.')\n    random_state = check_random_state(random_state)\n    centers, indices = _kmeans_plusplus(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\n    return (centers, indices)",
    ".sklearn.utils._testing.py@@assert_allclose": "def assert_allclose(actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg='', verbose=True):\n    dtypes = []\n    actual, desired = (np.asanyarray(actual), np.asanyarray(desired))\n    dtypes = [actual.dtype, desired.dtype]\n    if rtol is None:\n        rtols = [0.0001 if dtype == np.float32 else 1e-07 for dtype in dtypes]\n        rtol = max(rtols)\n    np_assert_allclose(actual, desired, rtol=rtol, atol=atol, equal_nan=equal_nan, err_msg=err_msg, verbose=verbose)",
    ".sklearn.cluster._kmeans.py@@_BaseKMeans.__init__": "def __init__(self, n_clusters, *, init, n_init, max_iter, tol, verbose, random_state):\n    self.n_clusters = n_clusters\n    self.init = init\n    self.max_iter = max_iter\n    self.tol = tol\n    self.n_init = n_init\n    self.verbose = verbose\n    self.random_state = random_state",
    ".sklearn.cluster._kmeans.py@@_kmeans_single_lloyd": "def _kmeans_single_lloyd(X, sample_weight, centers_init, max_iter=300, verbose=False, tol=0.0001, n_threads=1):\n    n_clusters = centers_init.shape[0]\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    labels = np.full(X.shape[0], -1, dtype=np.int32)\n    labels_old = labels.copy()\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n    if sp.issparse(X):\n        lloyd_iter = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        lloyd_iter = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n    strict_convergence = False\n    with threadpool_limits(limits=1, user_api='blas'):\n        for i in range(max_iter):\n            lloyd_iter(X, sample_weight, centers, centers_new, weight_in_clusters, labels, center_shift, n_threads)\n            if verbose:\n                inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n                print(f'Iteration {i}, inertia {inertia}.')\n            centers, centers_new = (centers_new, centers)\n            if np.array_equal(labels, labels_old):\n                if verbose:\n                    print(f'Converged at iteration {i}: strict convergence.')\n                strict_convergence = True\n                break\n            else:\n                center_shift_tot = (center_shift ** 2).sum()\n                if center_shift_tot <= tol:\n                    if verbose:\n                        print(f'Converged at iteration {i}: center shift {center_shift_tot} within tolerance {tol}.')\n                    break\n            labels_old[:] = labels\n        if not strict_convergence:\n            lloyd_iter(X, sample_weight, centers, centers, weight_in_clusters, labels, center_shift, n_threads, update_centers=False)\n    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n    return (labels, inertia, centers, i + 1)"
}