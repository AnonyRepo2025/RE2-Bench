{
    ".xarray.core.dataarray.py@@DataArray.__getitem__": "def __getitem__(self, key: Any) -> DataArray:\n    if isinstance(key, str):\n        return self._getitem_coord(key)\n    else:\n        return self.isel(indexers=self._item_key_to_dict(key))",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self, key):\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self, variable: Variable, name: Hashable | None | Default=_default) -> DataArray:\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n        indexes = self._indexes\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = {k: v for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))}\n        indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    else:\n        allowed_dims = set(variable.dims)\n        coords = {k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims}\n        indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.shape": "def shape(self) -> tuple[int]:\n    return (len(self.array),)",
    ".xarray.core.dataarray.py@@DataArray.shape": "def shape(self) -> tuple[int, ...]:\n    return self.variable.shape",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self: T_DataArray, variable: Variable=None, coords=None, name: Hashable | None | Default=_default, indexes=None) -> T_DataArray:\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if indexes is None:\n        indexes = self._indexes\n    if name is _default:\n        name = self.name\n    return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Sequence[tuple] | Mapping[Any, Any] | None=None, dims: Hashable | Sequence[Hashable] | None=None, name: Hashable=None, attrs: Mapping=None, indexes: dict[Hashable, Index]=None, fastpath: bool=False):\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n        assert indexes is not None\n    else:\n        if indexes is not None:\n            raise ValueError('Providing explicit indexes is not supported yet')\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None and (not isinstance(data, PANDAS_TYPES)):\n            attrs = getattr(data, 'attrs', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, fastpath=True)\n        indexes, coords = _create_indexes_from_coords(coords)\n    self._variable = variable\n    assert isinstance(coords, dict)\n    self._coords = coords\n    self._name = name\n    self._indexes = indexes\n    self._close = None",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '{!r} object has no attribute {!r}'.format(type(self).__name__, name):\n            raise\n        raise AttributeError(f\"cannot set attribute {name!r} on a {type(self).__name__!r} object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\") from e",
    ".xarray.core.dataarray.py@@DataArray.dtype": "def dtype(self) -> np.dtype:\n    return self.variable.dtype",
    ".xarray.core.variable.py@@Variable.dtype": "def dtype(self):\n    return self._data.dtype",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.dtype": "def dtype(self) -> np.dtype:\n    return self._dtype",
    ".xarray.core._reductions.py@@DataArrayReductions.max": "def max(self, dim: Union[None, Hashable, Sequence[Hashable]]=None, *, skipna: bool=None, keep_attrs: bool=None, **kwargs) -> 'DataArray':\n    return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reduce": "def reduce(self, func: Callable[..., Any], dim: None | Hashable | Sequence[Hashable]=None, *, axis: None | int | Sequence[int]=None, keep_attrs: bool=None, keepdims: bool=False, **kwargs: Any) -> DataArray:\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n    return self._replace_maybe_drop_dims(var)",
    ".xarray.core.variable.py@@Variable.reduce": "def reduce(self, func, dim=None, axis=None, keep_attrs=None, keepdims=False, **kwargs):\n    if dim == ...:\n        dim = None\n    if dim is not None and axis is not None:\n        raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n    if dim is not None:\n        axis = self.get_axis_num(dim)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        if axis is not None:\n            data = func(self.data, axis=axis, **kwargs)\n        else:\n            data = func(self.data, **kwargs)\n    if getattr(data, 'shape', ()) == self.shape:\n        dims = self.dims\n    else:\n        removed_axes = range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n        if keepdims:\n            slices = tuple((np.newaxis if i in removed_axes else slice(None, None) for i in range(self.ndim)))\n            if getattr(data, 'shape', None) is None:\n                data = np.asanyarray(data)[slices]\n            else:\n                data = data[slices]\n            dims = self.dims\n        else:\n            dims = [adim for n, adim in enumerate(self.dims) if n not in removed_axes]\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    return Variable(dims, data, attrs=attrs)",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if is_duck_array(self._data):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.utils.py@@is_duck_array": "def is_duck_array(value: Any) -> bool:\n    if isinstance(value, np.ndarray):\n        return True\n    return hasattr(value, 'ndim') and hasattr(value, 'shape') and hasattr(value, 'dtype') and hasattr(value, '__array_function__') and hasattr(value, '__array_ufunc__')",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.variable.py@@Variable.values": "def values(self):\n    return _as_array_or_item(self._data)",
    ".xarray.core.variable.py@@_as_array_or_item": "def _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__array__": "def __array__(self, dtype: DTypeLike=None) -> np.ndarray:\n    if dtype is None:\n        dtype = self.dtype\n    array = self.array\n    if isinstance(array, pd.PeriodIndex):\n        with suppress(AttributeError):\n            array = array.astype('object')\n    return np.asarray(array.values, dtype=dtype)",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError(f'`out` is not valid for {name}')\n    if invariant_0d and axis == ():\n        return values\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        if name in ['sum', 'prod']:\n            kwargs.pop('min_count', None)\n        func = getattr(np, name)\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', 'All-NaN slice encountered')\n            return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if not is_duck_dask_array(values):\n            raise\n        try:\n            return func(values, axis=axis, dtype=values.dtype, **kwargs)\n        except (AttributeError, TypeError):\n            raise NotImplementedError(f'{name} is not yet implemented on dask arrays')",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data, xp=np):\n    return data if is_duck_array(data) else xp.asarray(data)",
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    return _get_boolean_with_default('keep_attrs', default)",
    ".xarray.core.options.py@@_get_boolean_with_default": "def _get_boolean_with_default(option, default):\n    global_choice = OPTIONS[option]\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(f\"The global option {option} must be one of True, False or 'default'.\")",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    from .dataarray import DataArray\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, (Variable, DataArray)):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):\n        data = data.values\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray) and hasattr(data, '__array_function__'):\n        return data\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray) and data.dtype.kind in 'OMm':\n        data = _possibly_convert_objects(data)\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexingAdapter(data)\n    return data",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError(f'dimensions {dims} must have the same length as the number of data dimensions, ndim={self.ndim}')\n    return dims",
    ".xarray.core.indexes.py@@filter_indexes_from_coords": "def filter_indexes_from_coords(indexes: Mapping[Any, Index], filtered_coord_names: set) -> dict[Hashable, Index]:\n    filtered_indexes: dict[Any, Index] = dict(**indexes)\n    index_coord_names: dict[Hashable, set[Hashable]] = defaultdict(set)\n    for name, idx in indexes.items():\n        index_coord_names[id(idx)].add(name)\n    for idx_coord_names in index_coord_names.values():\n        if not idx_coord_names <= filtered_coord_names:\n            for k in idx_coord_names:\n                del filtered_indexes[k]\n    return filtered_indexes",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Hashable | None:\n    return self._name",
    ".xarray.core.ops.py@@func": "def func(self, *args, **kwargs):\n    try:\n        return getattr(self, name)(*args, **kwargs)\n    except AttributeError:\n        return f(self, *args, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.data": "def data(self) -> Any:\n    return self.variable.data",
    ".xarray.core.ops.py@@_call_possibly_missing_method": "def _call_possibly_missing_method(arg, name, args, kwargs):\n    try:\n        method = getattr(arg, name)\n    except AttributeError:\n        duck_array_ops.fail_on_dask_array_input(arg, func_name=name)\n        if hasattr(arg, 'data'):\n            duck_array_ops.fail_on_dask_array_input(arg.data, func_name=name)\n        raise\n    else:\n        return method(*args, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reindex": "def reindex(self, indexers: Mapping[Any, Any]=None, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value=dtypes.NA, **indexers_kwargs: Any) -> DataArray:\n    indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, 'reindex')\n    return alignment.reindex(self, indexers=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Mapping[Any, T] | None, kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is None or pos_kwargs == {}:\n        return cast(Mapping[Hashable, T], kw_kwargs)\n    if not is_dict_like(pos_kwargs):\n        raise ValueError(f'the first argument to .{func_name} must be a dictionary')\n    if kw_kwargs:\n        raise ValueError(f'cannot specify both keyword and positional arguments to .{func_name}')\n    return pos_kwargs",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> TypeGuard[dict]:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.alignment.py@@reindex": "def reindex(obj: DataAlignable, indexers: Mapping[Any, Any], method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False, exclude_vars: Iterable[Hashable]=frozenset()) -> DataAlignable:\n    aligner = Aligner((obj,), indexes=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value, sparse=sparse, exclude_vars=exclude_vars)\n    aligner.align()\n    return aligner.results[0]",
    ".xarray.core.alignment.py@@Aligner.__init__": "def __init__(self, objects: Iterable[DataAlignable], join: str='inner', indexes: Mapping[Any, Any]=None, exclude_dims: Iterable=frozenset(), exclude_vars: Iterable[Hashable]=frozenset(), method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False):\n    self.objects = tuple(objects)\n    self.objects_matching_indexes = ()\n    if join not in ['inner', 'outer', 'override', 'exact', 'left', 'right']:\n        raise ValueError(f'invalid value for join: {join}')\n    self.join = join\n    self.copy = copy\n    self.fill_value = fill_value\n    self.sparse = sparse\n    if method is None and tolerance is None:\n        self.reindex_kwargs = {}\n    else:\n        self.reindex_kwargs = {'method': method, 'tolerance': tolerance}\n    if isinstance(exclude_dims, str):\n        exclude_dims = [exclude_dims]\n    self.exclude_dims = frozenset(exclude_dims)\n    self.exclude_vars = frozenset(exclude_vars)\n    if indexes is None:\n        indexes = {}\n    self.indexes, self.index_vars = self._normalize_indexes(indexes)\n    self.all_indexes = {}\n    self.all_index_vars = {}\n    self.unindexed_dim_sizes = {}\n    self.aligned_indexes = {}\n    self.aligned_index_vars = {}\n    self.reindex = {}\n    self.results = tuple()",
    ".xarray.core.alignment.py@@Aligner._normalize_indexes": "def _normalize_indexes(self, indexes: Mapping[Any, Any]) -> tuple[NormalizedIndexes, NormalizedIndexVars]:\n    if isinstance(indexes, Indexes):\n        xr_variables = dict(indexes.variables)\n    else:\n        xr_variables = {}\n    xr_indexes: dict[Hashable, Index] = {}\n    for k, idx in indexes.items():\n        if not isinstance(idx, Index):\n            if getattr(idx, 'dims', (k,)) != (k,):\n                raise ValueError(f\"Indexer has dimensions {idx.dims} that are different from that to be indexed along '{k}'\")\n            data = as_compatible_data(idx)\n            pd_idx = safe_cast_to_index(data)\n            pd_idx.name = k\n            if isinstance(pd_idx, pd.MultiIndex):\n                idx = PandasMultiIndex(pd_idx, k)\n            else:\n                idx = PandasIndex(pd_idx, k, coord_dtype=data.dtype)\n            xr_variables.update(idx.create_variables())\n        xr_indexes[k] = idx\n    normalized_indexes = {}\n    normalized_index_vars = {}\n    for idx, index_vars in Indexes(xr_indexes, xr_variables).group_by_index():\n        coord_names_and_dims = []\n        all_dims = set()\n        for name, var in index_vars.items():\n            dims = var.dims\n            coord_names_and_dims.append((name, dims))\n            all_dims.update(dims)\n        exclude_dims = all_dims & self.exclude_dims\n        if exclude_dims == all_dims:\n            continue\n        elif exclude_dims:\n            excl_dims_str = ', '.join((str(d) for d in exclude_dims))\n            incl_dims_str = ', '.join((str(d) for d in all_dims - exclude_dims))\n            raise ValueError(f'cannot exclude dimension(s) {excl_dims_str} from alignment because these are used by an index together with non-excluded dimensions {incl_dims_str}')\n        key = (tuple(coord_names_and_dims), type(idx))\n        normalized_indexes[key] = idx\n        normalized_index_vars[key] = index_vars\n    return (normalized_indexes, normalized_index_vars)",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    elif hasattr(array, 'to_pandas_index'):\n        index = array.to_pandas_index()\n    elif hasattr(array, 'array') and isinstance(array.array, pd.Index):\n        index = array.array\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.indexes.py@@PandasIndex.__init__": "def __init__(self, array: Any, dim: Hashable, coord_dtype: Any=None):\n    index = utils.safe_cast_to_index(array).copy()\n    if index.name is None:\n        index.name = dim\n    self.index = index\n    self.dim = dim\n    if coord_dtype is None:\n        coord_dtype = get_valid_numpy_dtype(index)\n    self.coord_dtype = coord_dtype",
    ".xarray.core.indexes.py@@PandasIndex.create_variables": "def create_variables(self, variables: Mapping[Any, Variable] | None=None) -> IndexVars:\n    from .variable import IndexVariable\n    name = self.index.name\n    attrs: Mapping[Hashable, Any] | None\n    encoding: Mapping[Hashable, Any] | None\n    if variables is not None and name in variables:\n        var = variables[name]\n        attrs = var.attrs\n        encoding = var.encoding\n    else:\n        attrs = None\n        encoding = None\n    data = PandasIndexingAdapter(self.index, dtype=self.coord_dtype)\n    var = IndexVariable(self.dim, data, attrs=attrs, encoding=encoding)\n    return {name: var}",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__init__": "def __init__(self, array: pd.Index, dtype: DTypeLike=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        self._dtype = get_valid_numpy_dtype(array)\n    else:\n        self._dtype = np.dtype(dtype)",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError(f'{type(self).__name__} objects must be 1-dimensional')\n    if not isinstance(self._data, PandasIndexingAdapter):\n        self._data = PandasIndexingAdapter(self._data)",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes: dict[Any, T_PandasOrXarrayIndex], variables: dict[Any, Variable]):\n    self._indexes = indexes\n    self._variables = variables\n    self._dims: Mapping[Hashable, int] | None = None\n    self.__coord_name_id: dict[Any, int] | None = None\n    self.__id_index: dict[int, T_PandasOrXarrayIndex] | None = None\n    self.__id_coord_names: dict[int, tuple[Hashable, ...]] | None = None",
    ".xarray.core.indexes.py@@Indexes.group_by_index": "def group_by_index(self) -> list[tuple[T_PandasOrXarrayIndex, dict[Hashable, Variable]]]:\n    index_coords = []\n    for i in self._id_index:\n        index = self._id_index[i]\n        coords = {k: self._variables[k] for k in self._id_coord_names[i]}\n        index_coords.append((index, coords))\n    return index_coords",
    ".xarray.core.indexes.py@@Indexes._id_index": "def _id_index(self) -> dict[int, T_PandasOrXarrayIndex]:\n    if self.__id_index is None:\n        self.__id_index = {id(idx): idx for idx in self.get_unique()}\n    return self.__id_index",
    ".xarray.core.indexes.py@@Indexes.get_unique": "def get_unique(self) -> list[T_PandasOrXarrayIndex]:\n    unique_indexes: list[T_PandasOrXarrayIndex] = []\n    seen: set[T_PandasOrXarrayIndex] = set()\n    for index in self._indexes.values():\n        if index not in seen:\n            unique_indexes.append(index)\n            seen.add(index)\n    return unique_indexes",
    ".xarray.core.indexes.py@@Indexes._id_coord_names": "def _id_coord_names(self) -> dict[int, tuple[Hashable, ...]]:\n    if self.__id_coord_names is None:\n        id_coord_names: Mapping[int, list[Hashable]] = defaultdict(list)\n        for k, v in self._coord_name_id.items():\n            id_coord_names[v].append(k)\n        self.__id_coord_names = {k: tuple(v) for k, v in id_coord_names.items()}\n    return self.__id_coord_names",
    ".xarray.core.indexes.py@@Indexes._coord_name_id": "def _coord_name_id(self) -> dict[Any, int]:\n    if self.__coord_name_id is None:\n        self.__coord_name_id = {k: id(idx) for k, idx in self._indexes.items()}\n    return self.__coord_name_id",
    ".xarray.core.alignment.py@@Aligner.align": "def align(self) -> None:\n    if not self.indexes and len(self.objects) == 1:\n        obj, = self.objects\n        self.results = (obj.copy(deep=self.copy),)\n    self.find_matching_indexes()\n    self.find_matching_unindexed_dims()\n    self.assert_no_index_conflict()\n    self.align_indexes()\n    self.assert_unindexed_dim_sizes_equal()\n    if self.join == 'override':\n        self.override_indexes()\n    else:\n        self.reindex_all()",
    ".xarray.core.alignment.py@@Aligner.find_matching_indexes": "def find_matching_indexes(self) -> None:\n    all_indexes: dict[MatchingIndexKey, list[Index]]\n    all_index_vars: dict[MatchingIndexKey, list[dict[Hashable, Variable]]]\n    all_indexes_dim_sizes: dict[MatchingIndexKey, dict[Hashable, set]]\n    objects_matching_indexes: list[dict[MatchingIndexKey, Index]]\n    all_indexes = defaultdict(list)\n    all_index_vars = defaultdict(list)\n    all_indexes_dim_sizes = defaultdict(lambda: defaultdict(set))\n    objects_matching_indexes = []\n    for obj in self.objects:\n        obj_indexes, obj_index_vars = self._normalize_indexes(obj.xindexes)\n        objects_matching_indexes.append(obj_indexes)\n        for key, idx in obj_indexes.items():\n            all_indexes[key].append(idx)\n        for key, index_vars in obj_index_vars.items():\n            all_index_vars[key].append(index_vars)\n            for dim, size in calculate_dimensions(index_vars).items():\n                all_indexes_dim_sizes[key][dim].add(size)\n    self.objects_matching_indexes = tuple(objects_matching_indexes)\n    self.all_indexes = all_indexes\n    self.all_index_vars = all_index_vars\n    if self.join == 'override':\n        for dim_sizes in all_indexes_dim_sizes.values():\n            for dim, sizes in dim_sizes.items():\n                if len(sizes) > 1:\n                    raise ValueError(f\"cannot align objects with join='override' with matching indexes along dimension {dim!r} that don't have the same size\")",
    ".xarray.core.dataarray.py@@DataArray.xindexes": "def xindexes(self) -> Indexes:\n    return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})",
    ".xarray.core.indexes.py@@Indexes.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self) -> Iterator[T_PandasOrXarrayIndex]:\n    return iter(self._indexes)",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key) -> T_PandasOrXarrayIndex:\n    return self._indexes[key]",
    ".xarray.core.variable.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Any, Variable]) -> dict[Hashable, int]:\n    dims: dict[Hashable, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError(f'dimension {dim!r} already exists as a scalar variable')\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError(f'conflicting sizes for dimension {dim!r}: length {size} on {k!r} and length {dims[dim]} on {last_used!r}')\n    return dims",
    ".xarray.core.alignment.py@@Aligner.find_matching_unindexed_dims": "def find_matching_unindexed_dims(self) -> None:\n    unindexed_dim_sizes = defaultdict(set)\n    for obj in self.objects:\n        for dim in obj.dims:\n            if dim not in self.exclude_dims and dim not in obj.xindexes.dims:\n                unindexed_dim_sizes[dim].add(obj.sizes[dim])\n    self.unindexed_dim_sizes = unindexed_dim_sizes",
    ".xarray.core.indexes.py@@Indexes.dims": "def dims(self) -> Mapping[Hashable, int]:\n    from .variable import calculate_dimensions\n    if self._dims is None:\n        self._dims = calculate_dimensions(self._variables)\n    return Frozen(self._dims)",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.alignment.py@@Aligner.assert_no_index_conflict": "def assert_no_index_conflict(self) -> None:\n    matching_keys = set(self.all_indexes) | set(self.indexes)\n    coord_count: dict[Hashable, int] = defaultdict(int)\n    dim_count: dict[Hashable, int] = defaultdict(int)\n    for coord_names_dims, _ in matching_keys:\n        dims_set: set[Hashable] = set()\n        for name, dims in coord_names_dims:\n            coord_count[name] += 1\n            dims_set.update(dims)\n        for dim in dims_set:\n            dim_count[dim] += 1\n    for count, msg in [(coord_count, 'coordinates'), (dim_count, 'dimensions')]:\n        dup = {k: v for k, v in count.items() if v > 1}\n        if dup:\n            items_msg = ', '.join((f'{k!r} ({v} conflicting indexes)' for k, v in dup.items()))\n            raise ValueError(f\"cannot re-index or align objects with conflicting indexes found for the following {msg}: {items_msg}\\nConflicting indexes may occur when\\n- they relate to different sets of coordinate and/or dimension names\\n- they don't have the same type\\n- they may be used to reindex data along common dimensions\")",
    ".xarray.core.alignment.py@@Aligner.align_indexes": "def align_indexes(self) -> None:\n    aligned_indexes = {}\n    aligned_index_vars = {}\n    reindex = {}\n    new_indexes = {}\n    new_index_vars = {}\n    for key, matching_indexes in self.all_indexes.items():\n        matching_index_vars = self.all_index_vars[key]\n        dims = {d for coord in matching_index_vars[0].values() for d in coord.dims}\n        index_cls = key[1]\n        if self.join == 'override':\n            joined_index = matching_indexes[0]\n            joined_index_vars = matching_index_vars[0]\n            need_reindex = False\n        elif key in self.indexes:\n            joined_index = self.indexes[key]\n            joined_index_vars = self.index_vars[key]\n            cmp_indexes = list(zip([joined_index] + matching_indexes, [joined_index_vars] + matching_index_vars))\n            need_reindex = self._need_reindex(dims, cmp_indexes)\n        else:\n            if len(matching_indexes) > 1:\n                need_reindex = self._need_reindex(dims, list(zip(matching_indexes, matching_index_vars)))\n            else:\n                need_reindex = False\n            if need_reindex:\n                if self.join == 'exact':\n                    raise ValueError(\"cannot align objects with join='exact' where index/labels/sizes are not equal along these coordinates (dimensions): \" + ', '.join((f'{name!r} {dims!r}' for name, dims in key[0])))\n                joiner = self._get_index_joiner(index_cls)\n                joined_index = joiner(matching_indexes)\n                if self.join == 'left':\n                    joined_index_vars = matching_index_vars[0]\n                elif self.join == 'right':\n                    joined_index_vars = matching_index_vars[-1]\n                else:\n                    joined_index_vars = joined_index.create_variables()\n            else:\n                joined_index = matching_indexes[0]\n                joined_index_vars = matching_index_vars[0]\n        reindex[key] = need_reindex\n        aligned_indexes[key] = joined_index\n        aligned_index_vars[key] = joined_index_vars\n        for name, var in joined_index_vars.items():\n            new_indexes[name] = joined_index\n            new_index_vars[name] = var\n    for key, idx in self.indexes.items():\n        if key not in aligned_indexes:\n            index_vars = self.index_vars[key]\n            reindex[key] = False\n            aligned_indexes[key] = idx\n            aligned_index_vars[key] = index_vars\n            for name, var in index_vars.items():\n                new_indexes[name] = idx\n                new_index_vars[name] = var\n    self.aligned_indexes = aligned_indexes\n    self.aligned_index_vars = aligned_index_vars\n    self.reindex = reindex\n    self.new_indexes = Indexes(new_indexes, new_index_vars)",
    ".xarray.core.alignment.py@@Aligner._need_reindex": "def _need_reindex(self, dims, cmp_indexes) -> bool:\n    has_unindexed_dims = any((dim in self.unindexed_dim_sizes for dim in dims))\n    return not indexes_all_equal(cmp_indexes) or has_unindexed_dims",
    ".xarray.core.indexes.py@@indexes_all_equal": "def indexes_all_equal(elements: Sequence[tuple[Index, dict[Hashable, Variable]]]) -> bool:\n\n    def check_variables():\n        variables = [e[1] for e in elements]\n        return any((not variables[0][k].equals(other_vars[k]) for other_vars in variables[1:] for k in variables[0]))\n    indexes = [e[0] for e in elements]\n    same_type = all((type(indexes[0]) is type(other_idx) for other_idx in indexes[1:]))\n    if same_type:\n        try:\n            not_equal = any((not indexes[0].equals(other_idx) for other_idx in indexes[1:]))\n        except NotImplementedError:\n            not_equal = check_variables()\n    else:\n        not_equal = check_variables()\n    return not not_equal",
    ".xarray.core.indexes.py@@PandasIndex.equals": "def equals(self, other: Index):\n    if not isinstance(other, PandasIndex):\n        return False\n    return self.index.equals(other.index) and self.dim == other.dim",
    ".xarray.core.alignment.py@@Aligner.assert_unindexed_dim_sizes_equal": "def assert_unindexed_dim_sizes_equal(self) -> None:\n    for dim, sizes in self.unindexed_dim_sizes.items():\n        index_size = self.new_indexes.dims.get(dim)\n        if index_size is not None:\n            sizes.add(index_size)\n            add_err_msg = f' (note: an index is found along that dimension with size={index_size!r})'\n        else:\n            add_err_msg = ''\n        if len(sizes) > 1:\n            raise ValueError(f'cannot reindex or align along dimension {dim!r} because of conflicting dimension sizes: {sizes!r}' + add_err_msg)",
    ".xarray.core.alignment.py@@Aligner.reindex_all": "def reindex_all(self) -> None:\n    self.results = tuple((self._reindex_one(obj, matching_indexes) for obj, matching_indexes in zip(self.objects, self.objects_matching_indexes)))",
    ".xarray.core.alignment.py@@Aligner._reindex_one": "def _reindex_one(self, obj: DataAlignable, matching_indexes: dict[MatchingIndexKey, Index]) -> DataAlignable:\n    new_indexes, new_variables = self._get_indexes_and_vars(obj, matching_indexes)\n    dim_pos_indexers = self._get_dim_pos_indexers(matching_indexes)\n    new_obj = obj._reindex_callback(self, dim_pos_indexers, new_variables, new_indexes, self.fill_value, self.exclude_dims, self.exclude_vars)\n    new_obj.encoding = obj.encoding\n    return new_obj",
    ".xarray.core.alignment.py@@Aligner._get_indexes_and_vars": "def _get_indexes_and_vars(self, obj: DataAlignable, matching_indexes: dict[MatchingIndexKey, Index]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    new_indexes = {}\n    new_variables = {}\n    for key, aligned_idx in self.aligned_indexes.items():\n        index_vars = self.aligned_index_vars[key]\n        obj_idx = matching_indexes.get(key)\n        if obj_idx is None:\n            index_vars_dims = {d for var in index_vars.values() for d in var.dims}\n            if index_vars_dims <= set(obj.dims):\n                obj_idx = aligned_idx\n        if obj_idx is not None:\n            for name, var in index_vars.items():\n                new_indexes[name] = aligned_idx\n                new_variables[name] = var\n    return (new_indexes, new_variables)",
    ".xarray.core.alignment.py@@Aligner._get_dim_pos_indexers": "def _get_dim_pos_indexers(self, matching_indexes: dict[MatchingIndexKey, Index]) -> dict[Hashable, Any]:\n    dim_pos_indexers = {}\n    for key, aligned_idx in self.aligned_indexes.items():\n        obj_idx = matching_indexes.get(key)\n        if obj_idx is not None:\n            if self.reindex[key]:\n                indexers = obj_idx.reindex_like(aligned_idx, **self.reindex_kwargs)\n                dim_pos_indexers.update(indexers)\n    return dim_pos_indexers",
    ".xarray.core.dataarray.py@@DataArray._reindex_callback": "def _reindex_callback(self, aligner: alignment.Aligner, dim_pos_indexers: dict[Hashable, Any], variables: dict[Hashable, Variable], indexes: dict[Hashable, Index], fill_value: Any, exclude_dims: frozenset[Hashable], exclude_vars: frozenset[Hashable]) -> DataArray:\n    if isinstance(fill_value, dict):\n        fill_value = fill_value.copy()\n        sentinel = object()\n        value = fill_value.pop(self.name, sentinel)\n        if value is not sentinel:\n            fill_value[_THIS_ARRAY] = value\n    ds = self._to_temp_dataset()\n    reindexed = ds._reindex_callback(aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\n    return self._from_temp_dataset(reindexed)",
    ".xarray.core.dataarray.py@@DataArray._to_temp_dataset": "def _to_temp_dataset(self) -> Dataset:\n    return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)",
    ".xarray.core.dataarray.py@@DataArray._to_dataset_whole": "def _to_dataset_whole(self, name: Hashable=None, shallow_copy: bool=True) -> Dataset:\n    if name is None:\n        name = self.name\n    if name is None:\n        raise ValueError('unable to convert unnamed DataArray to a Dataset without providing an explicit name')\n    if name in self.coords:\n        raise ValueError('cannot create a Dataset from a DataArray with the same name as one of its coordinates')\n    variables = self._coords.copy()\n    variables[name] = self.variable\n    if shallow_copy:\n        for k in variables:\n            variables[k] = variables[k].copy(deep=False)\n    indexes = self._indexes\n    coord_names = set(self._coords)\n    return Dataset._construct_direct(variables, coord_names, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: 'DataArray'):\n    self._data = dataarray",
    ".xarray.core.coordinates.py@@Coordinates.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._names",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return set(self._data._coords)",
    ".xarray.core.utils.py@@ReprObject.__hash__": "def __hash__(self) -> int:\n    return hash((type(self), self._value))",
    ".xarray.core.dataset.py@@Dataset._construct_direct": "def _construct_direct(cls, variables: dict[Any, Variable], coord_names: set[Hashable], dims: dict[Any, int]=None, attrs: dict=None, indexes: dict[Any, Index]=None, encoding: dict=None, close: Callable[[], None]=None) -> Dataset:\n    if dims is None:\n        dims = calculate_dimensions(variables)\n    if indexes is None:\n        indexes = {}\n    obj = object.__new__(cls)\n    obj._variables = variables\n    obj._coord_names = coord_names\n    obj._dims = dims\n    obj._indexes = indexes\n    obj._attrs = attrs\n    obj._close = close\n    obj._encoding = encoding\n    return obj",
    ".xarray.core.dataset.py@@Dataset._reindex_callback": "def _reindex_callback(self, aligner: alignment.Aligner, dim_pos_indexers: dict[Hashable, Any], variables: dict[Hashable, Variable], indexes: dict[Hashable, Index], fill_value: Any, exclude_dims: frozenset[Hashable], exclude_vars: frozenset[Hashable]) -> Dataset:\n    new_variables = variables.copy()\n    new_indexes = indexes.copy()\n    for name, new_var in new_variables.items():\n        var = self._variables.get(name)\n        if var is not None:\n            new_var.attrs = var.attrs\n            new_var.encoding = var.encoding\n    for name, idx in self._indexes.items():\n        var = self._variables[name]\n        if set(var.dims) <= exclude_dims:\n            new_indexes[name] = idx\n            new_variables[name] = var\n    if not dim_pos_indexers:\n        if set(new_indexes) - set(self._indexes):\n            reindexed = self._overwrite_indexes(new_indexes, new_variables)\n        else:\n            reindexed = self.copy(deep=aligner.copy)\n    else:\n        to_reindex = {k: v for k, v in self.variables.items() if k not in variables and k not in exclude_vars}\n        reindexed_vars = alignment.reindex_variables(to_reindex, dim_pos_indexers, copy=aligner.copy, fill_value=fill_value, sparse=aligner.sparse)\n        new_variables.update(reindexed_vars)\n        new_coord_names = self._coord_names | set(new_indexes)\n        reindexed = self._replace_with_new_dims(new_variables, new_coord_names, indexes=new_indexes)\n    return reindexed",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self):\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.dataset.py@@Dataset.copy": "def copy(self, deep: bool=False, data: Mapping=None) -> Dataset:\n    if data is None:\n        data = {}\n    elif not utils.is_dict_like(data):\n        raise ValueError('Data must be dict-like')\n    if data:\n        var_keys = set(self.data_vars.keys())\n        data_keys = set(data.keys())\n        keys_not_in_vars = data_keys - var_keys\n        if keys_not_in_vars:\n            raise ValueError('Data must only contain variables in original dataset. Extra variables: {}'.format(keys_not_in_vars))\n        keys_missing_from_data = var_keys - data_keys\n        if keys_missing_from_data:\n            raise ValueError('Data must contain all variables in original dataset. Data is missing {}'.format(keys_missing_from_data))\n    indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n    variables = {}\n    for k, v in self._variables.items():\n        if k in index_vars:\n            variables[k] = index_vars[k]\n        else:\n            variables[k] = v.copy(deep=deep, data=data.get(k))\n    attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n    return self._replace(variables, indexes=indexes, attrs=attrs)",
    ".xarray.core.dataset.py@@Dataset.xindexes": "def xindexes(self) -> Indexes[Index]:\n    return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})",
    ".xarray.core.indexes.py@@Indexes.copy_indexes": "def copy_indexes(self, deep: bool=True) -> tuple[dict[Hashable, T_PandasOrXarrayIndex], dict[Hashable, Variable]]:\n    new_indexes = {}\n    new_index_vars = {}\n    for idx, coords in self.group_by_index():\n        new_idx = idx.copy(deep=deep)\n        idx_vars = idx.create_variables(coords)\n        new_indexes.update({k: new_idx for k in coords})\n        new_index_vars.update(idx_vars)\n    return (new_indexes, new_index_vars)",
    ".xarray.core.indexes.py@@PandasIndex.copy": "def copy(self, deep=True):\n    if deep:\n        index = self.index.copy(deep=True)\n    else:\n        index = self.index\n    return self._replace(index)",
    ".xarray.core.indexes.py@@PandasIndex._replace": "def _replace(self, index, dim=None, coord_dtype=None):\n    if dim is None:\n        dim = self.dim\n    if coord_dtype is None:\n        coord_dtype = self.coord_dtype\n    return type(self)(index, dim, coord_dtype)",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            data = copy.deepcopy(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return self._replace(data=data)",
    ".xarray.core.variable.py@@Variable._replace": "def _replace(self: T_Variable, dims=_default, data=_default, attrs=_default, encoding=_default) -> T_Variable:\n    if dims is _default:\n        dims = copy.copy(self._dims)\n    if data is _default:\n        data = copy.copy(self.data)\n    if attrs is _default:\n        attrs = copy.copy(self._attrs)\n    if encoding is _default:\n        encoding = copy.copy(self._encoding)\n    return type(self)(dims, data, attrs, encoding, fastpath=True)",
    ".xarray.core.dataset.py@@Dataset._replace": "def _replace(self, variables: dict[Hashable, Variable]=None, coord_names: set[Hashable]=None, dims: dict[Any, int]=None, attrs: dict[Hashable, Any] | None | Default=_default, indexes: dict[Hashable, Index]=None, encoding: dict | None | Default=_default, inplace: bool=False) -> Dataset:\n    if inplace:\n        if variables is not None:\n            self._variables = variables\n        if coord_names is not None:\n            self._coord_names = coord_names\n        if dims is not None:\n            self._dims = dims\n        if attrs is not _default:\n            self._attrs = attrs\n        if indexes is not None:\n            self._indexes = indexes\n        if encoding is not _default:\n            self._encoding = encoding\n        obj = self\n    else:\n        if variables is None:\n            variables = self._variables.copy()\n        if coord_names is None:\n            coord_names = self._coord_names.copy()\n        if dims is None:\n            dims = self._dims.copy()\n        if attrs is _default:\n            attrs = copy.copy(self._attrs)\n        if indexes is None:\n            indexes = self._indexes.copy()\n        if encoding is _default:\n            encoding = copy.copy(self._encoding)\n        obj = self._construct_direct(variables, coord_names, dims, attrs, indexes, encoding)\n    return obj",
    ".xarray.core.dataarray.py@@DataArray._from_temp_dataset": "def _from_temp_dataset(self, dataset: Dataset, name: Hashable | None | Default=_default) -> DataArray:\n    variable = dataset._variables.pop(_THIS_ARRAY)\n    coords = dataset._variables\n    indexes = dataset._indexes\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.encoding": "def encoding(self) -> dict[Hashable, Any]:\n    return self.variable.encoding",
    ".xarray.core.computation.py@@_ensure_numeric": "def _ensure_numeric(data: T_Xarray) -> T_Xarray:\n    from .dataset import Dataset\n\n    def to_floatable(x: DataArray) -> DataArray:\n        if x.dtype.kind == 'M':\n            return x.copy(data=datetime_to_numeric(x.data, offset=np.datetime64('1970-01-01'), datetime_unit='ns'))\n        elif x.dtype.kind == 'm':\n            return x.astype(float)\n        return x\n    if isinstance(data, Dataset):\n        return data.map(to_floatable)\n    else:\n        return to_floatable(data)",
    ".xarray.core.computation.py@@to_floatable": "def to_floatable(x: DataArray) -> DataArray:\n    if x.dtype.kind == 'M':\n        return x.copy(data=datetime_to_numeric(x.data, offset=np.datetime64('1970-01-01'), datetime_unit='ns'))\n    elif x.dtype.kind == 'm':\n        return x.astype(float)\n    return x",
    ".xarray.core.common.py@@zeros_like": "def zeros_like(other, dtype: DTypeLike=None):\n    return full_like(other, 0, dtype)",
    ".xarray.core.common.py@@full_like": "def full_like(other: Dataset, fill_value, dtype: DTypeLike | Mapping[Any, DTypeLike]=None) -> Dataset:\n    ...",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> TypeGuard[Hashable]:\n    return _is_scalar(value, include_0d)",
    ".xarray.core.utils.py@@_is_scalar": "def _is_scalar(value, include_0d):\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.common.py@@_full_like_variable": "def _full_like_variable(other, fill_value, dtype: DTypeLike=None):\n    from .variable import Variable\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(dtype if dtype is not None else other.dtype)\n    if is_duck_dask_array(other.data):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype, chunks=other.data.chunks)\n    else:\n        data = np.full_like(other.data, fill_value, dtype=dtype)\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)",
    ".xarray.core.pycompat.py@@is_duck_dask_array": "def is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
    ".xarray.core.pycompat.py@@is_dask_collection": "def is_dask_collection(x):\n    if dsk.available:\n        from dask.base import is_dask_collection\n        return is_dask_collection(x)\n    else:\n        return False",
    ".xarray.core.dataarray.py@@DataArray.attrs": "def attrs(self) -> dict[Any, Any]:\n    return self.variable.attrs",
    ".xarray.core.dataarray.py@@_check_data_shape": "def _check_data_shape(data, coords, dims):\n    if data is dtypes.NA:\n        data = np.nan\n    if coords is not None and utils.is_scalar(data, include_0d=False):\n        if utils.is_dict_like(coords):\n            if dims is None:\n                return data\n            else:\n                data_shape = tuple((as_variable(coords[k], k).size if k in coords.keys() else 1 for k in dims))\n        else:\n            data_shape = tuple((as_variable(coord, 'foo').size for coord in coords))\n        data = np.full(data_shape, data)\n    return data",
    ".xarray.core.dataarray.py@@_infer_coords_and_dims": "def _infer_coords_and_dims(shape, coords, dims) -> tuple[dict[Any, Variable], tuple[Hashable, ...]]:\n    if coords is not None and (not utils.is_dict_like(coords)) and (len(coords) != len(shape)):\n        raise ValueError(f'coords is not dict-like, but it has {len(coords)} items, which does not match the {len(shape)} dimensions of the data')\n    if isinstance(dims, str):\n        dims = (dims,)\n    if dims is None:\n        dims = [f'dim_{n}' for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            if utils.is_dict_like(coords):\n                dims = list(coords.keys())\n            else:\n                for n, (dim, coord) in enumerate(zip(dims, coords)):\n                    coord = as_variable(coord, name=dims[n]).to_index_variable()\n                    dims[n] = coord.name\n        dims = tuple(dims)\n    elif len(dims) != len(shape):\n        raise ValueError(f'different number of dimensions on data and dims: {len(shape)} vs {len(dims)}')\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError(f'dimension {d} is not a string')\n    new_coords: dict[Any, Variable] = {}\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var.to_index_variable()\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any((d not in dims for d in v.dims)):\n            raise ValueError(f'coordinate {k} has dimensions {v.dims}, but these are not a subset of the DataArray dimensions {dims}')\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError(f'conflicting sizes for dimension {d!r}: length {sizes[d]} on the data but length {s} on coordinate {k!r}')\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError(f'coordinate {k!r} is a DataArray dimension, but it has shape {v.shape!r} rather than expected shape {sizes[k]!r} matching the dimension size')\n    return (new_coords, dims)",
    ".xarray.core.coordinates.py@@Coordinates.__iter__": "def __iter__(self) -> Iterator['Hashable']:\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.merge.py@@_create_indexes_from_coords": "def _create_indexes_from_coords(coords, data_vars=None):\n    all_variables = dict(coords)\n    if data_vars is not None:\n        all_variables.update(data_vars)\n    indexes = {}\n    updated_coords = {}\n    index_vars = {k: v for k, v in all_variables.items() if k in coords or isinstance(v, pd.MultiIndex)}\n    for name, obj in index_vars.items():\n        variable = as_variable(obj, name=name)\n        if variable.dims == (name,):\n            idx, idx_vars = create_default_index_implicit(variable, all_variables)\n            indexes.update({k: idx for k in idx_vars})\n            updated_coords.update(idx_vars)\n            all_variables.update(idx_vars)\n        else:\n            updated_coords[name] = obj\n    return (indexes, updated_coords)",
    ".xarray.core.dataarray.py@@DataArray.isel": "def isel(self, indexers: Mapping[Any, Any]=None, drop: bool=False, missing_dims: ErrorChoiceWithWarn='raise', **indexers_kwargs: Any) -> DataArray:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    if any((is_fancy_indexer(idx) for idx in indexers.values())):\n        ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n        return self._from_temp_dataset(ds)\n    variable = self._variable.isel(indexers, missing_dims=missing_dims)\n    indexes, index_variables = isel_indexes(self.xindexes, indexers)\n    coords = {}\n    for coord_name, coord_value in self._coords.items():\n        if coord_name in index_variables:\n            coord_value = index_variables[coord_name]\n        else:\n            coord_indexers = {k: v for k, v in indexers.items() if k in coord_value.dims}\n            if coord_indexers:\n                coord_value = coord_value.isel(coord_indexers)\n                if drop and coord_value.ndim == 0:\n                    continue\n        coords[coord_name] = coord_value\n    return self._replace(variable=variable, coords=coords, indexes=indexes)",
    ".xarray.core.indexing.py@@is_fancy_indexer": "def is_fancy_indexer(indexer: Any) -> bool:\n    if isinstance(indexer, (int, slice)):\n        return False\n    if isinstance(indexer, np.ndarray):\n        return indexer.ndim > 1\n    if isinstance(indexer, list):\n        return bool(indexer) and (not isinstance(indexer[0], int))\n    return True",
    ".xarray.core.variable.py@@Variable.isel": "def isel(self: T_Variable, indexers: Mapping[Any, Any]=None, missing_dims: ErrorChoiceWithWarn='raise', **indexers_kwargs: Any) -> T_Variable:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n    key = tuple((indexers.get(dim, slice(None)) for dim in self.dims))\n    return self[key]",
    ".xarray.core.utils.py@@drop_dims_from_indexers": "def drop_dims_from_indexers(indexers: Mapping[Any, Any], dims: list | Mapping[Any, int], missing_dims: ErrorChoiceWithWarn) -> Mapping[Hashable, Any]:\n    if missing_dims == 'raise':\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            raise ValueError(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return indexers\n    elif missing_dims == 'warn':\n        indexers = dict(indexers)\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        for key in invalid:\n            indexers.pop(key)\n        return indexers\n    elif missing_dims == 'ignore':\n        return {key: val for key, val in indexers.items() if key in dims}\n    else:\n        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')",
    ".xarray.core.variable.py@@Variable.__getitem__": "def __getitem__(self: T_Variable, key) -> T_Variable:\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    data = as_indexable(self._data)[indexer]\n    if new_order:\n        data = np.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes": "def _broadcast_indexes(self, key):\n    key = self._item_key_to_tuple(key)\n    key = indexing.expanded_indexer(key, self.ndim)\n    key = tuple((k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key))\n    key = tuple((k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key))\n    if all((isinstance(k, BASIC_INDEXING_TYPES) for k in key)):\n        return self._broadcast_indexes_basic(key)\n    self._validate_indexers(key)\n    if all((not isinstance(k, Variable) for k in key)):\n        return self._broadcast_indexes_outer(key)\n    dims = []\n    for k, d in zip(key, self.dims):\n        if isinstance(k, Variable):\n            if len(k.dims) > 1:\n                return self._broadcast_indexes_vectorized(key)\n            dims.append(k.dims[0])\n        elif not isinstance(k, integer_types):\n            dims.append(d)\n    if len(set(dims)) == len(dims):\n        return self._broadcast_indexes_outer(key)\n    return self._broadcast_indexes_vectorized(key)",
    ".xarray.core.variable.py@@Variable._item_key_to_tuple": "def _item_key_to_tuple(self, key):\n    if utils.is_dict_like(key):\n        return tuple((key.get(dim, slice(None)) for dim in self.dims))\n    else:\n        return key",
    ".xarray.core.indexing.py@@expanded_indexer": "def expanded_indexer(key, ndim):\n    if not isinstance(key, tuple):\n        key = (key,)\n    new_key = []\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_basic": "def _broadcast_indexes_basic(self, key):\n    dims = tuple((dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    return (dims, BasicIndexer(key), None)",
    ".xarray.core.indexing.py@@BasicIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.__init__": "def __init__(self, key):\n    if type(self) is ExplicitIndexer:\n        raise TypeError('cannot instantiate base ExplicitIndexer objects')\n    self._key = tuple(key)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexingAdapter(array)\n    if is_duck_dask_array(array):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, '__array_function__'):\n        return NdArrayLikeIndexingAdapter(array)\n    raise TypeError(f'Invalid array type: {type(array)}')",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    array, key = self._indexing_array_and_key(key)\n    return array[key]",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter._indexing_array_and_key": "def _indexing_array_and_key(self, key):\n    if isinstance(key, OuterIndexer):\n        array = self.array\n        key = _outer_to_numpy_indexer(key, self.array.shape)\n    elif isinstance(key, VectorizedIndexer):\n        array = nputils.NumpyVIndexAdapter(self.array)\n        key = key.tuple\n    elif isinstance(key, BasicIndexer):\n        array = self.array\n        key = key.tuple + (Ellipsis,)\n    else:\n        raise TypeError(f'unexpected key type: {type(key)}')\n    return (array, key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.tuple": "def tuple(self):\n    return self._key",
    ".xarray.core.variable.py@@Variable._finalize_indexing_result": "def _finalize_indexing_result(self: T_Variable, dims, data) -> T_Variable:\n    return self._replace(dims=dims, data=data)",
    ".xarray.core.indexes.py@@isel_indexes": "def isel_indexes(indexes: Indexes[Index], indexers: Mapping[Any, Any]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    return _apply_indexes(indexes, indexers, 'isel')",
    ".xarray.core.indexes.py@@_apply_indexes": "def _apply_indexes(indexes: Indexes[Index], args: Mapping[Any, Any], func: str) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    new_indexes: dict[Hashable, Index] = {k: v for k, v in indexes.items()}\n    new_index_variables: dict[Hashable, Variable] = {}\n    for index, index_vars in indexes.group_by_index():\n        index_dims = {d for var in index_vars.values() for d in var.dims}\n        index_args = {k: v for k, v in args.items() if k in index_dims}\n        if index_args:\n            new_index = getattr(index, func)(index_args)\n            if new_index is not None:\n                new_indexes.update({k: new_index for k in index_vars})\n                new_index_vars = new_index.create_variables(index_vars)\n                new_index_variables.update(new_index_vars)\n            else:\n                for k in index_vars:\n                    new_indexes.pop(k, None)\n    return (new_indexes, new_index_variables)",
    ".xarray.core.indexes.py@@PandasIndex.isel": "def isel(self, indexers: Mapping[Any, int | slice | np.ndarray | Variable]) -> PandasIndex | None:\n    from .variable import Variable\n    indxr = indexers[self.dim]\n    if isinstance(indxr, Variable):\n        if indxr.dims != (self.dim,):\n            return None\n        else:\n            indxr = indxr.data\n    if not isinstance(indxr, slice) and is_scalar(indxr):\n        return None\n    return self._replace(self.index[indxr])",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__getitem__": "def __getitem__(self, indexer) -> PandasIndexingAdapter | NumpyIndexingAdapter | np.ndarray | np.datetime64 | np.timedelta64:\n    key = indexer.tuple\n    if isinstance(key, tuple) and len(key) == 1:\n        key, = key\n    if getattr(key, 'ndim', 0) > 1:\n        return NumpyIndexingAdapter(np.asarray(self))[indexer]\n    result = self.array[key]\n    if isinstance(result, pd.Index):\n        return type(self)(result, dtype=self.dtype)\n    else:\n        return self._convert_scalar(result)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter._convert_scalar": "def _convert_scalar(self, item):\n    if item is pd.NaT:\n        item = np.datetime64('NaT', 'ns')\n    elif isinstance(item, timedelta):\n        item = np.timedelta64(getattr(item, 'value', item), 'ns')\n    elif isinstance(item, pd.Timestamp):\n        item = np.asarray(item.to_datetime64())\n    elif self.dtype != object:\n        item = np.asarray(item, dtype=self.dtype)\n    return utils.to_0d_array(item)",
    ".xarray.core.utils.py@@to_0d_array": "def to_0d_array(value: Any) -> np.ndarray:\n    if np.isscalar(value) or (isinstance(value, np.ndarray) and value.ndim == 0):\n        return np.array(value)\n    else:\n        return to_0d_object_array(value)",
    ".xarray.core.variable.py@@IndexVariable._finalize_indexing_result": "def _finalize_indexing_result(self, dims, data):\n    if getattr(data, 'ndim', 0) != 1:\n        return Variable(dims, data, self._attrs, self._encoding)\n    else:\n        return self._replace(dims=dims, data=data)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__add__": "def __add__(self, other):\n    return self._binary_op(other, operator.add)",
    ".xarray.core.dataarray.py@@DataArray._binary_op": "def _binary_op(self, other, f: Callable, reflexive: bool=False):\n    if isinstance(other, (Dataset, groupby.GroupBy)):\n        return NotImplemented\n    if isinstance(other, DataArray):\n        align_type = OPTIONS['arithmetic_join']\n        self, other = align(self, other, join=align_type, copy=False)\n    other_variable = getattr(other, 'variable', other)\n    other_coords = getattr(other, 'coords', None)\n    variable = f(self.variable, other_variable) if not reflexive else f(other_variable, self.variable)\n    coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n    name = self._result_name(other)\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.alignment.py@@align": "def align(*objects: DataAlignable, join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA) -> tuple[DataAlignable, ...]:\n    aligner = Aligner(objects, join=join, copy=copy, indexes=indexes, exclude_dims=exclude, fill_value=fill_value)\n    aligner.align()\n    return aligner.results",
    ".xarray.core.common.py@@AbstractArray.sizes": "def sizes(self: Any) -> Mapping[Hashable, int]:\n    return Frozen(dict(zip(self.dims, self.shape)))",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__add__": "def __add__(self, other):\n    return self._binary_op(other, operator.add)",
    ".xarray.core.variable.py@@Variable._binary_op": "def _binary_op(self, other, f, reflexive=False):\n    if isinstance(other, (xr.DataArray, xr.Dataset)):\n        return NotImplemented\n    if reflexive and issubclass(type(self), type(other)):\n        other_data, self_data, dims = _broadcast_compat_data(other, self)\n    else:\n        self_data, other_data, dims = _broadcast_compat_data(self, other)\n    keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    with np.errstate(all='ignore'):\n        new_data = f(self_data, other_data) if not reflexive else f(other_data, self_data)\n    result = Variable(dims, new_data, attrs=attrs)\n    return result",
    ".xarray.core.variable.py@@_broadcast_compat_data": "def _broadcast_compat_data(self, other):\n    if all((hasattr(other, attr) for attr in ['dims', 'data', 'shape', 'encoding'])):\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return (self_data, other_data, dims)",
    ".xarray.core.variable.py@@_broadcast_compat_variables": "def _broadcast_compat_variables(*variables):\n    dims = tuple(_unified_dims(variables))\n    return tuple((var.set_dims(dims) if var.dims != dims else var for var in variables))",
    ".xarray.core.variable.py@@_unified_dims": "def _unified_dims(variables):\n    all_dims = {}\n    for var in variables:\n        var_dims = var.dims\n        if len(set(var_dims)) < len(var_dims):\n            raise ValueError(f'broadcasting cannot handle duplicate dimensions: {list(var_dims)!r}')\n        for d, s in zip(var_dims, var.shape):\n            if d not in all_dims:\n                all_dims[d] = s\n            elif all_dims[d] != s:\n                raise ValueError(f'operands cannot be broadcast together with mismatched lengths for dimension {d!r}: {(all_dims[d], s)}')\n    return all_dims",
    ".xarray.core.variable.py@@Variable.set_dims": "def set_dims(self, dims, shape=None):\n    if isinstance(dims, str):\n        dims = [dims]\n    if shape is None and utils.is_dict_like(dims):\n        shape = dims.values()\n    missing_dims = set(self.dims) - set(dims)\n    if missing_dims:\n        raise ValueError(f'new dimensions {dims!r} must be a superset of existing dimensions {self.dims!r}')\n    self_dims = set(self.dims)\n    expanded_dims = tuple((d for d in dims if d not in self_dims)) + self.dims\n    if self.dims == expanded_dims:\n        expanded_data = self.data\n    elif shape is not None:\n        dims_map = dict(zip(dims, shape))\n        tmp_shape = tuple((dims_map[d] for d in expanded_dims))\n        expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n    else:\n        expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n    expanded_var = Variable(expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True)\n    return expanded_var.transpose(*dims)",
    ".xarray.core.variable.py@@Variable.transpose": "def transpose(self, *dims, missing_dims: ErrorChoiceWithWarn='raise') -> Variable:\n    if len(dims) == 0:\n        dims = self.dims[::-1]\n    else:\n        dims = tuple(infix_dims(dims, self.dims, missing_dims))\n    if len(dims) < 2 or dims == self.dims:\n        return self.copy(deep=False)\n    axes = self.get_axis_num(dims)\n    data = as_indexable(self._data).transpose(axes)\n    return self._replace(dims=dims, data=data)",
    ".xarray.core.utils.py@@infix_dims": "def infix_dims(dims_supplied: Collection, dims_all: Collection, missing_dims: ErrorChoiceWithWarn='raise') -> Iterator:\n    if ... in dims_supplied:\n        if len(set(dims_all)) != len(dims_all):\n            raise ValueError('Cannot use ellipsis with repeated dims')\n        if list(dims_supplied).count(...) > 1:\n            raise ValueError('More than one ellipsis supplied')\n        other_dims = [d for d in dims_all if d not in dims_supplied]\n        existing_dims = drop_missing_dims(dims_supplied, dims_all, missing_dims)\n        for d in existing_dims:\n            if d is ...:\n                yield from other_dims\n            else:\n                yield d\n    else:\n        existing_dims = drop_missing_dims(dims_supplied, dims_all, missing_dims)\n        if set(existing_dims) ^ set(dims_all):\n            raise ValueError(f'{dims_supplied} must be a permuted list of {dims_all}, unless `...` is included')\n        yield from existing_dims",
    ".xarray.core.utils.py@@drop_missing_dims": "def drop_missing_dims(supplied_dims: Collection, dims: Collection, missing_dims: ErrorChoiceWithWarn) -> Collection:\n    if missing_dims == 'raise':\n        supplied_dims_set = {val for val in supplied_dims if val is not ...}\n        invalid = supplied_dims_set - set(dims)\n        if invalid:\n            raise ValueError(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return supplied_dims\n    elif missing_dims == 'warn':\n        invalid = set(supplied_dims) - set(dims)\n        if invalid:\n            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return [val for val in supplied_dims if val in dims or val is ...]\n    elif missing_dims == 'ignore':\n        return [val for val in supplied_dims if val in dims or val is ...]\n    else:\n        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')",
    ".xarray.core.coordinates.py@@Coordinates._merge_raw": "def _merge_raw(self, other, reflexive):\n    if other is None:\n        variables = dict(self.variables)\n        indexes = dict(self.xindexes)\n    else:\n        coord_list = [self, other] if not reflexive else [other, self]\n        variables, indexes = merge_coordinates_without_align(coord_list)\n    return (variables, indexes)",
    ".xarray.core.merge.py@@merge_coordinates_without_align": "def merge_coordinates_without_align(objects: list[Coordinates], prioritized: Mapping[Any, MergeElement]=None, exclude_dims: AbstractSet=frozenset(), combine_attrs: str='override') -> tuple[dict[Hashable, Variable], dict[Hashable, Index]]:\n    collected = collect_from_coordinates(objects)\n    if exclude_dims:\n        filtered: dict[Hashable, list[MergeElement]] = {}\n        for name, elements in collected.items():\n            new_elements = [(variable, index) for variable, index in elements if exclude_dims.isdisjoint(variable.dims)]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n    merged_coords, merged_indexes = merge_collected(filtered, prioritized, combine_attrs=combine_attrs)\n    merged_indexes = filter_indexes_from_coords(merged_indexes, set(merged_coords))\n    return (merged_coords, merged_indexes)",
    ".xarray.core.merge.py@@collect_from_coordinates": "def collect_from_coordinates(list_of_coords: list[Coordinates]) -> dict[Hashable, list[MergeElement]]:\n    grouped: dict[Hashable, list[MergeElement]] = defaultdict(list)\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.xindexes\n        for name, variable in variables.items():\n            grouped[name].append((variable, indexes.get(name)))\n    return grouped",
    ".xarray.core.coordinates.py@@Coordinates.xindexes": "def xindexes(self) -> Indexes[Index]:\n    return self._data.xindexes",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: dict[Hashable, list[MergeElement]], prioritized: Mapping[Any, MergeElement]=None, compat: str='minimal', combine_attrs: str | None='override', equals: dict[Hashable, bool]=None) -> tuple[dict[Hashable, Variable], dict[Hashable, Index]]:\n    if prioritized is None:\n        prioritized = {}\n    if equals is None:\n        equals = {}\n    _assert_compat_valid(compat)\n    _assert_prioritized_valid(grouped, prioritized)\n    merged_vars: dict[Hashable, Variable] = {}\n    merged_indexes: dict[Hashable, Index] = {}\n    index_cmp_cache: dict[tuple[int, int], bool | None] = {}\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for other_var, other_index in indexed_elements[1:]:\n                    if not indexes_equal(index, other_index, variable, other_var, index_cmp_cache):\n                        raise MergeError(f'conflicting values/indexes on objects to be combined fo coordinate {name!r}\\nfirst index: {index!r}\\nsecond index: {other_index!r}\\nfirst variable: {variable!r}\\nsecond variable: {other_var!r}\\n')\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError(f'conflicting attribute values on combined variable {name!r}:\\nfirst value: {variable.attrs!r}\\nsecond value: {other_variable.attrs!r}')\n                merged_vars[name] = variable\n                merged_vars[name].attrs = merge_attrs([var.attrs for var, _ in indexed_elements], combine_attrs=combine_attrs)\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat, equals.get(name, None))\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n                if name in merged_vars:\n                    merged_vars[name].attrs = merge_attrs([var.attrs for var in variables], combine_attrs=combine_attrs)\n    return (merged_vars, merged_indexes)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError(f'compat={compat!r} invalid: must be {set(_VALID_COMPAT)}')",
    ".xarray.core.merge.py@@_assert_prioritized_valid": "def _assert_prioritized_valid(grouped: dict[Hashable, list[MergeElement]], prioritized: Mapping[Any, MergeElement]) -> None:\n    prioritized_names = set(prioritized)\n    grouped_by_index: dict[int, list[Hashable]] = defaultdict(list)\n    indexes: dict[int, Index] = {}\n    for name, elements_list in grouped.items():\n        for _, index in elements_list:\n            if index is not None:\n                grouped_by_index[id(index)].append(name)\n                indexes[id(index)] = index\n    for index_id, index_coord_names in grouped_by_index.items():\n        index_names = set(index_coord_names)\n        common_names = index_names & prioritized_names\n        if common_names and len(common_names) != len(index_names):\n            common_names_str = ', '.join((f'{k!r}' for k in common_names))\n            index_names_str = ', '.join((f'{k!r}' for k in index_coord_names))\n            raise ValueError(f'cannot set or update variable(s) {common_names_str}, which would corrupt the following index built from coordinates {index_names_str}:\\n{indexes[index_id]!r}')",
    ".xarray.core.dataarray.py@@DataArray._result_name": "def _result_name(self, other: Any=None) -> Hashable | None:\n    other_name = getattr(other, 'name', _default)\n    if other_name is _default or other_name == self.name:\n        return self.name\n    else:\n        return None",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__imul__": "def __imul__(self, other):\n    return self._inplace_binary_op(other, operator.imul)",
    ".xarray.core.dataarray.py@@DataArray._inplace_binary_op": "def _inplace_binary_op(self, other, f: Callable):\n    if isinstance(other, groupby.GroupBy):\n        raise TypeError('in-place operations between a DataArray and a grouped object are not permitted')\n    other_coords = getattr(other, 'coords', None)\n    other_variable = getattr(other, 'variable', other)\n    try:\n        with self.coords._merge_inplace(other_coords):\n            f(self.variable, other_variable)\n    except MergeError as exc:\n        raise MergeError('Automatic alignment is not supported for in-place operations.\\nConsider aligning the indices manually or using a not-in-place operation.\\nSee https://github.com/pydata/xarray/issues/3910 for more explanations.') from exc\n    return self",
    ".xarray.core.coordinates.py@@Coordinates._merge_inplace": "def _merge_inplace(self, other):\n    if other is None:\n        yield\n    else:\n        prioritized = {k: (v, None) for k, v in self.variables.items() if k not in self.xindexes}\n        variables, indexes = merge_coordinates_without_align([self, other], prioritized)\n        yield\n        self._update_coords(variables, indexes)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__imul__": "def __imul__(self, other):\n    return self._inplace_binary_op(other, operator.imul)",
    ".xarray.core.variable.py@@Variable._inplace_binary_op": "def _inplace_binary_op(self, other, f):\n    if isinstance(other, xr.Dataset):\n        raise TypeError('cannot add a Dataset to a Variable in-place')\n    self_data, other_data, dims = _broadcast_compat_data(self, other)\n    if dims != self.dims:\n        raise ValueError('dimensions cannot change for in-place operations')\n    with np.errstate(all='ignore'):\n        self.values = f(self_data, other_data)\n    return self",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._update_coords": "def _update_coords(self, coords: Dict[Hashable, Variable], indexes: Mapping[Any, Index]) -> None:\n    coords_plus_data = coords.copy()\n    coords_plus_data[_THIS_ARRAY] = self._data.variable\n    dims = calculate_dimensions(coords_plus_data)\n    if not set(dims) <= set(self.dims):\n        raise ValueError('cannot add coordinates with new dimensions to a DataArray')\n    self._data._coords = coords\n    original_indexes = dict(self._data.xindexes)\n    original_indexes.update(indexes)\n    self._data._indexes = original_indexes",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self._data.dims",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__iadd__": "def __iadd__(self, other):\n    return self._inplace_binary_op(other, operator.iadd)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__iadd__": "def __iadd__(self, other):\n    return self._inplace_binary_op(other, operator.iadd)",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__init__": "def __init__(self, array):\n    self.array = array",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    if not isinstance(key, VectorizedIndexer):\n        rewritten_indexer = False\n        new_indexer = []\n        for idim, k in enumerate(key.tuple):\n            if isinstance(k, Iterable) and duck_array_ops.array_equiv(k, np.arange(self.array.shape[idim])):\n                new_indexer.append(slice(None))\n                rewritten_indexer = True\n            else:\n                new_indexer.append(k)\n        if rewritten_indexer:\n            key = type(key)(tuple(new_indexer))\n    if isinstance(key, BasicIndexer):\n        return self.array[key.tuple]\n    elif isinstance(key, VectorizedIndexer):\n        return self.array.vindex[key.tuple]\n    else:\n        assert isinstance(key, OuterIndexer)\n        key = key.tuple\n        try:\n            return self.array[key]\n        except NotImplementedError:\n            value = self.array\n            for axis, subkey in reversed(list(enumerate(key))):\n                value = value[(slice(None),) * axis + (subkey,)]\n            return value",
    ".xarray.core.indexing.py@@as_integer_slice": "def as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)",
    ".xarray.core.indexing.py@@as_integer_or_none": "def as_integer_or_none(value):\n    return None if value is None else operator.index(value)",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Hashable | Iterable[Hashable]) -> int | tuple[int, ...]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError(f'{dim!r} not found in array dimensions {self.dims!r}')",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.transpose": "def transpose(self, order):\n    return self.array.transpose(order)",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.transpose": "def transpose(self, order):\n    return self.array.transpose(order)",
    ".xarray.core.indexes.py@@PandasIndex.reindex_like": "def reindex_like(self, other: PandasIndex, method=None, tolerance=None) -> dict[Hashable, Any]:\n    if not self.index.is_unique:\n        raise ValueError(f'cannot reindex or align along dimension {self.dim!r} because the (pandas) index has duplicate values')\n    return {self.dim: get_indexer_nd(self.index, other.index, method, tolerance)}",
    ".xarray.core.indexes.py@@get_indexer_nd": "def get_indexer_nd(index, labels, method=None, tolerance=None):\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer",
    ".xarray.core.dataset.py@@Dataset.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.alignment.py@@reindex_variables": "def reindex_variables(variables: Mapping[Any, Variable], dim_pos_indexers: Mapping[Any, Any], copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False) -> dict[Hashable, Variable]:\n    new_variables = {}\n    dim_sizes = calculate_dimensions(variables)\n    masked_dims = set()\n    unchanged_dims = set()\n    for dim, indxr in dim_pos_indexers.items():\n        if (indxr < 0).any():\n            masked_dims.add(dim)\n        elif np.array_equal(indxr, np.arange(dim_sizes.get(dim, 0))):\n            unchanged_dims.add(dim)\n    for name, var in variables.items():\n        if isinstance(fill_value, dict):\n            fill_value_ = fill_value.get(name, dtypes.NA)\n        else:\n            fill_value_ = fill_value\n        if sparse:\n            var = var._as_sparse(fill_value=fill_value_)\n        indxr = tuple((slice(None) if d in unchanged_dims else dim_pos_indexers.get(d, slice(None)) for d in var.dims))\n        needs_masking = any((d in masked_dims for d in var.dims))\n        if needs_masking:\n            new_var = var._getitem_with_mask(indxr, fill_value=fill_value_)\n        elif all((is_full_slice(k) for k in indxr)):\n            new_var = var.copy(deep=copy)\n        else:\n            new_var = var[indxr]\n        new_variables[name] = new_var\n    return new_variables",
    ".xarray.core.utils.py@@is_full_slice": "def is_full_slice(value: Any) -> bool:\n    return isinstance(value, slice) and value == slice(None)",
    ".xarray.core.variable.py@@Variable._validate_indexers": "def _validate_indexers(self, key):\n    for dim, k in zip(self.dims, key):\n        if not isinstance(k, BASIC_INDEXING_TYPES):\n            if not isinstance(k, Variable):\n                k = np.asarray(k)\n                if k.ndim > 1:\n                    raise IndexError('Unlabeled multi-dimensional array cannot be used for indexing: {}'.format(k))\n            if k.dtype.kind == 'b':\n                if self.shape[self.get_axis_num(dim)] != len(k):\n                    raise IndexError('Boolean array size {:d} is used to index array with shape {:s}.'.format(len(k), str(self.shape)))\n                if k.ndim > 1:\n                    raise IndexError('{}-dimensional boolean indexing is not supported. '.format(k.ndim))\n                if getattr(k, 'dims', (dim,)) != (dim,):\n                    raise IndexError('Boolean indexer should be unlabeled or on the same dimension to the indexed array. Indexer is on {:s} but the target dimension is {:s}.'.format(str(k.dims), dim))",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_outer": "def _broadcast_indexes_outer(self, key):\n    dims = tuple((k.dims[0] if isinstance(k, Variable) else dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    new_key = []\n    for k in key:\n        if isinstance(k, Variable):\n            k = k.data\n        if not isinstance(k, BASIC_INDEXING_TYPES):\n            k = np.asarray(k)\n            if k.size == 0:\n                k = k.astype(int)\n            elif k.dtype.kind == 'b':\n                k, = np.nonzero(k)\n        new_key.append(k)\n    return (dims, OuterIndexer(tuple(new_key)), None)",
    ".xarray.core.indexing.py@@OuterIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if k.ndim != 1:\n                raise TypeError(f'invalid indexer array for {type(self).__name__}; must have exactly 1 dimension: {k!r}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@_outer_to_numpy_indexer": "def _outer_to_numpy_indexer(key, shape):\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple",
    ".xarray.core.dataset.py@@Dataset._replace_with_new_dims": "def _replace_with_new_dims(self, variables: dict[Hashable, Variable], coord_names: set=None, attrs: dict[Hashable, Any] | None | Default=_default, indexes: dict[Hashable, Index]=None, inplace: bool=False) -> Dataset:\n    dims = calculate_dimensions(variables)\n    return self._replace(variables, coord_names, dims, attrs, indexes, inplace=inplace)",
    ".xarray.core.duck_array_ops.py@@array_equiv": "def array_equiv(arr1, arr2):\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', \"In the future, 'NAT == x'\")\n            flag_array = (arr1 == arr2) | isnull(arr1) & isnull(arr2)\n            return bool(flag_array.all())\n    else:\n        return lazy_equiv",
    ".xarray.core.duck_array_ops.py@@lazy_array_equiv": "def lazy_array_equiv(arr1, arr2):\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_array and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        if tokenize(arr1) == tokenize(arr2):\n            return True\n        else:\n            return None\n    return None",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, (np.ndarray, dask_array_type)):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.variable.py@@Variable._getitem_with_mask": "def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(self.dtype)\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    if self.size:\n        if is_duck_dask_array(self._data):\n            actual_indexer = indexing.posify_mask_indexer(indexer)\n        else:\n            actual_indexer = indexer\n        data = as_indexable(self._data)[actual_indexer]\n        mask = indexing.create_mask(indexer, self.shape, data)\n        data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n    else:\n        mask = indexing.create_mask(indexer, self.shape)\n        data = np.broadcast_to(fill_value, getattr(mask, 'shape', ()))\n    if new_order:\n        data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.utils.py@@NdimSizeLenMixin.size": "def size(self: Any) -> int:\n    return int(np.prod(self.shape))",
    ".xarray.core.indexing.py@@create_mask": "def create_mask(indexer, shape, data=None):\n    if isinstance(indexer, OuterIndexer):\n        key = _outer_to_vectorized_indexer(indexer, shape).tuple\n        assert not any((isinstance(k, slice) for k in key))\n        mask = _masked_result_drop_slice(key, data)\n    elif isinstance(indexer, VectorizedIndexer):\n        key = indexer.tuple\n        base_mask = _masked_result_drop_slice(key, data)\n        slice_shape = tuple((np.arange(*k.indices(size)).size for k, size in zip(key, shape) if isinstance(k, slice)))\n        expanded_mask = base_mask[(Ellipsis,) + (np.newaxis,) * len(slice_shape)]\n        mask = duck_array_ops.broadcast_to(expanded_mask, base_mask.shape + slice_shape)\n    elif isinstance(indexer, BasicIndexer):\n        mask = any((k == -1 for k in indexer.tuple))\n    else:\n        raise TypeError(f'unexpected key type: {type(indexer)}')\n    return mask",
    ".xarray.core.indexing.py@@_outer_to_vectorized_indexer": "def _outer_to_vectorized_indexer(key, shape):\n    key = key.tuple\n    n_dim = len([k for k in key if not isinstance(k, integer_types)])\n    i_dim = 0\n    new_key = []\n    for k, size in zip(key, shape):\n        if isinstance(k, integer_types):\n            new_key.append(np.array(k).reshape((1,) * n_dim))\n        else:\n            if isinstance(k, slice):\n                k = np.arange(*k.indices(size))\n            assert k.dtype.kind in {'i', 'u'}\n            shape = [(1,) * i_dim + (k.size,) + (1,) * (n_dim - i_dim - 1)]\n            new_key.append(k.reshape(*shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))",
    ".xarray.core.indexing.py@@VectorizedIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    ndim = None\n    for k in key:\n        if isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if ndim is None:\n                ndim = k.ndim\n            elif ndim != k.ndim:\n                ndims = [k.ndim for k in key if isinstance(k, np.ndarray)]\n                raise ValueError(f'invalid indexer key: ndarray arguments have different numbers of dimensions: {ndims}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@_masked_result_drop_slice": "def _masked_result_drop_slice(key, data=None):\n    key = (k for k in key if not isinstance(k, slice))\n    chunks_hint = getattr(data, 'chunks', None)\n    new_keys = []\n    for k in key:\n        if isinstance(k, np.ndarray):\n            if is_duck_dask_array(data):\n                new_keys.append(_dask_array_with_chunks_hint(k, chunks_hint))\n            elif isinstance(data, sparse_array_type):\n                import sparse\n                new_keys.append(sparse.COO.from_numpy(k))\n            else:\n                new_keys.append(k)\n        else:\n            new_keys.append(k)\n    mask = _logical_any((k == -1 for k in new_keys))\n    return mask",
    ".xarray.core.indexing.py@@_logical_any": "def _logical_any(args):\n    return functools.reduce(operator.or_, args)",
    ".xarray.core.duck_array_ops.py@@where": "def where(condition, x, y):\n    return _where(condition, *as_shared_dtype([x, y]))",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    if any((isinstance(x, cupy_array_type) for x in scalars_or_arrays)):\n        import cupy as cp\n        arrays = [asarray(x, xp=cp) for x in scalars_or_arrays]\n    else:\n        arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)",
    ".xarray.core.indexing.py@@posify_mask_indexer": "def posify_mask_indexer(indexer):\n    key = tuple((_posify_mask_subindexer(k.ravel()).reshape(k.shape) if isinstance(k, np.ndarray) else k for k in indexer.tuple))\n    return type(indexer)(key)",
    ".xarray.core.indexing.py@@_posify_mask_subindexer": "def _posify_mask_subindexer(index):\n    masked = index == -1\n    unmasked_locs = np.flatnonzero(~masked)\n    if not unmasked_locs.size:\n        return np.zeros_like(index)\n    masked_locs = np.flatnonzero(masked)\n    prev_value = np.maximum(0, np.searchsorted(unmasked_locs, masked_locs) - 1)\n    new_index = index.copy()\n    new_index[masked_locs] = index[unmasked_locs[prev_value]]\n    return new_index",
    ".xarray.core.indexing.py@@_dask_array_with_chunks_hint": "def _dask_array_with_chunks_hint(array, chunks):\n    import dask.array as da\n    if len(chunks) < array.ndim:\n        raise ValueError('not enough chunks in hint')\n    new_chunks = []\n    for chunk, size in zip(chunks, array.shape):\n        new_chunks.append(chunk if size > 1 else (1,))\n    return da.from_array(array, new_chunks)",
    ".xarray.core.dataset.py@@Dataset.__getitem__": "def __getitem__(self, key: Mapping) -> Dataset:\n    ...",
    ".xarray.core.utils.py@@hashable": "def hashable(v: Any) -> bool:\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True",
    ".xarray.core.dataset.py@@Dataset._construct_dataarray": "def _construct_dataarray(self, name: Hashable) -> DataArray:\n    from .dataarray import DataArray\n    try:\n        variable = self._variables[name]\n    except KeyError:\n        _, name, variable = _get_virtual_variable(self._variables, name, self.dims)\n    needed_dims = set(variable.dims)\n    coords: dict[Hashable, Variable] = {}\n    for k in self._variables:\n        if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n            coords[k] = self.variables[k]\n    indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.dataset.py@@Dataset.reindex": "def reindex(self, indexers: Mapping[Any, Any]=None, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA, **indexers_kwargs: Any) -> Dataset:\n    indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, 'reindex')\n    return alignment.reindex(self, indexers=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.dataset.py@@Dataset.dims": "def dims(self) -> Mapping[Hashable, int]:\n    return Frozen(self._dims)",
    ".xarray.core.dataset.py@@Dataset.encoding": "def encoding(self) -> dict:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.dataset.py@@Dataset.isel": "def isel(self, indexers: Mapping[Any, Any]=None, drop: bool=False, missing_dims: ErrorChoiceWithWarn='raise', **indexers_kwargs: Any) -> Dataset:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    if any((is_fancy_indexer(idx) for idx in indexers.values())):\n        return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n    indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n    variables = {}\n    dims: dict[Hashable, int] = {}\n    coord_names = self._coord_names.copy()\n    indexes, index_variables = isel_indexes(self.xindexes, indexers)\n    for name, var in self._variables.items():\n        if name in index_variables:\n            var = index_variables[name]\n        else:\n            var_indexers = {k: v for k, v in indexers.items() if k in var.dims}\n            if var_indexers:\n                var = var.isel(var_indexers)\n                if drop and var.ndim == 0 and (name in coord_names):\n                    coord_names.remove(name)\n                    continue\n        variables[name] = var\n        dims.update(zip(var.dims, var.shape))\n    return self._construct_direct(variables=variables, coord_names=coord_names, dims=dims, attrs=self._attrs, indexes=indexes, encoding=self._encoding, close=self._close)",
    ".xarray.core._typed_ops.py@@DatasetOpsMixin.__radd__": "def __radd__(self, other):\n    return self._binary_op(other, operator.add, reflexive=True)",
    ".xarray.core.dataset.py@@Dataset._binary_op": "def _binary_op(self, other, f, reflexive=False, join=None):\n    from .dataarray import DataArray\n    if isinstance(other, groupby.GroupBy):\n        return NotImplemented\n    align_type = OPTIONS['arithmetic_join'] if join is None else join\n    if isinstance(other, (DataArray, Dataset)):\n        self, other = align(self, other, join=align_type, copy=False)\n    g = f if not reflexive else lambda x, y: f(y, x)\n    ds = self._calculate_binary_op(g, other, join=align_type)\n    return ds",
    ".xarray.core.dataset.py@@Dataset._calculate_binary_op": "def _calculate_binary_op(self, f, other, join='inner', inplace=False):\n\n    def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n        if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n            raise ValueError(f'datasets must have the same data variables for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}')\n        dest_vars = {}\n        for k in lhs_data_vars:\n            if k in rhs_data_vars:\n                dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n            elif join in ['left', 'outer']:\n                dest_vars[k] = f(lhs_vars[k], np.nan)\n        for k in rhs_data_vars:\n            if k not in dest_vars and join in ['right', 'outer']:\n                dest_vars[k] = f(rhs_vars[k], np.nan)\n        return dest_vars\n    if utils.is_dict_like(other) and (not isinstance(other, Dataset)):\n        new_data_vars = apply_over_both(self.data_vars, other, self.data_vars, other)\n        return Dataset(new_data_vars)\n    other_coords = getattr(other, 'coords', None)\n    ds = self.coords.merge(other_coords)\n    if isinstance(other, Dataset):\n        new_vars = apply_over_both(self.data_vars, other.data_vars, self.variables, other.variables)\n    else:\n        other_variable = getattr(other, 'variable', other)\n        new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n    ds._variables.update(new_vars)\n    ds._dims = calculate_dimensions(ds._variables)\n    return ds",
    ".xarray.core.common.py@@AttrAccessMixin.__getattr__": "def __getattr__(self, name: str) -> Any:\n    if name not in {'__dict__', '__setstate__'}:\n        for source in self._attr_sources:\n            with suppress(KeyError):\n                return source[name]\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {name!r}')",
    ".xarray.core.dataarray.py@@DataArray._attr_sources": "def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield from self._item_sources\n    yield self.attrs",
    ".xarray.core.dataarray.py@@DataArray._item_sources": "def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n    yield HybridMappingProxy(keys=self.dims, mapping={})",
    ".xarray.core.utils.py@@HybridMappingProxy.__init__": "def __init__(self, keys: Collection[K], mapping: Mapping[K, V]):\n    self._keys = keys\n    self.mapping = mapping",
    ".xarray.core.utils.py@@HybridMappingProxy.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    return self._data._getitem_coord(key)",
    ".xarray.core.dataset.py@@_get_virtual_variable": "def _get_virtual_variable(variables, key: Hashable, dim_sizes: Mapping=None) -> tuple[Hashable, Hashable, Variable]:\n    if dim_sizes is None:\n        dim_sizes = {}\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return (key, key, variable)\n    if not isinstance(key, str):\n        raise KeyError(key)\n    split_key = key.split('.', 1)\n    if len(split_key) != 2:\n        raise KeyError(key)\n    ref_name, var_name = split_key\n    ref_var = variables[ref_name]\n    if _contains_datetime_like_objects(ref_var):\n        ref_var = xr.DataArray(ref_var)\n        data = getattr(ref_var.dt, var_name).data\n    else:\n        data = getattr(ref_var, var_name).data\n    virtual_var = Variable(ref_var.dims, data)\n    return (ref_name, var_name, virtual_var)",
    ".xarray.core.dataset.py@@Dataset.coords": "def coords(self) -> DatasetCoordinates:\n    return DatasetCoordinates(self)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._data = dataset",
    ".xarray.core.coordinates.py@@Coordinates.merge": "def merge(self, other: 'Coordinates') -> 'Dataset':\n    from .dataset import Dataset\n    if other is None:\n        return self.to_dataset()\n    if not isinstance(other, Coordinates):\n        other = Dataset(coords=other).coords\n    coords, indexes = merge_coordinates_without_align([self, other])\n    coord_names = set(coords)\n    return Dataset._construct_direct(variables=coords, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen({k: v for k, v in self._data.variables.items() if k in self._names})",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return self._data._coord_names",
    ".xarray.core.dataset.py@@Dataset.data_vars": "def data_vars(self) -> DataVariables:\n    return DataVariables(self)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: Dataset):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core._typed_ops.py@@DatasetOpsMixin.__imul__": "def __imul__(self, other):\n    return self._inplace_binary_op(other, operator.imul)",
    ".xarray.core.dataset.py@@Dataset._inplace_binary_op": "def _inplace_binary_op(self, other, f):\n    from .dataarray import DataArray\n    if isinstance(other, groupby.GroupBy):\n        raise TypeError('in-place operations between a Dataset and a grouped object are not permitted')\n    if isinstance(other, (DataArray, Dataset)):\n        other = other.reindex_like(self, copy=False)\n    g = ops.inplace_to_noninplace_op(f)\n    ds = self._calculate_binary_op(g, other, inplace=True)\n    self._replace_with_new_dims(ds._variables, ds._coord_names, attrs=ds._attrs, indexes=ds._indexes, inplace=True)\n    return self",
    ".xarray.core.dataarray.py@@DataArray.reindex_like": "def reindex_like(self, other: DataArray | Dataset, method: str | None=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value=dtypes.NA) -> DataArray:\n    return alignment.reindex_like(self, other=other, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.alignment.py@@reindex_like": "def reindex_like(obj: DataAlignable, other: Dataset | DataArray, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA) -> DataAlignable:\n    if not other._indexes:\n        for dim in other.dims:\n            if dim in obj.dims:\n                other_size = other.sizes[dim]\n                obj_size = obj.sizes[dim]\n                if other_size != obj_size:\n                    raise ValueError(f'different size for unlabeled dimension on argument {dim!r}: {other_size!r} vs {obj_size!r}')\n    return reindex(obj, indexers=other.xindexes, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.dataset.py@@Dataset.sizes": "def sizes(self) -> Mapping[Hashable, int]:\n    return self.dims",
    ".xarray.core.dataarray.py@@DataArray.copy": "def copy(self: T_DataArray, deep: bool=True, data: Any=None) -> T_DataArray:\n    variable = self.variable.copy(deep=deep, data=data)\n    indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n    coords = {}\n    for k, v in self._coords.items():\n        if k in index_vars:\n            coords[k] = index_vars[k]\n        else:\n            coords[k] = v.copy(deep=deep)\n    return self._replace(variable, coords, indexes=indexes)",
    ".xarray.core.ops.py@@inplace_to_noninplace_op": "def inplace_to_noninplace_op(f):\n    return NON_INPLACE_OP[f]",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__mul__": "def __mul__(self, other):\n    return self._binary_op(other, operator.mul)",
    ".xarray.core._typed_ops.py@@DatasetOpsMixin.__iadd__": "def __iadd__(self, other):\n    return self._inplace_binary_op(other, operator.iadd)",
    ".xarray.core.dataset.py@@Dataset.reindex_like": "def reindex_like(self, other: Dataset | DataArray, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA) -> Dataset:\n    return alignment.reindex_like(self, other=other, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.dataset.py@@Dataset.apply_over_both": "def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n    if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n        raise ValueError(f'datasets must have the same data variables for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}')\n    dest_vars = {}\n    for k in lhs_data_vars:\n        if k in rhs_data_vars:\n            dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n        elif join in ['left', 'outer']:\n            dest_vars[k] = f(lhs_vars[k], np.nan)\n    for k in rhs_data_vars:\n        if k not in dest_vars and join in ['right', 'outer']:\n            dest_vars[k] = f(rhs_vars[k], np.nan)\n    return dest_vars",
    ".xarray.core.dataset.py@@DataVariables.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._dataset._variables and key not in self._dataset._coord_names",
    ".xarray.core.dataset.py@@Dataset.map": "def map(self, func: Callable, keep_attrs: bool=None, args: Iterable[Any]=(), **kwargs: Any) -> Dataset:\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    variables = {k: maybe_wrap_array(v, func(v, *args, **kwargs)) for k, v in self.data_vars.items()}\n    if keep_attrs:\n        for k, v in variables.items():\n            v._copy_attrs_from(self.data_vars[k])\n    attrs = self.attrs if keep_attrs else None\n    return type(self)(variables, attrs=attrs)",
    ".xarray.core.dataset.py@@DataVariables.__getitem__": "def __getitem__(self, key: Hashable) -> DataArray:\n    if key not in self._dataset._coord_names:\n        return cast('DataArray', self._dataset[key])\n    raise KeyError(key)",
    ".xarray.core.utils.py@@maybe_wrap_array": "def maybe_wrap_array(original, new_array):\n    if isinstance(new_array, np.ndarray) and new_array.shape == original.shape:\n        return original.__array_wrap__(new_array)\n    else:\n        return new_array",
    ".xarray.core.dataset.py@@Dataset.__init__": "def __init__(self, data_vars: Mapping[Any, Any]=None, coords: Mapping[Any, Any]=None, attrs: Mapping[Any, Any]=None):\n    if data_vars is None:\n        data_vars = {}\n    if coords is None:\n        coords = {}\n    both_data_and_coords = set(data_vars) & set(coords)\n    if both_data_and_coords:\n        raise ValueError(f'variables {both_data_and_coords!r} are found in both data_vars and coords')\n    if isinstance(coords, Dataset):\n        coords = coords.variables\n    variables, coord_names, dims, indexes, _ = merge_data_and_coords(data_vars, coords, compat='broadcast_equals')\n    self._attrs = dict(attrs) if attrs is not None else None\n    self._close = None\n    self._encoding = None\n    self._variables = variables\n    self._coord_names = coord_names\n    self._dims = dims\n    self._indexes = indexes",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data_vars, coords, compat='broadcast_equals', join='outer'):\n    indexes, coords = _create_indexes_from_coords(coords, data_vars)\n    objects = [data_vars, coords]\n    explicit_coords = coords.keys()\n    return merge_core(objects, compat, join, explicit_coords=explicit_coords, indexes=Indexes(indexes, coords))",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objects: Iterable[CoercibleMapping], compat: str='broadcast_equals', join: str='outer', combine_attrs: str | None='override', priority_arg: int | None=None, explicit_coords: Sequence | None=None, indexes: Mapping[Any, Any] | None=None, fill_value: object=dtypes.NA) -> _MergeResult:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned, indexes=indexes)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat, combine_attrs=combine_attrs)\n    dims = calculate_dimensions(variables)\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError(f'unable to determine if these variables should be coordinates or not in the merged result: {ambiguous_coords}')\n    attrs = merge_attrs([var.attrs for var in coerced if isinstance(var, (Dataset, DataArray))], combine_attrs)\n    return _MergeResult(variables, coord_names, dims, out_indexes, attrs)",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects: Iterable[CoercibleMapping]) -> list[DatasetLike]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables: DatasetLike = obj\n        else:\n            variables = {}\n            if isinstance(obj, PANDAS_TYPES):\n                obj = dict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.indexes.py@@Indexes.__contains__": "def __contains__(self, key) -> bool:\n    return key in self._indexes",
    ".xarray.core.merge.py@@collect_variables_and_indexes": "def collect_variables_and_indexes(list_of_mappings: list[DatasetLike], indexes: Mapping[Any, Any] | None=None) -> dict[Hashable, list[MergeElement]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n    grouped: dict[Hashable, list[MergeElement]] = defaultdict(list)\n\n    def append(name, variable, index):\n        grouped[name].append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping._indexes)\n            continue\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()\n                indexes = dict(variable._indexes)\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n            variable = as_variable(variable, name=name)\n            if name in indexes:\n                append(name, variable, indexes[name])\n            elif variable.dims == (name,):\n                idx, idx_vars = create_default_index_implicit(variable)\n                append_all(idx_vars, {k: idx for k in idx_vars})\n            else:\n                append(name, variable, None)\n    return grouped",
    ".xarray.core.merge.py@@append_all": "def append_all(variables, indexes):\n    for name, variable in variables.items():\n        append(name, variable, indexes.get(name))",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> Variable | IndexVariable:\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        if isinstance(obj[1], DataArray):\n            raise TypeError('Using a DataArray object to construct a variable is ambiguous, please extract the data using the .data property.')\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(f'variable {name!r} has invalid type {type(obj)!r}')\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(f'cannot set variable {name!r} with {data.ndim!r}-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.')\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError(f'unable to convert object into a variable without an explicit list of dimensions: {obj!r}')\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError(f'{name!r} has more than 1-dimension and the same name as one of its dimensions {obj.dims!r}. xarray disallows such variables because they conflict with the coordinates used to label dimensions.')\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.merge.py@@append": "def append(name, variable, index):\n    grouped[name].append((variable, index))",
    ".xarray.core.merge.py@@_get_priority_vars_and_indexes": "def _get_priority_vars_and_indexes(objects: list[DatasetLike], priority_arg: int | None, compat: str='equals') -> dict[Hashable, MergeElement]:\n    if priority_arg is None:\n        return {}\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped: dict[Hashable, MergeElement] = {}\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name: Hashable, variables: list[Variable], compat: str='broadcast_equals', equals: bool=None) -> Variable:\n    out = variables[0]\n    if len(variables) == 1 or compat == 'override':\n        return out\n    combine_method = None\n    if compat == 'minimal':\n        compat = 'broadcast_equals'\n    if compat == 'broadcast_equals':\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n    if compat == 'no_conflicts':\n        combine_method = 'fillna'\n    if equals is None:\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n            if equals is not True:\n                break\n        if equals is None:\n            out = out.compute()\n            for var in variables[1:]:\n                equals = getattr(out, compat)(var)\n                if not equals:\n                    break\n    if not equals:\n        raise MergeError(f\"conflicting values for variable {name!r} on objects to be combined. You can skip this check by specifying compat='override'.\")\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n    return out",
    ".xarray.core.merge.py@@merge_attrs": "def merge_attrs(variable_attrs, combine_attrs, context=None):\n    if not variable_attrs:\n        return None\n    if callable(combine_attrs):\n        return combine_attrs(variable_attrs, context=context)\n    elif combine_attrs == 'drop':\n        return {}\n    elif combine_attrs == 'override':\n        return dict(variable_attrs[0])\n    elif combine_attrs == 'no_conflicts':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            try:\n                result = compat_dict_union(result, attrs)\n            except ValueError as e:\n                raise MergeError(f\"combine_attrs='no_conflicts', but some values are not the same. Merging {str(result)} with {str(attrs)}\") from e\n        return result\n    elif combine_attrs == 'drop_conflicts':\n        result = {}\n        dropped_keys = set()\n        for attrs in variable_attrs:\n            result.update({key: value for key, value in attrs.items() if key not in result and key not in dropped_keys})\n            result = {key: value for key, value in result.items() if key not in attrs or equivalent(attrs[key], value)}\n            dropped_keys |= {key for key in attrs if key not in result}\n        return result\n    elif combine_attrs == 'identical':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            if not dict_equiv(result, attrs):\n                raise MergeError(f\"combine_attrs='identical', but attrs differ. First is {str(result)} , other is {str(attrs)}.\")\n        return result\n    else:\n        raise ValueError(f'Unrecognised value for combine_attrs={combine_attrs}')",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_mappings: Iterable[DatasetLike]) -> tuple[set[Hashable], set[Hashable]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names: set[Hashable] = set()\n    noncoord_names: set[Hashable] = set()\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError(f'coordinate {coord_name} shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.')",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core._typed_ops.py@@DatasetOpsMixin.__add__": "def __add__(self, other):\n    return self._binary_op(other, operator.add)",
    ".xarray.core.duck_array_ops.py@@datetime_to_numeric": "def datetime_to_numeric(array, offset=None, datetime_unit=None, dtype=float):\n    if offset is None:\n        if array.dtype.kind in 'Mm':\n            offset = _datetime_nanmin(array)\n        else:\n            offset = min(array)\n    array = array - offset\n    if not hasattr(array, 'dtype'):\n        array = np.array(array)\n    if array.dtype.kind in 'O':\n        return py_timedelta_to_float(array, datetime_unit or 'ns').astype(dtype)\n    elif array.dtype.kind in 'mM':\n        if datetime_unit:\n            array = array / np.timedelta64(1, datetime_unit)\n        return np.where(isnull(array), np.nan, array.astype(dtype))",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data.copy(deep=deep)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return self._replace(data=data)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.copy": "def copy(self, deep: bool=True) -> PandasIndexingAdapter:\n    array = self.array.copy(deep=True) if deep else self.array\n    return type(self)(array, self._dtype)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.indexes.py@@create_default_index_implicit": "def create_default_index_implicit(dim_variable: Variable, all_variables: Mapping | Iterable[Hashable] | None=None) -> tuple[PandasIndex, IndexVars]:\n    if all_variables is None:\n        all_variables = {}\n    if not isinstance(all_variables, Mapping):\n        all_variables = {k: None for k in all_variables}\n    name = dim_variable.dims[0]\n    array = getattr(dim_variable._data, 'array', None)\n    index: PandasIndex\n    if isinstance(array, pd.MultiIndex):\n        index = PandasMultiIndex(array, name)\n        index_vars = index.create_variables()\n        duplicate_names = [k for k in index_vars if k in all_variables and k != name]\n        if duplicate_names:\n            if len(duplicate_names) < len(index.index.names):\n                conflict = True\n            else:\n                duplicate_vars = [all_variables[k] for k in duplicate_names]\n                conflict = any((v is None or not dim_variable.equals(v) for v in duplicate_vars))\n            if conflict:\n                conflict_str = '\\n'.join(duplicate_names)\n                raise ValueError(f'conflicting MultiIndex level / variable name(s):\\n{conflict_str}')\n    else:\n        dim_var = {name: dim_variable}\n        index = PandasIndex.from_variables(dim_var)\n        index_vars = index.create_variables(dim_var)\n    return (index, index_vars)",
    ".xarray.core.indexes.py@@PandasIndex.from_variables": "def from_variables(cls, variables: Mapping[Any, Variable]) -> PandasIndex:\n    if len(variables) != 1:\n        raise ValueError(f'PandasIndex only accepts one variable, found {len(variables)} variables')\n    name, var = next(iter(variables.items()))\n    if var.ndim != 1:\n        raise ValueError(f'PandasIndex only accepts a 1-dimensional variable, variable {name!r} has {var.ndim} dimensions')\n    dim = var.dims[0]\n    data = getattr(var._data, 'array', var.data)\n    if isinstance(var._data, PandasMultiIndexingAdapter):\n        level = var._data.level\n        if level is not None:\n            data = var._data.array.get_level_values(level)\n    obj = cls(data, dim, coord_dtype=var.dtype)\n    assert not isinstance(obj.index, pd.MultiIndex)\n    obj.index.name = name\n    return obj",
    ".xarray.core.indexes.py@@indexes_equal": "def indexes_equal(index: Index, other_index: Index, variable: Variable, other_variable: Variable, cache: dict[tuple[int, int], bool | None]=None) -> bool:\n    if cache is None:\n        cache = {}\n    key = (id(index), id(other_index))\n    equal: bool | None = None\n    if key not in cache:\n        if type(index) is type(other_index):\n            try:\n                equal = index.equals(other_index)\n            except NotImplementedError:\n                equal = None\n            else:\n                cache[key] = equal\n        else:\n            equal = None\n    else:\n        equal = cache[key]\n    if equal is None:\n        equal = variable.equals(other_variable)\n    return cast(bool, equal)",
    ".xarray.core.common.py@@DataWithCoords.astype": "def astype(self: T, dtype, *, order=None, casting=None, subok=None, copy=None, keep_attrs=True) -> T:\n    from .computation import apply_ufunc\n    kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n    return apply_ufunc(duck_array_ops.astype, self, dtype, kwargs=kwargs, keep_attrs=keep_attrs, dask='allowed')",
    ".xarray.core.computation.py@@apply_ufunc": "def apply_ufunc(func: Callable, *args: Any, input_core_dims: Sequence[Sequence]=None, output_core_dims: Sequence[Sequence] | None=((),), exclude_dims: AbstractSet=frozenset(), vectorize: bool=False, join: str='exact', dataset_join: str='exact', dataset_fill_value: object=_NO_FILL_VALUE, keep_attrs: bool | str | None=None, kwargs: Mapping | None=None, dask: str='forbidden', output_dtypes: Sequence | None=None, output_sizes: Mapping[Any, int] | None=None, meta: Any=None, dask_gufunc_kwargs: dict[str, Any] | None=None) -> Any:\n    from .dataarray import DataArray\n    from .groupby import GroupBy\n    from .variable import Variable\n    if input_core_dims is None:\n        input_core_dims = ((),) * len(args)\n    elif len(input_core_dims) != len(args):\n        raise ValueError(f'input_core_dims must be None or a tuple with the length same to the number of arguments. Given {len(input_core_dims)} input_core_dims: {input_core_dims},  but number of args is {len(args)}.')\n    if kwargs is None:\n        kwargs = {}\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n    if exclude_dims:\n        if not isinstance(exclude_dims, set):\n            raise TypeError(f\"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead.\")\n        if not exclude_dims <= signature.all_core_dims:\n            raise ValueError(f'each dimension in `exclude_dims` must also be a core dimension in the function signature. Please make {exclude_dims - signature.all_core_dims} a core dimension')\n    if dask == 'parallelized':\n        if dask_gufunc_kwargs is None:\n            dask_gufunc_kwargs = {}\n        else:\n            dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n        if meta is not None:\n            warnings.warn('``meta`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('meta', meta)\n        if output_sizes is not None:\n            warnings.warn('``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('output_sizes', output_sizes)\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    if isinstance(keep_attrs, bool):\n        keep_attrs = 'override' if keep_attrs else 'drop'\n    variables_vfunc = functools.partial(apply_variable_ufunc, func, signature=signature, exclude_dims=exclude_dims, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n    if any((isinstance(a, GroupBy) for a in args)):\n        this_apply = functools.partial(apply_ufunc, func, input_core_dims=input_core_dims, output_core_dims=output_core_dims, exclude_dims=exclude_dims, join=join, dataset_join=dataset_join, dataset_fill_value=dataset_fill_value, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n        return apply_groupby_func(this_apply, *args)\n    elif any((is_dict_like(a) for a in args)):\n        return apply_dataset_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, dataset_join=dataset_join, fill_value=dataset_fill_value, keep_attrs=keep_attrs)\n    elif any((isinstance(a, DataArray) for a in args)):\n        return apply_dataarray_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, keep_attrs=keep_attrs)\n    elif any((isinstance(a, Variable) for a in args)):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)",
    ".xarray.core.computation.py@@_UFuncSignature.__init__": "def __init__(self, input_core_dims, output_core_dims=((),)):\n    self.input_core_dims = tuple((tuple(a) for a in input_core_dims))\n    self.output_core_dims = tuple((tuple(a) for a in output_core_dims))\n    self._all_input_core_dims = None\n    self._all_output_core_dims = None\n    self._all_core_dims = None",
    ".xarray.core.computation.py@@apply_dataarray_vfunc": "def apply_dataarray_vfunc(func, *args, signature, join='inner', exclude_dims=frozenset(), keep_attrs='override'):\n    from .dataarray import DataArray\n    if len(args) > 1:\n        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)\n    objs = _all_of_type(args, DataArray)\n    if keep_attrs == 'drop':\n        name = result_name(args)\n    else:\n        first_obj = _first_of_type(args, DataArray)\n        name = first_obj.name\n    result_coords, result_indexes = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)\n    data_vars = [getattr(a, 'variable', a) for a in args]\n    result_var = func(*data_vars)\n    if signature.num_outputs > 1:\n        out = tuple((DataArray(variable, coords=coords, indexes=indexes, name=name, fastpath=True) for variable, coords, indexes in zip(result_var, result_coords, result_indexes)))\n    else:\n        coords, = result_coords\n        indexes, = result_indexes\n        out = DataArray(result_var, coords=coords, indexes=indexes, name=name, fastpath=True)\n    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)\n    if isinstance(out, tuple):\n        for da in out:\n            da.attrs = attrs\n    else:\n        out.attrs = attrs\n    return out",
    ".xarray.core.computation.py@@_all_of_type": "def _all_of_type(args, kind):\n    return [arg for arg in args if isinstance(arg, kind)]",
    ".xarray.core.computation.py@@_first_of_type": "def _first_of_type(args, kind):\n    for arg in args:\n        if isinstance(arg, kind):\n            return arg\n    raise ValueError('This should be unreachable.')",
    ".xarray.core.computation.py@@build_output_coords_and_indexes": "def build_output_coords_and_indexes(args: list, signature: _UFuncSignature, exclude_dims: AbstractSet=frozenset(), combine_attrs: str='override') -> tuple[list[dict[Any, Variable]], list[dict[Any, Index]]]:\n    coords_list = _get_coords_list(args)\n    if len(coords_list) == 1 and (not exclude_dims):\n        unpacked_coords, = coords_list\n        merged_vars = dict(unpacked_coords.variables)\n        merged_indexes = dict(unpacked_coords.xindexes)\n    else:\n        merged_vars, merged_indexes = merge_coordinates_without_align(coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs)\n    output_coords = []\n    output_indexes = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered_coords = {k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)}\n            filtered_indexes = filter_indexes_from_coords(merged_indexes, set(filtered_coords))\n        else:\n            filtered_coords = merged_vars\n            filtered_indexes = merged_indexes\n        output_coords.append(filtered_coords)\n        output_indexes.append(filtered_indexes)\n    return (output_coords, output_indexes)",
    ".xarray.core.computation.py@@_get_coords_list": "def _get_coords_list(args) -> list[Coordinates]:\n    coords_list = []\n    for arg in args:\n        try:\n            coords = arg.coords\n        except AttributeError:\n            pass\n        else:\n            coords_list.append(coords)\n    return coords_list",
    ".xarray.core.computation.py@@_UFuncSignature.all_input_core_dims": "def all_input_core_dims(self):\n    if self._all_input_core_dims is None:\n        self._all_input_core_dims = frozenset((dim for dims in self.input_core_dims for dim in dims))\n    return self._all_input_core_dims",
    ".xarray.core.computation.py@@apply_variable_ufunc": "def apply_variable_ufunc(func, *args, signature, exclude_dims=frozenset(), dask='forbidden', output_dtypes=None, vectorize=False, keep_attrs='override', dask_gufunc_kwargs=None):\n    from .variable import Variable, as_compatible_data\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')), exclude_dims=exclude_dims)\n    broadcast_dims = tuple((dim for dim in dim_sizes if dim not in signature.all_core_dims))\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims) if isinstance(arg, Variable) else arg for arg, core_dims in zip(args, signature.input_core_dims)]\n    if any((is_duck_dask_array(array) for array in input_data)):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an argument, but handling for dask arrays has not been enabled. Either set the ``dask`` argument or load your data into memory first with ``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            numpy_func = func\n            if dask_gufunc_kwargs is None:\n                dask_gufunc_kwargs = {}\n            else:\n                dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n            allow_rechunk = dask_gufunc_kwargs.get('allow_rechunk', None)\n            if allow_rechunk is None:\n                for n, (data, core_dims) in enumerate(zip(input_data, signature.input_core_dims)):\n                    if is_duck_dask_array(data):\n                        for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                            if len(data.chunks[axis]) != 1:\n                                raise ValueError(f\"dimension {dim} on {n}th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.chunk(dict({dim}=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\")\n                dask_gufunc_kwargs['allow_rechunk'] = True\n            output_sizes = dask_gufunc_kwargs.pop('output_sizes', {})\n            if output_sizes:\n                output_sizes_renamed = {}\n                for key, value in output_sizes.items():\n                    if key not in signature.all_output_core_dims:\n                        raise ValueError(f\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\")\n                    output_sizes_renamed[signature.dims_map[key]] = value\n                dask_gufunc_kwargs['output_sizes'] = output_sizes_renamed\n            for key in signature.all_output_core_dims:\n                if key not in signature.all_input_core_dims and key not in output_sizes:\n                    raise ValueError(f\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\")\n\n            def func(*arrays):\n                import dask.array as da\n                res = da.apply_gufunc(numpy_func, signature.to_gufunc_string(exclude_dims), *arrays, vectorize=vectorize, output_dtypes=output_dtypes, **dask_gufunc_kwargs)\n                return res\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in apply_ufunc: {}'.format(dask))\n    elif vectorize:\n        func = _vectorize(func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims)\n    result_data = func(*input_data)\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs:\n        raise ValueError('applied function does not have the number of outputs specified in the ufunc signature. Result is not a tuple of {} elements: {!r}'.format(signature.num_outputs, result_data))\n    objs = _all_of_type(args, Variable)\n    attrs = merge_attrs([obj.attrs for obj in objs], combine_attrs=keep_attrs)\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError(f'applied function returned data with unexpected number of dimensions. Received {data.ndim} dimension(s) but expected {len(dims)} dimensions with names: {dims!r}')\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError('size of dimension {!r} on inputs was unexpectedly changed by applied function from {} to {}. Only dimensions specified in ``exclude_dims`` with xarray.apply_ufunc are allowed to change size.'.format(dim, dim_sizes[dim], new_size))\n        var.attrs = attrs\n        output.append(var)\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)",
    ".xarray.core.computation.py@@unified_dim_sizes": "def unified_dim_sizes(variables: Iterable[Variable], exclude_dims: AbstractSet=frozenset()) -> dict[Hashable, int]:\n    dim_sizes: dict[Hashable, int] = {}\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError(f'broadcasting cannot handle duplicate dimensions on a variable: {list(var.dims)}')\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError(f'operands cannot be broadcast together with mismatched lengths for dimension {dim}: {dim_sizes[dim]} vs {size}')\n    return dim_sizes",
    ".xarray.core.computation.py@@_UFuncSignature.all_core_dims": "def all_core_dims(self):\n    if self._all_core_dims is None:\n        self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n    return self._all_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_output_core_dims": "def all_output_core_dims(self):\n    if self._all_output_core_dims is None:\n        self._all_output_core_dims = frozenset((dim for dims in self.output_core_dims for dim in dims))\n    return self._all_output_core_dims",
    ".xarray.core.computation.py@@broadcast_compat_data": "def broadcast_compat_data(variable: Variable, broadcast_dims: tuple[Hashable, ...], core_dims: tuple[Hashable, ...]) -> Any:\n    data = variable.data\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n    if new_dims == old_dims:\n        return data\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError('operand to apply_ufunc has required core dimensions {}, but some of these dimensions are absent on an input variable: {}'.format(list(core_dims), missing_core_dims))\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError(f'operand to apply_ufunc encountered unexpected dimensions {unexpected_dims!r} on an input variable: these are core dimensions on other input or output variables')\n    old_broadcast_dims = tuple((d for d in broadcast_dims if d in set_old_dims))\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple((old_dims.index(d) for d in reordered_dims))\n        data = duck_array_ops.transpose(data, order)\n    if new_dims != reordered_dims:\n        key_parts: list[slice | None] = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n    return data",
    ".xarray.core.duck_array_ops.py@@astype": "def astype(data, dtype, **kwargs):\n    return data.astype(dtype, **kwargs)",
    ".xarray.core.computation.py@@_UFuncSignature.num_outputs": "def num_outputs(self):\n    return len(self.output_core_dims)"
}