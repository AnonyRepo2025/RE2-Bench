{
    ".xarray.core.utils.py@@peek_at": "def peek_at(iterable: Iterable[T]) -> Tuple[T, Iterator[T]]:\n    gen = iter(iterable)\n    peek = next(gen)\n    return (peek, itertools.chain([peek], gen))",
    ".xarray.core.groupby.py@@GroupBy._iter_grouped": "def _iter_grouped(self):\n    for indices in self._group_indices:\n        yield self._obj.isel(**{self._group_dim: indices})",
    ".xarray.core.dataarray.py@@DataArray.isel": "def isel(self, indexers: Mapping[Hashable, Any]=None, drop: bool=False, **indexers_kwargs: Any) -> 'DataArray':\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    ds = self._to_temp_dataset().isel(drop=drop, indexers=indexers)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Optional[Mapping[Hashable, T]], kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .%s must be a dictionary' % func_name)\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional arguments to .%s' % func_name)\n        return pos_kwargs\n    else:\n        return cast(Mapping[Hashable, T], kw_kwargs)",
    ".xarray.core.dataarray.py@@DataArray._to_temp_dataset": "def _to_temp_dataset(self) -> Dataset:\n    return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)",
    ".xarray.core.dataarray.py@@DataArray._to_dataset_whole": "def _to_dataset_whole(self, name: Hashable=None, shallow_copy: bool=True) -> Dataset:\n    if name is None:\n        name = self.name\n    if name is None:\n        raise ValueError('unable to convert unnamed DataArray to a Dataset without providing an explicit name')\n    if name in self.coords:\n        raise ValueError('cannot create a Dataset from a DataArray with the same name as one of its coordinates')\n    variables = self._coords.copy()\n    variables[name] = self.variable\n    if shallow_copy:\n        for k in variables:\n            variables[k] = variables[k].copy(deep=False)\n    coord_names = set(self._coords)\n    dataset = Dataset._from_vars_and_coord_names(variables, coord_names)\n    return dataset",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: 'DataArray'):\n    self._data = dataarray",
    ".xarray.core.coordinates.py@@Coordinates.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._names",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return set(self._data._coords)",
    ".xarray.core.utils.py@@ReprObject.__hash__": "def __hash__(self) -> int:\n    return hash((ReprObject, self._value))",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.dataset.py@@Dataset._from_vars_and_coord_names": "def _from_vars_and_coord_names(cls, variables, coord_names, attrs=None):\n    return cls._construct_direct(variables, coord_names, attrs=attrs)",
    ".xarray.core.dataset.py@@Dataset._construct_direct": "def _construct_direct(cls, variables, coord_names, dims=None, attrs=None, indexes=None, encoding=None, file_obj=None):\n    if dims is None:\n        dims = calculate_dimensions(variables)\n    obj = object.__new__(cls)\n    obj._variables = variables\n    obj._coord_names = coord_names\n    obj._dims = dims\n    obj._indexes = indexes\n    obj._attrs = attrs\n    obj._file_obj = file_obj\n    obj._encoding = encoding\n    obj._accessors = None\n    return obj",
    ".xarray.core.dataset.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> 'Dict[Any, int]':\n    dims: Dict[Any, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError('dimension %r already exists as a scalar variable' % dim)\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError('conflicting sizes for dimension %r: length %s on %r and length %s on %r' % (dim, size, k, dims[dim], last_used[dim]))\n    return dims",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.indexing.py@@PandasIndexAdapter.shape": "def shape(self) -> Tuple[int]:\n    return (len(self.array),)",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '%r object has no attribute %r' % (type(self).__name__, name):\n            raise\n        raise AttributeError(\"cannot set attribute %r on a %r object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\" % (name, type(self).__name__)) from e",
    ".xarray.core.dataset.py@@Dataset.isel": "def isel(self, indexers: Mapping[Hashable, Any]=None, drop: bool=False, **indexers_kwargs: Any) -> 'Dataset':\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    indexers_list = list(self._validate_indexers(indexers))\n    variables = OrderedDict()\n    indexes = OrderedDict()\n    for name, var in self.variables.items():\n        var_indexers = {k: v for k, v in indexers_list if k in var.dims}\n        if drop and name in var_indexers:\n            continue\n        if name in self.indexes:\n            new_var, new_index = isel_variable_and_index(name, var, self.indexes[name], var_indexers)\n            if new_index is not None:\n                indexes[name] = new_index\n        elif var_indexers:\n            new_var = var.isel(indexers=var_indexers)\n        else:\n            new_var = var.copy(deep=False)\n        variables[name] = new_var\n    coord_names = self._coord_names & variables.keys()\n    selected = self._replace_with_new_dims(variables, coord_names, indexes)\n    coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n    variables.update(coord_vars)\n    indexes.update(new_indexes)\n    coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n    return self._replace_with_new_dims(variables, coord_names, indexes=indexes)",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.dataset.py@@Dataset._validate_indexers": "def _validate_indexers(self, indexers: Mapping[Hashable, Any]) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:\n    from .dataarray import DataArray\n    invalid = indexers.keys() - self.dims.keys()\n    if invalid:\n        raise ValueError('dimensions %r do not exist' % invalid)\n    for k, v in indexers.items():\n        if isinstance(v, (int, slice, Variable)):\n            yield (k, v)\n        elif isinstance(v, DataArray):\n            yield (k, v.variable)\n        elif isinstance(v, tuple):\n            yield (k, as_variable(v))\n        elif isinstance(v, Dataset):\n            raise TypeError('cannot use a Dataset as an indexer')\n        elif isinstance(v, Sequence) and len(v) == 0:\n            yield (k, np.empty((0,), dtype='int64'))\n        else:\n            v = np.asarray(v)\n            if v.dtype.kind in 'US':\n                index = self.indexes[k]\n                if isinstance(index, pd.DatetimeIndex):\n                    v = v.astype('datetime64[ns]')\n                elif isinstance(index, xr.CFTimeIndex):\n                    v = _parse_array_of_cftime_strings(v, index.date_type)\n            if v.ndim > 1:\n                raise IndexError('Unlabeled multi-dimensional array cannot be used for indexing: {}'.format(k))\n            yield (k, v)",
    ".xarray.core.dataset.py@@Dataset.dims": "def dims(self) -> Mapping[Hashable, int]:\n    return Frozen(SortedKeysDict(self._dims))",
    ".xarray.core.utils.py@@SortedKeysDict.__init__": "def __init__(self, mapping: MutableMapping[K, V]=None):\n    self.mapping = {} if mapping is None else mapping",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.utils.py@@SortedKeysDict.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(sorted(self.mapping))",
    ".xarray.core.dataset.py@@Dataset.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.dataset.py@@Dataset.indexes": "def indexes(self) -> Indexes:\n    if self._indexes is None:\n        self._indexes = default_indexes(self._variables, self._dims)\n    return Indexes(self._indexes)",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, Variable], dims: Iterable) -> 'OrderedDict[Any, pd.Index]':\n    return OrderedDict(((key, coords[key].to_index()) for key in dims if key in coords))",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i) for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self):\n    return self.dims[0]",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes):\n    self._indexes = indexes",
    ".xarray.core.indexes.py@@Indexes.__contains__": "def __contains__(self, key):\n    return key in self._indexes",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key):\n    return self._indexes[key]",
    ".xarray.core.indexes.py@@isel_variable_and_index": "def isel_variable_and_index(name: Hashable, variable: Variable, index: pd.Index, indexers: Mapping[Hashable, Union[int, slice, np.ndarray, Variable]]) -> Tuple[Variable, Optional[pd.Index]]:\n    if not indexers:\n        return (variable.copy(deep=False), index)\n    if len(variable.dims) > 1:\n        raise NotImplementedError('indexing multi-dimensional variable with indexes is not supported yet')\n    new_variable = variable.isel(indexers)\n    if new_variable.dims != (name,):\n        return (new_variable, None)\n    dim, = variable.dims\n    indexer = indexers[dim]\n    if isinstance(indexer, Variable):\n        indexer = indexer.data\n    new_index = index[indexer]\n    return (new_variable, new_index)",
    ".xarray.core.variable.py@@Variable.isel": "def isel(self: VariableType, indexers: Mapping[Hashable, Any]=None, **indexers_kwargs: Any) -> VariableType:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    invalid = indexers.keys() - set(self.dims)\n    if invalid:\n        raise ValueError('dimensions %r do not exist' % invalid)\n    key = tuple((indexers.get(dim, slice(None)) for dim in self.dims))\n    return self[key]",
    ".xarray.core.variable.py@@Variable.__getitem__": "def __getitem__(self: VariableType, key) -> VariableType:\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    data = as_indexable(self._data)[indexer]\n    if new_order:\n        data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes": "def _broadcast_indexes(self, key):\n    key = self._item_key_to_tuple(key)\n    key = indexing.expanded_indexer(key, self.ndim)\n    key = tuple((k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key))\n    key = tuple((k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key))\n    if all((isinstance(k, BASIC_INDEXING_TYPES) for k in key)):\n        return self._broadcast_indexes_basic(key)\n    self._validate_indexers(key)\n    if all((not isinstance(k, Variable) for k in key)):\n        return self._broadcast_indexes_outer(key)\n    dims = []\n    for k, d in zip(key, self.dims):\n        if isinstance(k, Variable):\n            if len(k.dims) > 1:\n                return self._broadcast_indexes_vectorized(key)\n            dims.append(k.dims[0])\n        elif not isinstance(k, integer_types):\n            dims.append(d)\n    if len(set(dims)) == len(dims):\n        return self._broadcast_indexes_outer(key)\n    return self._broadcast_indexes_vectorized(key)",
    ".xarray.core.variable.py@@Variable._item_key_to_tuple": "def _item_key_to_tuple(self, key):\n    if utils.is_dict_like(key):\n        return tuple((key.get(dim, slice(None)) for dim in self.dims))\n    else:\n        return key",
    ".xarray.core.indexing.py@@expanded_indexer": "def expanded_indexer(key, ndim):\n    if not isinstance(key, tuple):\n        key = (key,)\n    new_key = []\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
    ".xarray.core.variable.py@@Variable._validate_indexers": "def _validate_indexers(self, key):\n    for dim, k in zip(self.dims, key):\n        if isinstance(k, BASIC_INDEXING_TYPES):\n            pass\n        else:\n            if not isinstance(k, Variable):\n                k = np.asarray(k)\n                if k.ndim > 1:\n                    raise IndexError('Unlabeled multi-dimensional array cannot be used for indexing: {}'.format(k))\n            if k.dtype.kind == 'b':\n                if self.shape[self.get_axis_num(dim)] != len(k):\n                    raise IndexError('Boolean array size {:d} is used to index array with shape {:s}.'.format(len(k), str(self.shape)))\n                if k.ndim > 1:\n                    raise IndexError('{}-dimensional boolean indexing is not supported. '.format(k.ndim))\n                if getattr(k, 'dims', (dim,)) != (dim,):\n                    raise IndexError('Boolean indexer should be unlabeled or on the same dimension to the indexed array. Indexer is on {:s} but the target dimension is {:s}.'.format(str(k.dims), dim))",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_outer": "def _broadcast_indexes_outer(self, key):\n    dims = tuple((k.dims[0] if isinstance(k, Variable) else dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    new_key = []\n    for k in key:\n        if isinstance(k, Variable):\n            k = k.data\n        if not isinstance(k, BASIC_INDEXING_TYPES):\n            k = np.asarray(k)\n            if k.dtype.kind == 'b':\n                k, = np.nonzero(k)\n        new_key.append(k)\n    return (dims, OuterIndexer(tuple(new_key)), None)",
    ".xarray.core.indexing.py@@OuterIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError('key must be a tuple: {!r}'.format(key))\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError('invalid indexer array, does not have integer dtype: {!r}'.format(k))\n            if k.ndim != 1:\n                raise TypeError('invalid indexer array for {}, must have exactly 1 dimension: '.format(type(self).__name__, k))\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError('unexpected indexer type for {}: {!r}'.format(type(self).__name__, k))\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.__init__": "def __init__(self, key):\n    if type(self) is ExplicitIndexer:\n        raise TypeError('cannot instantiate base ExplicitIndexer objects')\n    self._key = tuple(key)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, '__array_function__'):\n        return NdArrayLikeIndexingAdapter(array)\n    raise TypeError('Invalid array type: {}'.format(type(array)))",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__getitem__": "def __getitem__(self, indexer) -> Union[NumpyIndexingAdapter, np.ndarray, np.datetime64, np.timedelta64]:\n    key = indexer.tuple\n    if isinstance(key, tuple) and len(key) == 1:\n        key, = key\n    if getattr(key, 'ndim', 0) > 1:\n        return NumpyIndexingAdapter(self.array.values)[indexer]\n    result = self.array[key]\n    if isinstance(result, pd.Index):\n        result = PandasIndexAdapter(result, dtype=self.dtype)\n    else:\n        if result is pd.NaT:\n            result = np.datetime64('NaT', 'ns')\n        elif isinstance(result, timedelta):\n            result = np.timedelta64(getattr(result, 'value', result), 'ns')\n        elif isinstance(result, pd.Timestamp):\n            result = np.asarray(result.to_datetime64())\n        elif self.dtype != object:\n            result = np.asarray(result, dtype=self.dtype)\n        result = utils.to_0d_array(result)\n    return result",
    ".xarray.core.indexing.py@@ExplicitIndexer.tuple": "def tuple(self):\n    return self._key",
    ".xarray.core.indexing.py@@PandasIndexAdapter.dtype": "def dtype(self) -> np.dtype:\n    return self._dtype",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__init__": "def __init__(self, array: Any, dtype: DTypeLike=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        if isinstance(array, pd.PeriodIndex):\n            dtype = np.dtype('O')\n        elif hasattr(array, 'categories'):\n            dtype = array.categories.dtype\n        elif not utils.is_valid_numpy_dtype(array.dtype):\n            dtype = np.dtype('O')\n        else:\n            dtype = array.dtype\n    else:\n        dtype = np.dtype(dtype)\n    self._dtype = dtype",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.variable.py@@IndexVariable._finalize_indexing_result": "def _finalize_indexing_result(self, dims, data):\n    if getattr(data, 'ndim', 0) != 1:\n        return Variable(dims, data, self._attrs, self._encoding)\n    else:\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError('%s objects must be 1-dimensional' % type(self).__name__)\n    if not isinstance(self._data, PandasIndexAdapter):\n        self._data = PandasIndexAdapter(self._data)",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, Variable):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    data = getattr(data, 'values', data)\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray):\n        if hasattr(data, '__array_function__'):\n            if IS_NEP18_ACTIVE:\n                return data\n            else:\n                raise TypeError('Got an NumPy-like array type providing the __array_function__ protocol but NEP18 is not enabled. Check that numpy >= v1.16 and that the environment variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to \"1\"')\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'M':\n            data = np.asarray(data, 'datetime64[ns]')\n        elif data.dtype.kind == 'm':\n            data = np.asarray(data, 'timedelta64[ns]')\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError('dimensions %s must have the same length as the number of data dimensions, ndim=%s' % (dims, self.ndim))\n    return dims",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    array, key = self._indexing_array_and_key(key)\n    return array[key]",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter._indexing_array_and_key": "def _indexing_array_and_key(self, key):\n    if isinstance(key, OuterIndexer):\n        array = self.array\n        key = _outer_to_numpy_indexer(key, self.array.shape)\n    elif isinstance(key, VectorizedIndexer):\n        array = nputils.NumpyVIndexAdapter(self.array)\n        key = key.tuple\n    elif isinstance(key, BasicIndexer):\n        array = self.array\n        key = key.tuple + (Ellipsis,)\n    else:\n        raise TypeError('unexpected key type: {}'.format(type(key)))\n    return (array, key)",
    ".xarray.core.indexing.py@@_outer_to_numpy_indexer": "def _outer_to_numpy_indexer(key, shape):\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple",
    ".xarray.core.variable.py@@Variable._finalize_indexing_result": "def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.dataset.py@@Dataset._replace_with_new_dims": "def _replace_with_new_dims(self, variables: 'OrderedDict[Any, Variable]', coord_names: set=None, attrs: Optional['OrderedDict']=__default, indexes: 'OrderedDict[Any, pd.Index]'=__default, inplace: bool=False) -> 'Dataset':\n    dims = calculate_dimensions(variables)\n    return self._replace(variables, coord_names, dims, attrs, indexes, inplace=inplace)",
    ".xarray.core.dataset.py@@Dataset._replace": "def _replace(self, variables: 'OrderedDict[Any, Variable]'=None, coord_names: Set[Hashable]=None, dims: Dict[Any, int]=None, attrs: 'Optional[OrderedDict]'=__default, indexes: 'Optional[OrderedDict[Any, pd.Index]]'=__default, encoding: Optional[dict]=__default, inplace: bool=False) -> 'Dataset':\n    if inplace:\n        if variables is not None:\n            self._variables = variables\n        if coord_names is not None:\n            self._coord_names = coord_names\n        if dims is not None:\n            self._dims = dims\n        if attrs is not self.__default:\n            self._attrs = attrs\n        if indexes is not self.__default:\n            self._indexes = indexes\n        if encoding is not self.__default:\n            self._encoding = encoding\n        obj = self\n    else:\n        if variables is None:\n            variables = self._variables.copy()\n        if coord_names is None:\n            coord_names = self._coord_names.copy()\n        if dims is None:\n            dims = self._dims.copy()\n        if attrs is self.__default:\n            attrs = copy.copy(self._attrs)\n        if indexes is self.__default:\n            indexes = copy.copy(self._indexes)\n        if encoding is self.__default:\n            encoding = copy.copy(self._encoding)\n        obj = self._construct_direct(variables, coord_names, dims, attrs, indexes, encoding)\n    return obj",
    ".xarray.core.dataset.py@@Dataset._get_indexers_coords_and_indexes": "def _get_indexers_coords_and_indexes(self, indexers):\n    from .dataarray import DataArray\n    coords_list = []\n    for k, v in indexers.items():\n        if isinstance(v, DataArray):\n            if v.dtype.kind == 'b':\n                if v.ndim != 1:\n                    raise ValueError('{:d}d-boolean array is used for indexing along dimension {!r}, but only 1d boolean arrays are supported.'.format(v.ndim, k))\n                v_coords = v[v.values.nonzero()[0]].coords\n            else:\n                v_coords = v.coords\n            coords_list.append(v_coords)\n    coords, indexes = merge_coordinates_without_align(coords_list)\n    assert_coordinate_consistent(self, coords)\n    attached_coords = OrderedDict(((k, v) for k, v in coords.items() if k not in self._variables))\n    attached_indexes = OrderedDict(((k, v) for k, v in indexes.items() if k not in self._variables))\n    return (attached_coords, attached_indexes)",
    ".xarray.core.merge.py@@merge_coordinates_without_align": "def merge_coordinates_without_align(objects: 'List[Coordinates]', prioritized: Mapping[Hashable, MergeElement]=None, exclude_dims: AbstractSet=frozenset()) -> Tuple['OrderedDict[Hashable, Variable]', 'OrderedDict[Hashable, pd.Index]']:\n    collected = collect_from_coordinates(objects)\n    if exclude_dims:\n        filtered = OrderedDict()\n        for name, elements in collected.items():\n            new_elements = [(variable, index) for variable, index in elements if exclude_dims.isdisjoint(variable.dims)]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n    return merge_collected(filtered, prioritized)",
    ".xarray.core.merge.py@@collect_from_coordinates": "def collect_from_coordinates(list_of_coords: 'List[Coordinates]') -> 'OrderedDict[Hashable, List[MergeElement]]':\n    grouped = OrderedDict()\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.indexes\n        for name, variable in variables.items():\n            value = grouped.setdefault(name, [])\n            value.append((variable, indexes.get(name)))\n    return grouped",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: 'OrderedDict[Hashable, List[MergeElement]]', prioritized: Mapping[Hashable, MergeElement]=None, compat: str='minimal') -> Tuple['OrderedDict[Hashable, Variable]', 'OrderedDict[Hashable, pd.Index]']:\n    if prioritized is None:\n        prioritized = {}\n    _assert_compat_valid(compat)\n    merged_vars = OrderedDict()\n    merged_indexes = OrderedDict()\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for _, other_index in indexed_elements[1:]:\n                    if not index.equals(other_index):\n                        raise MergeError('conflicting values for index %r on objects to be combined:\\nfirst value: %r\\nsecond value: %r' % (name, index, other_index))\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError('conflicting attribute values on combined variable %r:\\nfirst value: %r\\nsecond value: %r' % (name, variable.attrs, other_variable.attrs))\n                merged_vars[name] = variable\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n    return (merged_vars, merged_indexes)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError('compat=%r invalid: must be %s' % (compat, set(_VALID_COMPAT)))",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.coordinates.py@@assert_coordinate_consistent": "def assert_coordinate_consistent(obj: Union['DataArray', 'Dataset'], coords: Mapping[Hashable, Variable]) -> None:\n    for k in obj.dims:\n        if k in coords and k in obj.coords:\n            if not coords[k].equals(obj[k].variable):\n                raise IndexError('dimension coordinate {!r} conflicts between indexed and indexing objects:\\n{}\\nvs.\\n{}'.format(k, obj[k], coords[k]))",
    ".xarray.core.dataarray.py@@DataArray._from_temp_dataset": "def _from_temp_dataset(self, dataset: Dataset, name: Hashable=__default) -> 'DataArray':\n    variable = dataset._variables.pop(_THIS_ARRAY)\n    coords = dataset._variables\n    return self._replace(variable, coords, name)",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self, variable: Variable=None, coords=None, name: Optional[Hashable]=__default) -> 'DataArray':\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if name is self.__default:\n        name = self.name\n    return type(self)(variable, coords, name=name, fastpath=True)",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Optional[Hashable]:\n    return self._name",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None]=None, dims: Union[Hashable, Sequence[Hashable], None]=None, name: Hashable=None, attrs: Mapping=None, encoding=None, indexes=None, fastpath: bool=False):\n    if encoding is not None:\n        warnings.warn('The `encoding` argument to `DataArray` is deprecated, and . will be removed in 0.15. Instead, specify the encoding when writing to disk or set the `encoding` attribute directly.', FutureWarning, stacklevel=2)\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n        assert encoding is None\n    else:\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n            elif isinstance(data, pdcompat.Panel):\n                coords = [data.items, data.major_axis, data.minor_axis]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None:\n            attrs = getattr(data, 'attrs', None)\n        if encoding is None:\n            encoding = getattr(data, 'encoding', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, encoding, fastpath=True)\n    self._variable = variable\n    assert isinstance(coords, OrderedDict)\n    self._coords = coords\n    self._name = name\n    self._accessors = None\n    self._indexes = indexes\n    self._file_obj = None",
    ".xarray.core.dataarray.py@@DataArray.quantile": "def quantile(self, q: Any, dim: Union[Hashable, Sequence[Hashable], None]=None, interpolation: str='linear', keep_attrs: bool=None) -> 'DataArray':\n    ds = self._to_temp_dataset().quantile(q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.quantile": "def quantile(self, q, dim=None, interpolation='linear', numeric_only=False, keep_attrs=None):\n    if isinstance(dim, str):\n        dims = {dim}\n    elif dim is None or dim is ALL_DIMS:\n        dims = set(self.dims)\n    else:\n        dims = set(dim)\n    _assert_empty([d for d in dims if d not in self.dims], 'Dataset does not contain the dimensions: %s')\n    q = np.asarray(q, dtype=np.float64)\n    variables = OrderedDict()\n    for name, var in self.variables.items():\n        reduce_dims = [d for d in var.dims if d in dims]\n        if reduce_dims or not var.dims:\n            if name not in self.coords:\n                if not numeric_only or np.issubdtype(var.dtype, np.number) or var.dtype == np.bool_:\n                    if len(reduce_dims) == var.ndim:\n                        reduce_dims = None\n                    variables[name] = var.quantile(q, dim=reduce_dims, interpolation=interpolation, keep_attrs=keep_attrs)\n        else:\n            variables[name] = var\n    coord_names = {k for k in self.coords if k in variables}\n    indexes = OrderedDict(((k, v) for k, v in self.indexes.items() if k in variables))\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self.attrs if keep_attrs else None\n    new = self._replace_with_new_dims(variables, coord_names=coord_names, attrs=attrs, indexes=indexes)\n    if 'quantile' in new.dims:\n        new.coords['quantile'] = Variable('quantile', q)\n    else:\n        new.coords['quantile'] = q\n    return new",
    ".xarray.core.utils.py@@SortedKeysDict.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.dataset.py@@_assert_empty": "def _assert_empty(args: tuple, msg: str='%s') -> None:\n    if args:\n        raise ValueError(msg % args)",
    ".xarray.core.dataset.py@@Dataset.coords": "def coords(self) -> DatasetCoordinates:\n    return DatasetCoordinates(self)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._data = dataset",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return self._data._coord_names",
    ".xarray.core.variable.py@@Variable.quantile": "def quantile(self, q, dim=None, interpolation='linear', keep_attrs=None):\n    if isinstance(self.data, dask_array_type):\n        raise TypeError('quantile does not work for arrays stored as dask arrays. Load the data via .compute() or .load() prior to calling this method.')\n    q = np.asarray(q, dtype=np.float64)\n    new_dims = list(self.dims)\n    if dim is not None:\n        axis = self.get_axis_num(dim)\n        if utils.is_scalar(dim):\n            new_dims.remove(dim)\n        else:\n            for d in dim:\n                new_dims.remove(d)\n    else:\n        axis = None\n        new_dims = []\n    if q.ndim != 0:\n        new_dims = ['quantile'] + new_dims\n    qs = np.nanpercentile(self.data, q * 100.0, axis=axis, interpolation=interpolation)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    return Variable(new_dims, qs, attrs)",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if hasattr(self._data, '__array_function__') or isinstance(self._data, dask_array_type):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    global_choice = OPTIONS['keep_attrs']\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(\"The global option keep_attrs must be one of True, False or 'default'.\")",
    ".xarray.core.coordinates.py@@Coordinates.__iter__": "def __iter__(self) -> Iterator['Hashable']:\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(OrderedDict(((k, v) for k, v in self._data.variables.items() if k in self._names)))",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self):\n    return iter(self._indexes)",
    ".xarray.core.coordinates.py@@Coordinates.__setitem__": "def __setitem__(self, key: Hashable, value: Any) -> None:\n    self.update({key: value})",
    ".xarray.core.coordinates.py@@Coordinates.update": "def update(self, other: Mapping[Hashable, Any]) -> None:\n    other_vars = getattr(other, 'variables', other)\n    coords, indexes = merge_coords([self.variables, other_vars], priority_arg=1, indexes=self.indexes)\n    self._update_coords(coords, indexes)",
    ".xarray.core.coordinates.py@@Coordinates.indexes": "def indexes(self) -> Indexes:\n    return self._data.indexes",
    ".xarray.core.merge.py@@merge_coords": "def merge_coords(objects: Iterable['CoercibleMapping'], compat: str='minimal', join: str='outer', priority_arg: Optional[int]=None, indexes: Optional[Mapping[Hashable, pd.Index]]=None, fill_value: object=dtypes.NA) -> Tuple['OrderedDict[Hashable, Variable]', 'OrderedDict[Hashable, pd.Index]']:\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    return (variables, out_indexes)",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects: Iterable['CoercibleMapping']) -> List['DatasetLike']:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables = obj\n        else:\n            variables = OrderedDict()\n            if isinstance(obj, PANDAS_TYPES):\n                obj = OrderedDict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = OrderedDict()\n            for k, v in variables.items():\n                if is_alignable(v):\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.alignment.py@@align": "def align(*objects, join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA):\n    if indexes is None:\n        indexes = {}\n    if not indexes and len(objects) == 1:\n        obj, = objects\n        return (obj.copy(deep=copy),)\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n    if join == 'override':\n        objects = _override_indexes(objects, all_indexes, exclude)\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if any((not index.equals(other) for other in matching_indexes)) or dim in unlabeled_dim_sizes:\n                joined_indexes[dim] = index\n        elif any((not matching_indexes[0].equals(other) for other in matching_indexes[1:])) or dim in unlabeled_dim_sizes:\n            if join == 'exact':\n                raise ValueError('indexes along dimension {!r} are not equal'.format(dim))\n            index = joiner(matching_indexes)\n            joined_indexes[dim] = index\n        else:\n            index = matching_indexes[0]\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension size(s) %r than the size of the aligned dimension labels: %r' % (dim, unlabeled_sizes, labeled_size))\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension sizes: %r' % (dim, sizes))\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n        if not valid_indexers:\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, **valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n    return tuple(result)",
    ".xarray.core.indexes.py@@Indexes.__len__": "def __len__(self):\n    return len(self._indexes)",
    ".xarray.core.alignment.py@@_get_joiner": "def _get_joiner(join):\n    if join == 'outer':\n        return functools.partial(functools.reduce, operator.or_)\n    elif join == 'inner':\n        return functools.partial(functools.reduce, operator.and_)\n    elif join == 'left':\n        return operator.itemgetter(0)\n    elif join == 'right':\n        return operator.itemgetter(-1)\n    elif join == 'exact':\n        return None\n    elif join == 'override':\n        return operator.itemgetter(0)\n    else:\n        raise ValueError('invalid value for join: %s' % join)",
    ".xarray.core.merge.py@@collect_variables_and_indexes": "def collect_variables_and_indexes(list_of_mappings: 'List[DatasetLike]') -> 'OrderedDict[Hashable, List[MergeElement]]':\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    grouped = OrderedDict()\n\n    def append(name, variable, index):\n        values = grouped.setdefault(name, [])\n        values.append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping.indexes)\n            continue\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()\n                indexes = OrderedDict(variable.indexes)\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n            variable = as_variable(variable, name=name)\n            if variable.dims == (name,):\n                variable = variable.to_index_variable()\n                index = variable.to_index()\n            else:\n                index = None\n            append(name, variable, index)\n    return grouped",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> 'Union[Variable, IndexVariable]':\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError('variable %r has invalid type %r' % (name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError('cannot set variable %r with %r-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.' % (name, data.ndim))\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError('unable to convert object into a variable without an explicit list of dimensions: %r' % obj)\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError('%r has more than 1-dimension and the same name as one of its dimensions %r. xarray disallows such variables because they conflict with the coordinates used to label dimensions.' % (name, obj.dims))\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> bool:\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.merge.py@@append": "def append(name, variable, index):\n    values = grouped.setdefault(name, [])\n    values.append((variable, index))",
    ".xarray.core.merge.py@@_get_priority_vars_and_indexes": "def _get_priority_vars_and_indexes(objects: List['DatasetLike'], priority_arg: Optional[int], compat: str='equals') -> 'OrderedDict[Hashable, MergeElement]':\n    if priority_arg is None:\n        return OrderedDict()\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped = OrderedDict()\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name: Hashable, variables: List[Variable], compat: str='broadcast_equals', equals: bool=None) -> Variable:\n    out = variables[0]\n    if len(variables) == 1 or compat == 'override':\n        return out\n    combine_method = None\n    if compat == 'minimal':\n        compat = 'broadcast_equals'\n    if compat == 'broadcast_equals':\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n    if compat == 'no_conflicts':\n        combine_method = 'fillna'\n    if equals is None:\n        out = out.compute()\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var)\n            if not equals:\n                break\n    if not equals:\n        raise MergeError(\"conflicting values for variable {!r} on objects to be combined. You can skip this check by specifying compat='override'.\".format(name))\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n    return out",
    ".xarray.core.variable.py@@assert_unique_multiindex_level_names": "def assert_unique_multiindex_level_names(variables):\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append('%r (%s)' % (n, var_name))\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n    for k, v in level_names.items():\n        if k in variables:\n            v.append('(%s)' % k)\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = '\\n'.join([', '.join(v) for v in duplicate_names])\n        raise ValueError('conflicting MultiIndex level name(s):\\n%s' % conflict_str)\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError('conflicting level / dimension names. {} already exists as a level name.'.format(d))",
    ".xarray.core.coordinates.py@@DatasetCoordinates._update_coords": "def _update_coords(self, coords: 'OrderedDict[Hashable, Variable]', indexes: Mapping[Hashable, pd.Index]) -> None:\n    from .dataset import calculate_dimensions\n    variables = self._data._variables.copy()\n    variables.update(coords)\n    dims = calculate_dimensions(variables)\n    new_coord_names = set(coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            new_coord_names.add(dim)\n    self._data._variables = variables\n    self._data._coord_names.update(new_coord_names)\n    self._data._dims = dims\n    original_indexes = OrderedDict(self._data.indexes)\n    original_indexes.update(indexes)\n    self._data._indexes = original_indexes",
    ".xarray.core.utils.py@@maybe_wrap_array": "def maybe_wrap_array(original, new_array):\n    if isinstance(new_array, np.ndarray) and new_array.shape == original.shape:\n        return original.__array_wrap__(new_array)\n    else:\n        return new_array",
    ".xarray.core.groupby.py@@GroupBy._infer_concat_args": "def _infer_concat_args(self, applied_example):\n    if self._group_dim in applied_example.dims:\n        coord = self._group\n        positions = self._group_indices\n    else:\n        coord = self._unique_coord\n        positions = None\n    dim, = coord.dims\n    if isinstance(coord, _DummyGroup):\n        coord = None\n    return (coord, dim, positions)",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.concat.py@@concat": "def concat(objs, dim, data_vars='all', coords='different', compat='equals', positions=None, fill_value=dtypes.NA, join='outer'):\n    from .dataset import Dataset\n    from .dataarray import DataArray\n    try:\n        first_obj, objs = utils.peek_at(objs)\n    except StopIteration:\n        raise ValueError('must supply at least one object to concatenate')\n    if compat not in _VALID_COMPAT:\n        raise ValueError(\"compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'\" % compat)\n    if isinstance(first_obj, DataArray):\n        f = _dataarray_concat\n    elif isinstance(first_obj, Dataset):\n        f = _dataset_concat\n    else:\n        raise TypeError('can only concatenate xarray Dataset and DataArray objects, got %s' % type(first_obj))\n    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)",
    ".xarray.core.concat.py@@_dataarray_concat": "def _dataarray_concat(arrays, dim, data_vars, coords, compat, positions, fill_value=dtypes.NA, join='outer'):\n    arrays = list(arrays)\n    if data_vars != 'all':\n        raise ValueError('data_vars is not a valid argument when concatenating DataArray objects')\n    datasets = []\n    for n, arr in enumerate(arrays):\n        if n == 0:\n            name = arr.name\n        elif name != arr.name:\n            if compat == 'identical':\n                raise ValueError('array names not identical')\n            else:\n                arr = arr.rename(name)\n        datasets.append(arr._to_temp_dataset())\n    ds = _dataset_concat(datasets, dim, data_vars, coords, compat, positions, fill_value=fill_value, join=join)\n    return arrays[0]._from_temp_dataset(ds, name)",
    ".xarray.core.concat.py@@_dataset_concat": "def _dataset_concat(datasets, dim, data_vars, coords, compat, positions, fill_value=dtypes.NA, join='outer'):\n    from .dataset import Dataset\n    dim, coord = _calc_concat_dim_coord(dim)\n    datasets = [ds.copy() for ds in datasets]\n    datasets = align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)\n    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n    dim_names = set(dim_coords)\n    unlabeled_dims = dim_names - coord_names\n    both_data_and_coords = coord_names & data_names\n    if both_data_and_coords:\n        raise ValueError('%r is a coordinate in some datasets but not others.' % both_data_and_coords)\n    dim_coords.pop(dim, None)\n    dims_sizes.pop(dim, None)\n    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n        datasets = [ds.expand_dims(dim) for ds in datasets]\n    concat_over, equals, concat_dim_lengths = _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat)\n    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                raise ValueError('variables %r are present in some datasets but not others. ' % absent_merge_vars)\n            for var in variables_to_merge:\n                to_merge[var].append(ds.variables[var])\n        for var in variables_to_merge:\n            result_vars[var] = unique_variable(var, to_merge[var], compat=compat, equals=equals.get(var, None))\n    else:\n        result_vars = OrderedDict()\n    result_vars.update(dim_coords)\n    result_attrs = datasets[0].attrs\n    result_encoding = datasets[0].encoding\n    for ds in datasets[1:]:\n        if compat == 'identical' and (not utils.dict_equiv(ds.attrs, result_attrs)):\n            raise ValueError('Dataset global attributes not equal.')\n\n    def ensure_common_dims(vars):\n        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n        if dim not in common_dims:\n            common_dims = (dim,) + common_dims\n        for var, dim_len in zip(vars, concat_dim_lengths):\n            if var.dims != common_dims:\n                common_shape = tuple((dims_sizes.get(d, dim_len) for d in common_dims))\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n    for k in datasets[0].variables:\n        if k in concat_over:\n            try:\n                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            except KeyError:\n                raise ValueError('%r is not present in all datasets.' % k)\n            combined = concat_vars(vars, dim, positions)\n            assert isinstance(combined, Variable)\n            result_vars[k] = combined\n    result = Dataset(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError('Variables %r are coordinates in some datasets but not others.' % absent_coord_names)\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n    result = result.drop(unlabeled_dims, errors='ignore')\n    if coord is not None:\n        result[coord.name] = coord\n    return result",
    ".xarray.core.concat.py@@_calc_concat_dim_coord": "def _calc_concat_dim_coord(dim):\n    from .dataarray import DataArray\n    if isinstance(dim, str):\n        coord = None\n    elif not isinstance(dim, (DataArray, Variable)):\n        dim_name = getattr(dim, 'name', None)\n        if dim_name is None:\n            dim_name = 'concat_dim'\n        coord = IndexVariable(dim_name, dim)\n        dim = dim_name\n    elif not isinstance(dim, DataArray):\n        coord = as_variable(dim).to_index_variable()\n        dim, = coord.dims\n    else:\n        coord = dim\n        dim, = coord.dims\n    return (dim, coord)",
    ".xarray.core.dataset.py@@Dataset.copy": "def copy(self, deep: bool=False, data: Mapping=None) -> 'Dataset':\n    if data is None:\n        variables = OrderedDict(((k, v.copy(deep=deep)) for k, v in self._variables.items()))\n    elif not utils.is_dict_like(data):\n        raise ValueError('Data must be dict-like')\n    else:\n        var_keys = set(self.data_vars.keys())\n        data_keys = set(data.keys())\n        keys_not_in_vars = data_keys - var_keys\n        if keys_not_in_vars:\n            raise ValueError('Data must only contain variables in original dataset. Extra variables: {}'.format(keys_not_in_vars))\n        keys_missing_from_data = var_keys - data_keys\n        if keys_missing_from_data:\n            raise ValueError('Data must contain all variables in original dataset. Data is missing {}'.format(keys_missing_from_data))\n        variables = OrderedDict(((k, v.copy(deep=deep, data=data.get(k))) for k, v in self._variables.items()))\n    attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n    return self._replace(variables, attrs=attrs)",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            if hasattr(data, '__array_function__') or isinstance(data, dask_array_type):\n                data = data.copy()\n            elif not isinstance(data, PandasIndexAdapter):\n                data = np.array(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.dataset.py@@Dataset.encoding": "def encoding(self) -> Dict:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.concat.py@@_parse_datasets": "def _parse_datasets(datasets):\n    dims = set()\n    all_coord_names = set()\n    data_vars = set()\n    dim_coords = dict()\n    dims_sizes = {}\n    for ds in datasets:\n        dims_sizes.update(ds.dims)\n        all_coord_names.update(ds.coords)\n        data_vars.update(ds.data_vars)\n        for dim in set(ds.dims) - dims:\n            if dim not in dim_coords:\n                dim_coords[dim] = ds.coords[dim].variable\n        dims = dims | set(ds.dims)\n    return (dim_coords, dims_sizes, all_coord_names, data_vars)",
    ".xarray.core.dataset.py@@Dataset.data_vars": "def data_vars(self) -> DataVariables:\n    return DataVariables(self)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core.concat.py@@_calc_concat_over": "def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n    concat_over = set()\n    equals = {}\n    if dim in dim_names:\n        concat_over_existing_dim = True\n        concat_over.add(dim)\n    else:\n        concat_over_existing_dim = False\n    concat_dim_lengths = []\n    for ds in datasets:\n        if concat_over_existing_dim:\n            if dim not in ds.dims:\n                if dim in ds:\n                    ds = ds.set_coords(dim)\n        concat_over.update((k for k, v in ds.variables.items() if dim in v.dims))\n        concat_dim_lengths.append(ds.dims.get(dim, 1))\n\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == 'different':\n                if compat == 'override':\n                    raise ValueError(\"Cannot specify both %s='different' and compat='override'.\" % subset)\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        v_lhs = datasets[0].variables[k].load()\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n            elif opt == 'all':\n                concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n            elif opt == 'minimal':\n                pass\n            else:\n                raise ValueError('unexpected value for %s: %s' % (subset, opt))\n        else:\n            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n            if invalid_vars:\n                if subset == 'coords':\n                    raise ValueError('some variables in coords are not coordinates on the first dataset: %s' % (invalid_vars,))\n                else:\n                    raise ValueError('some variables in data_vars are not data variables on the first dataset: %s' % (invalid_vars,))\n            concat_over.update(opt)\n    process_subset_opt(data_vars, 'data_vars')\n    process_subset_opt(coords, 'coords')\n    return (concat_over, equals, concat_dim_lengths)",
    ".xarray.core.utils.py@@SortedKeysDict.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.concat.py@@process_subset_opt": "def process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == 'different':\n            if compat == 'override':\n                raise ValueError(\"Cannot specify both %s='different' and compat='override'.\" % subset)\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    v_lhs = datasets[0].variables[k].load()\n                    computed = []\n                    for ds_rhs in datasets[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not getattr(v_lhs, compat)(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            for ds, v in zip(datasets[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n        elif opt == 'all':\n            concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n        elif opt == 'minimal':\n            pass\n        else:\n            raise ValueError('unexpected value for %s: %s' % (subset, opt))\n    else:\n        invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n        if invalid_vars:\n            if subset == 'coords':\n                raise ValueError('some variables in coords are not coordinates on the first dataset: %s' % (invalid_vars,))\n            else:\n                raise ValueError('some variables in data_vars are not data variables on the first dataset: %s' % (invalid_vars,))\n        concat_over.update(opt)",
    ".xarray.core.variable.py@@Variable.load": "def load(self, **kwargs):\n    if isinstance(self._data, dask_array_type):\n        self._data = as_compatible_data(self._data.compute(**kwargs))\n    elif not hasattr(self._data, '__array_function__'):\n        self._data = np.asarray(self._data)\n    return self",
    ".xarray.core.variable.py@@Variable.compute": "def compute(self, **kwargs):\n    new = self.copy(deep=False)\n    return new.load(**kwargs)",
    ".xarray.core.variable.py@@Variable.equals": "def equals(self, other, equiv=duck_array_ops.array_equiv):\n    other = getattr(other, 'variable', other)\n    try:\n        return self.dims == other.dims and (self._data is other._data or equiv(self.data, other.data))\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.duck_array_ops.py@@array_equiv": "def array_equiv(arr1, arr2):\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', \"In the future, 'NAT == x'\")\n        flag_array = (arr1 == arr2) | isnull(arr1) & isnull(arr2)\n        return bool(flag_array.all())",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data):\n    return data if isinstance(data, dask_array_type) or hasattr(data, '__array_function__') else np.asarray(data)",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, (np.ndarray, dask_array_type)):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError('`out` is not valid for {}'.format(name))\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        func = _dask_or_eager_func(name)\n    try:\n        return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if isinstance(values, dask_array_type):\n            try:\n                return func(values, axis=axis, dtype=values.dtype, **kwargs)\n            except (AttributeError, TypeError):\n                msg = '%s is not yet implemented on dask arrays' % name\n        else:\n            msg = '%s is not available with skipna=False with the installed version of numpy; upgrade to numpy 1.12 or newer to use skipna=True or skipna=None' % name\n        raise NotImplementedError(msg)",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> 'OrderedDict[Any, Any]':\n    if self._attrs is None:\n        self._attrs = OrderedDict()\n    return self._attrs",
    ".xarray.core.variable.py@@concat": "def concat(variables, dim='concat_dim', positions=None, shortcut=False):\n    variables = list(variables)\n    if all((isinstance(v, IndexVariable) for v in variables)):\n        return IndexVariable.concat(variables, dim, positions, shortcut)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut)",
    ".xarray.core.concat.py@@ensure_common_dims": "def ensure_common_dims(vars):\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, concat_dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple((dims_sizes.get(d, dim_len) for d in common_dims))\n            var = var.set_dims(common_dims, common_shape)\n        yield var",
    ".xarray.core.variable.py@@Variable.set_dims": "def set_dims(self, dims, shape=None):\n    if isinstance(dims, str):\n        dims = [dims]\n    if shape is None and utils.is_dict_like(dims):\n        shape = dims.values()\n    missing_dims = set(self.dims) - set(dims)\n    if missing_dims:\n        raise ValueError('new dimensions %r must be a superset of existing dimensions %r' % (dims, self.dims))\n    self_dims = set(self.dims)\n    expanded_dims = tuple((d for d in dims if d not in self_dims)) + self.dims\n    if self.dims == expanded_dims:\n        expanded_data = self.data\n    elif shape is not None:\n        dims_map = dict(zip(dims, shape))\n        tmp_shape = tuple((dims_map[d] for d in expanded_dims))\n        expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n    else:\n        expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n    expanded_var = Variable(expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True)\n    return expanded_var.transpose(*dims)",
    ".xarray.core.variable.py@@Variable.transpose": "def transpose(self, *dims) -> 'Variable':\n    if len(dims) == 0:\n        dims = self.dims[::-1]\n    axes = self.get_axis_num(dims)\n    if len(dims) < 2:\n        return self.copy(deep=False)\n    data = as_indexable(self._data).transpose(axes)\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Union[Hashable, Iterable[Hashable]]) -> Union[int, Tuple[int, ...]]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError('%r not found in array dimensions %r' % (dim, self.dims))",
    ".xarray.core.variable.py@@Variable.concat": "def concat(cls, variables, dim='concat_dim', positions=None, shortcut=False):\n    if not isinstance(dim, str):\n        dim, = dim.dims\n    variables = list(variables)\n    first_var = variables[0]\n    arrays = [v.data for v in variables]\n    if dim in first_var.dims:\n        axis = first_var.get_axis_num(dim)\n        dims = first_var.dims\n        data = duck_array_ops.concatenate(arrays, axis=axis)\n        if positions is not None:\n            indices = nputils.inverse_permutation(np.concatenate(positions))\n            data = duck_array_ops.take(data, indices, axis=axis)\n    else:\n        axis = 0\n        dims = (dim,) + first_var.dims\n        data = duck_array_ops.stack(arrays, axis=axis)\n    attrs = OrderedDict(first_var.attrs)\n    encoding = OrderedDict(first_var.encoding)\n    if not shortcut:\n        for var in variables:\n            if var.dims != first_var.dims:\n                raise ValueError('inconsistent dimensions')\n            utils.remove_incompatible_items(attrs, var.attrs)\n    return cls(dims, data, attrs, encoding)",
    ".xarray.core.duck_array_ops.py@@concatenate": "def concatenate(arrays, axis=0):\n    return _concatenate(as_shared_dtype(arrays), axis=axis)",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> 'OrderedDict[Any, Any]':\n    if self._attrs is None:\n        self._attrs = OrderedDict()\n    return self._attrs",
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self):\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.utils.py@@remove_incompatible_items": "def remove_incompatible_items(first_dict: MutableMapping[K, V], second_dict: Mapping[K, V], compat: Callable[[V, V], bool]=equivalent) -> None:\n    for k in list(first_dict):\n        if k not in second_dict or not compat(first_dict[k], second_dict[k]):\n            del first_dict[k]",
    ".xarray.core.dataset.py@@Dataset.__init__": "def __init__(self, data_vars: Mapping[Hashable, Any]=None, coords: Mapping[Hashable, Any]=None, attrs: Mapping[Hashable, Any]=None, compat=None):\n    if compat is not None:\n        warnings.warn('The `compat` argument to Dataset is deprecated and will be removed in 0.15.Instead, use `merge` to control how variables are combined', FutureWarning, stacklevel=2)\n    else:\n        compat = 'broadcast_equals'\n    self._variables = OrderedDict()\n    self._coord_names = set()\n    self._dims = {}\n    self._accessors = None\n    self._attrs = None\n    self._file_obj = None\n    if data_vars is None:\n        data_vars = {}\n    if coords is None:\n        coords = {}\n    self._set_init_vars_and_dims(data_vars, coords, compat)\n    if attrs is not None:\n        self._attrs = OrderedDict(attrs)\n    self._encoding = None",
    ".xarray.core.dataset.py@@Dataset._set_init_vars_and_dims": "def _set_init_vars_and_dims(self, data_vars, coords, compat):\n    both_data_and_coords = [k for k in data_vars if k in coords]\n    if both_data_and_coords:\n        raise ValueError('variables %r are found in both data_vars and coords' % both_data_and_coords)\n    if isinstance(coords, Dataset):\n        coords = coords.variables\n    variables, coord_names, dims, indexes = merge_data_and_coords(data_vars, coords, compat=compat)\n    self._variables = variables\n    self._coord_names = coord_names\n    self._dims = dims\n    self._indexes = indexes",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data, coords, compat='broadcast_equals', join='outer'):\n    objects = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(_extract_indexes_from_coords(coords))\n    return merge_core(objects, compat, join, explicit_coords=explicit_coords, indexes=indexes)",
    ".xarray.core.merge.py@@_extract_indexes_from_coords": "def _extract_indexes_from_coords(coords):\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield (name, variable.to_index())",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objects: Iterable['CoercibleMapping'], compat: str='broadcast_equals', join: str='outer', priority_arg: Optional[int]=None, explicit_coords: Optional[Sequence]=None, indexes: Optional[Mapping[Hashable, pd.Index]]=None, fill_value: object=dtypes.NA) -> _MergeResult:\n    from .dataset import calculate_dimensions\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    dims = calculate_dimensions(variables)\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError('unable to determine if these variables should be coordinates or not in the merged result: %s' % ambiguous_coords)\n    return _MergeResult(variables, coord_names, dims, out_indexes)",
    ".xarray.core.utils.py@@ReprObject.__eq__": "def __eq__(self, other) -> bool:\n    if isinstance(other, ReprObject):\n        return self._value == other._value\n    return False",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_mappings: Iterable['DatasetLike']) -> Tuple[Set[Hashable], Set[Hashable]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names: Set[Hashable] = set()\n    noncoord_names: Set[Hashable] = set()\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError('coordinate %s shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.' % coord_name)",
    ".xarray.core.dataset.py@@Dataset.set_coords": "def set_coords(self, names: 'Union[Hashable, Iterable[Hashable]]', inplace: bool=None) -> 'Dataset':\n    _check_inplace(inplace)\n    if isinstance(names, str) or not isinstance(names, Iterable):\n        names = [names]\n    else:\n        names = list(names)\n    self._assert_all_in_dataset(names)\n    obj = self.copy()\n    obj._coord_names.update(names)\n    return obj",
    ".xarray.core.utils.py@@_check_inplace": "def _check_inplace(inplace: Optional[bool]) -> None:\n    if inplace is not None:\n        raise TypeError(\"The `inplace` argument has been removed from xarray. You can achieve an identical effect with python's standard assignment.\")",
    ".xarray.core.dataset.py@@Dataset._assert_all_in_dataset": "def _assert_all_in_dataset(self, names: Iterable[Hashable], virtual_okay: bool=False) -> None:\n    bad_names = set(names) - set(self._variables)\n    if virtual_okay:\n        bad_names -= self.virtual_variables\n    if bad_names:\n        raise ValueError('One or more of the specified variables cannot be found in this dataset')",
    ".xarray.core.dataset.py@@Dataset.drop": "def drop(self, labels: Union[Hashable, Iterable[Hashable]], *, errors: str='raise') -> 'Dataset':\n    ...",
    ".xarray.core.dataset.py@@Dataset._drop_vars": "def _drop_vars(self, names: set, errors: str='raise') -> 'Dataset':\n    if errors == 'raise':\n        self._assert_all_in_dataset(names)\n    variables = OrderedDict(((k, v) for k, v in self._variables.items() if k not in names))\n    coord_names = {k for k in self._coord_names if k in variables}\n    indexes = OrderedDict(((k, v) for k, v in self.indexes.items() if k not in names))\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.groupby.py@@_maybe_reorder": "def _maybe_reorder(xarray_obj, dim, positions):\n    order = _inverse_permutation_indices(positions)\n    if order is None:\n        return xarray_obj\n    else:\n        return xarray_obj[{dim: order}]",
    ".xarray.core.groupby.py@@_inverse_permutation_indices": "def _inverse_permutation_indices(positions):\n    if not positions:\n        return None\n    if isinstance(positions[0], slice):\n        positions = _consolidate_slices(positions)\n        if positions == slice(None):\n            return None\n        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]\n    indices = nputils.inverse_permutation(np.concatenate(positions))\n    return indices",
    ".xarray.core.dataarray.py@@DataArray.transpose": "def transpose(self, *dims: Hashable, transpose_coords: bool=None) -> 'DataArray':\n    if dims:\n        if set(dims) ^ set(self.dims):\n            raise ValueError('arguments to transpose (%s) must be permuted array dimensions (%s)' % (dims, tuple(self.dims)))\n    variable = self.variable.transpose(*dims)\n    if transpose_coords:\n        coords = OrderedDict()\n        for name, coord in self.coords.items():\n            coord_dims = tuple((dim for dim in dims if dim in coord.dims))\n            coords[name] = coord.variable.transpose(*coord_dims)\n        return self._replace(variable, coords)\n    else:\n        if transpose_coords is None and any((self[c].ndim > 1 for c in self.coords)):\n            warnings.warn('This DataArray contains multi-dimensional coordinates. In the future, these coordinates will be transposed as well unless you specify transpose_coords=False.', FutureWarning, stacklevel=2)\n        return self._replace(variable)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.dataarray.py@@DataArray.__getitem__": "def __getitem__(self, key: Any) -> 'DataArray':\n    if isinstance(key, str):\n        return self._getitem_coord(key)\n    else:\n        return self.isel(indexers=self._item_key_to_dict(key))",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self, key):\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, self._level_coords, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self, variable: Variable, name: Optional[Hashable]=__default) -> 'DataArray':\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = OrderedDict(((k, v) for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))))\n    else:\n        allowed_dims = set(variable.dims)\n        coords = OrderedDict(((k, v) for k, v in self._coords.items() if set(v.dims) <= allowed_dims))\n    return self._replace(variable, coords, name)",
    ".xarray.core.dataarray.py@@DataArray.ndim": "def ndim(self) -> int:\n    return self.variable.ndim",
    ".xarray.core.dataarray.py@@DataArray.indexes": "def indexes(self) -> Indexes:\n    if self._indexes is None:\n        self._indexes = default_indexes(self._coords, self.dims)\n    return Indexes(self._indexes)",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data.copy(deep=deep)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.copy": "def copy(self, deep: bool=True) -> 'PandasIndexAdapter':\n    array = self.array.copy(deep=True) if deep else self.array\n    return PandasIndexAdapter(array, self._dtype)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.variable.py@@IndexVariable.level_names": "def level_names(self):\n    index = self.to_index()\n    if isinstance(index, pd.MultiIndex):\n        return index.names\n    else:\n        return None",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._update_coords": "def _update_coords(self, coords: 'OrderedDict[Hashable, Variable]', indexes: Mapping[Hashable, pd.Index]) -> None:\n    from .dataset import calculate_dimensions\n    coords_plus_data = coords.copy()\n    coords_plus_data[_THIS_ARRAY] = self._data.variable\n    dims = calculate_dimensions(coords_plus_data)\n    if not set(dims) <= set(self.dims):\n        raise ValueError('cannot add coordinates with new dimensions to a DataArray')\n    self._data._coords = coords\n    original_indexes = OrderedDict(self._data.indexes)\n    original_indexes.update(indexes)\n    self._data._indexes = original_indexes",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self._data.dims",
    ".xarray.core.groupby.py@@GroupBy._maybe_restore_empty_groups": "def _maybe_restore_empty_groups(self, combined):\n    if self._full_index is not None and self._group.name in combined.dims:\n        indexers = {self._group.name: self._full_index}\n        combined = combined.reindex(**indexers)\n    return combined",
    ".xarray.core.groupby.py@@GroupBy._maybe_unstack": "def _maybe_unstack(self, obj):\n    if self._stacked_dim is not None and self._stacked_dim in obj.dims:\n        obj = obj.unstack(self._stacked_dim)\n        for dim in self._inserted_dims:\n            if dim in obj.coords:\n                del obj.coords[dim]\n    return obj",
    ".xarray.core.dataarray.py@@DataArray.drop": "def drop(self, labels: Union[Hashable, Iterable[Hashable]], *, errors: str='raise') -> 'DataArray':\n    ...",
    ".xarray.core.variable.py@@Variable.to_index_variable": "def to_index_variable(self):\n    return IndexVariable(self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True)",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    if key in self._data.data_vars:\n        raise KeyError(key)\n    return cast('DataArray', self._data[key])",
    ".xarray.core.dataset.py@@DataVariables.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._dataset._variables and key not in self._dataset._coord_names",
    ".xarray.core.dataset.py@@Dataset.__getitem__": "def __getitem__(self, key: Any) -> 'Union[DataArray, Dataset]':\n    if utils.is_dict_like(key):\n        return self.isel(**cast(Mapping, key))\n    if hashable(key):\n        return self._construct_dataarray(key)\n    else:\n        return self._copy_listed(np.asarray(key))",
    ".xarray.core.utils.py@@hashable": "def hashable(v: Any) -> bool:\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True",
    ".xarray.core.dataset.py@@Dataset._construct_dataarray": "def _construct_dataarray(self, name: Hashable) -> 'DataArray':\n    from .dataarray import DataArray\n    try:\n        variable = self._variables[name]\n    except KeyError:\n        _, name, variable = _get_virtual_variable(self._variables, name, self._level_coords, self.dims)\n    needed_dims = set(variable.dims)\n    coords = OrderedDict()\n    for k in self.coords:\n        if set(self.variables[k].dims) <= needed_dims:\n            coords[k] = self.variables[k]\n    if self._indexes is None:\n        indexes = None\n    else:\n        indexes = OrderedDict(((k, v) for k, v in self._indexes.items() if k in coords))\n    return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.variable.py@@IndexVariable.load": "def load(self):\n    return self",
    ".xarray.core.variable.py@@IndexVariable.equals": "def equals(self, other, equiv=None):\n    if equiv is not None:\n        return super().equals(other, equiv)\n    other = getattr(other, 'variable', other)\n    try:\n        return self.dims == other.dims and self._data_equals(other)\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.variable.py@@IndexVariable._data_equals": "def _data_equals(self, other):\n    return self.to_index().equals(other.to_index())",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.transpose": "def transpose(self, order):\n    return self.array.transpose(order)",
    ".xarray.core.indexing.py@@as_integer_slice": "def as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)",
    ".xarray.core.indexing.py@@as_integer_or_none": "def as_integer_or_none(value):\n    return None if value is None else operator.index(value)"
}