{
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    global_choice = OPTIONS['keep_attrs']\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(\"The global option keep_attrs must be one of True, False or 'default'.\")",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> bool:\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.computation.py@@apply_ufunc": "def apply_ufunc(func: Callable, *args: Any, input_core_dims: Sequence[Sequence]=None, output_core_dims: Optional[Sequence[Sequence]]=((),), exclude_dims: AbstractSet=frozenset(), vectorize: bool=False, join: str='exact', dataset_join: str='exact', dataset_fill_value: object=_NO_FILL_VALUE, keep_attrs: bool=False, kwargs: Mapping=None, dask: str='forbidden', output_dtypes: Sequence=None, output_sizes: Mapping[Any, int]=None) -> Any:\n    from .groupby import GroupBy\n    from .dataarray import DataArray\n    from .variable import Variable\n    if input_core_dims is None:\n        input_core_dims = ((),) * len(args)\n    elif len(input_core_dims) != len(args):\n        raise ValueError('input_core_dims must be None or a tuple with the length same to the number of arguments. Given input_core_dims: {}, number of args: {}.'.format(input_core_dims, len(args)))\n    if kwargs is None:\n        kwargs = {}\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n    if exclude_dims and (not exclude_dims <= signature.all_core_dims):\n        raise ValueError('each dimension in `exclude_dims` must also be a core dimension in the function signature')\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n    if vectorize:\n        if signature.all_core_dims:\n            func = np.vectorize(func, otypes=output_dtypes, signature=signature.to_gufunc_string())\n        else:\n            func = np.vectorize(func, otypes=output_dtypes)\n    variables_vfunc = functools.partial(apply_variable_ufunc, func, signature=signature, exclude_dims=exclude_dims, keep_attrs=keep_attrs, dask=dask, output_dtypes=output_dtypes, output_sizes=output_sizes)\n    if any((isinstance(a, GroupBy) for a in args)):\n        this_apply = functools.partial(apply_ufunc, func, input_core_dims=input_core_dims, output_core_dims=output_core_dims, exclude_dims=exclude_dims, join=join, dataset_join=dataset_join, dataset_fill_value=dataset_fill_value, keep_attrs=keep_attrs, dask=dask)\n        return apply_groupby_func(this_apply, *args)\n    elif any((is_dict_like(a) for a in args)):\n        return apply_dataset_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, dataset_join=dataset_join, fill_value=dataset_fill_value, keep_attrs=keep_attrs)\n    elif any((isinstance(a, DataArray) for a in args)):\n        return apply_dataarray_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, keep_attrs=keep_attrs)\n    elif any((isinstance(a, Variable) for a in args)):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)",
    ".xarray.core.computation.py@@_UFuncSignature.__init__": "def __init__(self, input_core_dims, output_core_dims=((),)):\n    self.input_core_dims = tuple((tuple(a) for a in input_core_dims))\n    self.output_core_dims = tuple((tuple(a) for a in output_core_dims))\n    self._all_input_core_dims = None\n    self._all_output_core_dims = None\n    self._all_core_dims = None",
    ".xarray.core.computation.py@@_UFuncSignature.all_core_dims": "def all_core_dims(self):\n    if self._all_core_dims is None:\n        self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n    return self._all_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_input_core_dims": "def all_input_core_dims(self):\n    if self._all_input_core_dims is None:\n        self._all_input_core_dims = frozenset((dim for dims in self.input_core_dims for dim in dims))\n    return self._all_input_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_output_core_dims": "def all_output_core_dims(self):\n    if self._all_output_core_dims is None:\n        self._all_output_core_dims = frozenset((dim for dims in self.output_core_dims for dim in dims))\n    return self._all_output_core_dims",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.computation.py@@apply_variable_ufunc": "def apply_variable_ufunc(func, *args, signature, exclude_dims=frozenset(), dask='forbidden', output_dtypes=None, output_sizes=None, keep_attrs=False):\n    from .variable import Variable, as_compatible_data\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')), exclude_dims=exclude_dims)\n    broadcast_dims = tuple((dim for dim in dim_sizes if dim not in signature.all_core_dims))\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims) if isinstance(arg, Variable) else arg for arg, core_dims in zip(args, signature.input_core_dims)]\n    if any((isinstance(array, dask_array_type) for array in input_data)):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an argument, but handling for dask arrays has not been enabled. Either set the ``dask`` argument or load your data into memory first with ``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            input_dims = [broadcast_dims + dims for dims in signature.input_core_dims]\n            numpy_func = func\n\n            def func(*arrays):\n                return _apply_blockwise(numpy_func, arrays, input_dims, output_dims, signature, output_dtypes, output_sizes)\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in apply_ufunc: {}'.format(dask))\n    result_data = func(*input_data)\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs:\n        raise ValueError('applied function does not have the number of outputs specified in the ufunc signature. Result is not a tuple of {} elements: {!r}'.format(signature.num_outputs, result_data))\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError('applied function returned data with unexpected number of dimensions: {} vs {}, for dimensions {}'.format(data.ndim, len(dims), dims))\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError('size of dimension {!r} on inputs was unexpectedly changed by applied function from {} to {}. Only dimensions specified in ``exclude_dims`` with xarray.apply_ufunc are allowed to change size.'.format(dim, dim_sizes[dim], new_size))\n        if keep_attrs and isinstance(args[0], Variable):\n            var.attrs.update(args[0].attrs)\n        output.append(var)\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)",
    ".xarray.core.computation.py@@unified_dim_sizes": "def unified_dim_sizes(variables: Iterable[Variable], exclude_dims: AbstractSet=frozenset()) -> Dict[Hashable, int]:\n    dim_sizes: Dict[Hashable, int] = {}\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError('broadcasting cannot handle duplicate dimensions on a variable: %r' % list(var.dims))\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError('operands cannot be broadcast together with mismatched lengths for dimension %r: %s vs %s' % (dim, dim_sizes[dim], size))\n    return dim_sizes",
    ".xarray.core.computation.py@@broadcast_compat_data": "def broadcast_compat_data(variable: Variable, broadcast_dims: Tuple[Hashable, ...], core_dims: Tuple[Hashable, ...]) -> Any:\n    data = variable.data\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n    if new_dims == old_dims:\n        return data\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError('operand to apply_ufunc has required core dimensions {}, but some of these dimensions are absent on an input variable: {}'.format(list(core_dims), missing_core_dims))\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError('operand to apply_ufunc encountered unexpected dimensions %r on an input variable: these are core dimensions on other input or output variables' % unexpected_dims)\n    old_broadcast_dims = tuple((d for d in broadcast_dims if d in set_old_dims))\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple((old_dims.index(d) for d in reordered_dims))\n        data = duck_array_ops.transpose(data, order)\n    if new_dims != reordered_dims:\n        key_parts = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n    return data",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if hasattr(self._data, '__array_function__') or isinstance(self._data, dask_array_type):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError(f'`out` is not valid for {name}')\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        func = _dask_or_eager_func(name)\n    try:\n        return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if isinstance(values, dask_array_type):\n            try:\n                return func(values, axis=axis, dtype=values.dtype, **kwargs)\n            except (AttributeError, TypeError):\n                msg = '%s is not yet implemented on dask arrays' % name\n        else:\n            msg = '%s is not available with skipna=False with the installed version of numpy; upgrade to numpy 1.12 or newer to use skipna=True or skipna=None' % name\n        raise NotImplementedError(msg)",
    ".xarray.core.computation.py@@func": "def func(*arrays):\n    return _apply_blockwise(numpy_func, arrays, input_dims, output_dims, signature, output_dtypes, output_sizes)",
    ".xarray.core.computation.py@@_apply_blockwise": "def _apply_blockwise(func, args, input_dims, output_dims, signature, output_dtypes, output_sizes=None):\n    import dask.array\n    if signature.num_outputs > 1:\n        raise NotImplementedError(\"multiple outputs from apply_ufunc not yet supported with dask='parallelized'\")\n    if output_dtypes is None:\n        raise ValueError(\"output dtypes (output_dtypes) must be supplied to apply_func when using dask='parallelized'\")\n    if not isinstance(output_dtypes, list):\n        raise TypeError('output_dtypes must be a list of objects coercible to numpy dtypes, got {}'.format(output_dtypes))\n    if len(output_dtypes) != signature.num_outputs:\n        raise ValueError('apply_ufunc arguments output_dtypes and output_core_dims must have the same length: {} vs {}'.format(len(output_dtypes), signature.num_outputs))\n    dtype, = output_dtypes\n    if output_sizes is None:\n        output_sizes = {}\n    new_dims = signature.all_output_core_dims - signature.all_input_core_dims\n    if any((dim not in output_sizes for dim in new_dims)):\n        raise ValueError(\"when using dask='parallelized' with apply_ufunc, output core dimensions not found on inputs must have explicitly set sizes with ``output_sizes``: {}\".format(new_dims))\n    for n, (data, core_dims) in enumerate(zip(args, signature.input_core_dims)):\n        if isinstance(data, dask_array_type):\n            for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                if len(data.chunks[axis]) != 1:\n                    raise ValueError(\"dimension {!r} on {}th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, rechunk into a single dask array chunk along this dimension, i.e., ``.chunk({})``, but beware that this may significantly increase memory usage.\".format(dim, n, {dim: -1}))\n    out_ind, = output_dims\n    blockwise_args = []\n    for arg, dims in zip(args, input_dims):\n        ndim = getattr(arg, 'ndim', 0)\n        trimmed_dims = dims[-ndim:] if ndim else ()\n        blockwise_args.extend([arg, trimmed_dims])\n    return dask.array.blockwise(func, out_ind, *blockwise_args, dtype=dtype, concatenate=True, new_axes=output_sizes)",
    ".xarray.core.computation.py@@_UFuncSignature.num_outputs": "def num_outputs(self):\n    return len(self.output_core_dims)"
}