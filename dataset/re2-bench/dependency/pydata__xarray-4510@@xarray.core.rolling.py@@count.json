{
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    global_choice = OPTIONS['keep_attrs']\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(\"The global option keep_attrs must be one of True, False or 'default'.\")",
    ".xarray.core.rolling.py@@DataArrayRolling._counts": "def _counts(self, keep_attrs):\n    rolling_dim = {d: utils.get_temp_dimname(self.obj.dims, f'_rolling_dim_{d}') for d in self.dim}\n    counts = self.obj.notnull(keep_attrs=keep_attrs).rolling(center={d: self.center[i] for i, d in enumerate(self.dim)}, **{d: w for d, w in zip(self.dim, self.window)}).construct(rolling_dim, fill_value=False, keep_attrs=keep_attrs).sum(dim=list(rolling_dim.values()), skipna=False, keep_attrs=keep_attrs)\n    return counts",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.utils.py@@get_temp_dimname": "def get_temp_dimname(dims: Container[Hashable], new_dim: Hashable) -> Hashable:\n    while new_dim in dims:\n        new_dim = '_' + str(new_dim)\n    return new_dim",
    ".xarray.core.dataarray.py@@DataArray.func": "def func(self, other):\n    if isinstance(other, groupby.GroupBy):\n        raise TypeError('in-place operations between a DataArray and a grouped object are not permitted')\n    other_coords = getattr(other, 'coords', None)\n    other_variable = getattr(other, 'variable', other)\n    try:\n        with self.coords._merge_inplace(other_coords):\n            f(self.variable, other_variable)\n    except MergeError as exc:\n        raise MergeError('Automatic alignment is not supported for in-place operations.\\nConsider aligning the indices manually or using a not-in-place operation.\\nSee https://github.com/pydata/xarray/issues/3910 for more explanations.') from exc\n    return self",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if is_duck_array(self._data):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.utils.py@@is_duck_array": "def is_duck_array(value: Any) -> bool:\n    if isinstance(value, np.ndarray):\n        return True\n    return hasattr(value, 'ndim') and hasattr(value, 'shape') and hasattr(value, 'dtype') and hasattr(value, '__array_function__') and hasattr(value, '__array_ufunc__')",
    ".xarray.core.ops.py@@func": "def func(self, *args, **kwargs):\n    try:\n        return getattr(self, name)(*args, **kwargs)\n    except AttributeError:\n        return f(self, *args, **kwargs)",
    ".xarray.core.duck_array_ops.py@@notnull": "def notnull(data):\n    return ~isnull(data)",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, (np.ndarray, dask_array_type)):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data, xp=np):\n    return data if is_duck_array(data) else xp.asarray(data)",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError(f'`out` is not valid for {name}')\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        if name in ['sum', 'prod']:\n            kwargs.pop('min_count', None)\n        func = _dask_or_eager_func(name, dask_module=dask_module)\n    try:\n        return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if not is_duck_dask_array(values):\n            raise\n        try:\n            return func(values, axis=axis, dtype=values.dtype, **kwargs)\n        except (AttributeError, TypeError):\n            raise NotImplementedError(f'{name} is not yet implemented on dask arrays')",
    ".xarray.core.pycompat.py@@is_duck_dask_array": "def is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
    ".xarray.core.dataarray.py@@DataArray.__array_wrap__": "def __array_wrap__(self, obj, context=None) -> 'DataArray':\n    new_var = self.variable.__array_wrap__(obj, context)\n    return self._replace(new_var)",
    ".xarray.core.variable.py@@Variable.__array_wrap__": "def __array_wrap__(self, obj, context=None):\n    return Variable(self.dims, obj)",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, Variable):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    data = getattr(data, 'values', data)\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray):\n        if hasattr(data, '__array_function__'):\n            if IS_NEP18_ACTIVE:\n                return data\n            else:\n                raise TypeError('Got an NumPy-like array type providing the __array_function__ protocol but NEP18 is not enabled. Check that numpy >= v1.16 and that the environment variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to \"1\"')\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'M':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'm':\n            data = _possibly_convert_objects(data)\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError('dimensions %s must have the same length as the number of data dimensions, ndim=%s' % (dims, self.ndim))\n    return dims",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self, variable: Variable=None, coords=None, name: Union[Hashable, None, Default]=_default, indexes=None) -> 'DataArray':\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if name is _default:\n        name = self.name\n    return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Optional[Hashable]:\n    return self._name",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None]=None, dims: Union[Hashable, Sequence[Hashable], None]=None, name: Hashable=None, attrs: Mapping=None, indexes: Dict[Hashable, pd.Index]=None, fastpath: bool=False):\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n    else:\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n            elif isinstance(data, pdcompat.Panel):\n                coords = [data.items, data.major_axis, data.minor_axis]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None and (not isinstance(data, PANDAS_TYPES)):\n            attrs = getattr(data, 'attrs', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, fastpath=True)\n        indexes = dict(_extract_indexes_from_coords(coords))\n    self._variable = variable\n    assert isinstance(coords, dict)\n    self._coords = coords\n    self._name = name\n    self._indexes = indexes\n    self._file_obj = None",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '{!r} object has no attribute {!r}'.format(type(self).__name__, name):\n            raise\n        raise AttributeError(\"cannot set attribute %r on a %r object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\" % (name, type(self).__name__)) from e",
    ".xarray.core.dataarray.py@@DataArray.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    return self.variable.attrs",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.common.py@@DataWithCoords.rolling": "def rolling(self, dim: Mapping[Hashable, int]=None, min_periods: int=None, center: Union[bool, Mapping[Hashable, bool]]=False, keep_attrs: bool=None, **window_kwargs: int):\n    dim = either_dict_or_kwargs(dim, window_kwargs, 'rolling')\n    return self._rolling_cls(self, dim, min_periods=min_periods, center=center, keep_attrs=keep_attrs)",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Optional[Mapping[Hashable, T]], kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .%s must be a dictionary' % func_name)\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional arguments to .%s' % func_name)\n        return pos_kwargs\n    else:\n        return cast(Mapping[Hashable, T], kw_kwargs)",
    ".xarray.core.rolling.py@@DataArrayRolling.__init__": "def __init__(self, obj, windows, min_periods=None, center=False, keep_attrs=None):\n    super().__init__(obj, windows, min_periods=min_periods, center=center, keep_attrs=keep_attrs)\n    self.window_labels = self.obj[self.dim[0]]",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.dataarray.py@@DataArray.__getitem__": "def __getitem__(self, key: Any) -> 'DataArray':\n    if isinstance(key, str):\n        return self._getitem_coord(key)\n    else:\n        return self.isel(indexers=self._item_key_to_dict(key))",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self, key):\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, self._level_coords, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self, variable: Variable, name: Union[Hashable, None, Default]=_default) -> 'DataArray':\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n        indexes = self._indexes\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = {k: v for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))}\n        changed_dims = [k for k in variable.dims if variable.sizes[k] != self.sizes[k]]\n        indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n    else:\n        allowed_dims = set(variable.dims)\n        coords = {k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims}\n        indexes = propagate_indexes(self._indexes, exclude=set(self.dims) - allowed_dims)\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.indexes.py@@propagate_indexes": "def propagate_indexes(indexes: Optional[Dict[Hashable, pd.Index]], exclude: Optional[Any]=None) -> Optional[Dict[Hashable, pd.Index]]:\n    if exclude is None:\n        exclude = ()\n    if is_scalar(exclude):\n        exclude = (exclude,)\n    if indexes is not None:\n        new_indexes = {k: v for k, v in indexes.items() if k not in exclude}\n    else:\n        new_indexes = None\n    return new_indexes",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> bool:\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.rolling.py@@DataArrayRolling.construct": "def construct(self, window_dim=None, stride=1, fill_value=dtypes.NA, keep_attrs=None, **window_dim_kwargs):\n    from .dataarray import DataArray\n    keep_attrs = self._get_keep_attrs(keep_attrs)\n    if window_dim is None:\n        if len(window_dim_kwargs) == 0:\n            raise ValueError('Either window_dim or window_dim_kwargs need to be specified.')\n        window_dim = {d: window_dim_kwargs[d] for d in self.dim}\n    window_dim = self._mapping_to_list(window_dim, allow_default=False, allow_allsame=False)\n    stride = self._mapping_to_list(stride, default=1)\n    window = self.obj.variable.rolling_window(self.dim, self.window, window_dim, self.center, fill_value=fill_value)\n    attrs = self.obj.attrs if keep_attrs else {}\n    result = DataArray(window, dims=self.obj.dims + tuple(window_dim), coords=self.obj.coords, attrs=attrs, name=self.obj.name)\n    return result.isel(**{d: slice(None, None, s) for d, s in zip(self.dim, stride)})",
    ".xarray.core.variable.py@@Variable.rolling_window": "def rolling_window(self, dim, window, window_dim, center=False, fill_value=dtypes.NA):\n    if fill_value is dtypes.NA:\n        dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        array = self.astype(dtype, copy=False).data\n    else:\n        dtype = self.dtype\n        array = self.data\n    if isinstance(dim, list):\n        assert len(dim) == len(window)\n        assert len(dim) == len(window_dim)\n        assert len(dim) == len(center)\n    else:\n        dim = [dim]\n        window = [window]\n        window_dim = [window_dim]\n        center = [center]\n    axis = [self.get_axis_num(d) for d in dim]\n    new_dims = self.dims + tuple(window_dim)\n    return Variable(new_dims, duck_array_ops.rolling_window(array, axis=axis, window=window, center=center, fill_value=fill_value))",
    ".xarray.core.variable.py@@Variable.dtype": "def dtype(self):\n    return self._data.dtype",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Union[Hashable, Iterable[Hashable]]) -> Union[int, Tuple[int, ...]]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError(f'{dim!r} not found in array dimensions {self.dims!r}')",
    ".xarray.core.duck_array_ops.py@@rolling_window": "def rolling_window(array, axis, window, center, fill_value):\n    if is_duck_dask_array(array):\n        return dask_array_ops.rolling_window(array, axis, window, center, fill_value)\n    else:\n        return nputils.rolling_window(array, axis, window, center, fill_value)",
    ".xarray.core.dask_array_ops.py@@rolling_window": "def rolling_window(a, axis, window, center, fill_value):\n    import dask.array as da\n    if not hasattr(axis, '__len__'):\n        axis = [axis]\n        window = [window]\n        center = [center]\n    orig_shape = a.shape\n    depth = {d: 0 for d in range(a.ndim)}\n    offset = [0] * a.ndim\n    drop_size = [0] * a.ndim\n    pad_size = [0] * a.ndim\n    for ax, win, cent in zip(axis, window, center):\n        if ax < 0:\n            ax = a.ndim + ax\n        depth[ax] = int(win / 2)\n        offset[ax] = 1 if win % 2 == 0 else 0\n        if depth[ax] > min(a.chunks[ax]):\n            raise ValueError('For window size %d, every chunk should be larger than %d, but the smallest chunk size is %d. Rechunk your array\\nwith a larger chunk size or a chunk size that\\nmore evenly divides the shape of your array.' % (win, depth[ax], min(a.chunks[ax])))\n        if cent:\n            start = int(win / 2)\n            end = win - 1 - start\n        else:\n            start, end = (win - 1, 0)\n        pad_size[ax] = max(start, end) + offset[ax] - depth[ax]\n        drop_size[ax] = 0\n        if pad_size[ax] > 0:\n            if pad_size[ax] < depth[ax]:\n                drop_size[ax] = depth[ax] - pad_size[ax]\n                pad_size[ax] = depth[ax]\n    a = da.pad(a, [(p, 0) for p in pad_size], mode='constant', constant_values=fill_value)\n    boundary = {d: fill_value for d in range(a.ndim)}\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n\n    def func(x, window, axis):\n        x = np.asarray(x)\n        index = [slice(None)] * x.ndim\n        for ax, win in zip(axis, window):\n            x = nputils._rolling_window(x, win, ax)\n            index[ax] = slice(offset[ax], None)\n        return x[tuple(index)]\n    chunks = list(a.chunks) + window\n    new_axis = [a.ndim + i for i in range(len(axis))]\n    out = da.map_blocks(func, ag, dtype=a.dtype, new_axis=new_axis, chunks=chunks, window=window, axis=axis)\n    index = [slice(None)] * a.ndim\n    for ax in axis:\n        index[ax] = slice(drop_size[ax], drop_size[ax] + orig_shape[ax])\n    return out[tuple(index)]",
    ".xarray.core.dask_array_ops.py@@func": "def func(x, window, axis):\n    x = np.asarray(x)\n    index = [slice(None)] * x.ndim\n    for ax, win in zip(axis, window):\n        x = nputils._rolling_window(x, win, ax)\n        index[ax] = slice(offset[ax], None)\n    return x[tuple(index)]",
    ".xarray.core.nputils.py@@_rolling_window": "def _rolling_window(a, window, axis=-1):\n    axis = normalize_axis_index(axis, a.ndim)\n    a = np.swapaxes(a, axis, -1)\n    if window < 1:\n        raise ValueError(f'`window` must be at least 1. Given : {window}')\n    if window > a.shape[-1]:\n        raise ValueError(f'`window` is too long. Given : {window}')\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    rolling = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides, writeable=False)\n    return np.swapaxes(rolling, -2, axis)",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: 'DataArray'):\n    self._data = dataarray",
    ".xarray.core.dataarray.py@@_check_data_shape": "def _check_data_shape(data, coords, dims):\n    if data is dtypes.NA:\n        data = np.nan\n    if coords is not None and utils.is_scalar(data, include_0d=False):\n        if utils.is_dict_like(coords):\n            if dims is None:\n                return data\n            else:\n                data_shape = tuple((as_variable(coords[k], k).size if k in coords.keys() else 1 for k in dims))\n        else:\n            data_shape = tuple((as_variable(coord, 'foo').size for coord in coords))\n        data = np.full(data_shape, data)\n    return data",
    ".xarray.core.dataarray.py@@_infer_coords_and_dims": "def _infer_coords_and_dims(shape, coords, dims) -> 'Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]':\n    if coords is not None and (not utils.is_dict_like(coords)) and (len(coords) != len(shape)):\n        raise ValueError('coords is not dict-like, but it has %s items, which does not match the %s dimensions of the data' % (len(coords), len(shape)))\n    if isinstance(dims, str):\n        dims = (dims,)\n    if dims is None:\n        dims = ['dim_%s' % n for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            if utils.is_dict_like(coords):\n                raise ValueError('inferring DataArray dimensions from dictionary like ``coords`` is no longer supported. Use an explicit list of ``dims`` instead.')\n            for n, (dim, coord) in enumerate(zip(dims, coords)):\n                coord = as_variable(coord, name=dims[n]).to_index_variable()\n                dims[n] = coord.name\n        dims = tuple(dims)\n    elif len(dims) != len(shape):\n        raise ValueError('different number of dimensions on data and dims: %s vs %s' % (len(shape), len(dims)))\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError('dimension %s is not a string' % d)\n    new_coords: Dict[Any, Variable] = {}\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var.to_index_variable()\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any((d not in dims for d in v.dims)):\n            raise ValueError('coordinate %s has dimensions %s, but these are not a subset of the DataArray dimensions %s' % (k, v.dims, dims))\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError('conflicting sizes for dimension %r: length %s on the data but length %s on coordinate %r' % (d, sizes[d], s, k))\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError('coordinate %r is a DataArray dimension, but it has shape %r rather than expected shape %r matching the dimension size' % (k, v.shape, (sizes[k],)))\n    assert_unique_multiindex_level_names(new_coords)\n    return (new_coords, dims)",
    ".xarray.core.coordinates.py@@Coordinates.__iter__": "def __iter__(self) -> Iterator['Hashable']:\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return set(self._data._coords)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    return self._data._getitem_coord(key)",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> 'Union[Variable, IndexVariable]':\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError('variable {!r} has invalid type {!r}'.format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError('cannot set variable %r with %r-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.' % (name, data.ndim))\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError('unable to convert object into a variable without an explicit list of dimensions: %r' % obj)\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError('%r has more than 1-dimension and the same name as one of its dimensions %r. xarray disallows such variables because they conflict with the coordinates used to label dimensions.' % (name, obj.dims))\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data.copy(deep=deep)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.copy": "def copy(self, deep: bool=True) -> 'PandasIndexAdapter':\n    array = self.array.copy(deep=True) if deep else self.array\n    return PandasIndexAdapter(array, self._dtype)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__init__": "def __init__(self, array: Any, dtype: DTypeLike=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        if isinstance(array, pd.PeriodIndex):\n            dtype = np.dtype('O')\n        elif hasattr(array, 'categories'):\n            dtype = array.categories.dtype\n        elif not utils.is_valid_numpy_dtype(array.dtype):\n            dtype = np.dtype('O')\n        else:\n            dtype = array.dtype\n    else:\n        dtype = np.dtype(dtype)\n    self._dtype = dtype",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError('%s objects must be 1-dimensional' % type(self).__name__)\n    if not isinstance(self._data, PandasIndexAdapter):\n        self._data = PandasIndexAdapter(self._data)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.shape": "def shape(self) -> Tuple[int]:\n    return (len(self.array),)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.variable.py@@assert_unique_multiindex_level_names": "def assert_unique_multiindex_level_names(variables):\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append(f'{n!r} ({var_name})')\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n    for k, v in level_names.items():\n        if k in variables:\n            v.append('(%s)' % k)\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = '\\n'.join((', '.join(v) for v in duplicate_names))\n        raise ValueError('conflicting MultiIndex level name(s):\\n%s' % conflict_str)\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError('conflicting level / dimension names. {} already exists as a level name.'.format(d))",
    ".xarray.core.variable.py@@IndexVariable.level_names": "def level_names(self):\n    index = self.to_index()\n    if isinstance(index, pd.MultiIndex):\n        return index.names\n    else:\n        return None",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i) for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self):\n    return self.dims[0]",
    ".xarray.core.merge.py@@_extract_indexes_from_coords": "def _extract_indexes_from_coords(coords):\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield (name, variable.to_index())",
    ".xarray.core.dataarray.py@@DataArray.isel": "def isel(self, indexers: Mapping[Hashable, Any]=None, drop: bool=False, missing_dims: str='raise', **indexers_kwargs: Any) -> 'DataArray':\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    if any((is_fancy_indexer(idx) for idx in indexers.values())):\n        ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n        return self._from_temp_dataset(ds)\n    variable = self._variable.isel(indexers, missing_dims=missing_dims)\n    coords = {}\n    for coord_name, coord_value in self._coords.items():\n        coord_indexers = {k: v for k, v in indexers.items() if k in coord_value.dims}\n        if coord_indexers:\n            coord_value = coord_value.isel(coord_indexers)\n            if drop and coord_value.ndim == 0:\n                continue\n        coords[coord_name] = coord_value\n    return self._replace(variable=variable, coords=coords)",
    ".xarray.core.indexing.py@@is_fancy_indexer": "def is_fancy_indexer(indexer: Any) -> bool:\n    if isinstance(indexer, (int, slice)):\n        return False\n    if isinstance(indexer, np.ndarray):\n        return indexer.ndim > 1\n    if isinstance(indexer, list):\n        return bool(indexer) and (not isinstance(indexer[0], int))\n    return True",
    ".xarray.core.variable.py@@Variable.isel": "def isel(self: VariableType, indexers: Mapping[Hashable, Any]=None, missing_dims: str='raise', **indexers_kwargs: Any) -> VariableType:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n    key = tuple((indexers.get(dim, slice(None)) for dim in self.dims))\n    return self[key]",
    ".xarray.core.utils.py@@drop_dims_from_indexers": "def drop_dims_from_indexers(indexers: Mapping[Hashable, Any], dims: Union[list, Mapping[Hashable, int]], missing_dims: str) -> Mapping[Hashable, Any]:\n    if missing_dims == 'raise':\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            raise ValueError(f'dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return indexers\n    elif missing_dims == 'warn':\n        indexers = dict(indexers)\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            warnings.warn(f'dimensions {invalid} do not exist. Expected one or more of {dims}')\n        for key in invalid:\n            indexers.pop(key)\n        return indexers\n    elif missing_dims == 'ignore':\n        return {key: val for key, val in indexers.items() if key in dims}\n    else:\n        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')",
    ".xarray.core.variable.py@@Variable.__getitem__": "def __getitem__(self: VariableType, key) -> VariableType:\n    dims, indexer, new_order = self._broadcast_indexes(key)\n    data = as_indexable(self._data)[indexer]\n    if new_order:\n        data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n    return self._finalize_indexing_result(dims, data)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes": "def _broadcast_indexes(self, key):\n    key = self._item_key_to_tuple(key)\n    key = indexing.expanded_indexer(key, self.ndim)\n    key = tuple((k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key))\n    key = tuple((k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key))\n    if all((isinstance(k, BASIC_INDEXING_TYPES) for k in key)):\n        return self._broadcast_indexes_basic(key)\n    self._validate_indexers(key)\n    if all((not isinstance(k, Variable) for k in key)):\n        return self._broadcast_indexes_outer(key)\n    dims = []\n    for k, d in zip(key, self.dims):\n        if isinstance(k, Variable):\n            if len(k.dims) > 1:\n                return self._broadcast_indexes_vectorized(key)\n            dims.append(k.dims[0])\n        elif not isinstance(k, integer_types):\n            dims.append(d)\n    if len(set(dims)) == len(dims):\n        return self._broadcast_indexes_outer(key)\n    return self._broadcast_indexes_vectorized(key)",
    ".xarray.core.variable.py@@Variable._item_key_to_tuple": "def _item_key_to_tuple(self, key):\n    if utils.is_dict_like(key):\n        return tuple((key.get(dim, slice(None)) for dim in self.dims))\n    else:\n        return key",
    ".xarray.core.indexing.py@@expanded_indexer": "def expanded_indexer(key, ndim):\n    if not isinstance(key, tuple):\n        key = (key,)\n    new_key = []\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
    ".xarray.core.variable.py@@Variable._broadcast_indexes_basic": "def _broadcast_indexes_basic(self, key):\n    dims = tuple((dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)))\n    return (dims, BasicIndexer(key), None)",
    ".xarray.core.indexing.py@@BasicIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@as_integer_slice": "def as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)",
    ".xarray.core.indexing.py@@as_integer_or_none": "def as_integer_or_none(value):\n    return None if value is None else operator.index(value)",
    ".xarray.core.indexing.py@@ExplicitIndexer.__init__": "def __init__(self, key):\n    if type(self) is ExplicitIndexer:\n        raise TypeError('cannot instantiate base ExplicitIndexer objects')\n    self._key = tuple(key)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, '__array_function__'):\n        return NdArrayLikeIndexingAdapter(array)\n    raise TypeError('Invalid array type: {}'.format(type(array)))",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__init__": "def __init__(self, array):\n    self.array = array",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    if not isinstance(key, VectorizedIndexer):\n        rewritten_indexer = False\n        new_indexer = []\n        for idim, k in enumerate(key.tuple):\n            if isinstance(k, Iterable) and duck_array_ops.array_equiv(k, np.arange(self.array.shape[idim])):\n                new_indexer.append(slice(None))\n                rewritten_indexer = True\n            else:\n                new_indexer.append(k)\n        if rewritten_indexer:\n            key = type(key)(tuple(new_indexer))\n    if isinstance(key, BasicIndexer):\n        return self.array[key.tuple]\n    elif isinstance(key, VectorizedIndexer):\n        return self.array.vindex[key.tuple]\n    else:\n        assert isinstance(key, OuterIndexer)\n        key = key.tuple\n        try:\n            return self.array[key]\n        except NotImplementedError:\n            value = self.array\n            for axis, subkey in reversed(list(enumerate(key))):\n                value = value[(slice(None),) * axis + (subkey,)]\n            return value",
    ".xarray.core.indexing.py@@ExplicitIndexer.tuple": "def tuple(self):\n    return self._key",
    ".xarray.core.variable.py@@Variable._finalize_indexing_result": "def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__getitem__": "def __getitem__(self, indexer) -> Union[NumpyIndexingAdapter, np.ndarray, np.datetime64, np.timedelta64]:\n    key = indexer.tuple\n    if isinstance(key, tuple) and len(key) == 1:\n        key, = key\n    if getattr(key, 'ndim', 0) > 1:\n        return NumpyIndexingAdapter(self.array.values)[indexer]\n    result = self.array[key]\n    if isinstance(result, pd.Index):\n        result = PandasIndexAdapter(result, dtype=self.dtype)\n    else:\n        if result is pd.NaT:\n            result = np.datetime64('NaT', 'ns')\n        elif isinstance(result, timedelta):\n            result = np.timedelta64(getattr(result, 'value', result), 'ns')\n        elif isinstance(result, pd.Timestamp):\n            result = np.asarray(result.to_datetime64())\n        elif self.dtype != object:\n            result = np.asarray(result, dtype=self.dtype)\n        result = utils.to_0d_array(result)\n    return result",
    ".xarray.core.indexing.py@@PandasIndexAdapter.dtype": "def dtype(self) -> np.dtype:\n    return self._dtype",
    ".xarray.core.variable.py@@IndexVariable._finalize_indexing_result": "def _finalize_indexing_result(self, dims, data):\n    if getattr(data, 'ndim', 0) != 1:\n        return Variable(dims, data, self._attrs, self._encoding)\n    else:\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.common.py@@ImplementsArrayReduce.wrapped_func": "def wrapped_func(self, dim=None, axis=None, **kwargs):\n    return self.reduce(func, dim, axis, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reduce": "def reduce(self, func: Callable[..., Any], dim: Union[None, Hashable, Sequence[Hashable]]=None, axis: Union[None, int, Sequence[int]]=None, keep_attrs: bool=None, keepdims: bool=False, **kwargs: Any) -> 'DataArray':\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n    return self._replace_maybe_drop_dims(var)",
    ".xarray.core.variable.py@@Variable.reduce": "def reduce(self, func, dim=None, axis=None, keep_attrs=None, keepdims=False, **kwargs):\n    if dim == ...:\n        dim = None\n    if dim is not None and axis is not None:\n        raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n    if dim is not None:\n        axis = self.get_axis_num(dim)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        if axis is not None:\n            data = func(self.data, axis=axis, **kwargs)\n        else:\n            data = func(self.data, **kwargs)\n    if getattr(data, 'shape', ()) == self.shape:\n        dims = self.dims\n    else:\n        removed_axes = range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n        if keepdims:\n            slices = tuple((np.newaxis if i in removed_axes else slice(None, None) for i in range(self.ndim)))\n            if getattr(data, 'shape', None) is None:\n                data = np.asanyarray(data)[slices]\n            else:\n                data = data[slices]\n            dims = self.dims\n        else:\n            dims = [adim for n, adim in enumerate(self.dims) if n not in removed_axes]\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    return Variable(dims, data, attrs=attrs)",
    ".xarray.core.duck_array_ops.py@@_dask_or_eager_func": "def _dask_or_eager_func(name, eager_module=np, dask_module=dask_array, list_of_args=False, array_args=slice(1), requires_dask=None):\n    if dask_module is not None:\n\n        def f(*args, **kwargs):\n            if list_of_args:\n                dispatch_args = args[0]\n            else:\n                dispatch_args = args[array_args]\n            if any((is_duck_dask_array(a) for a in dispatch_args)):\n                try:\n                    wrapped = getattr(dask_module, name)\n                except AttributeError as e:\n                    raise AttributeError(f'{e}: requires dask >={requires_dask}')\n            else:\n                wrapped = getattr(eager_module, name)\n            return wrapped(*args, **kwargs)\n    else:\n\n        def f(*args, **kwargs):\n            return getattr(eager_module, name)(*args, **kwargs)\n    return f",
    ".xarray.core.variable.py@@Variable.func": "def func(self, other):\n    if isinstance(other, xr.Dataset):\n        raise TypeError('cannot add a Dataset to a Variable in-place')\n    self_data, other_data, dims = _broadcast_compat_data(self, other)\n    if dims != self.dims:\n        raise ValueError('dimensions cannot change for in-place operations')\n    with np.errstate(all='ignore'):\n        self.values = f(self_data, other_data)\n    return self",
    ".xarray.core.variable.py@@_broadcast_compat_data": "def _broadcast_compat_data(self, other):\n    if all((hasattr(other, attr) for attr in ['dims', 'data', 'shape', 'encoding'])):\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return (self_data, other_data, dims)",
    ".xarray.core.coordinates.py@@Coordinates._merge_raw": "def _merge_raw(self, other):\n    if other is None:\n        variables = dict(self.variables)\n        indexes = dict(self.indexes)\n    else:\n        variables, indexes = merge_coordinates_without_align([self, other])\n    return (variables, indexes)",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@Coordinates.indexes": "def indexes(self) -> Indexes:\n    return self._data.indexes",
    ".xarray.core.dataarray.py@@DataArray.indexes": "def indexes(self) -> Indexes:\n    if self._indexes is None:\n        self._indexes = default_indexes(self._coords, self.dims)\n    return Indexes(self._indexes)",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, Variable], dims: Iterable) -> Dict[Hashable, pd.Index]:\n    return {key: coords[key].to_index() for key in dims if key in coords}",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes):\n    self._indexes = indexes",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self):\n    return iter(self._indexes)",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key):\n    return self._indexes[key]",
    ".xarray.core.dataarray.py@@DataArray._result_name": "def _result_name(self, other: Any=None) -> Optional[Hashable]:\n    other_name = getattr(other, 'name', _default)\n    if other_name is _default or other_name == self.name:\n        return self.name\n    else:\n        return None",
    ".xarray.core.nputils.py@@rolling_window": "def rolling_window(a, axis, window, center, fill_value):\n    pads = [(0, 0) for s in a.shape]\n    if not hasattr(axis, '__len__'):\n        axis = [axis]\n        window = [window]\n        center = [center]\n    for ax, win, cent in zip(axis, window, center):\n        if cent:\n            start = int(win / 2)\n            end = win - 1 - start\n            pads[ax] = (start, end)\n        else:\n            pads[ax] = (win - 1, 0)\n    a = np.pad(a, pads, mode='constant', constant_values=fill_value)\n    for ax, win in zip(axis, window):\n        a = _rolling_window(a, win, ax)\n    return a",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    array, key = self._indexing_array_and_key(key)\n    return array[key]",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter._indexing_array_and_key": "def _indexing_array_and_key(self, key):\n    if isinstance(key, OuterIndexer):\n        array = self.array\n        key = _outer_to_numpy_indexer(key, self.array.shape)\n    elif isinstance(key, VectorizedIndexer):\n        array = nputils.NumpyVIndexAdapter(self.array)\n        key = key.tuple\n    elif isinstance(key, BasicIndexer):\n        array = self.array\n        key = key.tuple + (Ellipsis,)\n    else:\n        raise TypeError('unexpected key type: {}'.format(type(key)))\n    return (array, key)",
    ".xarray.core.dataarray.py@@DataArray.shape": "def shape(self) -> Tuple[int, ...]:\n    return self.variable.shape",
    ".xarray.core.dataarray.py@@DataArray._level_coords": "def _level_coords(self) -> Dict[Hashable, Hashable]:\n    level_coords: Dict[Hashable, Hashable] = {}\n    for cname, var in self._coords.items():\n        if var.ndim == 1 and isinstance(var, IndexVariable):\n            level_names = var.level_names\n            if level_names is not None:\n                dim, = var.dims\n                level_coords.update({lname: dim for lname in level_names})\n    return level_coords",
    ".xarray.core.dataset.py@@_get_virtual_variable": "def _get_virtual_variable(variables, key: Hashable, level_vars: Mapping=None, dim_sizes: Mapping=None) -> Tuple[Hashable, Hashable, Variable]:\n    if level_vars is None:\n        level_vars = {}\n    if dim_sizes is None:\n        dim_sizes = {}\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return (key, key, variable)\n    if not isinstance(key, str):\n        raise KeyError(key)\n    split_key = key.split('.', 1)\n    var_name: Optional[str]\n    if len(split_key) == 2:\n        ref_name, var_name = split_key\n    elif len(split_key) == 1:\n        ref_name, var_name = (key, None)\n    else:\n        raise KeyError(key)\n    if ref_name in level_vars:\n        dim_var = variables[level_vars[ref_name]]\n        ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n    else:\n        ref_var = variables[ref_name]\n    if var_name is None:\n        virtual_var = ref_var\n        var_name = key\n    else:\n        if _contains_datetime_like_objects(ref_var):\n            ref_var = xr.DataArray(ref_var)\n            data = getattr(ref_var.dt, var_name).data\n        else:\n            data = getattr(ref_var, var_name).data\n        virtual_var = Variable(ref_var.dims, data)\n    return (ref_name, var_name, virtual_var)",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.rolling.py@@DatasetRolling._counts": "def _counts(self, keep_attrs):\n    return self._dataset_implementation(DataArrayRolling._counts, keep_attrs=keep_attrs)",
    ".xarray.core.rolling.py@@DatasetRolling._dataset_implementation": "def _dataset_implementation(self, func, keep_attrs, **kwargs):\n    from .dataset import Dataset\n    keep_attrs = self._get_keep_attrs(keep_attrs)\n    reduced = {}\n    for key, da in self.obj.data_vars.items():\n        if any((d in da.dims for d in self.dim)):\n            reduced[key] = func(self.rollings[key], keep_attrs=keep_attrs, **kwargs)\n        else:\n            reduced[key] = self.obj[key].copy()\n            if not keep_attrs:\n                reduced[key].attrs = {}\n    attrs = self.obj.attrs if keep_attrs else {}\n    return Dataset(reduced, coords=self.obj.coords, attrs=attrs)",
    ".xarray.core.dataset.py@@Dataset.data_vars": "def data_vars(self) -> DataVariables:\n    return DataVariables(self)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core.dataset.py@@DataVariables.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    if key not in self._dataset._coord_names:\n        return cast('DataArray', self._dataset[key])\n    raise KeyError(key)",
    ".xarray.core.dataset.py@@Dataset.__getitem__": "def __getitem__(self, key: Mapping) -> 'Dataset':\n    ...",
    ".xarray.core.utils.py@@hashable": "def hashable(v: Any) -> bool:\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True",
    ".xarray.core.dataset.py@@Dataset._construct_dataarray": "def _construct_dataarray(self, name: Hashable) -> 'DataArray':\n    from .dataarray import DataArray\n    try:\n        variable = self._variables[name]\n    except KeyError:\n        _, name, variable = _get_virtual_variable(self._variables, name, self._level_coords, self.dims)\n    needed_dims = set(variable.dims)\n    coords: Dict[Hashable, Variable] = {}\n    for k in self.coords:\n        if set(self.variables[k].dims) <= needed_dims:\n            coords[k] = self.variables[k]\n    if self._indexes is None:\n        indexes = None\n    else:\n        indexes = {k: v for k, v in self._indexes.items() if k in coords}\n    return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.dataset.py@@Dataset.coords": "def coords(self) -> DatasetCoordinates:\n    return DatasetCoordinates(self)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._data = dataset",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen({k: v for k, v in self._data.variables.items() if k in self._names})",
    ".xarray.core.dataset.py@@Dataset.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self) -> Set[Hashable]:\n    return self._data._coord_names",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.dataset.py@@Dataset.__init__": "def __init__(self, data_vars: Mapping[Hashable, Any]=None, coords: Mapping[Hashable, Any]=None, attrs: Mapping[Hashable, Any]=None):\n    if data_vars is None:\n        data_vars = {}\n    if coords is None:\n        coords = {}\n    both_data_and_coords = set(data_vars) & set(coords)\n    if both_data_and_coords:\n        raise ValueError('variables %r are found in both data_vars and coords' % both_data_and_coords)\n    if isinstance(coords, Dataset):\n        coords = coords.variables\n    variables, coord_names, dims, indexes, _ = merge_data_and_coords(data_vars, coords, compat='broadcast_equals')\n    self._attrs = dict(attrs) if attrs is not None else None\n    self._file_obj = None\n    self._encoding = None\n    self._variables = variables\n    self._coord_names = coord_names\n    self._dims = dims\n    self._indexes = indexes",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data, coords, compat='broadcast_equals', join='outer'):\n    objects = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(_extract_indexes_from_coords(coords))\n    return merge_core(objects, compat, join, explicit_coords=explicit_coords, indexes=indexes)",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objects: Iterable['CoercibleMapping'], compat: str='broadcast_equals', join: str='outer', combine_attrs: Optional[str]='override', priority_arg: Optional[int]=None, explicit_coords: Optional[Sequence]=None, indexes: Optional[Mapping[Hashable, pd.Index]]=None, fill_value: object=dtypes.NA) -> _MergeResult:\n    from .dataarray import DataArray\n    from .dataset import Dataset, calculate_dimensions\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    dims = calculate_dimensions(variables)\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError('unable to determine if these variables should be coordinates or not in the merged result: %s' % ambiguous_coords)\n    attrs = merge_attrs([var.attrs for var in coerced if isinstance(var, Dataset) or isinstance(var, DataArray)], combine_attrs)\n    return _MergeResult(variables, coord_names, dims, out_indexes, attrs)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError('compat={!r} invalid: must be {}'.format(compat, set(_VALID_COMPAT)))",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects: Iterable['CoercibleMapping']) -> List['DatasetLike']:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables: 'DatasetLike' = obj\n        else:\n            variables = {}\n            if isinstance(obj, PANDAS_TYPES):\n                obj = dict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.alignment.py@@align": "def align(*objects: 'DataAlignable', join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA) -> Tuple['DataAlignable', ...]:\n    if indexes is None:\n        indexes = {}\n    if not indexes and len(objects) == 1:\n        obj, = objects\n        return (obj.copy(deep=copy),)\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n    if join == 'override':\n        objects = _override_indexes(objects, all_indexes, exclude)\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if any((not index.equals(other) for other in matching_indexes)) or dim in unlabeled_dim_sizes:\n                joined_indexes[dim] = index\n        elif any((not matching_indexes[0].equals(other) for other in matching_indexes[1:])) or dim in unlabeled_dim_sizes:\n            if join == 'exact':\n                raise ValueError(f'indexes along dimension {dim!r} are not equal')\n            index = joiner(matching_indexes)\n            joined_indexes[dim] = index\n        else:\n            index = matching_indexes[0]\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension size(s) %r than the size of the aligned dimension labels: %r' % (dim, unlabeled_sizes, labeled_size))\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension sizes: %r' % (dim, sizes))\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n        if not valid_indexers:\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, indexers=valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n    return tuple(result)",
    ".xarray.core.dataarray.py@@DataArray.copy": "def copy(self, deep: bool=True, data: Any=None) -> 'DataArray':\n    variable = self.variable.copy(deep=deep, data=data)\n    coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n    if self._indexes is None:\n        indexes = self._indexes\n    else:\n        indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n    return self._replace(variable, coords, indexes=indexes)",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            data = copy.deepcopy(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return self._replace(data=data)",
    ".xarray.core.variable.py@@Variable._replace": "def _replace(self, dims=_default, data=_default, attrs=_default, encoding=_default) -> 'Variable':\n    if dims is _default:\n        dims = copy.copy(self._dims)\n    if data is _default:\n        data = copy.copy(self.data)\n    if attrs is _default:\n        attrs = copy.copy(self._attrs)\n    if encoding is _default:\n        encoding = copy.copy(self._encoding)\n    return type(self)(dims, data, attrs, encoding, fastpath=True)",
    ".xarray.core.merge.py@@collect_variables_and_indexes": "def collect_variables_and_indexes(list_of_mappings: 'List[DatasetLike]') -> Dict[Hashable, List[MergeElement]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n\n    def append(name, variable, index):\n        values = grouped.setdefault(name, [])\n        values.append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping.indexes)\n            continue\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()\n                indexes = dict(variable.indexes)\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n            variable = as_variable(variable, name=name)\n            if variable.dims == (name,):\n                variable = variable.to_index_variable()\n                index = variable.to_index()\n            else:\n                index = None\n            append(name, variable, index)\n    return grouped",
    ".xarray.core.merge.py@@append_all": "def append_all(variables, indexes):\n    for name, variable in variables.items():\n        append(name, variable, indexes.get(name))",
    ".xarray.core.merge.py@@append": "def append(name, variable, index):\n    values = grouped.setdefault(name, [])\n    values.append((variable, index))",
    ".xarray.core.merge.py@@_get_priority_vars_and_indexes": "def _get_priority_vars_and_indexes(objects: List['DatasetLike'], priority_arg: Optional[int], compat: str='equals') -> Dict[Hashable, MergeElement]:\n    if priority_arg is None:\n        return {}\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped: Dict[Hashable, MergeElement] = {}\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: Dict[Hashable, List[MergeElement]], prioritized: Mapping[Hashable, MergeElement]=None, compat: str='minimal') -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n    if prioritized is None:\n        prioritized = {}\n    _assert_compat_valid(compat)\n    merged_vars: Dict[Hashable, Variable] = {}\n    merged_indexes: Dict[Hashable, pd.Index] = {}\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for _, other_index in indexed_elements[1:]:\n                    if not index.equals(other_index):\n                        raise MergeError('conflicting values for index %r on objects to be combined:\\nfirst value: %r\\nsecond value: %r' % (name, index, other_index))\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError('conflicting attribute values on combined variable %r:\\nfirst value: %r\\nsecond value: %r' % (name, variable.attrs, other_variable.attrs))\n                merged_vars[name] = variable\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n    return (merged_vars, merged_indexes)",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name: Hashable, variables: List[Variable], compat: str='broadcast_equals', equals: bool=None) -> Variable:\n    out = variables[0]\n    if len(variables) == 1 or compat == 'override':\n        return out\n    combine_method = None\n    if compat == 'minimal':\n        compat = 'broadcast_equals'\n    if compat == 'broadcast_equals':\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n    if compat == 'no_conflicts':\n        combine_method = 'fillna'\n    if equals is None:\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n            if equals is not True:\n                break\n        if equals is None:\n            out = out.compute()\n            for var in variables[1:]:\n                equals = getattr(out, compat)(var)\n                if not equals:\n                    break\n    if not equals:\n        raise MergeError(f\"conflicting values for variable {name!r} on objects to be combined. You can skip this check by specifying compat='override'.\")\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n    return out",
    ".xarray.core.dataset.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:\n    dims: Dict[Hashable, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError('dimension %r already exists as a scalar variable' % dim)\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError('conflicting sizes for dimension %r: length %s on %r and length %s on %r' % (dim, size, k, dims[dim], last_used[dim]))\n    return dims",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_mappings: Iterable['DatasetLike']) -> Tuple[Set[Hashable], Set[Hashable]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names: Set[Hashable] = set()\n    noncoord_names: Set[Hashable] = set()\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError('coordinate %s shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.' % coord_name)",
    ".xarray.core.merge.py@@merge_attrs": "def merge_attrs(variable_attrs, combine_attrs):\n    if not variable_attrs:\n        return None\n    if combine_attrs == 'drop':\n        return {}\n    elif combine_attrs == 'override':\n        return variable_attrs[0]\n    elif combine_attrs == 'no_conflicts':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            try:\n                result = compat_dict_union(result, attrs)\n            except ValueError:\n                raise MergeError(\"combine_attrs='no_conflicts', but some values are not the same. Merging %s with %s\" % (str(result), str(attrs)))\n        return result\n    elif combine_attrs == 'identical':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            if not dict_equiv(result, attrs):\n                raise MergeError(\"combine_attrs='identical', but attrs differ. First is %s , other is %s.\" % (str(result), str(attrs)))\n        return result\n    else:\n        raise ValueError('Unrecognised value for combine_attrs=%s' % combine_attrs)",
    ".xarray.core.dataset.py@@Dataset.func": "def func(self, other):\n    from .dataarray import DataArray\n    if isinstance(other, groupby.GroupBy):\n        raise TypeError('in-place operations between a Dataset and a grouped object are not permitted')\n    if isinstance(other, (DataArray, Dataset)):\n        other = other.reindex_like(self, copy=False)\n    g = ops.inplace_to_noninplace_op(f)\n    ds = self._calculate_binary_op(g, other, inplace=True)\n    self._replace_with_new_dims(ds._variables, ds._coord_names, attrs=ds._attrs, indexes=ds._indexes, inplace=True)\n    return self",
    ".xarray.core.dataset.py@@Dataset._calculate_binary_op": "def _calculate_binary_op(self, f, other, join='inner', inplace=False):\n\n    def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n        if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n            raise ValueError('datasets must have the same data variables for in-place arithmetic operations: %s, %s' % (list(lhs_data_vars), list(rhs_data_vars)))\n        dest_vars = {}\n        for k in lhs_data_vars:\n            if k in rhs_data_vars:\n                dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n            elif join in ['left', 'outer']:\n                dest_vars[k] = f(lhs_vars[k], np.nan)\n        for k in rhs_data_vars:\n            if k not in dest_vars and join in ['right', 'outer']:\n                dest_vars[k] = f(rhs_vars[k], np.nan)\n        return dest_vars\n    if utils.is_dict_like(other) and (not isinstance(other, Dataset)):\n        new_data_vars = apply_over_both(self.data_vars, other, self.data_vars, other)\n        return Dataset(new_data_vars)\n    other_coords = getattr(other, 'coords', None)\n    ds = self.coords.merge(other_coords)\n    if isinstance(other, Dataset):\n        new_vars = apply_over_both(self.data_vars, other.data_vars, self.variables, other.variables)\n    else:\n        other_variable = getattr(other, 'variable', other)\n        new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n    ds._variables.update(new_vars)\n    ds._dims = calculate_dimensions(ds._variables)\n    return ds",
    ".xarray.core.coordinates.py@@Coordinates.merge": "def merge(self, other: 'Coordinates') -> 'Dataset':\n    from .dataset import Dataset\n    if other is None:\n        return self.to_dataset()\n    if not isinstance(other, Coordinates):\n        other = Dataset(coords=other).coords\n    coords, indexes = merge_coordinates_without_align([self, other])\n    coord_names = set(coords)\n    merged = Dataset._construct_direct(variables=coords, coord_names=coord_names, indexes=indexes)\n    return merged",
    ".xarray.core.coordinates.py@@DatasetCoordinates.to_dataset": "def to_dataset(self) -> 'Dataset':\n    names = [name for name in self._data._variables if name in self._names]\n    return self._data._copy_listed(names)",
    ".xarray.core.dataset.py@@Dataset._copy_listed": "def _copy_listed(self, names: Iterable[Hashable]) -> 'Dataset':\n    variables: Dict[Hashable, Variable] = {}\n    coord_names = set()\n    indexes: Dict[Hashable, pd.Index] = {}\n    for name in names:\n        try:\n            variables[name] = self._variables[name]\n        except KeyError:\n            ref_name, var_name, var = _get_virtual_variable(self._variables, name, self._level_coords, self.dims)\n            variables[var_name] = var\n            if ref_name in self._coord_names or ref_name in self.dims:\n                coord_names.add(var_name)\n            if (var_name,) == var.dims:\n                indexes[var_name] = var.to_index()\n    needed_dims: Set[Hashable] = set()\n    for v in variables.values():\n        needed_dims.update(v.dims)\n    dims = {k: self.dims[k] for k in needed_dims}\n    for k in self._variables:\n        if k not in self._coord_names:\n            continue\n        if set(self.variables[k].dims) <= needed_dims:\n            variables[k] = self._variables[k]\n            coord_names.add(k)\n            if k in self.indexes:\n                indexes[k] = self.indexes[k]\n    return self._replace(variables, coord_names, dims, indexes=indexes)",
    ".xarray.core.dataset.py@@Dataset._replace": "def _replace(self, variables: Dict[Hashable, Variable]=None, coord_names: Set[Hashable]=None, dims: Dict[Any, int]=None, attrs: Union[Dict[Hashable, Any], None, Default]=_default, indexes: Union[Dict[Any, pd.Index], None, Default]=_default, encoding: Union[dict, None, Default]=_default, inplace: bool=False) -> 'Dataset':\n    if inplace:\n        if variables is not None:\n            self._variables = variables\n        if coord_names is not None:\n            self._coord_names = coord_names\n        if dims is not None:\n            self._dims = dims\n        if attrs is not _default:\n            self._attrs = attrs\n        if indexes is not _default:\n            self._indexes = indexes\n        if encoding is not _default:\n            self._encoding = encoding\n        obj = self\n    else:\n        if variables is None:\n            variables = self._variables.copy()\n        if coord_names is None:\n            coord_names = self._coord_names.copy()\n        if dims is None:\n            dims = self._dims.copy()\n        if attrs is _default:\n            attrs = copy.copy(self._attrs)\n        if indexes is _default:\n            indexes = copy.copy(self._indexes)\n        if encoding is _default:\n            encoding = copy.copy(self._encoding)\n        obj = self._construct_direct(variables, coord_names, dims, attrs, indexes, encoding)\n    return obj",
    ".xarray.core.dataset.py@@Dataset._construct_direct": "def _construct_direct(cls, variables, coord_names, dims=None, attrs=None, indexes=None, encoding=None, file_obj=None):\n    if dims is None:\n        dims = calculate_dimensions(variables)\n    obj = object.__new__(cls)\n    obj._variables = variables\n    obj._coord_names = coord_names\n    obj._dims = dims\n    obj._indexes = indexes\n    obj._attrs = attrs\n    obj._file_obj = file_obj\n    obj._encoding = encoding\n    return obj"
}