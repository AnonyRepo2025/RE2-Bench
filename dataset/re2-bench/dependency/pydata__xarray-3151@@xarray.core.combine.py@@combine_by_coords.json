{
    ".xarray.core.combine.py@@vars_as_keys": "def vars_as_keys(ds):\n    return tuple(sorted(ds))",
    ".xarray.core.dataset.py@@Dataset.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return iter(self.data_vars)",
    ".xarray.core.dataset.py@@Dataset.data_vars": "def data_vars(self) -> DataVariables:\n    return DataVariables(self)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: 'Dataset'):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core.dataset.py@@Dataset.__len__": "def __len__(self) -> int:\n    return len(self.data_vars)",
    ".xarray.core.dataset.py@@DataVariables.__len__": "def __len__(self) -> int:\n    return len(self._dataset._variables) - len(self._dataset._coord_names)",
    ".xarray.core.combine.py@@_infer_concat_order_from_coords": "def _infer_concat_order_from_coords(datasets):\n    concat_dims = []\n    tile_ids = [() for ds in datasets]\n    ds0 = datasets[0]\n    for dim in ds0.dims:\n        if dim in ds0:\n            indexes = [ds.indexes.get(dim) for ds in datasets]\n            if any((index is None for index in indexes)):\n                raise ValueError('Every dimension needs a coordinate for inferring concatenation order')\n            if not all((index.equals(indexes[0]) for index in indexes[1:])):\n                concat_dims.append(dim)\n                if all((index.is_monotonic_increasing for index in indexes)):\n                    ascending = True\n                elif all((index.is_monotonic_decreasing for index in indexes)):\n                    ascending = False\n                else:\n                    raise ValueError('Coordinate variable {} is neither monotonically increasing nor monotonically decreasing on all datasets'.format(dim))\n                if any((index.size == 0 for index in indexes)):\n                    raise ValueError('Cannot handle size zero dimensions')\n                first_items = pd.Index([index.take([0]) for index in indexes])\n                series = first_items.to_series()\n                rank = series.rank(method='dense', ascending=ascending)\n                order = rank.astype(int).values - 1\n                tile_ids = [tile_id + (position,) for tile_id, position in zip(tile_ids, order)]\n    if len(datasets) > 1 and (not concat_dims):\n        raise ValueError('Could not find any dimension coordinates to use to order the datasets for concatenation')\n    combined_ids = OrderedDict(zip(tile_ids, datasets))\n    return (combined_ids, concat_dims)",
    ".xarray.core.dataset.py@@Dataset.dims": "def dims(self) -> Mapping[Hashable, int]:\n    return Frozen(SortedKeysDict(self._dims))",
    ".xarray.core.utils.py@@SortedKeysDict.__init__": "def __init__(self, mapping: Optional[MutableMapping[K, V]]=None):\n    self.mapping = {} if mapping is None else mapping",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.utils.py@@SortedKeysDict.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(sorted(self.mapping))",
    ".xarray.core.dataset.py@@Dataset.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self._variables",
    ".xarray.core.dataset.py@@Dataset.indexes": "def indexes(self) -> 'Mapping[Any, pd.Index]':\n    if self._indexes is None:\n        self._indexes = default_indexes(self._variables, self._dims)\n    return Indexes(self._indexes)",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, Variable], dims: Iterable) -> 'OrderedDict[Any, pd.Index]':\n    return OrderedDict(((key, coords[key].to_index()) for key in dims if key in coords))",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i) for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.indexing.py@@PandasIndexAdapter.shape": "def shape(self):\n    return (len(self.array),)",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self):\n    return self.dims[0]",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    if self._initialized:\n        try:\n            self.__getattribute__(name)\n        except AttributeError:\n            raise AttributeError(\"cannot set attribute %r on a %r object. Use __setitem__ style assignment (e.g., `ds['name'] = ...`) instead to assign variables.\" % (name, type(self).__name__))\n    object.__setattr__(self, name, value)",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes):\n    self._indexes = indexes",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key):\n    return self._indexes[key]",
    ".xarray.core.combine.py@@_check_shape_tile_ids": "def _check_shape_tile_ids(combined_tile_ids):\n    tile_ids = combined_tile_ids.keys()\n    nesting_depths = [len(tile_id) for tile_id in tile_ids]\n    if not nesting_depths:\n        nesting_depths = [0]\n    if not set(nesting_depths) == {nesting_depths[0]}:\n        raise ValueError('The supplied objects do not form a hypercube because sub-lists do not have consistent depths')\n    for dim in range(nesting_depths[0]):\n        indices_along_dim = [tile_id[dim] for tile_id in tile_ids]\n        occurrences = Counter(indices_along_dim)\n        if len(set(occurrences.values())) != 1:\n            raise ValueError('The supplied objects do not form a hypercube because sub-lists do not have consistent lengths along dimension' + str(dim))",
    ".xarray.core.combine.py@@_combine_nd": "def _combine_nd(combined_ids, concat_dims, data_vars='all', coords='different', compat='no_conflicts', fill_value=dtypes.NA):\n    example_tile_id = next(iter(combined_ids.keys()))\n    n_dims = len(example_tile_id)\n    if len(concat_dims) != n_dims:\n        raise ValueError('concat_dims has length {} but the datasets passed are nested in a {}-dimensional structure'.format(len(concat_dims), n_dims))\n    for concat_dim in concat_dims:\n        combined_ids = _combine_all_along_first_dim(combined_ids, dim=concat_dim, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value)\n    combined_ds, = combined_ids.values()\n    return combined_ds",
    ".xarray.core.combine.py@@_combine_all_along_first_dim": "def _combine_all_along_first_dim(combined_ids, dim, data_vars, coords, compat, fill_value=dtypes.NA):\n    combined_ids = OrderedDict(sorted(combined_ids.items(), key=_new_tile_id))\n    grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)\n    new_combined_ids = {}\n    for new_id, group in grouped:\n        combined_ids = OrderedDict(sorted(group))\n        datasets = combined_ids.values()\n        new_combined_ids[new_id] = _combine_1d(datasets, dim, compat, data_vars, coords, fill_value)\n    return new_combined_ids",
    ".xarray.core.combine.py@@_new_tile_id": "def _new_tile_id(single_id_ds_pair):\n    tile_id, ds = single_id_ds_pair\n    return tile_id[1:]",
    ".xarray.core.combine.py@@_combine_1d": "def _combine_1d(datasets, concat_dim, compat='no_conflicts', data_vars='all', coords='different', fill_value=dtypes.NA):\n    if concat_dim is not None:\n        try:\n            combined = concat(datasets, dim=concat_dim, data_vars=data_vars, coords=coords, fill_value=fill_value)\n        except ValueError as err:\n            if 'encountered unexpected variable' in str(err):\n                raise ValueError('These objects cannot be combined using only xarray.combine_nested, instead either use xarray.combine_by_coords, or do it manually with xarray.concat, xarray.merge and xarray.align')\n            else:\n                raise\n    else:\n        combined = merge(datasets, compat=compat, fill_value=fill_value)\n    return combined",
    ".xarray.core.concat.py@@concat": "def concat(objs, dim=None, data_vars='all', coords='different', compat='equals', positions=None, indexers=None, mode=None, concat_over=None, fill_value=dtypes.NA):\n    from .dataset import Dataset\n    from .dataarray import DataArray\n    try:\n        first_obj, objs = utils.peek_at(objs)\n    except StopIteration:\n        raise ValueError('must supply at least one object to concatenate')\n    if dim is None:\n        warnings.warn(\"the `dim` argument to `concat` will be required in a future version of xarray; for now, setting it to the old default of 'concat_dim'\", FutureWarning, stacklevel=2)\n        dim = 'concat_dims'\n    if indexers is not None:\n        warnings.warn('indexers has been renamed to positions; the alias will be removed in a future version of xarray', FutureWarning, stacklevel=2)\n        positions = indexers\n    if mode is not None:\n        raise ValueError('`mode` is no longer a valid argument to xarray.concat; it has been split into the `data_vars` and `coords` arguments')\n    if concat_over is not None:\n        raise ValueError('`concat_over` is no longer a valid argument to xarray.concat; it has been split into the `data_vars` and `coords` arguments')\n    if isinstance(first_obj, DataArray):\n        f = _dataarray_concat\n    elif isinstance(first_obj, Dataset):\n        f = _dataset_concat\n    else:\n        raise TypeError('can only concatenate xarray Dataset and DataArray objects, got %s' % type(first_obj))\n    return f(objs, dim, data_vars, coords, compat, positions, fill_value)",
    ".xarray.core.utils.py@@peek_at": "def peek_at(iterable: Iterable[T]) -> Tuple[T, Iterator[T]]:\n    gen = iter(iterable)\n    peek = next(gen)\n    return (peek, itertools.chain([peek], gen))",
    ".xarray.core.concat.py@@_dataset_concat": "def _dataset_concat(datasets, dim, data_vars, coords, compat, positions, fill_value=dtypes.NA):\n    from .dataset import Dataset\n    if compat not in ['equals', 'identical']:\n        raise ValueError(\"compat=%r invalid: must be 'equals' or 'identical'\" % compat)\n    dim, coord = _calc_concat_dim_coord(dim)\n    datasets = [ds.copy() for ds in datasets]\n    datasets = align(*datasets, join='outer', copy=False, exclude=[dim], fill_value=fill_value)\n    concat_over, equals = _calc_concat_over(datasets, dim, data_vars, coords)\n\n    def insert_result_variable(k, v):\n        assert isinstance(v, Variable)\n        if k in datasets[0].coords:\n            result_coord_names.add(k)\n        result_vars[k] = v\n    result_vars = OrderedDict()\n    result_coord_names = set(datasets[0].coords)\n    result_attrs = datasets[0].attrs\n    result_encoding = datasets[0].encoding\n    for k, v in datasets[0].variables.items():\n        if k not in concat_over:\n            insert_result_variable(k, v)\n    for ds in datasets[1:]:\n        if compat == 'identical' and (not utils.dict_equiv(ds.attrs, result_attrs)):\n            raise ValueError('dataset global attributes not equal')\n        for k, v in ds.variables.items():\n            if k not in result_vars and k not in concat_over:\n                raise ValueError('encountered unexpected variable %r' % k)\n            elif (k in result_coord_names) != (k in ds.coords):\n                raise ValueError('%r is a coordinate in some datasets but not others' % k)\n            elif k in result_vars and k != dim:\n                if compat == 'identical' and (not utils.dict_equiv(v.attrs, result_vars[k].attrs)):\n                    raise ValueError('variable %s not identical across datasets' % k)\n                try:\n                    is_equal = equals[k]\n                except KeyError:\n                    result_vars[k].load()\n                    is_equal = v.equals(result_vars[k])\n                if not is_equal:\n                    raise ValueError('variable %s not equal across datasets' % k)\n    dim_lengths = [ds.dims.get(dim, 1) for ds in datasets]\n    non_concat_dims = {}\n    for ds in datasets:\n        non_concat_dims.update(ds.dims)\n    non_concat_dims.pop(dim, None)\n\n    def ensure_common_dims(vars):\n        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n        if dim not in common_dims:\n            common_dims = (dim,) + common_dims\n        for var, dim_len in zip(vars, dim_lengths):\n            if var.dims != common_dims:\n                common_shape = tuple((non_concat_dims.get(d, dim_len) for d in common_dims))\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            combined = concat_vars(vars, dim, positions)\n            insert_result_variable(k, combined)\n    result = Dataset(result_vars, attrs=result_attrs)\n    result = result.set_coords(result_coord_names)\n    result.encoding = result_encoding\n    if coord is not None:\n        result[coord.name] = coord\n    return result",
    ".xarray.core.concat.py@@_calc_concat_dim_coord": "def _calc_concat_dim_coord(dim):\n    from .dataarray import DataArray\n    if isinstance(dim, str):\n        coord = None\n    elif not isinstance(dim, (DataArray, Variable)):\n        dim_name = getattr(dim, 'name', None)\n        if dim_name is None:\n            dim_name = 'concat_dim'\n        coord = IndexVariable(dim_name, dim)\n        dim = dim_name\n    elif not isinstance(dim, DataArray):\n        coord = as_variable(dim).to_index_variable()\n        dim, = coord.dims\n    else:\n        coord = dim\n        dim, = coord.dims\n    return (dim, coord)",
    ".xarray.core.dataset.py@@Dataset.copy": "def copy(self, deep: bool=False, data: Mapping=None) -> 'Dataset':\n    if data is None:\n        variables = OrderedDict(((k, v.copy(deep=deep)) for k, v in self._variables.items()))\n    elif not utils.is_dict_like(data):\n        raise ValueError('Data must be dict-like')\n    else:\n        var_keys = set(self.data_vars.keys())\n        data_keys = set(data.keys())\n        keys_not_in_vars = data_keys - var_keys\n        if keys_not_in_vars:\n            raise ValueError('Data must only contain variables in original dataset. Extra variables: {}'.format(keys_not_in_vars))\n        keys_missing_from_data = var_keys - data_keys\n        if keys_missing_from_data:\n            raise ValueError('Data must contain all variables in original dataset. Data is missing {}'.format(keys_missing_from_data))\n        variables = OrderedDict(((k, v.copy(deep=deep, data=data.get(k))) for k, v in self._variables.items()))\n    attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n    return self._replace(variables, attrs=attrs)",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        if deep:\n            data = PandasIndexAdapter(self._data.array.copy(deep=True))\n        else:\n            data = self._data\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError('%s objects must be 1-dimensional' % type(self).__name__)\n    if not isinstance(self._data, PandasIndexAdapter):\n        self._data = PandasIndexAdapter(self._data)",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, Variable):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    data = getattr(data, 'values', data)\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'M':\n            data = np.asarray(data, 'datetime64[ns]')\n        elif data.dtype.kind == 'm':\n            data = np.asarray(data, 'timedelta64[ns]')\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError('dimensions %s must have the same length as the number of data dimensions, ndim=%s' % (dims, self.ndim))\n    return dims",
    ".xarray.core.dataset.py@@Dataset._replace": "def _replace(self, variables: 'OrderedDict[Any, Variable]'=None, coord_names: Optional[Set[Hashable]]=None, dims: Dict[Any, int]=None, attrs: 'Optional[OrderedDict]'=__default, indexes: 'Optional[OrderedDict[Any, pd.Index]]'=__default, encoding: Optional[dict]=__default, inplace: bool=False) -> 'Dataset':\n    if inplace:\n        if variables is not None:\n            self._variables = variables\n        if coord_names is not None:\n            self._coord_names = coord_names\n        if dims is not None:\n            self._dims = dims\n        if attrs is not self.__default:\n            self._attrs = attrs\n        if indexes is not self.__default:\n            self._indexes = indexes\n        if encoding is not self.__default:\n            self._encoding = encoding\n        obj = self\n    else:\n        if variables is None:\n            variables = self._variables.copy()\n        if coord_names is None:\n            coord_names = self._coord_names.copy()\n        if dims is None:\n            dims = self._dims.copy()\n        if attrs is self.__default:\n            attrs = copy.copy(self._attrs)\n        if indexes is self.__default:\n            indexes = copy.copy(self._indexes)\n        if encoding is self.__default:\n            encoding = copy.copy(self._encoding)\n        obj = self._construct_direct(variables, coord_names, dims, attrs, indexes, encoding)\n    return obj",
    ".xarray.core.dataset.py@@Dataset._construct_direct": "def _construct_direct(cls, variables, coord_names, dims, attrs=None, indexes=None, encoding=None, file_obj=None):\n    obj = object.__new__(cls)\n    obj._variables = variables\n    obj._coord_names = coord_names\n    obj._dims = dims\n    obj._indexes = indexes\n    obj._attrs = attrs\n    obj._file_obj = file_obj\n    obj._encoding = encoding\n    obj._initialized = True\n    return obj",
    ".xarray.core.alignment.py@@align": "def align(*objects, join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA):\n    if indexes is None:\n        indexes = {}\n    if not indexes and len(objects) == 1:\n        obj, = objects\n        return (obj.copy(deep=copy),)\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if any((not index.equals(other) for other in matching_indexes)) or dim in unlabeled_dim_sizes:\n                joined_indexes[dim] = index\n        elif any((not matching_indexes[0].equals(other) for other in matching_indexes[1:])) or dim in unlabeled_dim_sizes:\n            if join == 'exact':\n                raise ValueError('indexes along dimension {!r} are not equal'.format(dim))\n            index = joiner(matching_indexes)\n            joined_indexes[dim] = index\n        else:\n            index = matching_indexes[0]\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension size(s) %r than the size of the aligned dimension labels: %r' % (dim, unlabeled_sizes, labeled_size))\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError('arguments without labels along dimension %r cannot be aligned because they have different dimension sizes: %r' % (dim, sizes))\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n        if not valid_indexers:\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, **valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n    return tuple(result)",
    ".xarray.core.alignment.py@@_get_joiner": "def _get_joiner(join):\n    if join == 'outer':\n        return functools.partial(functools.reduce, operator.or_)\n    elif join == 'inner':\n        return functools.partial(functools.reduce, operator.and_)\n    elif join == 'left':\n        return operator.itemgetter(0)\n    elif join == 'right':\n        return operator.itemgetter(-1)\n    elif join == 'exact':\n        return None\n    else:\n        raise ValueError('invalid value for join: %s' % join)",
    ".xarray.core.dataset.py@@Dataset.encoding": "def encoding(self) -> Dict:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.concat.py@@_calc_concat_over": "def _calc_concat_over(datasets, dim, data_vars, coords):\n    concat_over = set()\n    equals = {}\n    if dim in datasets[0]:\n        concat_over.add(dim)\n    for ds in datasets:\n        concat_over.update((k for k, v in ds.variables.items() if dim in v.dims))\n\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == 'different':\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        v_lhs = datasets[0].variables[k].load()\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not v_lhs.equals(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n            elif opt == 'all':\n                concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n            elif opt == 'minimal':\n                pass\n            else:\n                raise ValueError('unexpected value for %s: %s' % (subset, opt))\n        else:\n            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n            if invalid_vars:\n                if subset == 'coords':\n                    raise ValueError('some variables in coords are not coordinates on the first dataset: %s' % (invalid_vars,))\n                else:\n                    raise ValueError('some variables in data_vars are not data variables on the first dataset: %s' % (invalid_vars,))\n            concat_over.update(opt)\n    process_subset_opt(data_vars, 'data_vars')\n    process_subset_opt(coords, 'coords')\n    return (concat_over, equals)",
    ".xarray.core.dataset.py@@Dataset.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.concat.py@@process_subset_opt": "def process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == 'different':\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    v_lhs = datasets[0].variables[k].load()\n                    computed = []\n                    for ds_rhs in datasets[1:]:\n                        v_rhs = ds_rhs.variables[k].compute()\n                        computed.append(v_rhs)\n                        if not v_lhs.equals(v_rhs):\n                            concat_over.add(k)\n                            equals[k] = False\n                            for ds, v in zip(datasets[1:], computed):\n                                ds.variables[k].data = v.data\n                            break\n                    else:\n                        equals[k] = True\n        elif opt == 'all':\n            concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n        elif opt == 'minimal':\n            pass\n        else:\n            raise ValueError('unexpected value for %s: %s' % (subset, opt))\n    else:\n        invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n        if invalid_vars:\n            if subset == 'coords':\n                raise ValueError('some variables in coords are not coordinates on the first dataset: %s' % (invalid_vars,))\n            else:\n                raise ValueError('some variables in data_vars are not data variables on the first dataset: %s' % (invalid_vars,))\n        concat_over.update(opt)",
    ".xarray.core.dataset.py@@Dataset.coords": "def coords(self) -> DatasetCoordinates:\n    return DatasetCoordinates(self)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset):\n    self._data = dataset",
    ".xarray.core.coordinates.py@@AbstractCoordinates.__iter__": "def __iter__(self):\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self):\n    return Frozen(OrderedDict(((k, v) for k, v in self._data.variables.items() if k in self._names)))",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self):\n    return self._data._coord_names",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> 'OrderedDict[Any, Any]':\n    if self._attrs is None:\n        self._attrs = OrderedDict()\n    return self._attrs",
    ".xarray.core.coordinates.py@@AbstractCoordinates.__contains__": "def __contains__(self, key):\n    return key in self._names",
    ".xarray.core.utils.py@@SortedKeysDict.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.variable.py@@concat": "def concat(variables, dim='concat_dim', positions=None, shortcut=False):\n    variables = list(variables)\n    if all((isinstance(v, IndexVariable) for v in variables)):\n        return IndexVariable.concat(variables, dim, positions, shortcut)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut)",
    ".xarray.core.concat.py@@ensure_common_dims": "def ensure_common_dims(vars):\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple((non_concat_dims.get(d, dim_len) for d in common_dims))\n            var = var.set_dims(common_dims, common_shape)\n        yield var",
    ".xarray.core.variable.py@@IndexVariable.concat": "def concat(cls, variables, dim='concat_dim', positions=None, shortcut=False):\n    if not isinstance(dim, str):\n        dim, = dim.dims\n    variables = list(variables)\n    first_var = variables[0]\n    if any((not isinstance(v, cls) for v in variables)):\n        raise TypeError('IndexVariable.concat requires that all input variables be IndexVariable objects')\n    indexes = [v._data.array for v in variables]\n    if not indexes:\n        data = []\n    else:\n        data = indexes[0].append(indexes[1:])\n        if positions is not None:\n            indices = nputils.inverse_permutation(np.concatenate(positions))\n            data = data.take(indices)\n    attrs = OrderedDict(first_var.attrs)\n    if not shortcut:\n        for var in variables:\n            if var.dims != first_var.dims:\n                raise ValueError('inconsistent dimensions')\n            utils.remove_incompatible_items(attrs, var.attrs)\n    return cls(first_var.dims, data, attrs)",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> 'OrderedDict[Any, Any]':\n    if self._attrs is None:\n        self._attrs = OrderedDict()\n    return self._attrs",
    ".xarray.core.utils.py@@remove_incompatible_items": "def remove_incompatible_items(first_dict: MutableMapping[K, V], second_dict: Mapping[K, V], compat: Callable[[V, V], bool]=equivalent) -> None:\n    for k in list(first_dict):\n        if k not in second_dict or not compat(first_dict[k], second_dict[k]):\n            del first_dict[k]",
    ".xarray.core.indexing.py@@PandasIndexAdapter.__init__": "def __init__(self, array, dtype=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        if isinstance(array, pd.PeriodIndex):\n            dtype = np.dtype('O')\n        elif hasattr(array, 'categories'):\n            dtype = array.categories.dtype\n        elif not utils.is_valid_numpy_dtype(array.dtype):\n            dtype = np.dtype('O')\n        else:\n            dtype = array.dtype\n    self._dtype = dtype",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.concat.py@@insert_result_variable": "def insert_result_variable(k, v):\n    assert isinstance(v, Variable)\n    if k in datasets[0].coords:\n        result_coord_names.add(k)\n    result_vars[k] = v",
    ".xarray.core.dataset.py@@Dataset.__init__": "def __init__(self, data_vars: Optional[Mapping[Hashable, Union['DataArray', Variable, Tuple[Hashable, Any], Tuple[Sequence[Hashable], Any]]]]=None, coords: Optional[Mapping[Hashable, Any]]=None, attrs: Optional[Mapping]=None, compat=None):\n    if compat is not None:\n        warnings.warn('The `compat` argument to Dataset is deprecated and will be removed in 0.13.Instead, use `merge` to control how variables are combined', FutureWarning, stacklevel=2)\n    else:\n        compat = 'broadcast_equals'\n    self._variables = OrderedDict()\n    self._coord_names = set()\n    self._dims = {}\n    self._attrs = None\n    self._file_obj = None\n    if data_vars is None:\n        data_vars = {}\n    if coords is None:\n        coords = {}\n    self._set_init_vars_and_dims(data_vars, coords, compat)\n    self._indexes = None\n    if attrs is not None:\n        self.attrs = attrs\n    self._encoding = None\n    self._initialized = True",
    ".xarray.core.dataset.py@@Dataset._set_init_vars_and_dims": "def _set_init_vars_and_dims(self, data_vars, coords, compat):\n    both_data_and_coords = [k for k in data_vars if k in coords]\n    if both_data_and_coords:\n        raise ValueError('variables %r are found in both data_vars and coords' % both_data_and_coords)\n    if isinstance(coords, Dataset):\n        coords = coords.variables\n    variables, coord_names, dims = merge_data_and_coords(data_vars, coords, compat=compat)\n    self._variables = variables\n    self._coord_names = coord_names\n    self._dims = dims",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data, coords, compat='broadcast_equals', join='outer'):\n    objs = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(extract_indexes(coords))\n    return merge_core(objs, compat, join, explicit_coords=explicit_coords, indexes=indexes)",
    ".xarray.core.merge.py@@extract_indexes": "def extract_indexes(coords):\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield (name, variable.to_index())",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objs, compat='broadcast_equals', join='outer', priority_arg=None, explicit_coords=None, indexes=None, fill_value=dtypes.NA) -> Tuple['OrderedDict[Hashable, Variable]', Set[Hashable], Dict[Hashable, int]]:\n    from .dataset import calculate_dimensions\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objs)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    expanded = expand_variable_dicts(aligned)\n    coord_names, noncoord_names = determine_coords(coerced)\n    priority_vars = _get_priority_vars(aligned, priority_arg, compat=compat)\n    variables = merge_variables(expanded, priority_vars, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n    dims = calculate_dimensions(variables)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError('unable to determine if these variables should be coordinates or not in the merged result: %s' % ambiguous_coords)\n    return (variables, coord_names, dims)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError('compat=%r invalid: must be %s' % (compat, set(_VALID_COMPAT)))",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables = obj\n        else:\n            variables = OrderedDict()\n            if isinstance(obj, PANDAS_TYPES):\n                obj = OrderedDict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for n, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(n)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(n)\n                    keys.append(k)\n                    targets.append(v)\n            out.append(OrderedDict(variables))\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: %r' % variables)\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    assert all((arg is not not_replaced for arg in out))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.merge.py@@expand_variable_dicts": "def expand_variable_dicts(list_of_variable_dicts: 'List[Union[Dataset, OrderedDict]]') -> 'List[Mapping[Any, Variable]]':\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    var_dicts = []\n    for variables in list_of_variable_dicts:\n        if isinstance(variables, Dataset):\n            var_dicts.append(variables.variables)\n            continue\n        sanitized_vars = OrderedDict()\n        for name, var in variables.items():\n            if isinstance(var, DataArray):\n                coords = var._coords.copy()\n                coords.pop(name, None)\n                var_dicts.append(coords)\n            var = as_variable(var, name=name)\n            sanitized_vars[name] = var\n        var_dicts.append(sanitized_vars)\n    return var_dicts",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> 'Union[Variable, IndexVariable]':\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError('variable %r has invalid type %r' % (name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError('cannot set variable %r with %r-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.' % (name, data.ndim))\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError('unable to convert object into a variable without an explicit list of dimensions: %r' % obj)\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError('%r has more than 1-dimension and the same name as one of its dimensions %r. xarray disallows such variables because they conflict with the coordinates used to label dimensions.' % (name, obj.dims))\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_variable_dicts):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names = set()\n    noncoord_names = set()\n    for variables in list_of_variable_dicts:\n        if isinstance(variables, Dataset):\n            coord_names.update(variables.coords)\n            noncoord_names.update(variables.data_vars)\n        else:\n            for name, var in variables.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@_get_priority_vars": "def _get_priority_vars(objects, priority_arg, compat='equals'):\n    if priority_arg is None:\n        priority_vars = {}\n    else:\n        expanded = expand_variable_dicts([objects[priority_arg]])\n        priority_vars = merge_variables(expanded, compat=compat)\n    return priority_vars",
    ".xarray.core.merge.py@@merge_variables": "def merge_variables(list_of_variables_dicts, priority_vars=None, compat='minimal'):\n    if priority_vars is None:\n        priority_vars = {}\n    _assert_compat_valid(compat)\n    dim_compat = min(compat, 'equals', key=_VALID_COMPAT.get)\n    lookup = OrderedDefaultDict(list)\n    for variables in list_of_variables_dicts:\n        for name, var in variables.items():\n            lookup[name].append(var)\n    merged = OrderedDict()\n    for name, var_list in lookup.items():\n        if name in priority_vars:\n            merged[name] = priority_vars[name]\n        else:\n            dim_variables = [var for var in var_list if (name,) == var.dims]\n            if dim_variables:\n                merged[name] = unique_variable(name, dim_variables, dim_compat)\n            else:\n                try:\n                    merged[name] = unique_variable(name, var_list, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n    return merged",
    ".xarray.core.merge.py@@OrderedDefaultDict.__init__": "def __init__(self, default_factory):\n    self.default_factory = default_factory\n    super().__init__()",
    ".xarray.core.merge.py@@OrderedDefaultDict.__missing__": "def __missing__(self, key):\n    self[key] = default = self.default_factory()\n    return default",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name, variables, compat='broadcast_equals'):\n    out = variables[0]\n    if len(variables) > 1:\n        combine_method = None\n        if compat == 'minimal':\n            compat = 'broadcast_equals'\n        if compat == 'broadcast_equals':\n            dim_lengths = broadcast_dimension_size(variables)\n            out = out.set_dims(dim_lengths)\n        if compat == 'no_conflicts':\n            combine_method = 'fillna'\n        for var in variables[1:]:\n            if not getattr(out, compat)(var):\n                raise MergeError('conflicting values for variable %r on objects to be combined:\\nfirst value: %r\\nsecond value: %r' % (name, out, var))\n            if combine_method:\n                out = getattr(out, combine_method)(var)\n                out.attrs = var.attrs\n    return out",
    ".xarray.core.variable.py@@assert_unique_multiindex_level_names": "def assert_unique_multiindex_level_names(variables):\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append('%r (%s)' % (n, var_name))\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n    for k, v in level_names.items():\n        if k in variables:\n            v.append('(%s)' % k)\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = '\\n'.join([', '.join(v) for v in duplicate_names])\n        raise ValueError('conflicting MultiIndex level name(s):\\n%s' % conflict_str)\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError('conflicting level / dimension names. {} already exists as a level name.'.format(d))",
    ".xarray.core.variable.py@@IndexVariable.level_names": "def level_names(self):\n    index = self.to_index()\n    if isinstance(index, pd.MultiIndex):\n        return index.names\n    else:\n        return None",
    ".xarray.core.dataset.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> 'Dict[Any, int]':\n    dims = {}\n    last_used = {}\n    scalar_vars = set((k for k, v in variables.items() if not v.dims))\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError('dimension %r already exists as a scalar variable' % dim)\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError('conflicting sizes for dimension %r: length %s on %r and length %s on %r' % (dim, size, k, dims[dim], last_used[dim]))\n    return dims",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError('coordinate %s shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.' % coord_name)",
    ".xarray.core.dataset.py@@Dataset.set_coords": "def set_coords(self, names: 'Union[Hashable, Iterable[Hashable]]', inplace: bool=None) -> 'Dataset':\n    inplace = _check_inplace(inplace)\n    if isinstance(names, str) or not isinstance(names, Iterable):\n        names = [names]\n    else:\n        names = list(names)\n    self._assert_all_in_dataset(names)\n    obj = self if inplace else self.copy()\n    obj._coord_names.update(names)\n    return obj",
    ".xarray.core.utils.py@@_check_inplace": "def _check_inplace(inplace: Optional[bool], default: bool=False) -> bool:\n    if inplace is None:\n        inplace = default\n    else:\n        warnings.warn('The inplace argument has been deprecated and will be removed in a future version of xarray.', FutureWarning, stacklevel=3)\n    return inplace",
    ".xarray.core.dataset.py@@Dataset._assert_all_in_dataset": "def _assert_all_in_dataset(self, names, virtual_okay=False):\n    bad_names = set(names) - set(self._variables)\n    if virtual_okay:\n        bad_names -= self.virtual_variables\n    if bad_names:\n        raise ValueError('One or more of the specified variables cannot be found in this dataset')",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            if isinstance(data, dask_array_type):\n                data = data.copy()\n            elif not isinstance(data, PandasIndexAdapter):\n                data = np.array(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.variable.py@@Variable.concat": "def concat(cls, variables, dim='concat_dim', positions=None, shortcut=False):\n    if not isinstance(dim, str):\n        dim, = dim.dims\n    variables = list(variables)\n    first_var = variables[0]\n    arrays = [v.data for v in variables]\n    if dim in first_var.dims:\n        axis = first_var.get_axis_num(dim)\n        dims = first_var.dims\n        data = duck_array_ops.concatenate(arrays, axis=axis)\n        if positions is not None:\n            indices = nputils.inverse_permutation(np.concatenate(positions))\n            data = duck_array_ops.take(data, indices, axis=axis)\n    else:\n        axis = 0\n        dims = (dim,) + first_var.dims\n        data = duck_array_ops.stack(arrays, axis=axis)\n    attrs = OrderedDict(first_var.attrs)\n    encoding = OrderedDict(first_var.encoding)\n    if not shortcut:\n        for var in variables:\n            if var.dims != first_var.dims:\n                raise ValueError('inconsistent dimensions')\n            utils.remove_incompatible_items(attrs, var.attrs)\n    return cls(dims, data, attrs, encoding)",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if isinstance(self._data, dask_array_type):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.variable.py@@Variable.values": "def values(self):\n    return _as_array_or_item(self._data)",
    ".xarray.core.variable.py@@_as_array_or_item": "def _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Union[Hashable, Iterable[Hashable]]) -> Union[int, Tuple[int, ...]]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError('%r not found in array dimensions %r' % (dim, self.dims))",
    ".xarray.core.duck_array_ops.py@@concatenate": "def concatenate(arrays, axis=0):\n    return _concatenate(as_shared_dtype(arrays), axis=axis)",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data):\n    return data if isinstance(data, dask_array_type) else np.asarray(data)",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError('`out` is not valid for {}'.format(name))\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        func = _dask_or_eager_func(name)\n    try:\n        return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if isinstance(values, dask_array_type):\n            try:\n                return func(values, axis=axis, dtype=values.dtype, **kwargs)\n            except (AttributeError, TypeError):\n                msg = '%s is not yet implemented on dask arrays' % name\n        else:\n            msg = '%s is not available with skipna=False with the installed version of numpy; upgrade to numpy 1.12 or newer to use skipna=True or skipna=None' % name\n        raise NotImplementedError(msg)",
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self):\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.variable.py@@IndexVariable.load": "def load(self):\n    return self",
    ".xarray.core.utils.py@@SortedKeysDict.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.dataset.py@@Dataset.reindex": "def reindex(self, indexers=None, method=None, tolerance=None, copy=True, fill_value=dtypes.NA, **indexers_kwargs):\n    indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, 'reindex')\n    bad_dims = [d for d in indexers if d not in self.dims]\n    if bad_dims:\n        raise ValueError('invalid reindex dimensions: %s' % bad_dims)\n    variables, indexes = alignment.reindex_variables(self.variables, self.sizes, self.indexes, indexers, method, tolerance, copy=copy, fill_value=fill_value)\n    coord_names = set(self._coord_names)\n    coord_names.update(indexers)\n    return self._replace_with_new_dims(variables, coord_names, indexes=indexes)",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Optional[Mapping[Hashable, T]], kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is not None:\n        if not is_dict_like(pos_kwargs):\n            raise ValueError('the first argument to .%s must be a dictionary' % func_name)\n        if kw_kwargs:\n            raise ValueError('cannot specify both keyword and positional arguments to .%s' % func_name)\n        return pos_kwargs\n    else:\n        return cast(Mapping[Hashable, T], kw_kwargs)",
    ".xarray.core.dataset.py@@Dataset.sizes": "def sizes(self) -> Mapping[Hashable, int]:\n    return self.dims",
    ".xarray.core.alignment.py@@reindex_variables": "def reindex_variables(variables: Mapping[Any, Variable], sizes: Mapping[Any, int], indexes: Mapping[Any, pd.Index], indexers: Mapping, method: Optional[str]=None, tolerance: Any=None, copy: bool=True, fill_value: Optional[Any]=dtypes.NA) -> 'Tuple[OrderedDict[Any, Variable], OrderedDict[Any, pd.Index]]':\n    from .dataarray import DataArray\n    reindexed = OrderedDict()\n    int_indexers = {}\n    new_indexes = OrderedDict(indexes)\n    masked_dims = set()\n    unchanged_dims = set()\n    for dim, indexer in indexers.items():\n        if isinstance(indexer, DataArray) and indexer.dims != (dim,):\n            warnings.warn('Indexer has dimensions {0:s} that are different from that to be indexed along {1:s}. This will behave differently in the future.'.format(str(indexer.dims), dim), FutureWarning, stacklevel=3)\n        target = new_indexes[dim] = utils.safe_cast_to_index(indexers[dim])\n        if dim in indexes:\n            index = indexes[dim]\n            if not index.is_unique:\n                raise ValueError('cannot reindex or align along dimension %r because the index has duplicate values' % dim)\n            int_indexer = get_indexer_nd(index, target, method, tolerance)\n            if (int_indexer < 0).any():\n                masked_dims.add(dim)\n            elif np.array_equal(int_indexer, np.arange(len(index))):\n                unchanged_dims.add(dim)\n            int_indexers[dim] = int_indexer\n        if dim in variables:\n            var = variables[dim]\n            args = (var.attrs, var.encoding)\n        else:\n            args = ()\n        reindexed[dim] = IndexVariable((dim,), target, *args)\n    for dim in sizes:\n        if dim not in indexes and dim in indexers:\n            existing_size = sizes[dim]\n            new_size = indexers[dim].size\n            if existing_size != new_size:\n                raise ValueError('cannot reindex or align along dimension %r without an index because its size %r is different from the size of the new index %r' % (dim, existing_size, new_size))\n    for name, var in variables.items():\n        if name not in indexers:\n            key = tuple((slice(None) if d in unchanged_dims else int_indexers.get(d, slice(None)) for d in var.dims))\n            needs_masking = any((d in masked_dims for d in var.dims))\n            if needs_masking:\n                new_var = var._getitem_with_mask(key, fill_value=fill_value)\n            elif all((is_full_slice(k) for k in key)):\n                new_var = var.copy(deep=copy)\n            else:\n                new_var = var[key]\n            reindexed[name] = new_var\n    return (reindexed, new_indexes)",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self):\n    return iter(self._indexes)",
    ".xarray.core.indexes.py@@Indexes.__contains__": "def __contains__(self, key):\n    return key in self._indexes",
    ".xarray.core.indexing.py@@get_indexer_nd": "def get_indexer_nd(index, labels, method=None, tolerance=None):\n    kwargs = _index_method_kwargs(method, tolerance)\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, **kwargs)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer",
    ".xarray.core.indexing.py@@_index_method_kwargs": "def _index_method_kwargs(method, tolerance):\n    kwargs = {}\n    if method is not None:\n        kwargs['method'] = method\n    if tolerance is not None:\n        kwargs['tolerance'] = tolerance\n    return kwargs",
    ".xarray.core.utils.py@@is_full_slice": "def is_full_slice(value: Any) -> bool:\n    return isinstance(value, slice) and value == slice(None)",
    ".xarray.core.dataset.py@@Dataset._replace_with_new_dims": "def _replace_with_new_dims(self, variables: 'OrderedDict[Any, Variable]', coord_names: set=None, attrs: 'Optional[OrderedDict]'=__default, indexes: 'Optional[OrderedDict[Any, pd.Index]]'=__default, inplace: bool=False) -> 'Dataset':\n    dims = calculate_dimensions(variables)\n    return self._replace(variables, coord_names, dims, attrs, indexes, inplace=inplace)",
    ".xarray.core.variable.py@@Variable.compute": "def compute(self, **kwargs):\n    new = self.copy(deep=False)\n    return new.load(**kwargs)",
    ".xarray.core.variable.py@@IndexVariable.equals": "def equals(self, other, equiv=None):\n    if equiv is not None:\n        return super().equals(other, equiv)\n    other = getattr(other, 'variable', other)\n    try:\n        return self.dims == other.dims and self._data_equals(other)\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.variable.py@@IndexVariable._data_equals": "def _data_equals(self, other):\n    return self.to_index().equals(other.to_index())",
    ".xarray.core.variable.py@@Variable.load": "def load(self, **kwargs):\n    if isinstance(self._data, dask_array_type):\n        self._data = as_compatible_data(self._data.compute(**kwargs))\n    elif not isinstance(self._data, np.ndarray):\n        self._data = np.asarray(self._data)\n    return self",
    ".xarray.core.variable.py@@Variable.equals": "def equals(self, other, equiv=duck_array_ops.array_equiv):\n    other = getattr(other, 'variable', other)\n    try:\n        return self.dims == other.dims and (self._data is other._data or equiv(self.data, other.data))\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.duck_array_ops.py@@array_equiv": "def array_equiv(arr1, arr2):\n    arr1, arr2 = as_like_arrays(arr1, arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', \"In the future, 'NAT == x'\")\n        flag_array = arr1 == arr2\n        flag_array |= isnull(arr1) & isnull(arr2)\n        return bool(flag_array.all())",
    ".xarray.core.duck_array_ops.py@@as_like_arrays": "def as_like_arrays(*data):\n    if all((isinstance(d, dask_array_type) for d in data)):\n        return data\n    else:\n        return tuple((np.asarray(d) for d in data))",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    try:\n        return _isnull(data)\n    except TypeError:\n        return np.zeros(data.shape, dtype=bool)",
    ".xarray.core.utils.py@@equivalent": "def equivalent(first: T, second: T) -> bool:\n    from . import duck_array_ops\n    if isinstance(first, np.ndarray) or isinstance(second, np.ndarray):\n        return duck_array_ops.array_equiv(first, second)\n    else:\n        return first is second or first == second or (pd.isnull(first) and pd.isnull(second))",
    ".xarray.core.variable.py@@Variable.set_dims": "def set_dims(self, dims, shape=None):\n    if isinstance(dims, str):\n        dims = [dims]\n    if shape is None and utils.is_dict_like(dims):\n        shape = dims.values()\n    missing_dims = set(self.dims) - set(dims)\n    if missing_dims:\n        raise ValueError('new dimensions %r must be a superset of existing dimensions %r' % (dims, self.dims))\n    self_dims = set(self.dims)\n    expanded_dims = tuple((d for d in dims if d not in self_dims)) + self.dims\n    if self.dims == expanded_dims:\n        expanded_data = self.data\n    elif shape is not None:\n        dims_map = dict(zip(dims, shape))\n        tmp_shape = tuple((dims_map[d] for d in expanded_dims))\n        expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n    else:\n        expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n    expanded_var = Variable(expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True)\n    return expanded_var.transpose(*dims)",
    ".xarray.core.variable.py@@Variable.transpose": "def transpose(self, *dims) -> 'Variable':\n    if len(dims) == 0:\n        dims = self.dims[::-1]\n    axes = self.get_axis_num(dims)\n    if len(dims) < 2:\n        return self.copy(deep=False)\n    data = as_indexable(self._data).transpose(axes)\n    return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    raise TypeError('Invalid array type: {}'.format(type(array)))",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.transpose": "def transpose(self, order):\n    return self.array.transpose(order)"
}