{
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.linear_model.base.py@@_preprocess_data": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order='K')\n    y = np.asarray(y, dtype=X.dtype)\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n            if normalize:\n                X_var *= X.shape[0]\n                X_scale = np.sqrt(X_var, X_var)\n                del X_var\n                X_scale[X_scale == 0] = 1\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        else:\n            X_offset = np.average(X, axis=0, weights=sample_weight)\n            X -= X_offset\n            if normalize:\n                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n    return (X, y, X_offset, y_offset, X_scale)",
    ".sklearn.linear_model.bayes.py@@ARDRegression.update_sigma": "def update_sigma(X, alpha_, lambda_, keep_lambda, n_samples):\n    sigma_ = pinvh(np.eye(n_samples) / alpha_ + np.dot(X[:, keep_lambda] * np.reshape(1.0 / lambda_[keep_lambda], [1, -1]), X[:, keep_lambda].T))\n    sigma_ = np.dot(sigma_, X[:, keep_lambda] * np.reshape(1.0 / lambda_[keep_lambda], [1, -1]))\n    sigma_ = -np.dot(np.reshape(1.0 / lambda_[keep_lambda], [-1, 1]) * X[:, keep_lambda].T, sigma_)\n    sigma_.flat[::sigma_.shape[1] + 1] += 1.0 / lambda_[keep_lambda]\n    return sigma_"
}