{
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.__init__": "def __init__(self, neg_label=0, pos_label=1, sparse_output=False):\n    if neg_label >= pos_label:\n        raise ValueError('neg_label={0} must be strictly less than pos_label={1}.'.format(neg_label, pos_label))\n    if sparse_output and (pos_label == 0 or neg_label != 0):\n        raise ValueError('Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={0} and neg_label={1}'.format(pos_label, neg_label))\n    self.neg_label = neg_label\n    self.pos_label = pos_label\n    self.sparse_output = sparse_output",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.fit_transform": "def fit_transform(self, y):\n    return self.fit(y).transform(y)",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.fit": "def fit(self, y):\n    self.y_type_ = type_of_target(y)\n    if 'multioutput' in self.y_type_:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    self.sparse_input_ = sp.issparse(y)\n    self.classes_ = unique_labels(y)\n    return self",
    ".sklearn.utils.multiclass.py@@type_of_target": "def type_of_target(y):\n    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, str))\n    if not valid:\n        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)\n    sparseseries = y.__class__.__name__ == 'SparseSeries'\n    if sparseseries:\n        raise ValueError(\"y cannot be class 'SparseSeries'.\")\n    if is_multilabel(y):\n        return 'multilabel-indicator'\n    try:\n        y = np.asarray(y)\n    except ValueError:\n        return 'unknown'\n    try:\n        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):\n            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.')\n    except IndexError:\n        pass\n    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], str))):\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] == 0:\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = '-multioutput'\n    else:\n        suffix = ''\n    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):\n        _assert_all_finite(y)\n        return 'continuous' + suffix\n    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):\n        return 'multiclass' + suffix\n    else:\n        return 'binary'",
    ".sklearn.utils.multiclass.py@@is_multilabel": "def is_multilabel(y):\n    if hasattr(y, '__array__'):\n        y = np.asarray(y)\n    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):\n        return False\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))\n    else:\n        labels = np.unique(y)\n        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))",
    ".sklearn.utils.multiclass.py@@unique_labels": "def unique_labels(*ys):\n    if not ys:\n        raise ValueError('No argument has been passed.')\n    ys_types = set((type_of_target(x) for x in ys))\n    if ys_types == {'binary', 'multiclass'}:\n        ys_types = {'multiclass'}\n    if len(ys_types) > 1:\n        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)\n    label_type = ys_types.pop()\n    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:\n        raise ValueError('Multi-label binary indicator input with different numbers of labels')\n    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n    if not _unique_labels:\n        raise ValueError('Unknown label type: %s' % repr(ys))\n    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))\n    if len(set((isinstance(label, str) for label in ys_labels))) > 1:\n        raise ValueError('Mix of label input types (string and number)')\n    return np.array(sorted(ys_labels))",
    ".sklearn.utils.multiclass.py@@_unique_multiclass": "def _unique_multiclass(y):\n    if hasattr(y, '__array__'):\n        return np.unique(np.asarray(y))\n    else:\n        return set(y)",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.transform": "def transform(self, y):\n    check_is_fitted(self, 'classes_')\n    y_is_multilabel = type_of_target(y).startswith('multilabel')\n    if y_is_multilabel and (not self.y_type_.startswith('multilabel')):\n        raise ValueError('The object was not fitted with multilabel input.')\n    return label_binarize(y, self.classes_, pos_label=self.pos_label, neg_label=self.neg_label, sparse_output=self.sparse_output)",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.preprocessing.label.py@@label_binarize": "def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):\n    if not isinstance(y, list):\n        y = check_array(y, accept_sparse='csr', ensure_2d=False, dtype=None)\n    elif _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    if neg_label >= pos_label:\n        raise ValueError('neg_label={0} must be strictly less than pos_label={1}.'.format(neg_label, pos_label))\n    if sparse_output and (pos_label == 0 or neg_label != 0):\n        raise ValueError('Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={0} and neg_label={1}'.format(pos_label, neg_label))\n    pos_switch = pos_label == 0\n    if pos_switch:\n        pos_label = -neg_label\n    y_type = type_of_target(y)\n    if 'multioutput' in y_type:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if y_type == 'unknown':\n        raise ValueError('The type of target data is not known')\n    n_samples = y.shape[0] if sp.issparse(y) else len(y)\n    n_classes = len(classes)\n    classes = np.asarray(classes)\n    if y_type == 'binary':\n        if n_classes == 1:\n            if sparse_output:\n                return sp.csr_matrix((n_samples, 1), dtype=int)\n            else:\n                Y = np.zeros((len(y), 1), dtype=np.int)\n                Y += neg_label\n                return Y\n        elif len(classes) >= 3:\n            y_type = 'multiclass'\n    sorted_class = np.sort(classes)\n    if y_type == 'multilabel-indicator' and classes.size != y.shape[1]:\n        raise ValueError('classes {0} missmatch with the labels {1}found in the data'.format(classes, unique_labels(y)))\n    if y_type in ('binary', 'multiclass'):\n        y = column_or_1d(y)\n        y_in_classes = np.in1d(y, classes)\n        y_seen = y[y_in_classes]\n        indices = np.searchsorted(sorted_class, y_seen)\n        indptr = np.hstack((0, np.cumsum(y_in_classes)))\n        data = np.empty_like(indices)\n        data.fill(pos_label)\n        Y = sp.csr_matrix((data, indices, indptr), shape=(n_samples, n_classes))\n    elif y_type == 'multilabel-indicator':\n        Y = sp.csr_matrix(y)\n        if pos_label != 1:\n            data = np.empty_like(Y.data)\n            data.fill(pos_label)\n            Y.data = data\n    else:\n        raise ValueError('%s target data is not supported with label binarization' % y_type)\n    if not sparse_output:\n        Y = Y.toarray()\n        Y = Y.astype(int, copy=False)\n        if neg_label != 0:\n            Y[Y == 0] = neg_label\n        if pos_switch:\n            Y[Y == pos_label] = 0\n    else:\n        Y.data = Y.data.astype(int, copy=False)\n    if np.any(classes != sorted_class):\n        indices = np.searchsorted(sorted_class, classes)\n        Y = Y[:, indices]\n    if y_type == 'binary':\n        if sparse_output:\n            Y = Y.getcol(-1)\n        else:\n            Y = Y[:, -1].reshape((-1, 1))\n    return Y",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.linear_model.ridge.py@@_BaseRidgeCV.fit": "def fit(self, X, y, sample_weight=None):\n    if self.cv is None:\n        estimator = _RidgeGCV(self.alphas, fit_intercept=self.fit_intercept, normalize=self.normalize, scoring=self.scoring, gcv_mode=self.gcv_mode, store_cv_values=self.store_cv_values)\n        estimator.fit(X, y, sample_weight=sample_weight)\n        self.alpha_ = estimator.alpha_\n        if self.store_cv_values:\n            self.cv_values_ = estimator.cv_values_\n    else:\n        if self.store_cv_values:\n            raise ValueError('cv!=None and store_cv_values=True  are incompatible')\n        parameters = {'alpha': self.alphas}\n        gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept, normalize=self.normalize), parameters, cv=self.cv, scoring=self.scoring)\n        gs.fit(X, y, sample_weight=sample_weight)\n        estimator = gs.best_estimator_\n        self.alpha_ = gs.best_estimator_.alpha\n    self.coef_ = estimator.coef_\n    self.intercept_ = estimator.intercept_\n    return self",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV.__init__": "def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, copy_X=True, gcv_mode=None, store_cv_values=False):\n    self.alphas = np.asarray(alphas)\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.scoring = scoring\n    self.copy_X = copy_X\n    self.gcv_mode = gcv_mode\n    self.store_cv_values = store_cv_values",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV.fit": "def fit(self, X, y, sample_weight=None):\n    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float64, np.float32], multi_output=True, y_numeric=True)\n    if sample_weight is not None and (not isinstance(sample_weight, float)):\n        sample_weight = check_array(sample_weight, ensure_2d=False, dtype=X.dtype)\n    n_samples, n_features = X.shape\n    X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X, sample_weight=sample_weight)\n    gcv_mode = self.gcv_mode\n    with_sw = len(np.shape(sample_weight))\n    if gcv_mode is None or gcv_mode == 'auto':\n        if sparse.issparse(X) or n_features > n_samples or with_sw:\n            gcv_mode = 'eigen'\n        else:\n            gcv_mode = 'svd'\n    elif gcv_mode == 'svd' and with_sw:\n        warnings.warn('non-uniform sample weights unsupported for svd, forcing usage of eigen')\n        gcv_mode = 'eigen'\n    if gcv_mode == 'eigen':\n        _pre_compute = self._pre_compute\n        _errors = self._errors\n        _values = self._values\n    elif gcv_mode == 'svd':\n        _pre_compute = self._pre_compute_svd\n        _errors = self._errors_svd\n        _values = self._values_svd\n    else:\n        raise ValueError('bad gcv_mode \"%s\"' % gcv_mode)\n    if sample_weight is not None:\n        X, y = _rescale_data(X, y, sample_weight)\n    centered_kernel = not sparse.issparse(X) and self.fit_intercept\n    v, Q, QT_y = _pre_compute(X, y, centered_kernel)\n    n_y = 1 if len(y.shape) == 1 else y.shape[1]\n    cv_values = np.zeros((n_samples * n_y, len(self.alphas)))\n    C = []\n    scorer = check_scoring(self, scoring=self.scoring, allow_none=True)\n    error = scorer is None\n    if np.any(self.alphas < 0):\n        raise ValueError('alphas cannot be negative. Got {} containing some negative value instead.'.format(self.alphas))\n    for i, alpha in enumerate(self.alphas):\n        if error:\n            out, c = _errors(float(alpha), y, v, Q, QT_y)\n        else:\n            out, c = _values(float(alpha), y, v, Q, QT_y)\n        cv_values[:, i] = out.ravel()\n        C.append(c)\n    if error:\n        best = cv_values.mean(axis=0).argmin()\n    else:\n\n        def identity_estimator():\n            pass\n        identity_estimator.decision_function = lambda y_predict: y_predict\n        identity_estimator.predict = lambda y_predict: y_predict\n        out = [scorer(identity_estimator, y.ravel(), cv_values[:, i]) for i in range(len(self.alphas))]\n        best = np.argmax(out)\n    self.alpha_ = self.alphas[best]\n    self.dual_coef_ = C[best]\n    self.coef_ = safe_sparse_dot(self.dual_coef_.T, X)\n    self._set_intercept(X_offset, y_offset, X_scale)\n    if self.store_cv_values:\n        if len(y.shape) == 1:\n            cv_values_shape = (n_samples, len(self.alphas))\n        else:\n            cv_values_shape = (n_samples, n_y, len(self.alphas))\n        self.cv_values_ = cv_values.reshape(cv_values_shape)\n    return self",
    ".sklearn.linear_model.base.py@@_preprocess_data": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order='K')\n    y = np.asarray(y, dtype=X.dtype)\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n            if normalize:\n                X_var *= X.shape[0]\n                X_scale = np.sqrt(X_var, X_var)\n                del X_var\n                X_scale[X_scale == 0] = 1\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        else:\n            X_offset = np.average(X, axis=0, weights=sample_weight)\n            X -= X_offset\n            if normalize:\n                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n    return (X, y, X_offset, y_offset, X_scale)",
    ".sklearn.utils.sparsefuncs.py@@mean_variance_axis": "def mean_variance_axis(X, axis):\n    _raise_error_wrong_axis(axis)\n    if isinstance(X, sp.csr_matrix):\n        if axis == 0:\n            return _csr_mean_var_axis0(X)\n        else:\n            return _csc_mean_var_axis0(X.T)\n    elif isinstance(X, sp.csc_matrix):\n        if axis == 0:\n            return _csc_mean_var_axis0(X)\n        else:\n            return _csr_mean_var_axis0(X.T)\n    else:\n        _raise_typeerror(X)",
    ".sklearn.utils.sparsefuncs.py@@_raise_error_wrong_axis": "def _raise_error_wrong_axis(axis):\n    if axis not in (0, 1):\n        raise ValueError('Unknown axis value: %d. Use 0 for rows, or 1 for columns' % axis)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._pre_compute": "def _pre_compute(self, X, y, centered_kernel=True):\n    K = safe_sparse_dot(X, X.T, dense_output=True)\n    if centered_kernel:\n        K += np.ones_like(K)\n    v, Q = linalg.eigh(K)\n    QT_y = np.dot(Q.T, y)\n    return (v, Q, QT_y)",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if sparse.issparse(a) or sparse.issparse(b):\n        ret = a * b\n        if dense_output and hasattr(ret, 'toarray'):\n            ret = ret.toarray()\n        return ret\n    else:\n        return np.dot(a, b)",
    ".sklearn.metrics.scorer.py@@check_scoring": "def check_scoring(estimator, scoring=None, allow_none=False):\n    if not hasattr(estimator, 'fit'):\n        raise TypeError(\"estimator should be an estimator implementing 'fit' method, %r was passed\" % estimator)\n    if isinstance(scoring, str):\n        return get_scorer(scoring)\n    elif callable(scoring):\n        module = getattr(scoring, '__module__', None)\n        if hasattr(module, 'startswith') and module.startswith('sklearn.metrics.') and (not module.startswith('sklearn.metrics.scorer')) and (not module.startswith('sklearn.metrics.tests.')):\n            raise ValueError('scoring value %r looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.' % scoring)\n        return get_scorer(scoring)\n    elif scoring is None:\n        if hasattr(estimator, 'score'):\n            return _passthrough_scorer\n        elif allow_none:\n            return None\n        else:\n            raise TypeError(\"If no scoring is specified, the estimator passed should have a 'score' method. The estimator %r does not.\" % estimator)\n    elif isinstance(scoring, Iterable):\n        raise ValueError('For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. {0} was passed.'.format(scoring))\n    else:\n        raise ValueError('scoring value should either be a callable, string or None. %r was passed' % scoring)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._errors": "def _errors(self, alpha, y, v, Q, QT_y):\n    G_diag, c = self._errors_and_values_helper(alpha, y, v, Q, QT_y)\n    return ((c / G_diag) ** 2, c)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._errors_and_values_helper": "def _errors_and_values_helper(self, alpha, y, v, Q, QT_y):\n    w = 1.0 / (v + alpha)\n    constant_column = np.var(Q, 0) < 1e-12\n    w[constant_column] = 0\n    c = np.dot(Q, self._diag_dot(w, QT_y))\n    G_diag = self._decomp_diag(w, Q)\n    if len(y.shape) != 1:\n        G_diag = G_diag[:, np.newaxis]\n    return (G_diag, c)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._diag_dot": "def _diag_dot(self, D, B):\n    if len(B.shape) > 1:\n        D = D[(slice(None),) + (np.newaxis,) * (len(B.shape) - 1)]\n    return D * B",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._decomp_diag": "def _decomp_diag(self, v_prime, Q):\n    return (v_prime * Q ** 2).sum(axis=-1)",
    ".sklearn.linear_model.base.py@@LinearModel._set_intercept": "def _set_intercept(self, X_offset, y_offset, X_scale):\n    if self.fit_intercept:\n        self.coef_ = self.coef_ / X_scale\n        self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n    else:\n        self.intercept_ = 0.0",
    ".sklearn.linear_model.ridge.py@@Ridge.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None):\n    super().__init__(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, max_iter=max_iter, tol=tol, solver=solver, random_state=random_state)",
    ".sklearn.linear_model.ridge.py@@_BaseRidge.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None):\n    self.alpha = alpha\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.copy_X = copy_X\n    self.max_iter = max_iter\n    self.tol = tol\n    self.solver = solver\n    self.random_state = random_state",
    ".sklearn.model_selection._search.py@@GridSearchCV.__init__": "def __init__(self, estimator, param_grid, scoring=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', error_score='raise-deprecating', return_train_score=False):\n    super().__init__(estimator=estimator, scoring=scoring, n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose, pre_dispatch=pre_dispatch, error_score=error_score, return_train_score=return_train_score)\n    self.param_grid = param_grid\n    _check_param_grid(param_grid)",
    ".sklearn.model_selection._search.py@@BaseSearchCV.__init__": "def __init__(self, estimator, scoring=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', error_score='raise-deprecating', return_train_score=True):\n    self.scoring = scoring\n    self.estimator = estimator\n    self.n_jobs = n_jobs\n    self.iid = iid\n    self.refit = refit\n    self.cv = cv\n    self.verbose = verbose\n    self.pre_dispatch = pre_dispatch\n    self.error_score = error_score\n    self.return_train_score = return_train_score",
    ".sklearn.model_selection._search.py@@_check_param_grid": "def _check_param_grid(param_grid):\n    if hasattr(param_grid, 'items'):\n        param_grid = [param_grid]\n    for p in param_grid:\n        for name, v in p.items():\n            if isinstance(v, np.ndarray) and v.ndim > 1:\n                raise ValueError('Parameter array should be one-dimensional.')\n            if isinstance(v, str) or not isinstance(v, (np.ndarray, Sequence)):\n                raise ValueError('Parameter values for parameter ({0}) need to be a sequence(but not a string) or np.ndarray.'.format(name))\n            if len(v) == 0:\n                raise ValueError('Parameter values for parameter ({0}) need to be a non-empty sequence.'.format(name))",
    ".sklearn.model_selection._search.py@@BaseSearchCV.fit": "def fit(self, X, y=None, groups=None, **fit_params):\n    estimator = self.estimator\n    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n    scorers, self.multimetric_ = _check_multimetric_scoring(self.estimator, scoring=self.scoring)\n    if self.multimetric_:\n        if self.refit is not False and (not isinstance(self.refit, str) or self.refit not in scorers) and (not callable(self.refit)):\n            raise ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. %r was passed.' % self.refit)\n        else:\n            refit_metric = self.refit\n    else:\n        refit_metric = 'score'\n    X, y, groups = indexable(X, y, groups)\n    n_splits = cv.get_n_splits(X, y, groups)\n    base_estimator = clone(self.estimator)\n    parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=self.pre_dispatch)\n    fit_and_score_kwargs = dict(scorer=scorers, fit_params=fit_params, return_train_score=self.return_train_score, return_n_test_samples=True, return_times=True, return_parameters=False, error_score=self.error_score, verbose=self.verbose)\n    results = {}\n    with parallel:\n        all_candidate_params = []\n        all_out = []\n\n        def evaluate_candidates(candidate_params):\n            candidate_params = list(candidate_params)\n            n_candidates = len(candidate_params)\n            if self.verbose > 0:\n                print('Fitting {0} folds for each of {1} candidates, totalling {2} fits'.format(n_splits, n_candidates, n_candidates * n_splits))\n            out = parallel((delayed(_fit_and_score)(clone(base_estimator), X, y, train=train, test=test, parameters=parameters, **fit_and_score_kwargs) for parameters, (train, test) in product(candidate_params, cv.split(X, y, groups))))\n            if len(out) < 1:\n                raise ValueError('No fits were performed. Was the CV iterator empty? Were there no candidates?')\n            elif len(out) != n_candidates * n_splits:\n                raise ValueError('cv.split and cv.get_n_splits returned inconsistent results. Expected {} splits, got {}'.format(n_splits, len(out) // n_candidates))\n            all_candidate_params.extend(candidate_params)\n            all_out.extend(out)\n            nonlocal results\n            results = self._format_results(all_candidate_params, scorers, n_splits, all_out)\n            return results\n        self._run_search(evaluate_candidates)\n    if self.refit or not self.multimetric_:\n        if callable(self.refit):\n            self.best_index_ = self.refit(results)\n            if not isinstance(self.best_index_, (int, np.integer)):\n                raise TypeError('best_index_ returned is not an integer')\n            if self.best_index_ < 0 or self.best_index_ >= len(results['params']):\n                raise IndexError('best_index_ index out of range')\n        else:\n            self.best_index_ = results['rank_test_%s' % refit_metric].argmin()\n            self.best_score_ = results['mean_test_%s' % refit_metric][self.best_index_]\n        self.best_params_ = results['params'][self.best_index_]\n    if self.refit:\n        self.best_estimator_ = clone(base_estimator).set_params(**self.best_params_)\n        refit_start_time = time.time()\n        if y is not None:\n            self.best_estimator_.fit(X, y, **fit_params)\n        else:\n            self.best_estimator_.fit(X, **fit_params)\n        refit_end_time = time.time()\n        self.refit_time_ = refit_end_time - refit_start_time\n    self.scorer_ = scorers if self.multimetric_ else scorers['score']\n    self.cv_results_ = results\n    self.n_splits_ = n_splits\n    return self",
    ".sklearn.base.py@@is_classifier": "def is_classifier(estimator):\n    return getattr(estimator, '_estimator_type', None) == 'classifier'",
    ".sklearn.model_selection._split.py@@check_cv": "def check_cv(cv='warn', y=None, classifier=False):\n    if cv is None or cv == 'warn':\n        warnings.warn(CV_WARNING, FutureWarning)\n        cv = 3\n    if isinstance(cv, numbers.Integral):\n        if classifier and y is not None and (type_of_target(y) in ('binary', 'multiclass')):\n            return StratifiedKFold(cv)\n        else:\n            return KFold(cv)\n    if not hasattr(cv, 'split') or isinstance(cv, str):\n        if not isinstance(cv, Iterable) or isinstance(cv, str):\n            raise ValueError('Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable. Got %s.' % cv)\n        return _CVIterableWrapper(cv)\n    return cv",
    ".sklearn.metrics.scorer.py@@_check_multimetric_scoring": "def _check_multimetric_scoring(estimator, scoring=None):\n    if callable(scoring) or scoring is None or isinstance(scoring, str):\n        scorers = {'score': check_scoring(estimator, scoring=scoring)}\n        return (scorers, False)\n    else:\n        err_msg_generic = 'scoring should either be a single string or callable for single metric evaluation or a list/tuple of strings or a dict of scorer name mapped to the callable for multiple metric evaluation. Got %s of type %s' % (repr(scoring), type(scoring))\n        if isinstance(scoring, (list, tuple, set)):\n            err_msg = 'The list/tuple elements must be unique strings of predefined scorers. '\n            invalid = False\n            try:\n                keys = set(scoring)\n            except TypeError:\n                invalid = True\n            if invalid:\n                raise ValueError(err_msg)\n            if len(keys) != len(scoring):\n                raise ValueError(err_msg + 'Duplicate elements were found in the given list. %r' % repr(scoring))\n            elif len(keys) > 0:\n                if not all((isinstance(k, str) for k in keys)):\n                    if any((callable(k) for k in keys)):\n                        raise ValueError(err_msg + 'One or more of the elements were callables. Use a dict of score name mapped to the scorer callable. Got %r' % repr(scoring))\n                    else:\n                        raise ValueError(err_msg + 'Non-string types were found in the given list. Got %r' % repr(scoring))\n                scorers = {scorer: check_scoring(estimator, scoring=scorer) for scorer in scoring}\n            else:\n                raise ValueError(err_msg + 'Empty list was given. %r' % repr(scoring))\n        elif isinstance(scoring, dict):\n            keys = set(scoring)\n            if not all((isinstance(k, str) for k in keys)):\n                raise ValueError('Non-string types were found in the keys of the given dict. scoring=%r' % repr(scoring))\n            if len(keys) == 0:\n                raise ValueError('An empty dict was passed. %r' % repr(scoring))\n            scorers = {key: check_scoring(estimator, scoring=scorer) for key, scorer in scoring.items()}\n        else:\n            raise ValueError(err_msg_generic)\n        return (scorers, True)",
    ".sklearn.utils.validation.py@@indexable": "def indexable(*iterables):\n    result = []\n    for X in iterables:\n        if sp.issparse(X):\n            result.append(X.tocsr())\n        elif hasattr(X, '__getitem__') or hasattr(X, 'iloc'):\n            result.append(X)\n        elif X is None:\n            result.append(X)\n        else:\n            result.append(np.array(X))\n    check_consistent_length(*result)\n    return result",
    ".sklearn.model_selection._split.py@@_BaseKFold.get_n_splits": "def get_n_splits(self, X=None, y=None, groups=None):\n    return self.n_splits",
    ".sklearn.base.py@@clone": "def clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\" % (repr(estimator), type(estimator)))\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError('Cannot clone object %s, as the constructor either does not set or modifies parameter %s' % (estimator, name))\n    return new_object",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key, None)\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.model_selection._search.py@@GridSearchCV._run_search": "def _run_search(self, evaluate_candidates):\n    evaluate_candidates(ParameterGrid(self.param_grid))",
    ".sklearn.model_selection._search.py@@ParameterGrid.__init__": "def __init__(self, param_grid):\n    if not isinstance(param_grid, (Mapping, Iterable)):\n        raise TypeError('Parameter grid is not a dict or a list ({!r})'.format(param_grid))\n    if isinstance(param_grid, Mapping):\n        param_grid = [param_grid]\n    for grid in param_grid:\n        if not isinstance(grid, dict):\n            raise TypeError('Parameter grid is not a dict ({!r})'.format(grid))\n        for key in grid:\n            if not isinstance(grid[key], Iterable):\n                raise TypeError('Parameter grid value is not iterable (key={!r}, value={!r})'.format(key, grid[key]))\n    self.param_grid = param_grid",
    ".sklearn.model_selection._search.py@@BaseSearchCV.evaluate_candidates": "def evaluate_candidates(candidate_params):\n    candidate_params = list(candidate_params)\n    n_candidates = len(candidate_params)\n    if self.verbose > 0:\n        print('Fitting {0} folds for each of {1} candidates, totalling {2} fits'.format(n_splits, n_candidates, n_candidates * n_splits))\n    out = parallel((delayed(_fit_and_score)(clone(base_estimator), X, y, train=train, test=test, parameters=parameters, **fit_and_score_kwargs) for parameters, (train, test) in product(candidate_params, cv.split(X, y, groups))))\n    if len(out) < 1:\n        raise ValueError('No fits were performed. Was the CV iterator empty? Were there no candidates?')\n    elif len(out) != n_candidates * n_splits:\n        raise ValueError('cv.split and cv.get_n_splits returned inconsistent results. Expected {} splits, got {}'.format(n_splits, len(out) // n_candidates))\n    all_candidate_params.extend(candidate_params)\n    all_out.extend(out)\n    nonlocal results\n    results = self._format_results(all_candidate_params, scorers, n_splits, all_out)\n    return results",
    ".sklearn.model_selection._search.py@@ParameterGrid.__len__": "def __len__(self):\n    product = partial(reduce, operator.mul)\n    return sum((product((len(v) for v in p.values())) if p else 1 for p in self.param_grid))",
    ".sklearn.model_selection._search.py@@ParameterGrid.__iter__": "def __iter__(self):\n    for p in self.param_grid:\n        items = sorted(p.items())\n        if not items:\n            yield {}\n        else:\n            keys, values = zip(*items)\n            for v in product(*values):\n                params = dict(zip(keys, v))\n                yield params",
    ".sklearn.model_selection._split.py@@_BaseKFold.split": "def split(self, X, y=None, groups=None):\n    X, y, groups = indexable(X, y, groups)\n    n_samples = _num_samples(X)\n    if self.n_splits > n_samples:\n        raise ValueError('Cannot have number of splits n_splits={0} greater than the number of samples: n_samples={1}.'.format(self.n_splits, n_samples))\n    for train, test in super().split(X, y, groups):\n        yield (train, test)",
    ".sklearn.model_selection._split.py@@BaseCrossValidator.split": "def split(self, X, y=None, groups=None):\n    X, y, groups = indexable(X, y, groups)\n    indices = np.arange(_num_samples(X))\n    for test_index in self._iter_test_masks(X, y, groups):\n        train_index = indices[np.logical_not(test_index)]\n        test_index = indices[test_index]\n        yield (train_index, test_index)",
    ".sklearn.model_selection._split.py@@BaseCrossValidator._iter_test_masks": "def _iter_test_masks(self, X=None, y=None, groups=None):\n    for test_index in self._iter_test_indices(X, y, groups):\n        test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n        test_mask[test_index] = True\n        yield test_mask",
    ".sklearn.model_selection._split.py@@KFold._iter_test_indices": "def _iter_test_indices(self, X, y=None, groups=None):\n    n_samples = _num_samples(X)\n    indices = np.arange(n_samples)\n    if self.shuffle:\n        check_random_state(self.random_state).shuffle(indices)\n    n_splits = self.n_splits\n    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n    fold_sizes[:n_samples % n_splits] += 1\n    current = 0\n    for fold_size in fold_sizes:\n        start, stop = (current, current + fold_size)\n        yield indices[start:stop]\n        current = stop",
    ".sklearn.model_selection._validation.py@@_fit_and_score": "def _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=False, return_estimator=False, error_score='raise-deprecating'):\n    if verbose > 1:\n        if parameters is None:\n            msg = ''\n        else:\n            msg = '%s' % ', '.join(('%s=%s' % (k, v) for k, v in parameters.items()))\n        print('[CV] %s %s' % (msg, (64 - len(msg)) * '.'))\n    fit_params = fit_params if fit_params is not None else {}\n    fit_params = {k: _index_param_value(X, v, train) for k, v in fit_params.items()}\n    train_scores = {}\n    if parameters is not None:\n        estimator.set_params(**parameters)\n    start_time = time.time()\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n    is_multimetric = not callable(scorer)\n    n_scorers = len(scorer.keys()) if is_multimetric else 1\n    try:\n        if y_train is None:\n            estimator.fit(X_train, **fit_params)\n        else:\n            estimator.fit(X_train, y_train, **fit_params)\n    except Exception as e:\n        fit_time = time.time() - start_time\n        score_time = 0.0\n        if error_score == 'raise':\n            raise\n        elif error_score == 'raise-deprecating':\n            warnings.warn(\"From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\", FutureWarning)\n            raise\n        elif isinstance(error_score, numbers.Number):\n            if is_multimetric:\n                test_scores = dict(zip(scorer.keys(), [error_score] * n_scorers))\n                if return_train_score:\n                    train_scores = dict(zip(scorer.keys(), [error_score] * n_scorers))\n            else:\n                test_scores = error_score\n                if return_train_score:\n                    train_scores = error_score\n            warnings.warn('Estimator fit failed. The score on this train-test partition for these parameters will be set to %f. Details: \\n%s' % (error_score, format_exception_only(type(e), e)[0]), FitFailedWarning)\n        else:\n            raise ValueError(\"error_score must be the string 'raise' or a numeric value. (Hint: if using 'raise', please make sure that it has been spelled correctly.)\")\n    else:\n        fit_time = time.time() - start_time\n        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n        score_time = time.time() - start_time - fit_time\n        if return_train_score:\n            train_scores = _score(estimator, X_train, y_train, scorer, is_multimetric)\n    if verbose > 2:\n        if is_multimetric:\n            for scorer_name in sorted(test_scores):\n                msg += ', %s=' % scorer_name\n                if return_train_score:\n                    msg += '(train=%.3f,' % train_scores[scorer_name]\n                    msg += ' test=%.3f)' % test_scores[scorer_name]\n                else:\n                    msg += '%.3f' % test_scores[scorer_name]\n        else:\n            msg += ', score='\n            msg += '%.3f' % test_scores if not return_train_score else '(train=%.3f, test=%.3f)' % (train_scores, test_scores)\n    if verbose > 1:\n        total_time = score_time + fit_time\n        print(_message_with_time('CV', msg, total_time))\n    ret = [train_scores, test_scores] if return_train_score else [test_scores]\n    if return_n_test_samples:\n        ret.append(_num_samples(X_test))\n    if return_times:\n        ret.extend([fit_time, score_time])\n    if return_parameters:\n        ret.append(parameters)\n    if return_estimator:\n        ret.append(estimator)\n    return ret",
    ".sklearn.model_selection._validation.py@@_index_param_value": "def _index_param_value(X, v, indices):\n    if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):\n        return v\n    if sp.issparse(v):\n        v = v.tocsr()\n    return safe_indexing(v, indices)",
    ".sklearn.utils.validation.py@@_is_arraylike": "def _is_arraylike(x):\n    return hasattr(x, '__len__') or hasattr(x, 'shape') or hasattr(x, '__array__')",
    ".sklearn.base.py@@BaseEstimator.set_params": "def set_params(self, **params):\n    if not params:\n        return self\n    valid_params = self.get_params(deep=True)\n    nested_params = defaultdict(dict)\n    for key, value in params.items():\n        key, delim, sub_key = key.partition('__')\n        if key not in valid_params:\n            raise ValueError('Invalid parameter %s for estimator %s. Check the list of available parameters with `estimator.get_params().keys()`.' % (key, self))\n        if delim:\n            nested_params[key][sub_key] = value\n        else:\n            setattr(self, key, value)\n            valid_params[key] = value\n    for key, sub_params in nested_params.items():\n        valid_params[key].set_params(**sub_params)\n    return self",
    ".sklearn.utils.metaestimators.py@@_safe_split": "def _safe_split(estimator, X, y, indices, train_indices=None):\n    if getattr(estimator, '_pairwise', False):\n        if not hasattr(X, 'shape'):\n            raise ValueError('Precomputed kernels or affinity matrices have to be passed as arrays or sparse matrices.')\n        if X.shape[0] != X.shape[1]:\n            raise ValueError('X should be a square kernel matrix')\n        if train_indices is None:\n            X_subset = X[np.ix_(indices, indices)]\n        else:\n            X_subset = X[np.ix_(indices, train_indices)]\n    else:\n        X_subset = safe_indexing(X, indices)\n    if y is not None:\n        y_subset = safe_indexing(y, indices)\n    else:\n        y_subset = None\n    return (X_subset, y_subset)",
    ".sklearn.utils.__init__.py@@safe_indexing": "def safe_indexing(X, indices):\n    if hasattr(X, 'iloc'):\n        indices = indices if indices.flags.writeable else indices.copy()\n        try:\n            return X.iloc[indices]\n        except ValueError:\n            warnings.warn('Copying input dataframe for slicing.', DataConversionWarning)\n            return X.copy().iloc[indices]\n    elif hasattr(X, 'shape'):\n        if hasattr(X, 'take') and (hasattr(indices, 'dtype') and indices.dtype.kind == 'i'):\n            return X.take(indices, axis=0)\n        else:\n            return X[indices]\n    else:\n        return [X[idx] for idx in indices]",
    ".sklearn.linear_model.ridge.py@@Ridge.fit": "def fit(self, X, y, sample_weight=None):\n    return super().fit(X, y, sample_weight=sample_weight)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._pre_compute_svd": "def _pre_compute_svd(self, X, y, centered_kernel=True):\n    if sparse.issparse(X):\n        raise TypeError('SVD not supported for sparse matrices')\n    if centered_kernel:\n        X = np.hstack((X, np.ones((X.shape[0], 1))))\n    U, s, _ = linalg.svd(X, full_matrices=0)\n    v = s ** 2\n    UT_y = np.dot(U.T, y)\n    return (v, U, UT_y)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._errors_svd": "def _errors_svd(self, alpha, y, v, U, UT_y):\n    G_diag, c = self._errors_and_values_svd_helper(alpha, y, v, U, UT_y)\n    return ((c / G_diag) ** 2, c)",
    ".sklearn.linear_model.ridge.py@@_RidgeGCV._errors_and_values_svd_helper": "def _errors_and_values_svd_helper(self, alpha, y, v, U, UT_y):\n    constant_column = np.var(U, 0) < 1e-12\n    w = (v + alpha) ** (-1) - alpha ** (-1)\n    w[constant_column] = -alpha ** (-1)\n    c = np.dot(U, self._diag_dot(w, UT_y)) + alpha ** (-1) * y\n    G_diag = self._decomp_diag(w, U) + alpha ** (-1)\n    if len(y.shape) != 1:\n        G_diag = G_diag[:, np.newaxis]\n    return (G_diag, c)",
    ".sklearn.utils.class_weight.py@@compute_sample_weight": "def compute_sample_weight(class_weight, y, indices=None):\n    y = np.atleast_1d(y)\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n    n_outputs = y.shape[1]\n    if isinstance(class_weight, str):\n        if class_weight not in ['balanced']:\n            raise ValueError('The only valid preset for class_weight is \"balanced\". Given \"%s\".' % class_weight)\n    elif indices is not None and (not isinstance(class_weight, str)):\n        raise ValueError('The only valid class_weight for subsampling is \"balanced\". Given \"%s\".' % class_weight)\n    elif n_outputs > 1:\n        if not hasattr(class_weight, '__iter__') or isinstance(class_weight, dict):\n            raise ValueError('For multi-output, class_weight should be a list of dicts, or a valid string.')\n        if len(class_weight) != n_outputs:\n            raise ValueError('For multi-output, number of elements in class_weight should match number of outputs.')\n    expanded_class_weight = []\n    for k in range(n_outputs):\n        y_full = y[:, k]\n        classes_full = np.unique(y_full)\n        classes_missing = None\n        if class_weight == 'balanced' or n_outputs == 1:\n            class_weight_k = class_weight\n        else:\n            class_weight_k = class_weight[k]\n        if indices is not None:\n            y_subsample = y[indices, k]\n            classes_subsample = np.unique(y_subsample)\n            weight_k = np.take(compute_class_weight(class_weight_k, classes_subsample, y_subsample), np.searchsorted(classes_subsample, classes_full), mode='clip')\n            classes_missing = set(classes_full) - set(classes_subsample)\n        else:\n            weight_k = compute_class_weight(class_weight_k, classes_full, y_full)\n        weight_k = weight_k[np.searchsorted(classes_full, y_full)]\n        if classes_missing:\n            weight_k[np.in1d(y_full, list(classes_missing))] = 0.0\n        expanded_class_weight.append(weight_k)\n    expanded_class_weight = np.prod(expanded_class_weight, axis=0, dtype=np.float64)\n    return expanded_class_weight",
    ".sklearn.utils.class_weight.py@@compute_class_weight": "def compute_class_weight(class_weight, classes, y):\n    from ..preprocessing import LabelEncoder\n    if set(y) - set(classes):\n        raise ValueError('classes should include all valid labels that can be in y')\n    if class_weight is None or len(class_weight) == 0:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n    elif class_weight == 'balanced':\n        le = LabelEncoder()\n        y_ind = le.fit_transform(y)\n        if not all(np.in1d(classes, le.classes_)):\n            raise ValueError('classes should have valid labels that are in y')\n        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n        weight = recip_freq[le.transform(classes)]\n    else:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n        if not isinstance(class_weight, dict):\n            raise ValueError(\"class_weight must be dict, 'balanced', or None, got: %r\" % class_weight)\n        for c in class_weight:\n            i = np.searchsorted(classes, c)\n            if i >= len(classes) or classes[i] != c:\n                raise ValueError('Class label {} not present.'.format(c))\n            else:\n                weight[i] = class_weight[c]\n    return weight",
    ".sklearn.preprocessing.label.py@@LabelEncoder.fit_transform": "def fit_transform(self, y):\n    y = column_or_1d(y, warn=True)\n    self.classes_, y = _encode(y, encode=True)\n    return y",
    ".sklearn.preprocessing.label.py@@_encode": "def _encode(values, uniques=None, encode=False):\n    if values.dtype == object:\n        try:\n            res = _encode_python(values, uniques, encode)\n        except TypeError:\n            raise TypeError('argument must be a string or number')\n        return res\n    else:\n        return _encode_numpy(values, uniques, encode)",
    ".sklearn.preprocessing.label.py@@_encode_numpy": "def _encode_numpy(values, uniques=None, encode=False):\n    if uniques is None:\n        if encode:\n            uniques, encoded = np.unique(values, return_inverse=True)\n            return (uniques, encoded)\n        else:\n            return np.unique(values)\n    if encode:\n        diff = _encode_check_unknown(values, uniques)\n        if diff:\n            raise ValueError('y contains previously unseen labels: %s' % str(diff))\n        encoded = np.searchsorted(uniques, values)\n        return (uniques, encoded)\n    else:\n        return uniques",
    ".sklearn.preprocessing.label.py@@LabelEncoder.transform": "def transform(self, y):\n    check_is_fitted(self, 'classes_')\n    y = column_or_1d(y, warn=True)\n    if _num_samples(y) == 0:\n        return np.array([])\n    _, y = _encode(y, uniques=self.classes_, encode=True)\n    return y",
    ".sklearn.preprocessing.label.py@@_encode_check_unknown": "def _encode_check_unknown(values, uniques, return_mask=False):\n    if values.dtype == object:\n        uniques_set = set(uniques)\n        diff = list(set(values) - uniques_set)\n        if return_mask:\n            if diff:\n                valid_mask = np.array([val in uniques_set for val in values])\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return (diff, valid_mask)\n        else:\n            return diff\n    else:\n        unique_values = np.unique(values)\n        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))\n        if return_mask:\n            if diff:\n                valid_mask = np.in1d(values, uniques)\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return (diff, valid_mask)\n        else:\n            return diff",
    ".sklearn.linear_model.base.py@@_rescale_data": "def _rescale_data(X, y, sample_weight):\n    n_samples = X.shape[0]\n    sample_weight = np.full(n_samples, sample_weight, dtype=np.array(sample_weight).dtype)\n    sample_weight = np.sqrt(sample_weight)\n    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n    X = safe_sparse_dot(sw_matrix, X)\n    y = safe_sparse_dot(sw_matrix, y)\n    return (X, y)",
    ".sklearn.utils.multiclass.py@@_unique_indicator": "def _unique_indicator(y):\n    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])"
}