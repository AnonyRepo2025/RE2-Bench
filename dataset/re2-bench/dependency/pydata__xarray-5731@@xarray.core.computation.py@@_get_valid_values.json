{
    ".xarray.core.common.py@@DataWithCoords.isnull": "def isnull(self, keep_attrs: bool=None):\n    from .computation import apply_ufunc\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    return apply_ufunc(duck_array_ops.isnull, self, dask='allowed', keep_attrs=keep_attrs)",
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default):\n    return _get_boolean_with_default('keep_attrs', default)",
    ".xarray.core.options.py@@_get_boolean_with_default": "def _get_boolean_with_default(option, default):\n    global_choice = OPTIONS[option]\n    if global_choice == 'default':\n        return default\n    elif global_choice in [True, False]:\n        return global_choice\n    else:\n        raise ValueError(f\"The global option {option} must be one of True, False or 'default'.\")",
    ".xarray.core.computation.py@@apply_ufunc": "def apply_ufunc(func: Callable, *args: Any, input_core_dims: Sequence[Sequence]=None, output_core_dims: Optional[Sequence[Sequence]]=((),), exclude_dims: AbstractSet=frozenset(), vectorize: bool=False, join: str='exact', dataset_join: str='exact', dataset_fill_value: object=_NO_FILL_VALUE, keep_attrs: Union[bool, str]=None, kwargs: Mapping=None, dask: str='forbidden', output_dtypes: Sequence=None, output_sizes: Mapping[Any, int]=None, meta: Any=None, dask_gufunc_kwargs: Dict[str, Any]=None) -> Any:\n    from .dataarray import DataArray\n    from .groupby import GroupBy\n    from .variable import Variable\n    if input_core_dims is None:\n        input_core_dims = ((),) * len(args)\n    elif len(input_core_dims) != len(args):\n        raise ValueError(f'input_core_dims must be None or a tuple with the length same to the number of arguments. Given {len(input_core_dims)} input_core_dims: {input_core_dims},  but number of args is {len(args)}.')\n    if kwargs is None:\n        kwargs = {}\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n    if exclude_dims:\n        if not isinstance(exclude_dims, set):\n            raise TypeError(f\"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead.\")\n        if not exclude_dims <= signature.all_core_dims:\n            raise ValueError(f'each dimension in `exclude_dims` must also be a core dimension in the function signature. Please make {exclude_dims - signature.all_core_dims} a core dimension')\n    if dask == 'parallelized':\n        if dask_gufunc_kwargs is None:\n            dask_gufunc_kwargs = {}\n        else:\n            dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n        if meta is not None:\n            warnings.warn('``meta`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('meta', meta)\n        if output_sizes is not None:\n            warnings.warn('``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('output_sizes', output_sizes)\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    if isinstance(keep_attrs, bool):\n        keep_attrs = 'override' if keep_attrs else 'drop'\n    variables_vfunc = functools.partial(apply_variable_ufunc, func, signature=signature, exclude_dims=exclude_dims, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n    if any((isinstance(a, GroupBy) for a in args)):\n        this_apply = functools.partial(apply_ufunc, func, input_core_dims=input_core_dims, output_core_dims=output_core_dims, exclude_dims=exclude_dims, join=join, dataset_join=dataset_join, dataset_fill_value=dataset_fill_value, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n        return apply_groupby_func(this_apply, *args)\n    elif any((is_dict_like(a) for a in args)):\n        return apply_dataset_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, dataset_join=dataset_join, fill_value=dataset_fill_value, keep_attrs=keep_attrs)\n    elif any((isinstance(a, DataArray) for a in args)):\n        return apply_dataarray_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, keep_attrs=keep_attrs)\n    elif any((isinstance(a, Variable) for a in args)):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)",
    ".xarray.core.computation.py@@_UFuncSignature.__init__": "def __init__(self, input_core_dims, output_core_dims=((),)):\n    self.input_core_dims = tuple((tuple(a) for a in input_core_dims))\n    self.output_core_dims = tuple((tuple(a) for a in output_core_dims))\n    self._all_input_core_dims = None\n    self._all_output_core_dims = None\n    self._all_core_dims = None",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> bool:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.common.py@@AttrAccessMixin.__getattr__": "def __getattr__(self, name: str) -> Any:\n    if name not in {'__dict__', '__setstate__'}:\n        for source in self._attr_sources:\n            with suppress(KeyError):\n                return source[name]\n    raise AttributeError('{!r} object has no attribute {!r}'.format(type(self).__name__, name))",
    ".xarray.core.dataarray.py@@DataArray._attr_sources": "def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield from self._item_sources\n    yield self.attrs",
    ".xarray.core.dataarray.py@@DataArray._item_sources": "def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n    yield HybridMappingProxy(keys=self.dims, mapping={})\n    yield HybridMappingProxy(keys=self._level_coords, mapping={})",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: 'DataArray'):\n    self._data = dataarray",
    ".xarray.core.utils.py@@HybridMappingProxy.__init__": "def __init__(self, keys: Collection[K], mapping: Mapping[K, V]):\n    self._keys = keys\n    self.mapping = mapping",
    ".xarray.core.utils.py@@HybridMappingProxy.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> 'DataArray':\n    return self._data._getitem_coord(key)",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self, key):\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, self._level_coords, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> Tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self):\n    return self._dims",
    ".xarray.core.dataarray.py@@DataArray.shape": "def shape(self) -> Tuple[int, ...]:\n    return self.variable.shape",
    ".xarray.core.variable.py@@Variable.shape": "def shape(self):\n    return self._data.shape",
    ".xarray.core.dataarray.py@@DataArray._level_coords": "def _level_coords(self) -> Dict[Hashable, Hashable]:\n    level_coords: Dict[Hashable, Hashable] = {}\n    for cname, var in self._coords.items():\n        if var.ndim == 1 and isinstance(var, IndexVariable):\n            level_names = var.level_names\n            if level_names is not None:\n                dim, = var.dims\n                level_coords.update({lname: dim for lname in level_names})\n    return level_coords",
    ".xarray.core.dataset.py@@_get_virtual_variable": "def _get_virtual_variable(variables, key: Hashable, level_vars: Mapping=None, dim_sizes: Mapping=None) -> Tuple[Hashable, Hashable, Variable]:\n    if level_vars is None:\n        level_vars = {}\n    if dim_sizes is None:\n        dim_sizes = {}\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return (key, key, variable)\n    if not isinstance(key, str):\n        raise KeyError(key)\n    split_key = key.split('.', 1)\n    var_name: Optional[str]\n    if len(split_key) == 2:\n        ref_name, var_name = split_key\n    elif len(split_key) == 1:\n        ref_name, var_name = (key, None)\n    else:\n        raise KeyError(key)\n    if ref_name in level_vars:\n        dim_var = variables[level_vars[ref_name]]\n        ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n    else:\n        ref_var = variables[ref_name]\n    if var_name is None:\n        virtual_var = ref_var\n        var_name = key\n    else:\n        if _contains_datetime_like_objects(ref_var):\n            ref_var = xr.DataArray(ref_var)\n            data = getattr(ref_var.dt, var_name).data\n        else:\n            data = getattr(ref_var, var_name).data\n        virtual_var = Variable(ref_var.dims, data)\n    return (ref_name, var_name, virtual_var)",
    ".xarray.core.dataarray.py@@DataArray.attrs": "def attrs(self) -> Dict[Any, Any]:\n    return self.variable.attrs",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> Dict[Hashable, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.computation.py@@apply_dataarray_vfunc": "def apply_dataarray_vfunc(func, *args, signature, join='inner', exclude_dims=frozenset(), keep_attrs='override'):\n    from .dataarray import DataArray\n    if len(args) > 1:\n        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)\n    objs = _all_of_type(args, DataArray)\n    if keep_attrs == 'drop':\n        name = result_name(args)\n    else:\n        first_obj = _first_of_type(args, DataArray)\n        name = first_obj.name\n    result_coords = build_output_coords(args, signature, exclude_dims, combine_attrs=keep_attrs)\n    data_vars = [getattr(a, 'variable', a) for a in args]\n    result_var = func(*data_vars)\n    if signature.num_outputs > 1:\n        out = tuple((DataArray(variable, coords, name=name, fastpath=True) for variable, coords in zip(result_var, result_coords)))\n    else:\n        coords, = result_coords\n        out = DataArray(result_var, coords, name=name, fastpath=True)\n    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)\n    if isinstance(out, tuple):\n        for da in out:\n            da.attrs = attrs\n    else:\n        out.attrs = attrs\n    return out",
    ".xarray.core.computation.py@@_all_of_type": "def _all_of_type(args, kind):\n    return [arg for arg in args if isinstance(arg, kind)]",
    ".xarray.core.computation.py@@result_name": "def result_name(objects: list) -> Any:\n    names = {getattr(obj, 'name', _DEFAULT_NAME) for obj in objects}\n    names.discard(_DEFAULT_NAME)\n    if len(names) == 1:\n        name, = names\n    else:\n        name = None\n    return name",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Optional[Hashable]:\n    return self._name",
    ".xarray.core.utils.py@@ReprObject.__hash__": "def __hash__(self) -> int:\n    return hash((type(self), self._value))",
    ".xarray.core.computation.py@@build_output_coords": "def build_output_coords(args: list, signature: _UFuncSignature, exclude_dims: AbstractSet=frozenset(), combine_attrs: str='override') -> 'List[Dict[Any, Variable]]':\n    coords_list = _get_coords_list(args)\n    if len(coords_list) == 1 and (not exclude_dims):\n        unpacked_coords, = coords_list\n        merged_vars = dict(unpacked_coords.variables)\n    else:\n        merged_vars, unused_indexes = merge_coordinates_without_align(coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs)\n    output_coords = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered = {k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)}\n        else:\n            filtered = merged_vars\n        output_coords.append(filtered)\n    return output_coords",
    ".xarray.core.computation.py@@_get_coords_list": "def _get_coords_list(args) -> List[Coordinates]:\n    coords_list = []\n    for arg in args:\n        try:\n            coords = arg.coords\n        except AttributeError:\n            pass\n        else:\n            coords_list.append(coords)\n    return coords_list",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.computation.py@@_UFuncSignature.all_input_core_dims": "def all_input_core_dims(self):\n    if self._all_input_core_dims is None:\n        self._all_input_core_dims = frozenset((dim for dims in self.input_core_dims for dim in dims))\n    return self._all_input_core_dims",
    ".xarray.core.computation.py@@apply_variable_ufunc": "def apply_variable_ufunc(func, *args, signature, exclude_dims=frozenset(), dask='forbidden', output_dtypes=None, vectorize=False, keep_attrs='override', dask_gufunc_kwargs=None):\n    from .variable import Variable, as_compatible_data\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')), exclude_dims=exclude_dims)\n    broadcast_dims = tuple((dim for dim in dim_sizes if dim not in signature.all_core_dims))\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims) if isinstance(arg, Variable) else arg for arg, core_dims in zip(args, signature.input_core_dims)]\n    if any((is_duck_dask_array(array) for array in input_data)):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an argument, but handling for dask arrays has not been enabled. Either set the ``dask`` argument or load your data into memory first with ``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            numpy_func = func\n            if dask_gufunc_kwargs is None:\n                dask_gufunc_kwargs = {}\n            else:\n                dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n            allow_rechunk = dask_gufunc_kwargs.get('allow_rechunk', None)\n            if allow_rechunk is None:\n                for n, (data, core_dims) in enumerate(zip(input_data, signature.input_core_dims)):\n                    if is_duck_dask_array(data):\n                        for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                            if len(data.chunks[axis]) != 1:\n                                raise ValueError(f\"dimension {dim} on {n}th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.chunk(dict({dim}=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\")\n                dask_gufunc_kwargs['allow_rechunk'] = True\n            output_sizes = dask_gufunc_kwargs.pop('output_sizes', {})\n            if output_sizes:\n                output_sizes_renamed = {}\n                for key, value in output_sizes.items():\n                    if key not in signature.all_output_core_dims:\n                        raise ValueError(f\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\")\n                    output_sizes_renamed[signature.dims_map[key]] = value\n                dask_gufunc_kwargs['output_sizes'] = output_sizes_renamed\n            for key in signature.all_output_core_dims:\n                if key not in signature.all_input_core_dims and key not in output_sizes:\n                    raise ValueError(f\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\")\n\n            def func(*arrays):\n                import dask.array as da\n                res = da.apply_gufunc(numpy_func, signature.to_gufunc_string(exclude_dims), *arrays, vectorize=vectorize, output_dtypes=output_dtypes, **dask_gufunc_kwargs)\n                return res\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in apply_ufunc: {}'.format(dask))\n    elif vectorize:\n        func = _vectorize(func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims)\n    result_data = func(*input_data)\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs:\n        raise ValueError('applied function does not have the number of outputs specified in the ufunc signature. Result is not a tuple of {} elements: {!r}'.format(signature.num_outputs, result_data))\n    objs = _all_of_type(args, Variable)\n    attrs = merge_attrs([obj.attrs for obj in objs], combine_attrs=keep_attrs)\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError(f'applied function returned data with unexpected number of dimensions. Received {data.ndim} dimension(s) but expected {len(dims)} dimensions with names: {dims!r}')\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError('size of dimension {!r} on inputs was unexpectedly changed by applied function from {} to {}. Only dimensions specified in ``exclude_dims`` with xarray.apply_ufunc are allowed to change size.'.format(dim, dim_sizes[dim], new_size))\n        var.attrs = attrs\n        output.append(var)\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)",
    ".xarray.core.computation.py@@unified_dim_sizes": "def unified_dim_sizes(variables: Iterable[Variable], exclude_dims: AbstractSet=frozenset()) -> Dict[Hashable, int]:\n    dim_sizes: Dict[Hashable, int] = {}\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError(f'broadcasting cannot handle duplicate dimensions on a variable: {list(var.dims)}')\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError(f'operands cannot be broadcast together with mismatched lengths for dimension {dim}: {dim_sizes[dim]} vs {size}')\n    return dim_sizes",
    ".xarray.core.computation.py@@_UFuncSignature.all_core_dims": "def all_core_dims(self):\n    if self._all_core_dims is None:\n        self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n    return self._all_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_output_core_dims": "def all_output_core_dims(self):\n    if self._all_output_core_dims is None:\n        self._all_output_core_dims = frozenset((dim for dims in self.output_core_dims for dim in dims))\n    return self._all_output_core_dims",
    ".xarray.core.computation.py@@broadcast_compat_data": "def broadcast_compat_data(variable: Variable, broadcast_dims: Tuple[Hashable, ...], core_dims: Tuple[Hashable, ...]) -> Any:\n    data = variable.data\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n    if new_dims == old_dims:\n        return data\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError('operand to apply_ufunc has required core dimensions {}, but some of these dimensions are absent on an input variable: {}'.format(list(core_dims), missing_core_dims))\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError(f'operand to apply_ufunc encountered unexpected dimensions {unexpected_dims!r} on an input variable: these are core dimensions on other input or output variables')\n    old_broadcast_dims = tuple((d for d in broadcast_dims if d in set_old_dims))\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple((old_dims.index(d) for d in reordered_dims))\n        data = duck_array_ops.transpose(data, order)\n    if new_dims != reordered_dims:\n        key_parts: List[Optional[slice]] = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n    return data",
    ".xarray.core.variable.py@@Variable.data": "def data(self):\n    if is_duck_array(self._data):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.utils.py@@is_duck_array": "def is_duck_array(value: Any) -> bool:\n    if isinstance(value, np.ndarray):\n        return True\n    return hasattr(value, 'ndim') and hasattr(value, 'shape') and hasattr(value, 'dtype') and hasattr(value, '__array_function__') and hasattr(value, '__array_ufunc__')",
    ".xarray.core.pycompat.py@@is_duck_dask_array": "def is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
    ".xarray.core.pycompat.py@@is_dask_collection": "def is_dask_collection(x):\n    if DuckArrayModule('dask').available:\n        from dask.base import is_dask_collection\n        return is_dask_collection(x)\n    else:\n        return False",
    ".xarray.core.pycompat.py@@DuckArrayModule.__init__": "def __init__(self, mod):\n    try:\n        duck_array_module = import_module(mod)\n        duck_array_version = LooseVersion(duck_array_module.__version__)\n        if mod == 'dask':\n            duck_array_type = (import_module('dask.array').Array,)\n        elif mod == 'pint':\n            duck_array_type = (duck_array_module.Quantity,)\n        elif mod == 'cupy':\n            duck_array_type = (duck_array_module.ndarray,)\n        elif mod == 'sparse':\n            duck_array_type = (duck_array_module.SparseArray,)\n        else:\n            raise NotImplementedError\n    except ImportError:\n        duck_array_module = None\n        duck_array_version = LooseVersion('0.0.0')\n        duck_array_type = ()\n    self.module = duck_array_module\n    self.version = duck_array_version\n    self.type = duck_array_type\n    self.available = duck_array_module is not None",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, (np.ndarray, dask_array_type)):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data, xp=np):\n    return data if is_duck_array(data) else xp.asarray(data)",
    ".xarray.core.computation.py@@_UFuncSignature.num_outputs": "def num_outputs(self):\n    return len(self.output_core_dims)",
    ".xarray.core.merge.py@@merge_attrs": "def merge_attrs(variable_attrs, combine_attrs, context=None):\n    if not variable_attrs:\n        return None\n    if callable(combine_attrs):\n        return combine_attrs(variable_attrs, context=context)\n    elif combine_attrs == 'drop':\n        return {}\n    elif combine_attrs == 'override':\n        return dict(variable_attrs[0])\n    elif combine_attrs == 'no_conflicts':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            try:\n                result = compat_dict_union(result, attrs)\n            except ValueError as e:\n                raise MergeError(f\"combine_attrs='no_conflicts', but some values are not the same. Merging {str(result)} with {str(attrs)}\") from e\n        return result\n    elif combine_attrs == 'drop_conflicts':\n        result = {}\n        dropped_keys = set()\n        for attrs in variable_attrs:\n            result.update({key: value for key, value in attrs.items() if key not in result and key not in dropped_keys})\n            result = {key: value for key, value in result.items() if key not in attrs or equivalent(attrs[key], value)}\n            dropped_keys |= {key for key in attrs if key not in result}\n        return result\n    elif combine_attrs == 'identical':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            if not dict_equiv(result, attrs):\n                raise MergeError(f\"combine_attrs='identical', but attrs differ. First is {str(result)} , other is {str(attrs)}.\")\n        return result\n    else:\n        raise ValueError(f'Unrecognised value for combine_attrs={combine_attrs}')",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, Variable):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):\n        data = data.values\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray) and hasattr(data, '__array_function__'):\n        return data\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray) and data.dtype.kind in 'OMm':\n        data = _possibly_convert_objects(data)\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexingAdapter(data)\n    return data",
    ".xarray.core.variable.py@@Variable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    self._data = as_compatible_data(data, fastpath=fastpath)\n    self._dims = self._parse_dimensions(dims)\n    self._attrs = None\n    self._encoding = None\n    if attrs is not None:\n        self.attrs = attrs\n    if encoding is not None:\n        self.encoding = encoding",
    ".xarray.core.variable.py@@Variable._parse_dimensions": "def _parse_dimensions(self, dims):\n    if isinstance(dims, str):\n        dims = (dims,)\n    dims = tuple(dims)\n    if len(dims) != self.ndim:\n        raise ValueError(f'dimensions {dims} must have the same length as the number of data dimensions, ndim={self.ndim}')\n    return dims",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.common.py@@AbstractArray.sizes": "def sizes(self: Any) -> Mapping[Hashable, int]:\n    return Frozen(dict(zip(self.dims, self.shape)))",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Union[Sequence[Tuple], Mapping[Any, Any], None]=None, dims: Union[Hashable, Sequence[Hashable], None]=None, name: Hashable=None, attrs: Mapping=None, indexes: Dict[Hashable, pd.Index]=None, fastpath: bool=False):\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n    else:\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n            elif isinstance(data, pdcompat.Panel):\n                coords = [data.items, data.major_axis, data.minor_axis]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None and (not isinstance(data, PANDAS_TYPES)):\n            attrs = getattr(data, 'attrs', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, fastpath=True)\n        indexes = dict(_extract_indexes_from_coords(coords))\n    self._variable = variable\n    assert isinstance(coords, dict)\n    self._coords = coords\n    self._name = name\n    self._indexes = indexes\n    self._close = None",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '{!r} object has no attribute {!r}'.format(type(self).__name__, name):\n            raise\n        raise AttributeError(f\"cannot set attribute {name!r} on a {type(self).__name__!r} object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\") from e",
    ".xarray.core.arithmetic.py@@SupportsArithmetic.__array_ufunc__": "def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n    from .computation import apply_ufunc\n    out = kwargs.get('out', ())\n    for x in inputs + out:\n        if not isinstance(x, self._HANDLED_TYPES + (SupportsArithmetic,)):\n            return NotImplemented\n    if ufunc.signature is not None:\n        raise NotImplementedError('{} not supported: xarray objects do not directly implement generalized ufuncs. Instead, use xarray.apply_ufunc or explicitly convert to xarray objects to NumPy arrays (e.g., with `.values`).'.format(ufunc))\n    if method != '__call__':\n        raise NotImplementedError('{} method for ufunc {} is not implemented on xarray objects, which currently only support the __call__ method. As an alternative, consider explicitly converting xarray objects to NumPy arrays (e.g., with `.values`).'.format(method, ufunc))\n    if any((isinstance(o, SupportsArithmetic) for o in out)):\n        raise NotImplementedError('xarray objects are not yet supported in the `out` argument for ufuncs. As an alternative, consider explicitly converting xarray objects to NumPy arrays (e.g., with `.values`).')\n    join = dataset_join = OPTIONS['arithmetic_join']\n    return apply_ufunc(ufunc, *inputs, input_core_dims=((),) * ufunc.nin, output_core_dims=((),) * ufunc.nout, join=join, dataset_join=dataset_join, dataset_fill_value=np.nan, kwargs=kwargs, dask='allowed', keep_attrs=_get_keep_attrs(default=True))",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects, join='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.alignment.py@@align": "def align(*objects: 'DataAlignable', join='inner', copy=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA) -> Tuple['DataAlignable', ...]:\n    if indexes is None:\n        indexes = {}\n    if not indexes and len(objects) == 1:\n        obj, = objects\n        return (obj.copy(deep=copy),)\n    all_indexes = defaultdict(list)\n    all_coords = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                all_coords[dim].append(obj.coords[dim])\n                try:\n                    index = obj.xindexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n    if join == 'override':\n        objects = _override_indexes(objects, all_indexes, exclude)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index, _ = PandasIndex.from_pandas_index(safe_cast_to_index(indexes[dim]), dim)\n            if any((not index.equals(other) for other in matching_indexes)) or dim in unlabeled_dim_sizes:\n                joined_indexes[dim] = indexes[dim]\n        elif any((not matching_indexes[0].equals(other) for other in matching_indexes[1:])) or dim in unlabeled_dim_sizes:\n            if join == 'exact':\n                raise ValueError(f'indexes along dimension {dim!r} are not equal')\n            joiner = _get_joiner(join, type(matching_indexes[0]))\n            index = joiner(matching_indexes)\n            index = maybe_coerce_to_str(index.to_pandas_index(), all_coords[dim])\n            joined_indexes[dim] = index\n        else:\n            index = all_coords[dim][0]\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            if isinstance(index, PandasIndex):\n                labeled_size = index.to_pandas_index().size\n            else:\n                labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError(f'arguments without labels along dimension {dim!r} cannot be aligned because they have different dimension size(s) {unlabeled_sizes!r} than the size of the aligned dimension labels: {labeled_size!r}')\n    for dim, sizes in unlabeled_dim_sizes.items():\n        if dim not in all_indexes and len(sizes) > 1:\n            raise ValueError(f'arguments without labels along dimension {dim!r} cannot be aligned because they have different dimension sizes: {sizes!r}')\n    result = []\n    for obj in objects:\n        valid_indexers = {}\n        for k, index in joined_indexes.items():\n            if k in obj.dims:\n                if isinstance(index, Index):\n                    valid_indexers[k] = index.to_pandas_index()\n                else:\n                    valid_indexers[k] = index\n        if not valid_indexers:\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, fill_value=fill_value, indexers=valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n    return tuple(result)",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError(f'{type(self).__name__} objects must be 1-dimensional')\n    if not isinstance(self._data, PandasIndexingAdapter):\n        self._data = PandasIndexingAdapter(self._data)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__init__": "def __init__(self, array: pd.Index, dtype: DTypeLike=None):\n    self.array = utils.safe_cast_to_index(array)\n    if dtype is None:\n        if isinstance(array, pd.PeriodIndex):\n            dtype_ = np.dtype('O')\n        elif hasattr(array, 'categories'):\n            dtype_ = array.categories.dtype\n        elif not utils.is_valid_numpy_dtype(array.dtype):\n            dtype_ = np.dtype('O')\n        else:\n            dtype_ = array.dtype\n    else:\n        dtype_ = np.dtype(dtype)\n    self._dtype = dtype_",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    elif hasattr(array, 'to_pandas_index'):\n        index = array.to_pandas_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.shape": "def shape(self) -> Tuple[int]:\n    return (len(self.array),)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self, variable: Variable, name: Union[Hashable, None, Default]=_default) -> 'DataArray':\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n        indexes = self._indexes\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = {k: v for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))}\n        changed_dims = [k for k in variable.dims if variable.sizes[k] != self.sizes[k]]\n        indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n    else:\n        allowed_dims = set(variable.dims)\n        coords = {k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims}\n        indexes = propagate_indexes(self._indexes, exclude=set(self.dims) - allowed_dims)\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.indexes.py@@propagate_indexes": "def propagate_indexes(indexes: Optional[Dict[Hashable, Index]], exclude: Optional[Any]=None) -> Optional[Dict[Hashable, Index]]:\n    if exclude is None:\n        exclude = ()\n    if is_scalar(exclude):\n        exclude = (exclude,)\n    if indexes is not None:\n        new_indexes = {k: v for k, v in indexes.items() if k not in exclude}\n    else:\n        new_indexes = None\n    return new_indexes",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> TypeGuard[Hashable]:\n    return _is_scalar(value, include_0d)",
    ".xarray.core.utils.py@@_is_scalar": "def _is_scalar(value, include_0d):\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__')))",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self: T_DataArray, variable: Variable=None, coords=None, name: Union[Hashable, None, Default]=_default, indexes=None) -> T_DataArray:\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if name is _default:\n        name = self.name\n    return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.xindexes": "def xindexes(self) -> Indexes:\n    if self._indexes is None:\n        self._indexes = default_indexes(self._coords, self.dims)\n    return Indexes(self._indexes)",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, 'Variable'], dims: Iterable) -> Dict[Hashable, Index]:\n    return {key: coords[key]._to_xindex() for key in dims if key in coords}",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes: Mapping[Any, Union[pd.Index, Index]]) -> None:\n    self._indexes = indexes",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key) -> pd.Index:\n    return self._indexes[key]",
    ".xarray.core.dataarray.py@@DataArray.copy": "def copy(self: T_DataArray, deep: bool=True, data: Any=None) -> T_DataArray:\n    variable = self.variable.copy(deep=deep, data=data)\n    coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n    if self._indexes is None:\n        indexes = self._indexes\n    else:\n        indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n    return self._replace(variable, coords, indexes=indexes)",
    ".xarray.core.variable.py@@Variable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data\n        if isinstance(data, indexing.MemoryCachedArray):\n            data = indexing.MemoryCachedArray(data.array)\n        if deep:\n            data = copy.deepcopy(data)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return self._replace(data=data)",
    ".xarray.core.variable.py@@Variable._replace": "def _replace(self: T_Variable, dims=_default, data=_default, attrs=_default, encoding=_default) -> T_Variable:\n    if dims is _default:\n        dims = copy.copy(self._dims)\n    if data is _default:\n        data = copy.copy(self.data)\n    if attrs is _default:\n        attrs = copy.copy(self._attrs)\n    if encoding is _default:\n        encoding = copy.copy(self._encoding)\n    return type(self)(dims, data, attrs, encoding, fastpath=True)",
    ".xarray.core.dataarray.py@@DataArray.encoding": "def encoding(self) -> Dict[Hashable, Any]:\n    return self.variable.encoding",
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self):\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.computation.py@@_first_of_type": "def _first_of_type(args, kind):\n    for arg in args:\n        if isinstance(arg, kind):\n            return arg\n    raise ValueError('This should be unreachable.')",
    ".xarray.core.merge.py@@merge_coordinates_without_align": "def merge_coordinates_without_align(objects: 'List[Coordinates]', prioritized: Mapping[Any, MergeElement]=None, exclude_dims: AbstractSet=frozenset(), combine_attrs: str='override') -> Tuple[Dict[Hashable, Variable], Dict[Hashable, Index]]:\n    collected = collect_from_coordinates(objects)\n    if exclude_dims:\n        filtered: Dict[Hashable, List[MergeElement]] = {}\n        for name, elements in collected.items():\n            new_elements = [(variable, index) for variable, index in elements if exclude_dims.isdisjoint(variable.dims)]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n    return merge_collected(filtered, prioritized, combine_attrs=combine_attrs)",
    ".xarray.core.merge.py@@collect_from_coordinates": "def collect_from_coordinates(list_of_coords: 'List[Coordinates]') -> Dict[Hashable, List[MergeElement]]:\n    grouped: Dict[Hashable, List[Tuple[Variable, Optional[Index]]]] = {}\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.xindexes\n        for name, variable in variables.items():\n            value = grouped.setdefault(name, [])\n            value.append((variable, indexes.get(name)))\n    return grouped",
    ".xarray.core.coordinates.py@@Coordinates.xindexes": "def xindexes(self) -> Indexes:\n    return self._data.xindexes",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: Dict[Hashable, List[MergeElement]], prioritized: Mapping[Any, MergeElement]=None, compat: str='minimal', combine_attrs='override') -> Tuple[Dict[Hashable, Variable], Dict[Hashable, Index]]:\n    if prioritized is None:\n        prioritized = {}\n    _assert_compat_valid(compat)\n    merged_vars: Dict[Hashable, Variable] = {}\n    merged_indexes: Dict[Hashable, Index] = {}\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for _, other_index in indexed_elements[1:]:\n                    if not index.equals(other_index):\n                        raise MergeError(f'conflicting values for index {name!r} on objects to be combined:\\nfirst value: {index!r}\\nsecond value: {other_index!r}')\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError(f'conflicting attribute values on combined variable {name!r}:\\nfirst value: {variable.attrs!r}\\nsecond value: {other_variable.attrs!r}')\n                merged_vars[name] = variable\n                merged_vars[name].attrs = merge_attrs([var.attrs for var, _ in indexed_elements], combine_attrs=combine_attrs)\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n                if name in merged_vars:\n                    merged_vars[name].attrs = merge_attrs([var.attrs for var in variables], combine_attrs=combine_attrs)\n    return (merged_vars, merged_indexes)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError('compat={!r} invalid: must be {}'.format(compat, set(_VALID_COMPAT)))",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.common.py@@ImplementsArrayReduce.wrapped_func": "def wrapped_func(self, dim=None, axis=None, **kwargs):\n    return self.reduce(func, dim, axis, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reduce": "def reduce(self, func: Callable[..., Any], dim: Union[None, Hashable, Sequence[Hashable]]=None, axis: Union[None, int, Sequence[int]]=None, keep_attrs: bool=None, keepdims: bool=False, **kwargs: Any) -> 'DataArray':\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n    return self._replace_maybe_drop_dims(var)",
    ".xarray.core.variable.py@@Variable.reduce": "def reduce(self, func, dim=None, axis=None, keep_attrs=None, keepdims=False, **kwargs):\n    if dim == ...:\n        dim = None\n    if dim is not None and axis is not None:\n        raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n    if dim is not None:\n        axis = self.get_axis_num(dim)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        if axis is not None:\n            data = func(self.data, axis=axis, **kwargs)\n        else:\n            data = func(self.data, **kwargs)\n    if getattr(data, 'shape', ()) == self.shape:\n        dims = self.dims\n    else:\n        removed_axes = range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n        if keepdims:\n            slices = tuple((np.newaxis if i in removed_axes else slice(None, None) for i in range(self.ndim)))\n            if getattr(data, 'shape', None) is None:\n                data = np.asanyarray(data)[slices]\n            else:\n                data = data[slices]\n            dims = self.dims\n        else:\n            dims = [adim for n, adim in enumerate(self.dims) if n not in removed_axes]\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self._attrs if keep_attrs else None\n    return Variable(dims, data, attrs=attrs)",
    ".xarray.core.common.py@@AbstractArray.__bool__": "def __bool__(self: Any) -> bool:\n    return bool(self.values)",
    ".xarray.core.dataarray.py@@DataArray.values": "def values(self) -> np.ndarray:\n    return self.variable.values",
    ".xarray.core.variable.py@@Variable.values": "def values(self):\n    return _as_array_or_item(self._data)",
    ".xarray.core.variable.py@@_as_array_or_item": "def _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data",
    ".xarray.core.variable.py@@IndexVariable.level_names": "def level_names(self):\n    index = self.to_index()\n    if isinstance(index, pd.MultiIndex):\n        return index.names\n    else:\n        return None",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i) for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self):\n    return self.dims[0]",
    ".xarray.core.variable.py@@Variable._to_xindex": "def _to_xindex(self):\n    index_var = self.to_index_variable()\n    index = index_var.to_index()\n    dim = index_var.dims[0]\n    if isinstance(index, pd.MultiIndex):\n        return PandasMultiIndex(index, dim)\n    else:\n        return PandasIndex(index, dim)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self):\n    return self",
    ".xarray.core.indexes.py@@PandasIndex.__init__": "def __init__(self, array: Any, dim: Hashable):\n    self.index = utils.safe_cast_to_index(array)\n    self.dim = dim",
    ".xarray.core.indexes.py@@PandasIndex.equals": "def equals(self, other):\n    return self.index.equals(other.index)",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep=True, data=None):\n    if data is None:\n        data = self._data.copy(deep=deep)\n    else:\n        data = as_compatible_data(data)\n        if self.shape != data.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(data.shape, self.shape))\n    return self._replace(data=data)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.copy": "def copy(self, deep: bool=True) -> 'PandasIndexingAdapter':\n    array = self.array.copy(deep=True) if deep else self.array\n    return type(self)(array, self._dtype)",
    ".xarray.core.indexes.py@@PandasIndex.copy": "def copy(self, deep=True):\n    return type(self)(self.index.copy(deep=deep), self.dim)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__invert__": "def __invert__(self):\n    return self._unary_op(operator.invert)",
    ".xarray.core.dataarray.py@@DataArray._unary_op": "def _unary_op(self, f: Callable, *args, **kwargs):\n    keep_attrs = kwargs.pop('keep_attrs', None)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=True)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        with np.errstate(all='ignore'):\n            da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n        if keep_attrs:\n            da.attrs = self.attrs\n        return da",
    ".xarray.core.dataarray.py@@DataArray.__array_wrap__": "def __array_wrap__(self, obj, context=None) -> 'DataArray':\n    new_var = self.variable.__array_wrap__(obj, context)\n    return self._replace(new_var)",
    ".xarray.core.variable.py@@Variable.__array_wrap__": "def __array_wrap__(self, obj, context=None):\n    return Variable(self.dims, obj)",
    ".xarray.core.common.py@@DataWithCoords.where": "def where(self, cond, other=dtypes.NA, drop: bool=False):\n    from .alignment import align\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if callable(cond):\n        cond = cond(self)\n    if drop:\n        if other is not dtypes.NA:\n            raise ValueError('cannot set `other` if drop=True')\n        if not isinstance(cond, (Dataset, DataArray)):\n            raise TypeError(f'cond argument is {cond!r} but must be a {Dataset!r} or {DataArray!r}')\n        self, cond = align(self, cond)\n        if isinstance(cond, Dataset):\n            clipcond = cond.to_array().any('variable')\n        else:\n            clipcond = cond\n        nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n        indexers = {k: np.unique(v) for k, v in nonzeros}\n        self = self.isel(**indexers)\n        cond = cond.isel(**indexers)\n    return ops.where_method(self, cond, other)",
    ".xarray.core.ops.py@@where_method": "def where_method(self, cond, other=dtypes.NA):\n    from .computation import apply_ufunc\n    join = 'inner' if other is dtypes.NA else 'exact'\n    return apply_ufunc(duck_array_ops.where_method, self, cond, other, join=join, dataset_join=join, dask='allowed', keep_attrs=True)",
    ".xarray.core.duck_array_ops.py@@where_method": "def where_method(data, cond, other=dtypes.NA):\n    if other is dtypes.NA:\n        other = dtypes.get_fill_value(data.dtype)\n    return where(cond, data, other)",
    ".xarray.core.dtypes.py@@get_fill_value": "def get_fill_value(dtype):\n    _, fill_value = maybe_promote(dtype)\n    return fill_value",
    ".xarray.core.dtypes.py@@maybe_promote": "def maybe_promote(dtype):\n    if np.issubdtype(dtype, np.floating):\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.timedelta64):\n        fill_value = np.timedelta64('NaT')\n    elif np.issubdtype(dtype, np.integer):\n        dtype = np.float32 if dtype.itemsize <= 2 else np.float64\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.complexfloating):\n        fill_value = np.nan + np.nan * 1j\n    elif np.issubdtype(dtype, np.datetime64):\n        fill_value = np.datetime64('NaT')\n    else:\n        dtype = object\n        fill_value = np.nan\n    return (np.dtype(dtype), fill_value)",
    ".xarray.core.duck_array_ops.py@@where": "def where(condition, x, y):\n    return _where(condition, *as_shared_dtype([x, y]))",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    if any((isinstance(x, cupy_array_type) for x in scalars_or_arrays)):\n        import cupy as cp\n        arrays = [asarray(x, xp=cp) for x in scalars_or_arrays]\n    else:\n        arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)"
}