{
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", FutureWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n        if all((isinstance(dtype, np.dtype) for dtype in dtypes_orig)):\n            dtype_orig = np.result_type(*array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)\n                    array = array.astype(dtype, casting='unsafe', copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    y = np.asarray(y)\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, msg_dtype if msg_dtype is not None else X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.base.py@@is_classifier": "def is_classifier(estimator):\n    return getattr(estimator, '_estimator_type', None) == 'classifier'",
    ".sklearn.model_selection._split.py@@check_cv": "def check_cv(cv=5, y=None, classifier=False):\n    cv = 5 if cv is None else cv\n    if isinstance(cv, numbers.Integral):\n        if classifier and y is not None and (type_of_target(y) in ('binary', 'multiclass')):\n            return StratifiedKFold(cv)\n        else:\n            return KFold(cv)\n    if not hasattr(cv, 'split') or isinstance(cv, str):\n        if not isinstance(cv, Iterable) or isinstance(cv, str):\n            raise ValueError('Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable. Got %s.' % cv)\n        return _CVIterableWrapper(cv)\n    return cv",
    ".sklearn.utils.multiclass.py@@type_of_target": "def type_of_target(y):\n    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, str))\n    if not valid:\n        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)\n    sparse_pandas = y.__class__.__name__ in ['SparseSeries', 'SparseArray']\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n    if is_multilabel(y):\n        return 'multilabel-indicator'\n    try:\n        y = np.asarray(y)\n    except ValueError:\n        return 'unknown'\n    try:\n        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):\n            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.')\n    except IndexError:\n        pass\n    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], str))):\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] == 0:\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = '-multioutput'\n    else:\n        suffix = ''\n    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):\n        _assert_all_finite(y)\n        return 'continuous' + suffix\n    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):\n        return 'multiclass' + suffix\n    else:\n        return 'binary'",
    ".sklearn.utils.multiclass.py@@is_multilabel": "def is_multilabel(y):\n    if hasattr(y, '__array__') or isinstance(y, Sequence):\n        y = np.asarray(y)\n    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):\n        return False\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))\n    else:\n        labels = np.unique(y)\n        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))",
    ".sklearn.model_selection._split.py@@StratifiedKFold.__init__": "def __init__(self, n_splits=5, shuffle=False, random_state=None):\n    super().__init__(n_splits, shuffle, random_state)",
    ".sklearn.model_selection._split.py@@_BaseKFold.__init__": "def __init__(self, n_splits, shuffle, random_state):\n    if not isinstance(n_splits, numbers.Integral):\n        raise ValueError('The number of folds must be of Integral type. %s of type %s was passed.' % (n_splits, type(n_splits)))\n    n_splits = int(n_splits)\n    if n_splits <= 1:\n        raise ValueError('k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits={0}.'.format(n_splits))\n    if not isinstance(shuffle, bool):\n        raise TypeError('shuffle must be True or False; got {0}'.format(shuffle))\n    if not shuffle and random_state is not None:\n        warnings.warn('Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.', FutureWarning)\n    self.n_splits = n_splits\n    self.shuffle = shuffle\n    self.random_state = random_state",
    ".sklearn.metrics._scorer.py@@check_scoring": "def check_scoring(estimator, scoring=None, allow_none=False):\n    if not hasattr(estimator, 'fit'):\n        raise TypeError(\"estimator should be an estimator implementing 'fit' method, %r was passed\" % estimator)\n    if isinstance(scoring, str):\n        return get_scorer(scoring)\n    elif callable(scoring):\n        module = getattr(scoring, '__module__', None)\n        if hasattr(module, 'startswith') and module.startswith('sklearn.metrics.') and (not module.startswith('sklearn.metrics._scorer')) and (not module.startswith('sklearn.metrics.tests.')):\n            raise ValueError('scoring value %r looks like it is a metric function rather than a scorer. A scorer should require an estimator as its first parameter. Please use `make_scorer` to convert a metric to a scorer.' % scoring)\n        return get_scorer(scoring)\n    elif scoring is None:\n        if hasattr(estimator, 'score'):\n            return _passthrough_scorer\n        elif allow_none:\n            return None\n        else:\n            raise TypeError(\"If no scoring is specified, the estimator passed should have a 'score' method. The estimator %r does not.\" % estimator)\n    elif isinstance(scoring, Iterable):\n        raise ValueError('For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. {0} was passed.'.format(scoring))\n    else:\n        raise ValueError('scoring value should either be a callable, string or None. %r was passed' % scoring)",
    ".sklearn.feature_selection._rfe.py@@RFE.__init__": "def __init__(self, estimator, n_features_to_select=None, step=1, verbose=0):\n    self.estimator = estimator\n    self.n_features_to_select = n_features_to_select\n    self.step = step\n    self.verbose = verbose",
    ".sklearn.model_selection._split.py@@StratifiedKFold.split": "def split(self, X, y, groups=None):\n    y = check_array(y, ensure_2d=False, dtype=None)\n    return super().split(X, y, groups)",
    ".sklearn.model_selection._split.py@@_BaseKFold.split": "def split(self, X, y=None, groups=None):\n    X, y, groups = indexable(X, y, groups)\n    n_samples = _num_samples(X)\n    if self.n_splits > n_samples:\n        raise ValueError('Cannot have number of splits n_splits={0} greater than the number of samples: n_samples={1}.'.format(self.n_splits, n_samples))\n    for train, test in super().split(X, y, groups):\n        yield (train, test)",
    ".sklearn.utils.validation.py@@indexable": "def indexable(*iterables):\n    result = []\n    for X in iterables:\n        if sp.issparse(X):\n            result.append(X.tocsr())\n        elif hasattr(X, '__getitem__') or hasattr(X, 'iloc'):\n            result.append(X)\n        elif X is None:\n            result.append(X)\n        else:\n            result.append(np.array(X))\n    check_consistent_length(*result)\n    return result",
    ".sklearn.model_selection._split.py@@BaseCrossValidator.split": "def split(self, X, y=None, groups=None):\n    X, y, groups = indexable(X, y, groups)\n    indices = np.arange(_num_samples(X))\n    for test_index in self._iter_test_masks(X, y, groups):\n        train_index = indices[np.logical_not(test_index)]\n        test_index = indices[test_index]\n        yield (train_index, test_index)",
    ".sklearn.model_selection._split.py@@StratifiedKFold._iter_test_masks": "def _iter_test_masks(self, X, y=None, groups=None):\n    test_folds = self._make_test_folds(X, y)\n    for i in range(self.n_splits):\n        yield (test_folds == i)",
    ".sklearn.model_selection._split.py@@StratifiedKFold._make_test_folds": "def _make_test_folds(self, X, y=None):\n    rng = check_random_state(self.random_state)\n    y = np.asarray(y)\n    type_of_target_y = type_of_target(y)\n    allowed_target_types = ('binary', 'multiclass')\n    if type_of_target_y not in allowed_target_types:\n        raise ValueError('Supported target types are: {}. Got {!r} instead.'.format(allowed_target_types, type_of_target_y))\n    y = column_or_1d(y)\n    _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n    _, class_perm = np.unique(y_idx, return_inverse=True)\n    y_encoded = class_perm[y_inv]\n    n_classes = len(y_idx)\n    y_counts = np.bincount(y_encoded)\n    min_groups = np.min(y_counts)\n    if np.all(self.n_splits > y_counts):\n        raise ValueError('n_splits=%d cannot be greater than the number of members in each class.' % self.n_splits)\n    if self.n_splits > min_groups:\n        warnings.warn('The least populated class in y has only %d members, which is less than n_splits=%d.' % (min_groups, self.n_splits), UserWarning)\n    y_order = np.sort(y_encoded)\n    allocation = np.asarray([np.bincount(y_order[i::self.n_splits], minlength=n_classes) for i in range(self.n_splits)])\n    test_folds = np.empty(len(y), dtype='i')\n    for k in range(n_classes):\n        folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n        if self.shuffle:\n            rng.shuffle(folds_for_class)\n        test_folds[y_encoded == k] = folds_for_class\n    return test_folds",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.feature_selection._rfe.py@@_rfe_single_fit": "def _rfe_single_fit(rfe, estimator, X, y, train, test, scorer):\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n    return rfe._fit(X_train, y_train, lambda estimator, features: _score(estimator, X_test[:, features], y_test, scorer)).scores_",
    ".sklearn.utils.metaestimators.py@@_safe_split": "def _safe_split(estimator, X, y, indices, train_indices=None):\n    if getattr(estimator, '_pairwise', False):\n        if not hasattr(X, 'shape'):\n            raise ValueError('Precomputed kernels or affinity matrices have to be passed as arrays or sparse matrices.')\n        if X.shape[0] != X.shape[1]:\n            raise ValueError('X should be a square kernel matrix')\n        if train_indices is None:\n            X_subset = X[np.ix_(indices, indices)]\n        else:\n            X_subset = X[np.ix_(indices, train_indices)]\n    else:\n        X_subset = _safe_indexing(X, indices)\n    if y is not None:\n        y_subset = _safe_indexing(y, indices)\n    else:\n        y_subset = None\n    return (X_subset, y_subset)",
    ".sklearn.svm._base.py@@BaseLibSVM._pairwise": "def _pairwise(self):\n    return self.kernel == 'precomputed'",
    ".sklearn.utils.__init__.py@@_safe_indexing": "def _safe_indexing(X, indices, axis=0):\n    if indices is None:\n        return X\n    if axis not in (0, 1):\n        raise ValueError(\"'axis' should be either 0 (to index rows) or 1 (to index  column). Got {} instead.\".format(axis))\n    indices_dtype = _determine_key_type(indices)\n    if axis == 0 and indices_dtype == 'str':\n        raise ValueError(\"String indexing is not supported with 'axis=0'\")\n    if axis == 1 and X.ndim != 2:\n        raise ValueError(\"'X' should be a 2D NumPy array, 2D sparse matrix or pandas dataframe when indexing the columns (i.e. 'axis=1'). Got {} instead with {} dimension(s).\".format(type(X), X.ndim))\n    if axis == 1 and indices_dtype == 'str' and (not hasattr(X, 'loc')):\n        raise ValueError('Specifying the columns using strings is only supported for pandas DataFrames')\n    if hasattr(X, 'iloc'):\n        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n    elif hasattr(X, 'shape'):\n        return _array_indexing(X, indices, indices_dtype, axis=axis)\n    else:\n        return _list_indexing(X, indices, indices_dtype)",
    ".sklearn.utils.__init__.py@@_determine_key_type": "def _determine_key_type(key):\n    err_msg = 'No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed'\n    dtype_to_str = {int: 'int', str: 'str', bool: 'bool', np.bool_: 'bool'}\n    array_dtype_to_str = {'i': 'int', 'u': 'int', 'b': 'bool', 'O': 'str', 'U': 'str', 'S': 'str'}\n    if key is None:\n        return None\n    if isinstance(key, tuple(dtype_to_str.keys())):\n        try:\n            return dtype_to_str[type(key)]\n        except KeyError:\n            raise ValueError(err_msg)\n    if isinstance(key, slice):\n        if key.start is None and key.stop is None:\n            return None\n        key_start_type = _determine_key_type(key.start)\n        key_stop_type = _determine_key_type(key.stop)\n        if key_start_type is not None and key_stop_type is not None:\n            if key_start_type != key_stop_type:\n                raise ValueError(err_msg)\n        if key_start_type is not None:\n            return key_start_type\n        return key_stop_type\n    if isinstance(key, list):\n        unique_key = set(key)\n        key_type = {_determine_key_type(elt) for elt in unique_key}\n        if not key_type:\n            return None\n        if len(key_type) != 1:\n            raise ValueError(err_msg)\n        return key_type.pop()\n    if hasattr(key, 'dtype'):\n        try:\n            return array_dtype_to_str[key.dtype.kind]\n        except KeyError:\n            raise ValueError(err_msg)\n    raise ValueError(err_msg)",
    ".sklearn.utils.__init__.py@@_array_indexing": "def _array_indexing(array, key, key_dtype, axis):\n    if np_version < (1, 12) or issparse(array):\n        if key_dtype == 'bool':\n            key = np.asarray(key)\n    return array[key] if axis == 0 else array[:, key]",
    ".sklearn.feature_selection._rfe.py@@RFE._fit": "def _fit(self, X, y, step_score=None):\n    tags = self._get_tags()\n    X, y = check_X_y(X, y, 'csc', ensure_min_features=2, force_all_finite=not tags.get('allow_nan', True))\n    n_features = X.shape[1]\n    if self.n_features_to_select is None:\n        n_features_to_select = n_features // 2\n    else:\n        n_features_to_select = self.n_features_to_select\n    if 0.0 < self.step < 1.0:\n        step = int(max(1, self.step * n_features))\n    else:\n        step = int(self.step)\n    if step <= 0:\n        raise ValueError('Step must be >0')\n    support_ = np.ones(n_features, dtype=np.bool)\n    ranking_ = np.ones(n_features, dtype=np.int)\n    if step_score:\n        self.scores_ = []\n    while np.sum(support_) > n_features_to_select:\n        features = np.arange(n_features)[support_]\n        estimator = clone(self.estimator)\n        if self.verbose > 0:\n            print('Fitting estimator with %d features.' % np.sum(support_))\n        estimator.fit(X[:, features], y)\n        if hasattr(estimator, 'coef_'):\n            coefs = estimator.coef_\n        else:\n            coefs = getattr(estimator, 'feature_importances_', None)\n        if coefs is None:\n            raise RuntimeError('The classifier does not expose \"coef_\" or \"feature_importances_\" attributes')\n        if coefs.ndim > 1:\n            ranks = np.argsort(safe_sqr(coefs).sum(axis=0))\n        else:\n            ranks = np.argsort(safe_sqr(coefs))\n        ranks = np.ravel(ranks)\n        threshold = min(step, np.sum(support_) - n_features_to_select)\n        if step_score:\n            self.scores_.append(step_score(estimator, features))\n        support_[features[ranks][:threshold]] = False\n        ranking_[np.logical_not(support_)] += 1\n    features = np.arange(n_features)[support_]\n    self.estimator_ = clone(self.estimator)\n    self.estimator_.fit(X[:, features], y)\n    if step_score:\n        self.scores_.append(step_score(self.estimator_, features))\n    self.n_features_ = support_.sum()\n    self.support_ = support_\n    self.ranking_ = ranking_\n    return self",
    ".sklearn.base.py@@BaseEstimator._get_tags": "def _get_tags(self):\n    collected_tags = {}\n    for base_class in reversed(inspect.getmro(self.__class__)):\n        if hasattr(base_class, '_more_tags'):\n            more_tags = base_class._more_tags(self)\n            collected_tags.update(more_tags)\n    return collected_tags",
    ".sklearn.base.py@@BaseEstimator._more_tags": "def _more_tags(self):\n    return _DEFAULT_TAGS",
    ".sklearn.feature_selection._rfe.py@@RFE._more_tags": "def _more_tags(self):\n    estimator_tags = self.estimator._get_tags()\n    return {'poor_score': True, 'allow_nan': estimator_tags.get('allow_nan', True)}",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.base.py@@clone": "def clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\" % (repr(estimator), type(estimator)))\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError('Cannot clone object %s, as the constructor either does not set or modifies parameter %s' % (estimator, name))\n    return new_object",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        try:\n            value = getattr(self, key)\n        except AttributeError:\n            warnings.warn('From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.', FutureWarning)\n            value = None\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.svm._classes.py@@SVC.__init__": "def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None):\n    super().__init__(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, tol=tol, C=C, nu=0.0, shrinking=shrinking, probability=probability, cache_size=cache_size, class_weight=class_weight, verbose=verbose, max_iter=max_iter, decision_function_shape=decision_function_shape, break_ties=break_ties, random_state=random_state)",
    ".sklearn.svm._base.py@@BaseSVC.__init__": "def __init__(self, kernel, degree, gamma, coef0, tol, C, nu, shrinking, probability, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state, break_ties):\n    self.decision_function_shape = decision_function_shape\n    self.break_ties = break_ties\n    super().__init__(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, tol=tol, C=C, nu=nu, epsilon=0.0, shrinking=shrinking, probability=probability, cache_size=cache_size, class_weight=class_weight, verbose=verbose, max_iter=max_iter, random_state=random_state)",
    ".sklearn.svm._base.py@@BaseLibSVM.__init__": "def __init__(self, kernel, degree, gamma, coef0, tol, C, nu, epsilon, shrinking, probability, cache_size, class_weight, verbose, max_iter, random_state):\n    if self._impl not in LIBSVM_IMPL:\n        raise ValueError('impl should be one of %s, %s was given' % (LIBSVM_IMPL, self._impl))\n    if gamma == 0:\n        msg = \"The gamma value of 0.0 is invalid. Use 'auto' to set gamma to a value of 1 / n_features.\"\n        raise ValueError(msg)\n    self.kernel = kernel\n    self.degree = degree\n    self.gamma = gamma\n    self.coef0 = coef0\n    self.tol = tol\n    self.C = C\n    self.nu = nu\n    self.epsilon = epsilon\n    self.shrinking = shrinking\n    self.probability = probability\n    self.cache_size = cache_size\n    self.class_weight = class_weight\n    self.verbose = verbose\n    self.max_iter = max_iter\n    self.random_state = random_state",
    ".sklearn.svm._base.py@@BaseLibSVM.fit": "def fit(self, X, y, sample_weight=None):\n    rnd = check_random_state(self.random_state)\n    sparse = sp.isspmatrix(X)\n    if sparse and self.kernel == 'precomputed':\n        raise TypeError('Sparse precomputed kernels are not supported.')\n    self._sparse = sparse and (not callable(self.kernel))\n    X, y = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr', accept_large_sparse=False)\n    y = self._validate_targets(y)\n    sample_weight = np.asarray([] if sample_weight is None else sample_weight, dtype=np.float64)\n    solver_type = LIBSVM_IMPL.index(self._impl)\n    if solver_type != 2 and X.shape[0] != y.shape[0]:\n        raise ValueError('X and y have incompatible shapes.\\n' + 'X has %s samples, but y has %s.' % (X.shape[0], y.shape[0]))\n    if self.kernel == 'precomputed' and X.shape[0] != X.shape[1]:\n        raise ValueError('Precomputed matrix must be a square matrix. Input is a {}x{} matrix.'.format(X.shape[0], X.shape[1]))\n    if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:\n        raise ValueError('sample_weight and X have incompatible shapes: %r vs %r\\nNote: Sparse matrices cannot be indexed w/boolean masks (use `indices=True` in CV).' % (sample_weight.shape, X.shape))\n    if isinstance(self.gamma, str):\n        if self.gamma == 'scale':\n            X_var = X.multiply(X).mean() - X.mean() ** 2 if sparse else X.var()\n            self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n        elif self.gamma == 'auto':\n            self._gamma = 1.0 / X.shape[1]\n        else:\n            raise ValueError(\"When 'gamma' is a string, it should be either 'scale' or 'auto'. Got '{}' instead.\".format(self.gamma))\n    else:\n        self._gamma = self.gamma\n    kernel = self.kernel\n    if callable(kernel):\n        kernel = 'precomputed'\n    fit = self._sparse_fit if self._sparse else self._dense_fit\n    if self.verbose:\n        print('[LibSVM]', end='')\n    seed = rnd.randint(np.iinfo('i').max)\n    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n    self.shape_fit_ = X.shape\n    self._intercept_ = self.intercept_.copy()\n    self._dual_coef_ = self.dual_coef_\n    if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:\n        self.intercept_ *= -1\n        self.dual_coef_ = -self.dual_coef_\n    return self",
    ".sklearn.svm._base.py@@BaseSVC._validate_targets": "def _validate_targets(self, y):\n    y_ = column_or_1d(y, warn=True)\n    check_classification_targets(y)\n    cls, y = np.unique(y_, return_inverse=True)\n    self.class_weight_ = compute_class_weight(self.class_weight, cls, y_)\n    if len(cls) < 2:\n        raise ValueError('The number of classes has to be greater than one; got %d class' % len(cls))\n    self.classes_ = cls\n    return np.asarray(y, dtype=np.float64, order='C')",
    ".sklearn.utils.multiclass.py@@check_classification_targets": "def check_classification_targets(y):\n    y_type = type_of_target(y)\n    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', 'multilabel-sequences']:\n        raise ValueError('Unknown label type: %r' % y_type)",
    ".sklearn.utils.class_weight.py@@compute_class_weight": "def compute_class_weight(class_weight, classes, y):\n    from ..preprocessing import LabelEncoder\n    if set(y) - set(classes):\n        raise ValueError('classes should include all valid labels that can be in y')\n    if class_weight is None or len(class_weight) == 0:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n    elif class_weight == 'balanced':\n        le = LabelEncoder()\n        y_ind = le.fit_transform(y)\n        if not all(np.in1d(classes, le.classes_)):\n            raise ValueError('classes should have valid labels that are in y')\n        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n        weight = recip_freq[le.transform(classes)]\n    else:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n        if not isinstance(class_weight, dict):\n            raise ValueError(\"class_weight must be dict, 'balanced', or None, got: %r\" % class_weight)\n        for c in class_weight:\n            i = np.searchsorted(classes, c)\n            if i >= len(classes) or classes[i] != c:\n                raise ValueError('Class label {} not present.'.format(c))\n            else:\n                weight[i] = class_weight[c]\n    return weight",
    ".sklearn.svm._base.py@@BaseLibSVM._dense_fit": "def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n    if callable(self.kernel):\n        self.__Xfit = X\n        X = self._compute_kernel(X)\n        if X.shape[0] != X.shape[1]:\n            raise ValueError('X.shape[0] should be equal to X.shape[1]')\n    libsvm.set_verbosity_wrap(self.verbose)\n    self.support_, self.support_vectors_, self._n_support, self.dual_coef_, self.intercept_, self.probA_, self.probB_, self.fit_status_ = libsvm.fit(X, y, svm_type=solver_type, sample_weight=sample_weight, class_weight=self.class_weight_, kernel=kernel, C=self.C, nu=self.nu, probability=self.probability, degree=self.degree, shrinking=self.shrinking, tol=self.tol, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma, epsilon=self.epsilon, max_iter=self.max_iter, random_seed=random_seed)\n    self._warn_from_fit_status()",
    ".sklearn.svm._base.py@@BaseLibSVM._warn_from_fit_status": "def _warn_from_fit_status(self):\n    assert self.fit_status_ in (0, 1)\n    if self.fit_status_ == 1:\n        warnings.warn('Solver terminated early (max_iter=%i).  Consider pre-processing your data with StandardScaler or MinMaxScaler.' % self.max_iter, ConvergenceWarning)",
    ".sklearn.svm._base.py@@BaseLibSVM.coef_": "def coef_(self):\n    if self.kernel != 'linear':\n        raise AttributeError('coef_ is only available when using a linear kernel')\n    coef = self._get_coef()\n    if sp.issparse(coef):\n        coef.data.flags.writeable = False\n    else:\n        coef.flags.writeable = False\n    return coef",
    ".sklearn.svm._base.py@@BaseSVC._get_coef": "def _get_coef(self):\n    if self.dual_coef_.shape[0] == 1:\n        coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)\n    else:\n        coef = _one_vs_one_coef(self.dual_coef_, self._n_support, self.support_vectors_)\n        if sp.issparse(coef[0]):\n            coef = sp.vstack(coef).tocsr()\n        else:\n            coef = np.vstack(coef)\n    return coef",
    ".sklearn.svm._base.py@@_one_vs_one_coef": "def _one_vs_one_coef(dual_coef, n_support, support_vectors):\n    n_class = dual_coef.shape[0] + 1\n    coef = []\n    sv_locs = np.cumsum(np.hstack([[0], n_support]))\n    for class1 in range(n_class):\n        sv1 = support_vectors[sv_locs[class1]:sv_locs[class1 + 1], :]\n        for class2 in range(class1 + 1, n_class):\n            sv2 = support_vectors[sv_locs[class2]:sv_locs[class2 + 1], :]\n            alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]]\n            alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]\n            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))\n    return coef",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n    if sparse.issparse(a) and sparse.issparse(b) and dense_output and hasattr(ret, 'toarray'):\n        return ret.toarray()\n    return ret",
    ".sklearn.utils.__init__.py@@safe_sqr": "def safe_sqr(X, copy=True):\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False)\n    if issparse(X):\n        if copy:\n            X = X.copy()\n        X.data **= 2\n    elif copy:\n        X = X ** 2\n    else:\n        X **= 2\n    return X",
    ".sklearn.model_selection._validation.py@@_score": "def _score(estimator, X_test, y_test, scorer):\n    if isinstance(scorer, dict):\n        scorer = _MultimetricScorer(**scorer)\n    if y_test is None:\n        scores = scorer(estimator, X_test)\n    else:\n        scores = scorer(estimator, X_test, y_test)\n    error_msg = 'scoring must return a number, got %s (%s) instead. (scorer=%s)'\n    if isinstance(scores, dict):\n        for name, score in scores.items():\n            if hasattr(score, 'item'):\n                with suppress(ValueError):\n                    score = score.item()\n            if not isinstance(score, numbers.Number):\n                raise ValueError(error_msg % (score, type(score), name))\n            scores[name] = score\n    else:\n        if hasattr(scores, 'item'):\n            with suppress(ValueError):\n                scores = scores.item()\n        if not isinstance(scores, numbers.Number):\n            raise ValueError(error_msg % (scores, type(scores), scorer))\n    return scores",
    ".sklearn.metrics._scorer.py@@_passthrough_scorer": "def _passthrough_scorer(estimator, *args, **kwargs):\n    return estimator.score(*args, **kwargs)",
    ".sklearn.base.py@@ClassifierMixin.score": "def score(self, X, y, sample_weight=None):\n    from .metrics import accuracy_score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)",
    ".sklearn.svm._base.py@@BaseSVC.predict": "def predict(self, X):\n    check_is_fitted(self)\n    if self.break_ties and self.decision_function_shape == 'ovo':\n        raise ValueError(\"break_ties must be False when decision_function_shape is 'ovo'\")\n    if self.break_ties and self.decision_function_shape == 'ovr' and (len(self.classes_) > 2):\n        y = np.argmax(self.decision_function(X), axis=1)\n    else:\n        y = super().predict(X)\n    return self.classes_.take(np.asarray(y, dtype=np.intp))",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes='deprecated', msg=None, all_or_any='deprecated'):\n    if attributes != 'deprecated':\n        warnings.warn('Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.', FutureWarning)\n    if all_or_any != 'deprecated':\n        warnings.warn('Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.', FutureWarning)\n    if isclass(estimator):\n        raise TypeError('{} is a class, not an instance.'.format(estimator))\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    attrs = [v for v in vars(estimator) if (v.endswith('_') or v.startswith('_')) and (not v.startswith('__'))]\n    if not attrs:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.svm._base.py@@BaseLibSVM.predict": "def predict(self, X):\n    X = self._validate_for_predict(X)\n    predict = self._sparse_predict if self._sparse else self._dense_predict\n    return predict(X)",
    ".sklearn.svm._base.py@@BaseLibSVM._validate_for_predict": "def _validate_for_predict(self, X):\n    check_is_fitted(self)\n    X = check_array(X, accept_sparse='csr', dtype=np.float64, order='C', accept_large_sparse=False)\n    if self._sparse and (not sp.isspmatrix(X)):\n        X = sp.csr_matrix(X)\n    if self._sparse:\n        X.sort_indices()\n    if sp.issparse(X) and (not self._sparse) and (not callable(self.kernel)):\n        raise ValueError('cannot use sparse input in %r trained on dense data' % type(self).__name__)\n    n_samples, n_features = X.shape\n    if self.kernel == 'precomputed':\n        if X.shape[1] != self.shape_fit_[0]:\n            raise ValueError('X.shape[1] = %d should be equal to %d, the number of samples at training time' % (X.shape[1], self.shape_fit_[0]))\n    elif n_features != self.shape_fit_[1]:\n        raise ValueError('X.shape[1] = %d should be equal to %d, the number of features at training time' % (n_features, self.shape_fit_[1]))\n    return X",
    ".sklearn.svm._base.py@@BaseLibSVM._dense_predict": "def _dense_predict(self, X):\n    X = self._compute_kernel(X)\n    if X.ndim == 1:\n        X = check_array(X, order='C', accept_large_sparse=False)\n    kernel = self.kernel\n    if callable(self.kernel):\n        kernel = 'precomputed'\n        if X.shape[1] != self.shape_fit_[0]:\n            raise ValueError('X.shape[1] = %d should be equal to %d, the number of samples at training time' % (X.shape[1], self.shape_fit_[0]))\n    svm_type = LIBSVM_IMPL.index(self._impl)\n    return libsvm.predict(X, self.support_, self.support_vectors_, self._n_support, self._dual_coef_, self._intercept_, self.probA_, self.probB_, svm_type=svm_type, kernel=kernel, degree=self.degree, coef0=self.coef0, gamma=self._gamma, cache_size=self.cache_size)",
    ".sklearn.svm._base.py@@BaseLibSVM._compute_kernel": "def _compute_kernel(self, X):\n    if callable(self.kernel):\n        kernel = self.kernel(X, self.__Xfit)\n        if sp.issparse(kernel):\n            kernel = kernel.toarray()\n        X = np.asarray(kernel, dtype=np.float64, order='C')\n    return X",
    ".sklearn.metrics._classification.py@@accuracy_score": "def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    check_consistent_length(y_true, y_pred, sample_weight)\n    if y_type.startswith('multilabel'):\n        differing_labels = count_nonzero(y_true - y_pred, axis=1)\n        score = differing_labels == 0\n    else:\n        score = y_true == y_pred\n    return _weighted_sum(score, sample_weight, normalize)",
    ".sklearn.metrics._classification.py@@_check_targets": "def _check_targets(y_true, y_pred):\n    check_consistent_length(y_true, y_pred)\n    type_true = type_of_target(y_true)\n    type_pred = type_of_target(y_pred)\n    y_type = {type_true, type_pred}\n    if y_type == {'binary', 'multiclass'}:\n        y_type = {'multiclass'}\n    if len(y_type) > 1:\n        raise ValueError(\"Classification metrics can't handle a mix of {0} and {1} targets\".format(type_true, type_pred))\n    y_type = y_type.pop()\n    if y_type not in ['binary', 'multiclass', 'multilabel-indicator']:\n        raise ValueError('{0} is not supported'.format(y_type))\n    if y_type in ['binary', 'multiclass']:\n        y_true = column_or_1d(y_true)\n        y_pred = column_or_1d(y_pred)\n        if y_type == 'binary':\n            unique_values = np.union1d(y_true, y_pred)\n            if len(unique_values) > 2:\n                y_type = 'multiclass'\n    if y_type.startswith('multilabel'):\n        y_true = csr_matrix(y_true)\n        y_pred = csr_matrix(y_pred)\n        y_type = 'multilabel-indicator'\n    return (y_type, y_true, y_pred)",
    ".sklearn.metrics._classification.py@@_weighted_sum": "def _weighted_sum(sample_score, sample_weight, normalize=False):\n    if normalize:\n        return np.average(sample_score, weights=sample_weight)\n    elif sample_weight is not None:\n        return np.dot(sample_score, sample_weight)\n    else:\n        return sample_score.sum()",
    ".sklearn.feature_selection._rfe.py@@RFE.fit": "def fit(self, X, y):\n    return self._fit(X, y)",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.svm._base.py@@BaseLibSVM._sparse_fit": "def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n    X.data = np.asarray(X.data, dtype=np.float64, order='C')\n    X.sort_indices()\n    kernel_type = self._sparse_kernels.index(kernel)\n    libsvm_sparse.set_verbosity_wrap(self.verbose)\n    self.support_, self.support_vectors_, dual_coef_data, self.intercept_, self._n_support, self.probA_, self.probB_, self.fit_status_ = libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices, X.indptr, y, solver_type, kernel_type, self.degree, self._gamma, self.coef0, self.tol, self.C, self.class_weight_, sample_weight, self.nu, self.cache_size, self.epsilon, int(self.shrinking), int(self.probability), self.max_iter, random_seed)\n    self._warn_from_fit_status()\n    if hasattr(self, 'classes_'):\n        n_class = len(self.classes_) - 1\n    else:\n        n_class = 1\n    n_SV = self.support_vectors_.shape[0]\n    dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n    if not n_SV:\n        self.dual_coef_ = sp.csr_matrix([])\n    else:\n        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class)\n        self.dual_coef_ = sp.csr_matrix((dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV))",
    ".sklearn.svm._base.py@@BaseLibSVM._sparse_predict": "def _sparse_predict(self, X):\n    kernel = self.kernel\n    if callable(kernel):\n        kernel = 'precomputed'\n    kernel_type = self._sparse_kernels.index(kernel)\n    C = 0.0\n    return libsvm_sparse.libsvm_sparse_predict(X.data, X.indices, X.indptr, self.support_vectors_.data, self.support_vectors_.indices, self.support_vectors_.indptr, self._dual_coef_.data, self._intercept_, LIBSVM_IMPL.index(self._impl), kernel_type, self.degree, self._gamma, self.coef0, self.tol, C, self.class_weight_, self.nu, self.epsilon, self.shrinking, self.probability, self._n_support, self.probA_, self.probB_)",
    ".sklearn.metrics._scorer.py@@get_scorer": "def get_scorer(scoring):\n    if isinstance(scoring, str):\n        try:\n            if scoring == 'brier_score_loss':\n                scorer = brier_score_loss_scorer\n            else:\n                scorer = SCORERS[scoring]\n        except KeyError:\n            raise ValueError('%r is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.' % scoring)\n    else:\n        scorer = scoring\n    return scorer",
    ".sklearn.metrics._scorer.py@@_BaseScorer.__call__": "def __call__(self, estimator, X, y_true, sample_weight=None):\n    if self._deprecation_msg is not None:\n        warnings.warn(self._deprecation_msg, category=FutureWarning, stacklevel=2)\n    return self._score(partial(_cached_call, None), estimator, X, y_true, sample_weight=sample_weight)",
    ".sklearn.metrics._scorer.py@@_PredictScorer._score": "def _score(self, method_caller, estimator, X, y_true, sample_weight=None):\n    y_pred = method_caller(estimator, 'predict', X)\n    if sample_weight is not None:\n        return self._sign * self._score_func(y_true, y_pred, sample_weight=sample_weight, **self._kwargs)\n    else:\n        return self._sign * self._score_func(y_true, y_pred, **self._kwargs)",
    ".sklearn.metrics._scorer.py@@_cached_call": "def _cached_call(cache, estimator, method, *args, **kwargs):\n    if cache is None:\n        return getattr(estimator, method)(*args, **kwargs)\n    try:\n        return cache[method]\n    except KeyError:\n        result = getattr(estimator, method)(*args, **kwargs)\n        cache[method] = result\n        return result",
    ".sklearn.metrics._classification.py@@zero_one_loss": "def zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None):\n    score = accuracy_score(y_true, y_pred, normalize=normalize, sample_weight=sample_weight)\n    if normalize:\n        return 1 - score\n    else:\n        if sample_weight is not None:\n            n_samples = np.sum(sample_weight)\n        else:\n            n_samples = _num_samples(y_true)\n        return n_samples - score",
    ".sklearn.model_selection._split.py@@KFold.__init__": "def __init__(self, n_splits=5, shuffle=False, random_state=None):\n    super().__init__(n_splits, shuffle, random_state)",
    ".sklearn.model_selection._split.py@@BaseCrossValidator._iter_test_masks": "def _iter_test_masks(self, X=None, y=None, groups=None):\n    for test_index in self._iter_test_indices(X, y, groups):\n        test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n        test_mask[test_index] = True\n        yield test_mask",
    ".sklearn.model_selection._split.py@@KFold._iter_test_indices": "def _iter_test_indices(self, X, y=None, groups=None):\n    n_samples = _num_samples(X)\n    indices = np.arange(n_samples)\n    if self.shuffle:\n        check_random_state(self.random_state).shuffle(indices)\n    n_splits = self.n_splits\n    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n    fold_sizes[:n_samples % n_splits] += 1\n    current = 0\n    for fold_size in fold_sizes:\n        start, stop = (current, current + fold_size)\n        yield indices[start:stop]\n        current = stop",
    ".sklearn.base.py@@BaseEstimator.__getstate__": "def __getstate__(self):\n    try:\n        state = super().__getstate__()\n    except AttributeError:\n        state = self.__dict__.copy()\n    if type(self).__module__.startswith('sklearn.'):\n        return dict(state.items(), _sklearn_version=__version__)\n    else:\n        return state",
    ".sklearn.utils.fixes.py@@_parse_version": "def _parse_version(version_string):\n    version = []\n    for x in version_string.split('.'):\n        try:\n            version.append(int(x))\n        except ValueError:\n            version.append(x)\n    return tuple(version)",
    ".sklearn.utils.deprecation.py@@deprecated.__init__": "def __init__(self, extra=''):\n    self.extra = extra",
    ".sklearn.utils.deprecation.py@@deprecated.__call__": "def __call__(self, obj):\n    if isinstance(obj, type):\n        return self._decorate_class(obj)\n    elif isinstance(obj, property):\n        return self._decorate_property(obj)\n    else:\n        return self._decorate_fun(obj)",
    ".sklearn.utils.deprecation.py@@deprecated._decorate_fun": "def _decorate_fun(self, fun):\n    msg = 'Function %s is deprecated' % fun.__name__\n    if self.extra:\n        msg += '; %s' % self.extra\n\n    @functools.wraps(fun)\n    def wrapped(*args, **kwargs):\n        warnings.warn(msg, category=FutureWarning)\n        return fun(*args, **kwargs)\n    wrapped.__doc__ = self._update_doc(wrapped.__doc__)\n    wrapped.__wrapped__ = fun\n    return wrapped",
    ".sklearn.utils.deprecation.py@@deprecated._update_doc": "def _update_doc(self, olddoc):\n    newdoc = 'DEPRECATED'\n    if self.extra:\n        newdoc = '%s: %s' % (newdoc, self.extra)\n    if olddoc:\n        newdoc = '%s\\n\\n%s' % (newdoc, olddoc)\n    return newdoc",
    ".sklearn.utils.deprecation.py@@deprecated._decorate_class": "def _decorate_class(self, cls):\n    msg = 'Class %s is deprecated' % cls.__name__\n    if self.extra:\n        msg += '; %s' % self.extra\n    init = cls.__init__\n\n    def wrapped(*args, **kwargs):\n        warnings.warn(msg, category=FutureWarning)\n        return init(*args, **kwargs)\n    cls.__init__ = wrapped\n    wrapped.__name__ = '__init__'\n    wrapped.__doc__ = self._update_doc(init.__doc__)\n    wrapped.deprecated_original = init\n    return cls",
    ".sklearn.metrics._scorer.py@@make_scorer": "def make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs):\n    sign = 1 if greater_is_better else -1\n    if needs_proba and needs_threshold:\n        raise ValueError('Set either needs_proba or needs_threshold to True, but not both.')\n    if needs_proba:\n        cls = _ProbaScorer\n    elif needs_threshold:\n        cls = _ThresholdScorer\n    else:\n        cls = _PredictScorer\n    return cls(score_func, sign, kwargs)",
    ".sklearn.metrics._scorer.py@@_BaseScorer.__init__": "def __init__(self, score_func, sign, kwargs):\n    self._kwargs = kwargs\n    self._score_func = score_func\n    self._sign = sign\n    self._deprecation_msg = None",
    ".sklearn.utils.metaestimators.py@@if_delegate_has_method": "def if_delegate_has_method(delegate):\n    if isinstance(delegate, list):\n        delegate = tuple(delegate)\n    if not isinstance(delegate, tuple):\n        delegate = (delegate,)\n    return lambda fn: _IffHasAttrDescriptor(fn, delegate, attribute_name=fn.__name__)",
    ".sklearn.utils.metaestimators.py@@_IffHasAttrDescriptor.__init__": "def __init__(self, fn, delegate_names, attribute_name):\n    self.fn = fn\n    self.delegate_names = delegate_names\n    self.attribute_name = attribute_name\n    update_wrapper(self, fn)",
    ".sklearn.base.py@@BaseEstimator.__setstate__": "def __setstate__(self, state):\n    if type(self).__module__.startswith('sklearn.'):\n        pickle_version = state.pop('_sklearn_version', 'pre-0.18')\n        if pickle_version != __version__:\n            warnings.warn('Trying to unpickle estimator {0} from version {1} when using version {2}. This might lead to breaking code or invalid results. Use at your own risk.'.format(self.__class__.__name__, pickle_version, __version__), UserWarning)\n    try:\n        super().__setstate__(state)\n    except AttributeError:\n        self.__dict__.update(state)",
    ".sklearn.model_selection._split.py@@GroupKFold.split": "def split(self, X, y=None, groups=None):\n    return super().split(X, y, groups)",
    ".sklearn.model_selection._split.py@@GroupKFold._iter_test_indices": "def _iter_test_indices(self, X, y, groups):\n    if groups is None:\n        raise ValueError(\"The 'groups' parameter should not be None.\")\n    groups = check_array(groups, ensure_2d=False, dtype=None)\n    unique_groups, groups = np.unique(groups, return_inverse=True)\n    n_groups = len(unique_groups)\n    if self.n_splits > n_groups:\n        raise ValueError('Cannot have number of splits n_splits=%d greater than the number of groups: %d.' % (self.n_splits, n_groups))\n    n_samples_per_group = np.bincount(groups)\n    indices = np.argsort(n_samples_per_group)[::-1]\n    n_samples_per_group = n_samples_per_group[indices]\n    n_samples_per_fold = np.zeros(self.n_splits)\n    group_to_fold = np.zeros(len(unique_groups))\n    for group_index, weight in enumerate(n_samples_per_group):\n        lightest_fold = np.argmin(n_samples_per_fold)\n        n_samples_per_fold[lightest_fold] += weight\n        group_to_fold[indices[group_index]] = lightest_fold\n    indices = group_to_fold[groups]\n    for f in range(self.n_splits):\n        yield np.where(indices == f)[0]",
    ".sklearn.base.py@@MultiOutputMixin._more_tags": "def _more_tags(self):\n    return {'multioutput': True}",
    ".sklearn.ensemble._forest.py@@RandomForestClassifier.__init__": "def __init__(self, n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None):\n    super().__init__(base_estimator=DecisionTreeClassifier(), n_estimators=n_estimators, estimator_params=('criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'random_state', 'ccp_alpha'), bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start, class_weight=class_weight, max_samples=max_samples)\n    self.criterion = criterion\n    self.max_depth = max_depth\n    self.min_samples_split = min_samples_split\n    self.min_samples_leaf = min_samples_leaf\n    self.min_weight_fraction_leaf = min_weight_fraction_leaf\n    self.max_features = max_features\n    self.max_leaf_nodes = max_leaf_nodes\n    self.min_impurity_decrease = min_impurity_decrease\n    self.min_impurity_split = min_impurity_split\n    self.ccp_alpha = ccp_alpha",
    ".sklearn.tree._classes.py@@DecisionTreeClassifier.__init__": "def __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0):\n    super().__init__(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, max_leaf_nodes=max_leaf_nodes, class_weight=class_weight, random_state=random_state, min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split, presort=presort, ccp_alpha=ccp_alpha)",
    ".sklearn.tree._classes.py@@BaseDecisionTree.__init__": "def __init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, min_impurity_decrease, min_impurity_split, class_weight=None, presort='deprecated', ccp_alpha=0.0):\n    self.criterion = criterion\n    self.splitter = splitter\n    self.max_depth = max_depth\n    self.min_samples_split = min_samples_split\n    self.min_samples_leaf = min_samples_leaf\n    self.min_weight_fraction_leaf = min_weight_fraction_leaf\n    self.max_features = max_features\n    self.random_state = random_state\n    self.max_leaf_nodes = max_leaf_nodes\n    self.min_impurity_decrease = min_impurity_decrease\n    self.min_impurity_split = min_impurity_split\n    self.class_weight = class_weight\n    self.presort = presort\n    self.ccp_alpha = ccp_alpha",
    ".sklearn.ensemble._forest.py@@ForestClassifier.__init__": "def __init__(self, base_estimator, n_estimators=100, estimator_params=tuple(), bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, max_samples=None):\n    super().__init__(base_estimator, n_estimators=n_estimators, estimator_params=estimator_params, bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start, class_weight=class_weight, max_samples=max_samples)",
    ".sklearn.ensemble._forest.py@@BaseForest.__init__": "def __init__(self, base_estimator, n_estimators=100, estimator_params=tuple(), bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, max_samples=None):\n    super().__init__(base_estimator=base_estimator, n_estimators=n_estimators, estimator_params=estimator_params)\n    self.bootstrap = bootstrap\n    self.oob_score = oob_score\n    self.n_jobs = n_jobs\n    self.random_state = random_state\n    self.verbose = verbose\n    self.warm_start = warm_start\n    self.class_weight = class_weight\n    self.max_samples = max_samples",
    ".sklearn.ensemble._base.py@@BaseEnsemble.__init__": "def __init__(self, base_estimator, n_estimators=10, estimator_params=tuple()):\n    self.base_estimator = base_estimator\n    self.n_estimators = n_estimators\n    self.estimator_params = estimator_params",
    ".sklearn.ensemble._forest.py@@BaseForest.fit": "def fit(self, X, y, sample_weight=None):\n    X = check_array(X, accept_sparse='csc', dtype=DTYPE)\n    y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n    if sample_weight is not None:\n        sample_weight = check_array(sample_weight, ensure_2d=False)\n    if issparse(X):\n        X.sort_indices()\n    self.n_features_ = X.shape[1]\n    y = np.atleast_1d(y)\n    if y.ndim == 2 and y.shape[1] == 1:\n        warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().', DataConversionWarning, stacklevel=2)\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n    self.n_outputs_ = y.shape[1]\n    y, expanded_class_weight = self._validate_y_class_weight(y)\n    if getattr(y, 'dtype', None) != DOUBLE or not y.flags.contiguous:\n        y = np.ascontiguousarray(y, dtype=DOUBLE)\n    if expanded_class_weight is not None:\n        if sample_weight is not None:\n            sample_weight = sample_weight * expanded_class_weight\n        else:\n            sample_weight = expanded_class_weight\n    n_samples_bootstrap = _get_n_samples_bootstrap(n_samples=X.shape[0], max_samples=self.max_samples)\n    self._validate_estimator()\n    if not self.bootstrap and self.oob_score:\n        raise ValueError('Out of bag estimation only available if bootstrap=True')\n    random_state = check_random_state(self.random_state)\n    if not self.warm_start or not hasattr(self, 'estimators_'):\n        self.estimators_ = []\n    n_more_estimators = self.n_estimators - len(self.estimators_)\n    if n_more_estimators < 0:\n        raise ValueError('n_estimators=%d must be larger or equal to len(estimators_)=%d when warm_start==True' % (self.n_estimators, len(self.estimators_)))\n    elif n_more_estimators == 0:\n        warn('Warm-start fitting without increasing n_estimators does not fit new trees.')\n    else:\n        if self.warm_start and len(self.estimators_) > 0:\n            random_state.randint(MAX_INT, size=len(self.estimators_))\n        trees = [self._make_estimator(append=False, random_state=random_state) for i in range(n_more_estimators)]\n        trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, **_joblib_parallel_args(prefer='threads'))((delayed(_parallel_build_trees)(t, self, X, y, sample_weight, i, len(trees), verbose=self.verbose, class_weight=self.class_weight, n_samples_bootstrap=n_samples_bootstrap) for i, t in enumerate(trees)))\n        self.estimators_.extend(trees)\n    if self.oob_score:\n        self._set_oob_score(X, y)\n    if hasattr(self, 'classes_') and self.n_outputs_ == 1:\n        self.n_classes_ = self.n_classes_[0]\n        self.classes_ = self.classes_[0]\n    return self",
    ".sklearn.ensemble._forest.py@@ForestClassifier._validate_y_class_weight": "def _validate_y_class_weight(self, y):\n    check_classification_targets(y)\n    y = np.copy(y)\n    expanded_class_weight = None\n    if self.class_weight is not None:\n        y_original = np.copy(y)\n    self.classes_ = []\n    self.n_classes_ = []\n    y_store_unique_indices = np.zeros(y.shape, dtype=np.int)\n    for k in range(self.n_outputs_):\n        classes_k, y_store_unique_indices[:, k] = np.unique(y[:, k], return_inverse=True)\n        self.classes_.append(classes_k)\n        self.n_classes_.append(classes_k.shape[0])\n    y = y_store_unique_indices\n    if self.class_weight is not None:\n        valid_presets = ('balanced', 'balanced_subsample')\n        if isinstance(self.class_weight, str):\n            if self.class_weight not in valid_presets:\n                raise ValueError('Valid presets for class_weight include \"balanced\" and \"balanced_subsample\". Given \"%s\".' % self.class_weight)\n            if self.warm_start:\n                warn('class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight(\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.')\n        if self.class_weight != 'balanced_subsample' or not self.bootstrap:\n            if self.class_weight == 'balanced_subsample':\n                class_weight = 'balanced'\n            else:\n                class_weight = self.class_weight\n            expanded_class_weight = compute_sample_weight(class_weight, y_original)\n    return (y, expanded_class_weight)",
    ".sklearn.ensemble._forest.py@@_get_n_samples_bootstrap": "def _get_n_samples_bootstrap(n_samples, max_samples):\n    if max_samples is None:\n        return n_samples\n    if isinstance(max_samples, numbers.Integral):\n        if not 1 <= max_samples <= n_samples:\n            msg = '`max_samples` must be in range 1 to {} but got value {}'\n            raise ValueError(msg.format(n_samples, max_samples))\n        return max_samples\n    if isinstance(max_samples, numbers.Real):\n        if not 0 < max_samples < 1:\n            msg = '`max_samples` must be in range (0, 1) but got value {}'\n            raise ValueError(msg.format(max_samples))\n        return int(round(n_samples * max_samples))\n    msg = \"`max_samples` should be int or float, but got type '{}'\"\n    raise TypeError(msg.format(type(max_samples)))",
    ".sklearn.ensemble._base.py@@BaseEnsemble._validate_estimator": "def _validate_estimator(self, default=None):\n    if not isinstance(self.n_estimators, numbers.Integral):\n        raise ValueError('n_estimators must be an integer, got {0}.'.format(type(self.n_estimators)))\n    if self.n_estimators <= 0:\n        raise ValueError('n_estimators must be greater than zero, got {0}.'.format(self.n_estimators))\n    if self.base_estimator is not None:\n        self.base_estimator_ = self.base_estimator\n    else:\n        self.base_estimator_ = default\n    if self.base_estimator_ is None:\n        raise ValueError('base_estimator cannot be None')",
    ".sklearn.ensemble._base.py@@BaseEnsemble._make_estimator": "def _make_estimator(self, append=True, random_state=None):\n    estimator = clone(self.base_estimator_)\n    estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n    if random_state is not None:\n        _set_random_states(estimator, random_state)\n    if append:\n        self.estimators_.append(estimator)\n    return estimator",
    ".sklearn.base.py@@BaseEstimator.set_params": "def set_params(self, **params):\n    if not params:\n        return self\n    valid_params = self.get_params(deep=True)\n    nested_params = defaultdict(dict)\n    for key, value in params.items():\n        key, delim, sub_key = key.partition('__')\n        if key not in valid_params:\n            raise ValueError('Invalid parameter %s for estimator %s. Check the list of available parameters with `estimator.get_params().keys()`.' % (key, self))\n        if delim:\n            nested_params[key][sub_key] = value\n        else:\n            setattr(self, key, value)\n            valid_params[key] = value\n    for key, sub_params in nested_params.items():\n        valid_params[key].set_params(**sub_params)\n    return self",
    ".sklearn.ensemble._base.py@@_set_random_states": "def _set_random_states(estimator, random_state=None):\n    random_state = check_random_state(random_state)\n    to_set = {}\n    for key in sorted(estimator.get_params(deep=True)):\n        if key == 'random_state' or key.endswith('__random_state'):\n            to_set[key] = random_state.randint(MAX_RAND_SEED)\n    if to_set:\n        estimator.set_params(**to_set)",
    ".sklearn.utils.fixes.py@@_joblib_parallel_args": "def _joblib_parallel_args(**kwargs):\n    import joblib\n    if joblib.__version__ >= LooseVersion('0.12'):\n        return kwargs\n    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})\n    if extra_args:\n        raise NotImplementedError('unhandled arguments %s with joblib %s' % (list(extra_args), joblib.__version__))\n    args = {}\n    if 'prefer' in kwargs:\n        prefer = kwargs['prefer']\n        if prefer not in ['threads', 'processes', None]:\n            raise ValueError('prefer=%s is not supported' % prefer)\n        args['backend'] = {'threads': 'threading', 'processes': 'multiprocessing', None: None}[prefer]\n    if 'require' in kwargs:\n        require = kwargs['require']\n        if require not in [None, 'sharedmem']:\n            raise ValueError('require=%s is not supported' % require)\n        if require == 'sharedmem':\n            args['backend'] = 'threading'\n    return args",
    ".sklearn.ensemble._forest.py@@_parallel_build_trees": "def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose=0, class_weight=None, n_samples_bootstrap=None):\n    if verbose > 1:\n        print('building tree %d of %d' % (tree_idx + 1, n_trees))\n    if forest.bootstrap:\n        n_samples = X.shape[0]\n        if sample_weight is None:\n            curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n        else:\n            curr_sample_weight = sample_weight.copy()\n        indices = _generate_sample_indices(tree.random_state, n_samples, n_samples_bootstrap)\n        sample_counts = np.bincount(indices, minlength=n_samples)\n        curr_sample_weight *= sample_counts\n        if class_weight == 'subsample':\n            with catch_warnings():\n                simplefilter('ignore', DeprecationWarning)\n                curr_sample_weight *= compute_sample_weight('auto', y, indices)\n        elif class_weight == 'balanced_subsample':\n            curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n        tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n    else:\n        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    return tree",
    ".sklearn.ensemble._forest.py@@_generate_sample_indices": "def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):\n    random_instance = check_random_state(random_state)\n    sample_indices = random_instance.randint(0, n_samples, n_samples_bootstrap)\n    return sample_indices",
    ".sklearn.tree._classes.py@@DecisionTreeClassifier.fit": "def fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None):\n    super().fit(X, y, sample_weight=sample_weight, check_input=check_input, X_idx_sorted=X_idx_sorted)\n    return self",
    ".sklearn.tree._classes.py@@BaseDecisionTree.fit": "def fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None):\n    random_state = check_random_state(self.random_state)\n    if self.ccp_alpha < 0.0:\n        raise ValueError('ccp_alpha must be greater than or equal to 0')\n    if check_input:\n        X = check_array(X, dtype=DTYPE, accept_sparse='csc')\n        y = check_array(y, ensure_2d=False, dtype=None)\n        if issparse(X):\n            X.sort_indices()\n            if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:\n                raise ValueError('No support for np.int64 index based sparse matrices')\n    n_samples, self.n_features_ = X.shape\n    is_classification = is_classifier(self)\n    y = np.atleast_1d(y)\n    expanded_class_weight = None\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n    self.n_outputs_ = y.shape[1]\n    if is_classification:\n        check_classification_targets(y)\n        y = np.copy(y)\n        self.classes_ = []\n        self.n_classes_ = []\n        if self.class_weight is not None:\n            y_original = np.copy(y)\n        y_encoded = np.zeros(y.shape, dtype=np.int)\n        for k in range(self.n_outputs_):\n            classes_k, y_encoded[:, k] = np.unique(y[:, k], return_inverse=True)\n            self.classes_.append(classes_k)\n            self.n_classes_.append(classes_k.shape[0])\n        y = y_encoded\n        if self.class_weight is not None:\n            expanded_class_weight = compute_sample_weight(self.class_weight, y_original)\n        self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)\n    if getattr(y, 'dtype', None) != DOUBLE or not y.flags.contiguous:\n        y = np.ascontiguousarray(y, dtype=DOUBLE)\n    max_depth = 2 ** 31 - 1 if self.max_depth is None else self.max_depth\n    max_leaf_nodes = -1 if self.max_leaf_nodes is None else self.max_leaf_nodes\n    if isinstance(self.min_samples_leaf, numbers.Integral):\n        if not 1 <= self.min_samples_leaf:\n            raise ValueError('min_samples_leaf must be at least 1 or in (0, 0.5], got %s' % self.min_samples_leaf)\n        min_samples_leaf = self.min_samples_leaf\n    else:\n        if not 0.0 < self.min_samples_leaf <= 0.5:\n            raise ValueError('min_samples_leaf must be at least 1 or in (0, 0.5], got %s' % self.min_samples_leaf)\n        min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))\n    if isinstance(self.min_samples_split, numbers.Integral):\n        if not 2 <= self.min_samples_split:\n            raise ValueError('min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer %s' % self.min_samples_split)\n        min_samples_split = self.min_samples_split\n    else:\n        if not 0.0 < self.min_samples_split <= 1.0:\n            raise ValueError('min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float %s' % self.min_samples_split)\n        min_samples_split = int(ceil(self.min_samples_split * n_samples))\n        min_samples_split = max(2, min_samples_split)\n    min_samples_split = max(min_samples_split, 2 * min_samples_leaf)\n    if isinstance(self.max_features, str):\n        if self.max_features == 'auto':\n            if is_classification:\n                max_features = max(1, int(np.sqrt(self.n_features_)))\n            else:\n                max_features = self.n_features_\n        elif self.max_features == 'sqrt':\n            max_features = max(1, int(np.sqrt(self.n_features_)))\n        elif self.max_features == 'log2':\n            max_features = max(1, int(np.log2(self.n_features_)))\n        else:\n            raise ValueError('Invalid value for max_features. Allowed string values are \"auto\", \"sqrt\" or \"log2\".')\n    elif self.max_features is None:\n        max_features = self.n_features_\n    elif isinstance(self.max_features, numbers.Integral):\n        max_features = self.max_features\n    elif self.max_features > 0.0:\n        max_features = max(1, int(self.max_features * self.n_features_))\n    else:\n        max_features = 0\n    self.max_features_ = max_features\n    if len(y) != n_samples:\n        raise ValueError('Number of labels=%d does not match number of samples=%d' % (len(y), n_samples))\n    if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n        raise ValueError('min_weight_fraction_leaf must in [0, 0.5]')\n    if max_depth <= 0:\n        raise ValueError('max_depth must be greater than zero. ')\n    if not 0 < max_features <= self.n_features_:\n        raise ValueError('max_features must be in (0, n_features]')\n    if not isinstance(max_leaf_nodes, numbers.Integral):\n        raise ValueError('max_leaf_nodes must be integral number but was %r' % max_leaf_nodes)\n    if -1 < max_leaf_nodes < 2:\n        raise ValueError('max_leaf_nodes {0} must be either None or larger than 1'.format(max_leaf_nodes))\n    if sample_weight is not None:\n        if getattr(sample_weight, 'dtype', None) != DOUBLE or not sample_weight.flags.contiguous:\n            sample_weight = np.ascontiguousarray(sample_weight, dtype=DOUBLE)\n        if len(sample_weight.shape) > 1:\n            raise ValueError('Sample weights array has more than one dimension: %d' % len(sample_weight.shape))\n        if len(sample_weight) != n_samples:\n            raise ValueError('Number of weights=%d does not match number of samples=%d' % (len(sample_weight), n_samples))\n    if expanded_class_weight is not None:\n        if sample_weight is not None:\n            sample_weight = sample_weight * expanded_class_weight\n        else:\n            sample_weight = expanded_class_weight\n    if sample_weight is None:\n        min_weight_leaf = self.min_weight_fraction_leaf * n_samples\n    else:\n        min_weight_leaf = self.min_weight_fraction_leaf * np.sum(sample_weight)\n    if self.min_impurity_split is not None:\n        warnings.warn('The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.', FutureWarning)\n        min_impurity_split = self.min_impurity_split\n    else:\n        min_impurity_split = 1e-07\n    if min_impurity_split < 0.0:\n        raise ValueError('min_impurity_split must be greater than or equal to 0')\n    if self.min_impurity_decrease < 0.0:\n        raise ValueError('min_impurity_decrease must be greater than or equal to 0')\n    if self.presort != 'deprecated':\n        warnings.warn(\"The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\", FutureWarning)\n    criterion = self.criterion\n    if not isinstance(criterion, Criterion):\n        if is_classification:\n            criterion = CRITERIA_CLF[self.criterion](self.n_outputs_, self.n_classes_)\n        else:\n            criterion = CRITERIA_REG[self.criterion](self.n_outputs_, n_samples)\n    SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    splitter = self.splitter\n    if not isinstance(self.splitter, Splitter):\n        splitter = SPLITTERS[self.splitter](criterion, self.max_features_, min_samples_leaf, min_weight_leaf, random_state)\n    if is_classifier(self):\n        self.tree_ = Tree(self.n_features_, self.n_classes_, self.n_outputs_)\n    else:\n        self.tree_ = Tree(self.n_features_, np.array([1] * self.n_outputs_, dtype=np.intp), self.n_outputs_)\n    if max_leaf_nodes < 0:\n        builder = DepthFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, self.min_impurity_decrease, min_impurity_split)\n    else:\n        builder = BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes, self.min_impurity_decrease, min_impurity_split)\n    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n    if self.n_outputs_ == 1 and is_classifier(self):\n        self.n_classes_ = self.n_classes_[0]\n        self.classes_ = self.classes_[0]\n    self._prune_tree()\n    return self",
    ".sklearn.tree._classes.py@@BaseDecisionTree._prune_tree": "def _prune_tree(self):\n    check_is_fitted(self)\n    if self.ccp_alpha < 0.0:\n        raise ValueError('ccp_alpha must be greater than or equal to 0')\n    if self.ccp_alpha == 0.0:\n        return\n    if is_classifier(self):\n        n_classes = np.atleast_1d(self.n_classes_)\n        pruned_tree = Tree(self.n_features_, n_classes, self.n_outputs_)\n    else:\n        pruned_tree = Tree(self.n_features_, np.array([1] * self.n_outputs_, dtype=np.intp), self.n_outputs_)\n    _build_pruned_tree_ccp(pruned_tree, self.tree_, self.ccp_alpha)\n    self.tree_ = pruned_tree",
    ".sklearn.ensemble._forest.py@@BaseForest.feature_importances_": "def feature_importances_(self):\n    check_is_fitted(self)\n    all_importances = Parallel(n_jobs=self.n_jobs, **_joblib_parallel_args(prefer='threads'))((delayed(getattr)(tree, 'feature_importances_') for tree in self.estimators_ if tree.tree_.node_count > 1))\n    if not all_importances:\n        return np.zeros(self.n_features_, dtype=np.float64)\n    all_importances = np.mean(all_importances, axis=0, dtype=np.float64)\n    return all_importances / np.sum(all_importances)",
    ".sklearn.ensemble._forest.py@@ForestClassifier.predict": "def predict(self, X):\n    proba = self.predict_proba(X)\n    if self.n_outputs_ == 1:\n        return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n    else:\n        n_samples = proba[0].shape[0]\n        class_type = self.classes_[0].dtype\n        predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n        for k in range(self.n_outputs_):\n            predictions[:, k] = self.classes_[k].take(np.argmax(proba[k], axis=1), axis=0)\n        return predictions",
    ".sklearn.ensemble._forest.py@@ForestClassifier.predict_proba": "def predict_proba(self, X):\n    check_is_fitted(self)\n    X = self._validate_X_predict(X)\n    n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n    all_proba = [np.zeros((X.shape[0], j), dtype=np.float64) for j in np.atleast_1d(self.n_classes_)]\n    lock = threading.Lock()\n    Parallel(n_jobs=n_jobs, verbose=self.verbose, **_joblib_parallel_args(require='sharedmem'))((delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock) for e in self.estimators_))\n    for proba in all_proba:\n        proba /= len(self.estimators_)\n    if len(all_proba) == 1:\n        return all_proba[0]\n    else:\n        return all_proba",
    ".sklearn.ensemble._forest.py@@BaseForest._validate_X_predict": "def _validate_X_predict(self, X):\n    check_is_fitted(self)\n    return self.estimators_[0]._validate_X_predict(X, check_input=True)",
    ".sklearn.tree._classes.py@@BaseDecisionTree._validate_X_predict": "def _validate_X_predict(self, X, check_input):\n    if check_input:\n        X = check_array(X, dtype=DTYPE, accept_sparse='csr')\n        if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n            raise ValueError('No support for np.int64 index based sparse matrices')\n    n_features = X.shape[1]\n    if self.n_features_ != n_features:\n        raise ValueError('Number of features of the model must match the input. Model n_features is %s and input n_features is %s ' % (self.n_features_, n_features))\n    return X",
    ".sklearn.ensemble._base.py@@_partition_estimators": "def _partition_estimators(n_estimators, n_jobs):\n    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs, dtype=np.int)\n    n_estimators_per_job[:n_estimators % n_jobs] += 1\n    starts = np.cumsum(n_estimators_per_job)\n    return (n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist())",
    ".sklearn.ensemble._forest.py@@_accumulate_prediction": "def _accumulate_prediction(predict, X, out, lock):\n    prediction = predict(X, check_input=False)\n    with lock:\n        if len(out) == 1:\n            out[0] += prediction\n        else:\n            for i in range(len(out)):\n                out[i] += prediction[i]",
    ".sklearn.tree._classes.py@@DecisionTreeClassifier.predict_proba": "def predict_proba(self, X, check_input=True):\n    check_is_fitted(self)\n    X = self._validate_X_predict(X, check_input)\n    proba = self.tree_.predict(X)\n    if self.n_outputs_ == 1:\n        proba = proba[:, :self.n_classes_]\n        normalizer = proba.sum(axis=1)[:, np.newaxis]\n        normalizer[normalizer == 0.0] = 1.0\n        proba /= normalizer\n        return proba\n    else:\n        all_proba = []\n        for k in range(self.n_outputs_):\n            proba_k = proba[:, k, :self.n_classes_[k]]\n            normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n            normalizer[normalizer == 0.0] = 1.0\n            proba_k /= normalizer\n            all_proba.append(proba_k)\n        return all_proba",
    ".sklearn.tree._classes.py@@BaseDecisionTree.feature_importances_": "def feature_importances_(self):\n    check_is_fitted(self)\n    return self.tree_.compute_feature_importances()"
}