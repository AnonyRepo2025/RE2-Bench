{
    ".sklearn.compose._column_transformer.py@@_get_column_indices": "def _get_column_indices(X, key):\n    n_columns = X.shape[1]\n    if _check_key_type(key, int):\n        if isinstance(key, int):\n            return [key]\n        elif isinstance(key, slice):\n            return list(range(n_columns)[key])\n        else:\n            return list(key)\n    elif _check_key_type(key, six.string_types):\n        try:\n            all_columns = list(X.columns)\n        except AttributeError:\n            raise ValueError('Specifying the columns using strings is only supported for pandas DataFrames')\n        if isinstance(key, six.string_types):\n            columns = [key]\n        elif isinstance(key, slice):\n            start, stop = (key.start, key.stop)\n            if start is not None:\n                start = all_columns.index(start)\n            if stop is not None:\n                stop = all_columns.index(stop) + 1\n            else:\n                stop = n_columns + 1\n            return list(range(n_columns)[slice(start, stop)])\n        else:\n            columns = list(key)\n        return [all_columns.index(col) for col in columns]\n    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):\n        return list(np.arange(n_columns)[key])\n    else:\n        raise ValueError('No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed')",
    ".sklearn.compose._column_transformer.py@@_check_key_type": "def _check_key_type(key, superclass):\n    if isinstance(key, superclass):\n        return True\n    if isinstance(key, slice):\n        return isinstance(key.start, (superclass, type(None))) and isinstance(key.stop, (superclass, type(None)))\n    if isinstance(key, list):\n        return all((isinstance(x, superclass) for x in key))\n    if hasattr(key, 'dtype'):\n        if superclass is int:\n            return key.dtype.kind == 'i'\n        else:\n            return key.dtype.kind in ('O', 'U', 'S')\n    return False",
    ".sklearn.utils.metaestimators.py@@_BaseComposition._validate_names": "def _validate_names(self, names):\n    if len(set(names)) != len(names):\n        raise ValueError('Names provided are not unique: {0!r}'.format(list(names)))\n    invalid_names = set(names).intersection(self.get_params(deep=False))\n    if invalid_names:\n        raise ValueError('Estimator names conflict with constructor arguments: {0!r}'.format(sorted(invalid_names)))\n    invalid_names = [name for name in names if '__' in name]\n    if invalid_names:\n        raise ValueError('Estimator names must not contain __: got {0!r}'.format(invalid_names))",
    ".sklearn.utils.metaestimators.py@@_BaseComposition._get_params": "def _get_params(self, attr, deep=True):\n    out = super(_BaseComposition, self).get_params(deep=deep)\n    if not deep:\n        return out\n    estimators = getattr(self, attr)\n    out.update(estimators)\n    for name, estimator in estimators:\n        if hasattr(estimator, 'get_params'):\n            for key, value in six.iteritems(estimator.get_params(deep=True)):\n                out['%s__%s' % (name, key)] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key, None)\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.externals.joblib.parallel.py@@Parallel.__init__": "def __init__(self, n_jobs=1, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=None, max_nbytes='1M', mmap_mode='r'):\n    active_backend, default_n_jobs = get_active_backend()\n    if backend is None and n_jobs == 1:\n        n_jobs = default_n_jobs\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.timeout = timeout\n    self.pre_dispatch = pre_dispatch\n    if isinstance(max_nbytes, _basestring):\n        max_nbytes = memstr_to_bytes(max_nbytes)\n    self._backend_args = dict(max_nbytes=max_nbytes, mmap_mode=mmap_mode, temp_folder=temp_folder, verbose=max(0, self.verbose - 50))\n    if DEFAULT_MP_CONTEXT is not None:\n        self._backend_args['context'] = DEFAULT_MP_CONTEXT\n    if backend is None:\n        backend = active_backend\n    elif isinstance(backend, ParallelBackendBase):\n        pass\n    elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):\n        self._backend_args['context'] = backend\n        backend = MultiprocessingBackend()\n    else:\n        try:\n            backend_factory = BACKENDS[backend]\n        except KeyError:\n            raise ValueError('Invalid backend: %s, expected one of %r' % (backend, sorted(BACKENDS.keys())))\n        backend = backend_factory()\n    if batch_size == 'auto' or (isinstance(batch_size, Integral) and batch_size > 0):\n        self.batch_size = batch_size\n    else:\n        raise ValueError(\"batch_size must be 'auto' or a positive integer, got: %r\" % batch_size)\n    self._backend = backend\n    self._output = None\n    self._jobs = list()\n    self._managed_backend = False\n    self._lock = threading.Lock()",
    ".sklearn.externals.joblib.parallel.py@@get_active_backend": "def get_active_backend():\n    active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    if active_backend_and_jobs is not None:\n        return active_backend_and_jobs\n    active_backend = BACKENDS[DEFAULT_BACKEND]()\n    return (active_backend, DEFAULT_N_JOBS)",
    ".sklearn.externals.joblib.disk.py@@memstr_to_bytes": "def memstr_to_bytes(text):\n    kilo = 1024\n    units = dict(K=kilo, M=kilo ** 2, G=kilo ** 3)\n    try:\n        size = int(units[text[-1]] * float(text[:-1]))\n    except (KeyError, ValueError):\n        raise ValueError(\"Invalid literal for size give: %s (type %s) should be alike '10G', '500M', '50K'.\" % (text, type(text)))\n    return size",
    ".sklearn.externals.joblib.parallel.py@@Parallel.__call__": "def __call__(self, iterable):\n    if self._jobs:\n        raise ValueError('This Parallel instance is already running')\n    self._aborting = False\n    if not self._managed_backend:\n        n_jobs = self._initialize_backend()\n    else:\n        n_jobs = self._effective_n_jobs()\n    iterator = iter(iterable)\n    pre_dispatch = self.pre_dispatch\n    if pre_dispatch == 'all' or n_jobs == 1:\n        self._original_iterator = None\n        self._pre_dispatch_amount = 0\n    else:\n        self._original_iterator = iterator\n        if hasattr(pre_dispatch, 'endswith'):\n            pre_dispatch = eval(pre_dispatch)\n        self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n        iterator = itertools.islice(iterator, pre_dispatch)\n    self._start_time = time.time()\n    self.n_dispatched_batches = 0\n    self.n_dispatched_tasks = 0\n    self.n_completed_tasks = 0\n    try:\n        while self.dispatch_one_batch(iterator):\n            self._iterating = True\n        else:\n            self._iterating = False\n        if pre_dispatch == 'all' or n_jobs == 1:\n            self._iterating = False\n        self.retrieve()\n        elapsed_time = time.time() - self._start_time\n        self._print('Done %3i out of %3i | elapsed: %s finished', (len(self._output), len(self._output), short_format_time(elapsed_time)))\n    finally:\n        if not self._managed_backend:\n            self._terminate_backend()\n        self._jobs = list()\n    output = self._output\n    self._output = None\n    return output",
    ".sklearn.externals.joblib.parallel.py@@Parallel._initialize_backend": "def _initialize_backend(self):\n    try:\n        n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self, **self._backend_args)\n        if self.timeout is not None and (not self._backend.supports_timeout):\n            warnings.warn(\"The backend class {!r} does not support timeout. You have set 'timeout={}' in Parallel but the 'timeout' parameter will not be used.\".format(self._backend.__class__.__name__, self.timeout))\n    except FallbackToBackend as e:\n        self._backend = e.backend\n        n_jobs = self._initialize_backend()\n    return n_jobs",
    ".sklearn.externals.joblib._parallel_backends.py@@MultiprocessingBackend.configure": "def configure(self, n_jobs=1, parallel=None, **backend_args):\n    n_jobs = self.effective_n_jobs(n_jobs)\n    if n_jobs == 1:\n        raise FallbackToBackend(SequentialBackend())\n    already_forked = int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))\n    if already_forked:\n        raise ImportError('[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == \\'__main__\\'\". Please see the joblib documentation on Parallel for more information')\n    os.environ[self.JOBLIB_SPAWNED_PROCESS] = '1'\n    gc.collect()\n    self._pool = MemmapingPool(n_jobs, **backend_args)\n    self.parallel = parallel\n    return n_jobs",
    ".sklearn.externals.joblib._parallel_backends.py@@MultiprocessingBackend.effective_n_jobs": "def effective_n_jobs(self, n_jobs):\n    if mp is None:\n        return 1\n    if mp.current_process().daemon:\n        if n_jobs != 1:\n            warnings.warn('Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1', stacklevel=3)\n        return 1\n    if not isinstance(threading.current_thread(), threading._MainThread):\n        warnings.warn('Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1', stacklevel=3)\n        return 1\n    return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)",
    ".sklearn.externals.joblib._parallel_backends.py@@PoolManagerMixin.effective_n_jobs": "def effective_n_jobs(self, n_jobs):\n    if n_jobs == 0:\n        raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    elif mp is None or n_jobs is None:\n        return 1\n    elif n_jobs < 0:\n        n_jobs = max(mp.cpu_count() + 1 + n_jobs, 1)\n    return n_jobs",
    ".sklearn.externals.joblib._parallel_backends.py@@FallbackToBackend.__init__": "def __init__(self, backend):\n    self.backend = backend",
    ".sklearn.externals.joblib._parallel_backends.py@@ParallelBackendBase.configure": "def configure(self, n_jobs=1, parallel=None, **backend_args):\n    self.parallel = parallel\n    return self.effective_n_jobs(n_jobs)",
    ".sklearn.externals.joblib._parallel_backends.py@@SequentialBackend.effective_n_jobs": "def effective_n_jobs(self, n_jobs):\n    if n_jobs == 0:\n        raise ValueError('n_jobs == 0 in Parallel has no meaning')\n    return 1",
    ".sklearn.externals.joblib.parallel.py@@Parallel.dispatch_one_batch": "def dispatch_one_batch(self, iterator):\n    if self.batch_size == 'auto':\n        batch_size = self._backend.compute_batch_size()\n    else:\n        batch_size = self.batch_size\n    with self._lock:\n        tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n        if len(tasks) == 0:\n            return False\n        else:\n            self._dispatch(tasks)\n            return True",
    ".sklearn.externals.joblib._parallel_backends.py@@ParallelBackendBase.compute_batch_size": "def compute_batch_size(self):\n    return 1",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__init__": "def __init__(self, iterator_slice):\n    self.items = list(iterator_slice)\n    self._size = len(self.items)",
    ".sklearn.compose._column_transformer.py@@_get_column": "def _get_column(X, key):\n    if _check_key_type(key, int):\n        column_names = False\n    elif _check_key_type(key, six.string_types):\n        column_names = True\n    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):\n        column_names = False\n        if hasattr(X, 'loc'):\n            column_names = True\n    else:\n        raise ValueError('No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed')\n    if column_names:\n        if hasattr(X, 'loc'):\n            return X.loc[:, key]\n        else:\n            raise ValueError('Specifying the columns using strings is only supported for pandas DataFrames')\n    elif hasattr(X, 'iloc'):\n        return X.iloc[:, key]\n    else:\n        return X[:, key]",
    ".sklearn.externals.joblib.parallel.py@@delayed": "def delayed(function, check_pickle=True):\n    if check_pickle:\n        pickle.dumps(function)\n\n    def delayed_function(*args, **kwargs):\n        return (function, args, kwargs)\n    try:\n        delayed_function = functools.wraps(function)(delayed_function)\n    except AttributeError:\n        ' functools.wraps fails on some callable objects '\n    return delayed_function",
    ".sklearn.base.py@@clone": "def clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params'):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\" % (repr(estimator), type(estimator)))\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in six.iteritems(new_object_params):\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is param2:\n            continue\n        if isinstance(param1, np.ndarray):\n            if not isinstance(param2, type(param1)):\n                equality_test = False\n            elif param1.ndim > 0 and param1.shape[0] > 0 and isinstance(param2, np.ndarray) and (param2.ndim > 0) and (param2.shape[0] > 0):\n                equality_test = param1.shape == param2.shape and param1.dtype == param2.dtype and (_first_and_last_element(param1) == _first_and_last_element(param2))\n            else:\n                equality_test = np.all(param1 == param2)\n        elif sparse.issparse(param1):\n            if not sparse.issparse(param2):\n                equality_test = False\n            elif param1.size == 0 or param2.size == 0:\n                equality_test = param1.__class__ == param2.__class__ and param1.size == 0 and (param2.size == 0)\n            else:\n                equality_test = param1.__class__ == param2.__class__ and _first_and_last_element(param1) == _first_and_last_element(param2) and (param1.nnz == param2.nnz) and (param1.shape == param2.shape)\n        else:\n            equality_test = param1 == param2\n        if equality_test:\n            warnings.warn('Estimator %s modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.' % type(estimator).__name__, DeprecationWarning)\n        else:\n            raise RuntimeError('Cannot clone object %s, as the constructor does not seem to set parameter %s' % (estimator, name))\n    return new_object",
    ".sklearn.externals.six.py@@iteritems": "def iteritems(d, **kw):\n    return iter(getattr(d, _iteritems)(**kw))",
    ".sklearn.externals.joblib.parallel.py@@delayed_function": "def delayed_function(*args, **kwargs):\n    return (function, args, kwargs)",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__len__": "def __len__(self):\n    return self._size",
    ".sklearn.externals.joblib.parallel.py@@Parallel._dispatch": "def _dispatch(self, batch):\n    if self._aborting:\n        return\n    self.n_dispatched_tasks += len(batch)\n    self.n_dispatched_batches += 1\n    dispatch_timestamp = time.time()\n    cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n    job = self._backend.apply_async(batch, callback=cb)\n    self._jobs.append(job)",
    ".sklearn.externals.joblib.parallel.py@@BatchCompletionCallBack.__init__": "def __init__(self, dispatch_timestamp, batch_size, parallel):\n    self.dispatch_timestamp = dispatch_timestamp\n    self.batch_size = batch_size\n    self.parallel = parallel",
    ".sklearn.externals.joblib._parallel_backends.py@@SequentialBackend.apply_async": "def apply_async(self, func, callback=None):\n    result = ImmediateResult(func)\n    if callback:\n        callback(result)\n    return result",
    ".sklearn.externals.joblib._parallel_backends.py@@ImmediateResult.__init__": "def __init__(self, batch):\n    self.results = batch()",
    ".sklearn.externals.joblib.parallel.py@@BatchedCalls.__call__": "def __call__(self):\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]",
    ".sklearn.pipeline.py@@_fit_one_transformer": "def _fit_one_transformer(transformer, X, y, weight=None, **fit_params):\n    return transformer.fit(X, y)",
    ".sklearn.externals.joblib.parallel.py@@BatchCompletionCallBack.__call__": "def __call__(self, out):\n    self.parallel.n_completed_tasks += self.batch_size\n    this_batch_duration = time.time() - self.dispatch_timestamp\n    self.parallel._backend.batch_completed(self.batch_size, this_batch_duration)\n    self.parallel.print_progress()\n    if self.parallel._original_iterator is not None:\n        self.parallel.dispatch_next()",
    ".sklearn.externals.joblib._parallel_backends.py@@ParallelBackendBase.batch_completed": "def batch_completed(self, batch_size, duration):",
    ".sklearn.externals.joblib.parallel.py@@Parallel.print_progress": "def print_progress(self):\n    if not self.verbose:\n        return\n    elapsed_time = time.time() - self._start_time\n    if self._original_iterator is not None:\n        if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n            return\n        self._print('Done %3i tasks      | elapsed: %s', (self.n_completed_tasks, short_format_time(elapsed_time)))\n    else:\n        index = self.n_completed_tasks\n        total_tasks = self.n_dispatched_tasks\n        if not index == 0:\n            cursor = total_tasks - index + 1 - self._pre_dispatch_amount\n            frequency = total_tasks // self.verbose + 1\n            is_last_item = index + 1 == total_tasks\n            if is_last_item or cursor % frequency:\n                return\n        remaining_time = elapsed_time / index * (self.n_dispatched_tasks - index * 1.0)\n        self._print('Done %3i out of %3i | elapsed: %s remaining: %s', (index, total_tasks, short_format_time(elapsed_time), short_format_time(remaining_time)))",
    ".sklearn.externals.joblib.parallel.py@@Parallel.retrieve": "def retrieve(self):\n    self._output = list()\n    while self._iterating or len(self._jobs) > 0:\n        if len(self._jobs) == 0:\n            time.sleep(0.01)\n            continue\n        with self._lock:\n            job = self._jobs.pop(0)\n        try:\n            if getattr(self._backend, 'supports_timeout', False):\n                self._output.extend(job.get(timeout=self.timeout))\n            else:\n                self._output.extend(job.get())\n        except BaseException as exception:\n            self._aborting = True\n            backend = self._backend\n            if backend is not None and hasattr(backend, 'abort_everything'):\n                ensure_ready = self._managed_backend\n                backend.abort_everything(ensure_ready=ensure_ready)\n            if not isinstance(exception, TransportableException):\n                raise\n            else:\n                this_report = format_outer_frames(context=10, stack_start=1)\n                report = 'Multiprocessing exception:\\n%s\\n---------------------------------------------------------------------------\\nSub-process traceback:\\n---------------------------------------------------------------------------\\n%s' % (this_report, exception.message)\n                exception_type = _mk_exception(exception.etype)[0]\n                exception = exception_type(report)\n                raise exception",
    ".sklearn.externals.joblib._parallel_backends.py@@ImmediateResult.get": "def get(self):\n    return self.results",
    ".sklearn.externals.joblib.logger.py@@short_format_time": "def short_format_time(t):\n    t = _squeeze_time(t)\n    if t > 60:\n        return '%4.1fmin' % (t / 60.0)\n    else:\n        return ' %5.1fs' % t",
    ".sklearn.externals.joblib.logger.py@@_squeeze_time": "def _squeeze_time(t):\n    if sys.platform.startswith('win'):\n        return max(0, t - 0.1)\n    else:\n        return t",
    ".sklearn.externals.joblib.parallel.py@@Parallel._print": "def _print(self, msg, msg_args):\n    if not self.verbose:\n        return\n    if self.verbose < 50:\n        writer = sys.stderr.write\n    else:\n        writer = sys.stdout.write\n    msg = msg % msg_args\n    writer('[%s]: %s\\n' % (self, msg))",
    ".sklearn.externals.joblib.parallel.py@@Parallel._terminate_backend": "def _terminate_backend(self):\n    if self._backend is not None:\n        self._backend.terminate()",
    ".sklearn.externals.joblib._parallel_backends.py@@ParallelBackendBase.terminate": "def terminate(self):",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.__init__": "def __init__(self, func=None, inverse_func=None, validate=True, accept_sparse=False, pass_y='deprecated', check_inverse=True, kw_args=None, inv_kw_args=None):\n    self.func = func\n    self.inverse_func = inverse_func\n    self.validate = validate\n    self.accept_sparse = accept_sparse\n    self.pass_y = pass_y\n    self.check_inverse = check_inverse\n    self.kw_args = kw_args\n    self.inv_kw_args = inv_kw_args",
    ".sklearn.preprocessing._function_transformer.py@@FunctionTransformer.fit": "def fit(self, X, y=None):\n    if self.validate:\n        X = check_array(X, self.accept_sparse)\n    if self.check_inverse and (not (self.func is None or self.inverse_func is None)):\n        self._check_inverse_transform(X)\n    return self",
    ".sklearn.preprocessing.data.py@@StandardScaler.__init__": "def __init__(self, copy=True, with_mean=True, with_std=True):\n    self.with_mean = with_mean\n    self.with_std = with_std\n    self.copy = copy",
    ".sklearn.preprocessing.data.py@@StandardScaler.fit": "def fit(self, X, y=None):\n    self._reset()\n    return self.partial_fit(X, y)",
    ".sklearn.preprocessing.data.py@@StandardScaler._reset": "def _reset(self):\n    if hasattr(self, 'scale_'):\n        del self.scale_\n        del self.n_samples_seen_\n        del self.mean_\n        del self.var_",
    ".sklearn.preprocessing.data.py@@StandardScaler.partial_fit": "def partial_fit(self, X, y=None):\n    X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy, warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n    if sparse.issparse(X):\n        if self.with_mean:\n            raise ValueError('Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.')\n        if self.with_std:\n            if not hasattr(self, 'n_samples_seen_'):\n                self.mean_, self.var_ = mean_variance_axis(X, axis=0)\n                self.n_samples_seen_ = X.shape[0]\n            else:\n                self.mean_, self.var_, self.n_samples_seen_ = incr_mean_variance_axis(X, axis=0, last_mean=self.mean_, last_var=self.var_, last_n=self.n_samples_seen_)\n        else:\n            self.mean_ = None\n            self.var_ = None\n            if not hasattr(self, 'n_samples_seen_'):\n                self.n_samples_seen_ = X.shape[0]\n            else:\n                self.n_samples_seen_ += X.shape[0]\n    else:\n        if not hasattr(self, 'n_samples_seen_'):\n            self.mean_ = 0.0\n            self.n_samples_seen_ = 0\n            if self.with_std:\n                self.var_ = 0.0\n            else:\n                self.var_ = None\n        if not self.with_mean and (not self.with_std):\n            self.mean_ = None\n            self.var_ = None\n            self.n_samples_seen_ += X.shape[0]\n        else:\n            self.mean_, self.var_, self.n_samples_seen_ = _incremental_mean_and_var(X, self.mean_, self.var_, self.n_samples_seen_)\n    if self.with_std:\n        self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n    else:\n        self.scale_ = None\n    return self",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    array_orig = array\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of strings will be interpreted as decimal numbers if parameter 'dtype' is 'numeric'. It is recommended that you convert the array to type np.float64 before passing it to check_array.\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(X.sum()):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.validation.py@@_shape_repr": "def _shape_repr(shape):\n    if len(shape) == 0:\n        return '()'\n    joined = ', '.join(('%d' % e for e in shape))\n    if len(shape) == 1:\n        joined += ','\n    return '(%s)' % joined",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        return x.shape[0]\n    else:\n        return len(x)",
    ".sklearn.utils.extmath.py@@_incremental_mean_and_var": "def _incremental_mean_and_var(X, last_mean=0.0, last_variance=None, last_sample_count=0):\n    last_sum = last_mean * last_sample_count\n    new_sum = X.sum(axis=0)\n    new_sample_count = X.shape[0]\n    updated_sample_count = last_sample_count + new_sample_count\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n    if last_variance is None:\n        updated_variance = None\n    else:\n        new_unnormalized_variance = X.var(axis=0) * new_sample_count\n        if last_sample_count == 0:\n            updated_unnormalized_variance = new_unnormalized_variance\n        else:\n            last_over_new_count = last_sample_count / new_sample_count\n            last_unnormalized_variance = last_variance * last_sample_count\n            updated_unnormalized_variance = last_unnormalized_variance + new_unnormalized_variance + last_over_new_count / updated_sample_count * (last_sum / last_over_new_count - new_sum) ** 2\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n    return (updated_mean, updated_variance, updated_sample_count)",
    ".sklearn.preprocessing.data.py@@_handle_zeros_in_scale": "def _handle_zeros_in_scale(scale, copy=True):\n    if np.isscalar(scale):\n        if scale == 0.0:\n            scale = 1.0\n        return scale\n    elif isinstance(scale, np.ndarray):\n        if copy:\n            scale = scale.copy()\n        scale[scale == 0.0] = 1.0\n        return scale",
    ".sklearn.feature_extraction.dict_vectorizer.py@@DictVectorizer.__init__": "def __init__(self, dtype=np.float64, separator='=', sparse=True, sort=True):\n    self.dtype = dtype\n    self.separator = separator\n    self.sparse = sparse\n    self.sort = sort",
    ".sklearn.feature_extraction.dict_vectorizer.py@@DictVectorizer.fit": "def fit(self, X, y=None):\n    feature_names = []\n    vocab = {}\n    for x in X:\n        for f, v in six.iteritems(x):\n            if isinstance(v, six.string_types):\n                f = '%s%s%s' % (f, self.separator, v)\n            if f not in vocab:\n                feature_names.append(f)\n                vocab[f] = len(vocab)\n    if self.sort:\n        feature_names.sort()\n        vocab = dict(((f, i) for i, f in enumerate(feature_names)))\n    self.feature_names_ = feature_names\n    self.vocabulary_ = vocab\n    return self"
}