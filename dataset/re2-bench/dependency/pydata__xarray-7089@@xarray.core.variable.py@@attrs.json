{
    ".xarray.core.variable.py@@Variable.encoding": "def encoding(self) -> dict[Any, Any]:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.utils.py@@NdimSizeLenMixin.ndim": "def ndim(self: Any) -> int:\n    return len(self.shape)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.shape": "def shape(self) -> tuple[int, ...]:\n    return (len(self.array),)",
    ".xarray.core.common.py@@AbstractArray.__array__": "def __array__(self: Any, dtype: DTypeLike=None) -> np.ndarray:\n    return np.asarray(self.values, dtype=dtype)",
    ".xarray.core.variable.py@@Variable.values": "def values(self):\n    return _as_array_or_item(self._data)",
    ".xarray.core.variable.py@@_as_array_or_item": "def _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__array__": "def __array__(self, dtype: DTypeLike=None) -> np.ndarray:\n    if dtype is None:\n        dtype = self.dtype\n    array = self.array\n    if isinstance(array, pd.PeriodIndex):\n        with suppress(AttributeError):\n            array = array.astype('object')\n    return np.asarray(array.values, dtype=dtype)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.dtype": "def dtype(self) -> np.dtype:\n    return self._dtype",
    ".xarray.core.formatting.py@@format_items": "def format_items(x):\n    x = np.asarray(x)\n    timedelta_format = 'datetime'\n    if np.issubdtype(x.dtype, np.timedelta64):\n        x = np.asarray(x, dtype='timedelta64[ns]')\n        day_part = x[~pd.isnull(x)].astype('timedelta64[D]').astype('timedelta64[ns]')\n        time_needed = x[~pd.isnull(x)] != day_part\n        day_needed = day_part != np.timedelta64(0, 'ns')\n        if np.logical_not(day_needed).all():\n            timedelta_format = 'time'\n        elif np.logical_not(time_needed).all():\n            timedelta_format = 'date'\n    formatted = [format_item(xi, timedelta_format) for xi in x]\n    return formatted",
    ".xarray.core.formatting.py@@format_item": "def format_item(x, timedelta_format=None, quote_strings=True):\n    if isinstance(x, (np.datetime64, datetime)):\n        return format_timestamp(x)\n    if isinstance(x, (np.timedelta64, timedelta)):\n        return format_timedelta(x, timedelta_format=timedelta_format)\n    elif isinstance(x, (str, bytes)):\n        return repr(x) if quote_strings else x\n    elif hasattr(x, 'dtype') and np.issubdtype(x.dtype, np.floating):\n        return f'{x.item():.4}'\n    else:\n        return str(x)",
    ".xarray.core.utils.py@@NdimSizeLenMixin.size": "def size(self: Any) -> int:\n    return math.prod(self.shape)",
    ".xarray.core.dataarray.py@@DataArray.dims": "def dims(self) -> tuple[Hashable, ...]:\n    return self.variable.dims",
    ".xarray.core.dataarray.py@@DataArray.variable": "def variable(self) -> Variable:\n    return self._variable",
    ".xarray.core.variable.py@@Variable.dims": "def dims(self) -> tuple[Hashable, ...]:\n    return self._dims",
    ".xarray.core.dataarray.py@@DataArray.coords": "def coords(self) -> DataArrayCoordinates:\n    return DataArrayCoordinates(self)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__init__": "def __init__(self, dataarray: DataArray):\n    self._data = dataarray",
    ".xarray.core.formatting.py@@unindexed_dims_repr": "def unindexed_dims_repr(dims, coords, max_rows: int | None=None):\n    unindexed_dims = [d for d in dims if d not in coords]\n    if unindexed_dims:\n        dims_start = 'Dimensions without coordinates: '\n        dims_str = _element_formatter(unindexed_dims, col_width=len(dims_start), max_rows=max_rows)\n        return dims_start + dims_str\n    else:\n        return None",
    ".xarray.core.coordinates.py@@Coordinates.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._names",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._names": "def _names(self) -> set[Hashable]:\n    return set(self._data._coords)",
    ".xarray.core.dataarray.py@@DataArray.attrs": "def attrs(self) -> dict[Any, Any]:\n    return self.variable.attrs",
    ".xarray.core.variable.py@@Variable.attrs": "def attrs(self) -> dict[Any, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.dataset.py@@Dataset.__init__": "def __init__(self, data_vars: Mapping[Any, Any] | None=None, coords: Mapping[Any, Any] | None=None, attrs: Mapping[Any, Any] | None=None) -> None:\n    if data_vars is None:\n        data_vars = {}\n    if coords is None:\n        coords = {}\n    both_data_and_coords = set(data_vars) & set(coords)\n    if both_data_and_coords:\n        raise ValueError(f'variables {both_data_and_coords!r} are found in both data_vars and coords')\n    if isinstance(coords, Dataset):\n        coords = coords.variables\n    variables, coord_names, dims, indexes, _ = merge_data_and_coords(data_vars, coords, compat='broadcast_equals')\n    self._attrs = dict(attrs) if attrs is not None else None\n    self._close = None\n    self._encoding = None\n    self._variables = variables\n    self._coord_names = coord_names\n    self._dims = dims\n    self._indexes = indexes",
    ".xarray.core.merge.py@@merge_data_and_coords": "def merge_data_and_coords(data_vars, coords, compat='broadcast_equals', join='outer'):\n    indexes, coords = _create_indexes_from_coords(coords, data_vars)\n    objects = [data_vars, coords]\n    explicit_coords = coords.keys()\n    return merge_core(objects, compat, join, explicit_coords=explicit_coords, indexes=Indexes(indexes, coords))",
    ".xarray.core.merge.py@@_create_indexes_from_coords": "def _create_indexes_from_coords(coords, data_vars=None):\n    all_variables = dict(coords)\n    if data_vars is not None:\n        all_variables.update(data_vars)\n    indexes = {}\n    updated_coords = {}\n    index_vars = {k: v for k, v in all_variables.items() if k in coords or isinstance(v, pd.MultiIndex)}\n    for name, obj in index_vars.items():\n        variable = as_variable(obj, name=name)\n        if variable.dims == (name,):\n            idx, idx_vars = create_default_index_implicit(variable, all_variables)\n            indexes.update({k: idx for k in idx_vars})\n            updated_coords.update(idx_vars)\n            all_variables.update(idx_vars)\n        else:\n            updated_coords[name] = obj\n    return (indexes, updated_coords)",
    ".xarray.core.indexes.py@@Indexes.__init__": "def __init__(self, indexes: dict[Any, T_PandasOrXarrayIndex], variables: dict[Any, Variable]):\n    self._indexes = indexes\n    self._variables = variables\n    self._dims: Mapping[Hashable, int] | None = None\n    self.__coord_name_id: dict[Any, int] | None = None\n    self.__id_index: dict[int, T_PandasOrXarrayIndex] | None = None\n    self.__id_coord_names: dict[int, tuple[Hashable, ...]] | None = None",
    ".xarray.core.merge.py@@merge_core": "def merge_core(objects: Iterable[CoercibleMapping], compat: CompatOptions='broadcast_equals', join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='override', priority_arg: int | None=None, explicit_coords: Sequence | None=None, indexes: Mapping[Any, Any] | None=None, fill_value: object=dtypes.NA) -> _MergeResult:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned, indexes=indexes)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat, combine_attrs=combine_attrs)\n    dims = calculate_dimensions(variables)\n    coord_names, noncoord_names = determine_coords(coerced)\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError(f'unable to determine if these variables should be coordinates or not in the merged result: {ambiguous_coords}')\n    attrs = merge_attrs([var.attrs for var in coerced if isinstance(var, (Dataset, DataArray))], combine_attrs)\n    return _MergeResult(variables, coord_names, dims, out_indexes, attrs)",
    ".xarray.core.merge.py@@_assert_compat_valid": "def _assert_compat_valid(compat):\n    if compat not in _VALID_COMPAT:\n        raise ValueError(f'compat={compat!r} invalid: must be {set(_VALID_COMPAT)}')",
    ".xarray.core.utils.py@@Frozen.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self.mapping",
    ".xarray.core.merge.py@@coerce_pandas_values": "def coerce_pandas_values(objects: Iterable[CoercibleMapping]) -> list[DatasetLike]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables: DatasetLike = obj\n        else:\n            variables = {}\n            if isinstance(obj, PANDAS_TYPES):\n                obj = dict(obj.items())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out",
    ".xarray.core.alignment.py@@deep_align": "def deep_align(objects: Iterable[Any], join: JoinOptions='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for position, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(position)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            current_out = {}\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    positions.append(position)\n                    keys.append(k)\n                    targets.append(v)\n                    current_out[k] = not_replaced\n                else:\n                    current_out[k] = v\n            out.append(current_out)\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))\n        else:\n            out.append(variables)\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n    for arg in out:\n        assert arg is not not_replaced\n        if is_dict_like(arg):\n            assert all((value is not not_replaced for value in arg.values()))\n    return out",
    ".xarray.core.alignment.py@@is_alignable": "def is_alignable(obj):\n    return isinstance(obj, (DataArray, Dataset))",
    ".xarray.core.utils.py@@is_dict_like": "def is_dict_like(value: Any) -> TypeGuard[Mapping]:\n    return hasattr(value, 'keys') and hasattr(value, '__getitem__')",
    ".xarray.core.alignment.py@@align": "def align(*objects: DataAlignable, join: JoinOptions='inner', copy: bool=True, indexes=None, exclude=frozenset(), fill_value=dtypes.NA) -> tuple[DataAlignable, ...]:\n    aligner = Aligner(objects, join=join, copy=copy, indexes=indexes, exclude_dims=exclude, fill_value=fill_value)\n    aligner.align()\n    return aligner.results",
    ".xarray.core.alignment.py@@Aligner.__init__": "def __init__(self, objects: Iterable[DataAlignable], join: str='inner', indexes: Mapping[Any, Any]=None, exclude_dims: Iterable=frozenset(), exclude_vars: Iterable[Hashable]=frozenset(), method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False):\n    self.objects = tuple(objects)\n    self.objects_matching_indexes = ()\n    if join not in ['inner', 'outer', 'override', 'exact', 'left', 'right']:\n        raise ValueError(f'invalid value for join: {join}')\n    self.join = join\n    self.copy = copy\n    self.fill_value = fill_value\n    self.sparse = sparse\n    if method is None and tolerance is None:\n        self.reindex_kwargs = {}\n    else:\n        self.reindex_kwargs = {'method': method, 'tolerance': tolerance}\n    if isinstance(exclude_dims, str):\n        exclude_dims = [exclude_dims]\n    self.exclude_dims = frozenset(exclude_dims)\n    self.exclude_vars = frozenset(exclude_vars)\n    if indexes is None:\n        indexes = {}\n    self.indexes, self.index_vars = self._normalize_indexes(indexes)\n    self.all_indexes = {}\n    self.all_index_vars = {}\n    self.unindexed_dim_sizes = {}\n    self.aligned_indexes = {}\n    self.aligned_index_vars = {}\n    self.reindex = {}\n    self.results = tuple()",
    ".xarray.core.alignment.py@@Aligner._normalize_indexes": "def _normalize_indexes(self, indexes: Mapping[Any, Any]) -> tuple[NormalizedIndexes, NormalizedIndexVars]:\n    if isinstance(indexes, Indexes):\n        xr_variables = dict(indexes.variables)\n    else:\n        xr_variables = {}\n    xr_indexes: dict[Hashable, Index] = {}\n    for k, idx in indexes.items():\n        if not isinstance(idx, Index):\n            if getattr(idx, 'dims', (k,)) != (k,):\n                raise ValueError(f\"Indexer has dimensions {idx.dims} that are different from that to be indexed along '{k}'\")\n            data = as_compatible_data(idx)\n            pd_idx = safe_cast_to_index(data)\n            pd_idx.name = k\n            if isinstance(pd_idx, pd.MultiIndex):\n                idx = PandasMultiIndex(pd_idx, k)\n            else:\n                idx = PandasIndex(pd_idx, k, coord_dtype=data.dtype)\n            xr_variables.update(idx.create_variables())\n        xr_indexes[k] = idx\n    normalized_indexes = {}\n    normalized_index_vars = {}\n    for idx, index_vars in Indexes(xr_indexes, xr_variables).group_by_index():\n        coord_names_and_dims = []\n        all_dims: set[Hashable] = set()\n        for name, var in index_vars.items():\n            dims = var.dims\n            coord_names_and_dims.append((name, dims))\n            all_dims.update(dims)\n        exclude_dims = all_dims & self.exclude_dims\n        if exclude_dims == all_dims:\n            continue\n        elif exclude_dims:\n            excl_dims_str = ', '.join((str(d) for d in exclude_dims))\n            incl_dims_str = ', '.join((str(d) for d in all_dims - exclude_dims))\n            raise ValueError(f'cannot exclude dimension(s) {excl_dims_str} from alignment because these are used by an index together with non-excluded dimensions {incl_dims_str}')\n        key = (tuple(coord_names_and_dims), type(idx))\n        normalized_indexes[key] = idx\n        normalized_index_vars[key] = index_vars\n    return (normalized_indexes, normalized_index_vars)",
    ".xarray.core.indexes.py@@Indexes.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.utils.py@@Frozen.__init__": "def __init__(self, mapping: Mapping[K, V]):\n    self.mapping = mapping",
    ".xarray.core.utils.py@@Frozen.__iter__": "def __iter__(self) -> Iterator[K]:\n    return iter(self.mapping)",
    ".xarray.core.indexes.py@@Indexes.__iter__": "def __iter__(self) -> Iterator[T_PandasOrXarrayIndex]:\n    return iter(self._indexes)",
    ".xarray.core.indexes.py@@Indexes.group_by_index": "def group_by_index(self) -> list[tuple[T_PandasOrXarrayIndex, dict[Hashable, Variable]]]:\n    index_coords = []\n    for i in self._id_index:\n        index = self._id_index[i]\n        coords = {k: self._variables[k] for k in self._id_coord_names[i]}\n        index_coords.append((index, coords))\n    return index_coords",
    ".xarray.core.indexes.py@@Indexes._id_index": "def _id_index(self) -> dict[int, T_PandasOrXarrayIndex]:\n    if self.__id_index is None:\n        self.__id_index = {id(idx): idx for idx in self.get_unique()}\n    return self.__id_index",
    ".xarray.core.indexes.py@@Indexes.get_unique": "def get_unique(self) -> list[T_PandasOrXarrayIndex]:\n    unique_indexes: list[T_PandasOrXarrayIndex] = []\n    seen: set[int] = set()\n    for index in self._indexes.values():\n        index_id = id(index)\n        if index_id not in seen:\n            unique_indexes.append(index)\n            seen.add(index_id)\n    return unique_indexes",
    ".xarray.core.alignment.py@@Aligner.align": "def align(self) -> None:\n    if not self.indexes and len(self.objects) == 1:\n        obj, = self.objects\n        self.results = (obj.copy(deep=self.copy),)\n    self.find_matching_indexes()\n    self.find_matching_unindexed_dims()\n    self.assert_no_index_conflict()\n    self.align_indexes()\n    self.assert_unindexed_dim_sizes_equal()\n    if self.join == 'override':\n        self.override_indexes()\n    else:\n        self.reindex_all()",
    ".xarray.core.alignment.py@@Aligner.find_matching_indexes": "def find_matching_indexes(self) -> None:\n    all_indexes: dict[MatchingIndexKey, list[Index]]\n    all_index_vars: dict[MatchingIndexKey, list[dict[Hashable, Variable]]]\n    all_indexes_dim_sizes: dict[MatchingIndexKey, dict[Hashable, set]]\n    objects_matching_indexes: list[dict[MatchingIndexKey, Index]]\n    all_indexes = defaultdict(list)\n    all_index_vars = defaultdict(list)\n    all_indexes_dim_sizes = defaultdict(lambda: defaultdict(set))\n    objects_matching_indexes = []\n    for obj in self.objects:\n        obj_indexes, obj_index_vars = self._normalize_indexes(obj.xindexes)\n        objects_matching_indexes.append(obj_indexes)\n        for key, idx in obj_indexes.items():\n            all_indexes[key].append(idx)\n        for key, index_vars in obj_index_vars.items():\n            all_index_vars[key].append(index_vars)\n            for dim, size in calculate_dimensions(index_vars).items():\n                all_indexes_dim_sizes[key][dim].add(size)\n    self.objects_matching_indexes = tuple(objects_matching_indexes)\n    self.all_indexes = all_indexes\n    self.all_index_vars = all_index_vars\n    if self.join == 'override':\n        for dim_sizes in all_indexes_dim_sizes.values():\n            for dim, sizes in dim_sizes.items():\n                if len(sizes) > 1:\n                    raise ValueError(f\"cannot align objects with join='override' with matching indexes along dimension {dim!r} that don't have the same size\")",
    ".xarray.core.alignment.py@@Aligner.find_matching_unindexed_dims": "def find_matching_unindexed_dims(self) -> None:\n    unindexed_dim_sizes = defaultdict(set)\n    for obj in self.objects:\n        for dim in obj.dims:\n            if dim not in self.exclude_dims and dim not in obj.xindexes.dims:\n                unindexed_dim_sizes[dim].add(obj.sizes[dim])\n    self.unindexed_dim_sizes = unindexed_dim_sizes",
    ".xarray.core.alignment.py@@Aligner.assert_no_index_conflict": "def assert_no_index_conflict(self) -> None:\n    matching_keys = set(self.all_indexes) | set(self.indexes)\n    coord_count: dict[Hashable, int] = defaultdict(int)\n    dim_count: dict[Hashable, int] = defaultdict(int)\n    for coord_names_dims, _ in matching_keys:\n        dims_set: set[Hashable] = set()\n        for name, dims in coord_names_dims:\n            coord_count[name] += 1\n            dims_set.update(dims)\n        for dim in dims_set:\n            dim_count[dim] += 1\n    for count, msg in [(coord_count, 'coordinates'), (dim_count, 'dimensions')]:\n        dup = {k: v for k, v in count.items() if v > 1}\n        if dup:\n            items_msg = ', '.join((f'{k!r} ({v} conflicting indexes)' for k, v in dup.items()))\n            raise ValueError(f\"cannot re-index or align objects with conflicting indexes found for the following {msg}: {items_msg}\\nConflicting indexes may occur when\\n- they relate to different sets of coordinate and/or dimension names\\n- they don't have the same type\\n- they may be used to reindex data along common dimensions\")",
    ".xarray.core.alignment.py@@Aligner.align_indexes": "def align_indexes(self) -> None:\n    aligned_indexes = {}\n    aligned_index_vars = {}\n    reindex = {}\n    new_indexes = {}\n    new_index_vars = {}\n    for key, matching_indexes in self.all_indexes.items():\n        matching_index_vars = self.all_index_vars[key]\n        dims = {d for coord in matching_index_vars[0].values() for d in coord.dims}\n        index_cls = key[1]\n        if self.join == 'override':\n            joined_index = matching_indexes[0]\n            joined_index_vars = matching_index_vars[0]\n            need_reindex = False\n        elif key in self.indexes:\n            joined_index = self.indexes[key]\n            joined_index_vars = self.index_vars[key]\n            cmp_indexes = list(zip([joined_index] + matching_indexes, [joined_index_vars] + matching_index_vars))\n            need_reindex = self._need_reindex(dims, cmp_indexes)\n        else:\n            if len(matching_indexes) > 1:\n                need_reindex = self._need_reindex(dims, list(zip(matching_indexes, matching_index_vars)))\n            else:\n                need_reindex = False\n            if need_reindex:\n                if self.join == 'exact':\n                    raise ValueError(\"cannot align objects with join='exact' where index/labels/sizes are not equal along these coordinates (dimensions): \" + ', '.join((f'{name!r} {dims!r}' for name, dims in key[0])))\n                joiner = self._get_index_joiner(index_cls)\n                joined_index = joiner(matching_indexes)\n                if self.join == 'left':\n                    joined_index_vars = matching_index_vars[0]\n                elif self.join == 'right':\n                    joined_index_vars = matching_index_vars[-1]\n                else:\n                    joined_index_vars = joined_index.create_variables()\n            else:\n                joined_index = matching_indexes[0]\n                joined_index_vars = matching_index_vars[0]\n        reindex[key] = need_reindex\n        aligned_indexes[key] = joined_index\n        aligned_index_vars[key] = joined_index_vars\n        for name, var in joined_index_vars.items():\n            new_indexes[name] = joined_index\n            new_index_vars[name] = var\n    for key, idx in self.indexes.items():\n        if key not in aligned_indexes:\n            index_vars = self.index_vars[key]\n            reindex[key] = False\n            aligned_indexes[key] = idx\n            aligned_index_vars[key] = index_vars\n            for name, var in index_vars.items():\n                new_indexes[name] = idx\n                new_index_vars[name] = var\n    self.aligned_indexes = aligned_indexes\n    self.aligned_index_vars = aligned_index_vars\n    self.reindex = reindex\n    self.new_indexes = Indexes(new_indexes, new_index_vars)",
    ".xarray.core.alignment.py@@Aligner.assert_unindexed_dim_sizes_equal": "def assert_unindexed_dim_sizes_equal(self) -> None:\n    for dim, sizes in self.unindexed_dim_sizes.items():\n        index_size = self.new_indexes.dims.get(dim)\n        if index_size is not None:\n            sizes.add(index_size)\n            add_err_msg = f' (note: an index is found along that dimension with size={index_size!r})'\n        else:\n            add_err_msg = ''\n        if len(sizes) > 1:\n            raise ValueError(f'cannot reindex or align along dimension {dim!r} because of conflicting dimension sizes: {sizes!r}' + add_err_msg)",
    ".xarray.core.alignment.py@@Aligner.reindex_all": "def reindex_all(self) -> None:\n    self.results = tuple((self._reindex_one(obj, matching_indexes) for obj, matching_indexes in zip(self.objects, self.objects_matching_indexes)))",
    ".xarray.core.merge.py@@collect_variables_and_indexes": "def collect_variables_and_indexes(list_of_mappings: list[DatasetLike], indexes: Mapping[Any, Any] | None=None) -> dict[Hashable, list[MergeElement]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if indexes is None:\n        indexes = {}\n    grouped: dict[Hashable, list[MergeElement]] = defaultdict(list)\n\n    def append(name, variable, index):\n        grouped[name].append((variable, index))\n\n    def append_all(variables, indexes):\n        for name, variable in variables.items():\n            append(name, variable, indexes.get(name))\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            append_all(mapping.variables, mapping._indexes)\n            continue\n        for name, variable in mapping.items():\n            if isinstance(variable, DataArray):\n                coords = variable._coords.copy()\n                indexes = dict(variable._indexes)\n                coords.pop(name, None)\n                indexes.pop(name, None)\n                append_all(coords, indexes)\n            variable = as_variable(variable, name=name)\n            if name in indexes:\n                append(name, variable, indexes[name])\n            elif variable.dims == (name,):\n                idx, idx_vars = create_default_index_implicit(variable)\n                append_all(idx_vars, {k: idx for k in idx_vars})\n            else:\n                append(name, variable, None)\n    return grouped",
    ".xarray.core.variable.py@@as_variable": "def as_variable(obj, name=None) -> Variable | IndexVariable:\n    from .dataarray import DataArray\n    if isinstance(obj, DataArray):\n        obj = obj.variable\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        if isinstance(obj[1], DataArray):\n            raise TypeError('Using a DataArray object to construct a variable is ambiguous, please extract the data using the .data property.')\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            raise error.__class__('Could not convert tuple of form (dims, data[, attrs, encoding]): {} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(f'variable {name!r} has invalid type {type(obj)!r}')\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(f'cannot set variable {name!r} with {data.ndim!r}-dimensional data without explicit dimension names. Pass a tuple of (dims, data) instead.')\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError(f'unable to convert object into a variable without an explicit list of dimensions: {obj!r}')\n    if name is not None and name in obj.dims:\n        if obj.ndim != 1:\n            raise MissingDimensionsError(f'{name!r} has more than 1-dimension and the same name as one of its dimensions {obj.dims!r}. xarray disallows such variables because they conflict with the coordinates used to label dimensions.')\n        obj = obj.to_index_variable()\n    return obj",
    ".xarray.core.variable.py@@as_compatible_data": "def as_compatible_data(data, fastpath=False):\n    from .dataarray import DataArray\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        return _maybe_wrap_data(data)\n    if isinstance(data, (Variable, DataArray)):\n        return data.data\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n    if isinstance(data, pd.Timestamp):\n        data = np.datetime64(data.value, 'ns')\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):\n        data = data.values\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n    if not isinstance(data, np.ndarray) and (hasattr(data, '__array_function__') or hasattr(data, '__array_namespace__')):\n        return data\n    data = np.asarray(data)\n    if isinstance(data, np.ndarray) and data.dtype.kind in 'OMm':\n        data = _possibly_convert_objects(data)\n    return _maybe_wrap_data(data)",
    ".xarray.core.variable.py@@_maybe_wrap_data": "def _maybe_wrap_data(data):\n    if isinstance(data, pd.Index):\n        return PandasIndexingAdapter(data)\n    return data",
    ".xarray.core.indexes.py@@Indexes.__contains__": "def __contains__(self, key) -> bool:\n    return key in self._indexes",
    ".xarray.core.merge.py@@append": "def append(name, variable, index):\n    grouped[name].append((variable, index))",
    ".xarray.core.merge.py@@_get_priority_vars_and_indexes": "def _get_priority_vars_and_indexes(objects: list[DatasetLike], priority_arg: int | None, compat: CompatOptions='equals') -> dict[Hashable, MergeElement]:\n    if priority_arg is None:\n        return {}\n    collected = collect_variables_and_indexes([objects[priority_arg]])\n    variables, indexes = merge_collected(collected, compat=compat)\n    grouped: dict[Hashable, MergeElement] = {}\n    for name, variable in variables.items():\n        grouped[name] = (variable, indexes.get(name))\n    return grouped",
    ".xarray.core.merge.py@@merge_collected": "def merge_collected(grouped: dict[Hashable, list[MergeElement]], prioritized: Mapping[Any, MergeElement]=None, compat: CompatOptions='minimal', combine_attrs: CombineAttrsOptions='override', equals: dict[Hashable, bool] | None=None) -> tuple[dict[Hashable, Variable], dict[Hashable, Index]]:\n    if prioritized is None:\n        prioritized = {}\n    if equals is None:\n        equals = {}\n    _assert_compat_valid(compat)\n    _assert_prioritized_valid(grouped, prioritized)\n    merged_vars: dict[Hashable, Variable] = {}\n    merged_indexes: dict[Hashable, Index] = {}\n    index_cmp_cache: dict[tuple[int, int], bool | None] = {}\n    for name, elements_list in grouped.items():\n        if name in prioritized:\n            variable, index = prioritized[name]\n            merged_vars[name] = variable\n            if index is not None:\n                merged_indexes[name] = index\n        else:\n            indexed_elements = [(variable, index) for variable, index in elements_list if index is not None]\n            if indexed_elements:\n                variable, index = indexed_elements[0]\n                for other_var, other_index in indexed_elements[1:]:\n                    if not indexes_equal(index, other_index, variable, other_var, index_cmp_cache):\n                        raise MergeError(f'conflicting values/indexes on objects to be combined fo coordinate {name!r}\\nfirst index: {index!r}\\nsecond index: {other_index!r}\\nfirst variable: {variable!r}\\nsecond variable: {other_var!r}\\n')\n                if compat == 'identical':\n                    for other_variable, _ in indexed_elements[1:]:\n                        if not dict_equiv(variable.attrs, other_variable.attrs):\n                            raise MergeError(f'conflicting attribute values on combined variable {name!r}:\\nfirst value: {variable.attrs!r}\\nsecond value: {other_variable.attrs!r}')\n                merged_vars[name] = variable\n                merged_vars[name].attrs = merge_attrs([var.attrs for var, _ in indexed_elements], combine_attrs=combine_attrs)\n                merged_indexes[name] = index\n            else:\n                variables = [variable for variable, _ in elements_list]\n                try:\n                    merged_vars[name] = unique_variable(name, variables, compat, equals.get(name, None))\n                except MergeError:\n                    if compat != 'minimal':\n                        raise\n                if name in merged_vars:\n                    merged_vars[name].attrs = merge_attrs([var.attrs for var in variables], combine_attrs=combine_attrs)\n    return (merged_vars, merged_indexes)",
    ".xarray.core.merge.py@@_assert_prioritized_valid": "def _assert_prioritized_valid(grouped: dict[Hashable, list[MergeElement]], prioritized: Mapping[Any, MergeElement]) -> None:\n    prioritized_names = set(prioritized)\n    grouped_by_index: dict[int, list[Hashable]] = defaultdict(list)\n    indexes: dict[int, Index] = {}\n    for name, elements_list in grouped.items():\n        for _, index in elements_list:\n            if index is not None:\n                grouped_by_index[id(index)].append(name)\n                indexes[id(index)] = index\n    for index_id, index_coord_names in grouped_by_index.items():\n        index_names = set(index_coord_names)\n        common_names = index_names & prioritized_names\n        if common_names and len(common_names) != len(index_names):\n            common_names_str = ', '.join((f'{k!r}' for k in common_names))\n            index_names_str = ', '.join((f'{k!r}' for k in index_coord_names))\n            raise ValueError(f'cannot set or update variable(s) {common_names_str}, which would corrupt the following index built from coordinates {index_names_str}:\\n{indexes[index_id]!r}')",
    ".xarray.core.merge.py@@unique_variable": "def unique_variable(name: Hashable, variables: list[Variable], compat: CompatOptions='broadcast_equals', equals: bool | None=None) -> Variable:\n    out = variables[0]\n    if len(variables) == 1 or compat == 'override':\n        return out\n    combine_method = None\n    if compat == 'minimal':\n        compat = 'broadcast_equals'\n    if compat == 'broadcast_equals':\n        dim_lengths = broadcast_dimension_size(variables)\n        out = out.set_dims(dim_lengths)\n    if compat == 'no_conflicts':\n        combine_method = 'fillna'\n    if equals is None:\n        for var in variables[1:]:\n            equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n            if equals is not True:\n                break\n        if equals is None:\n            out = out.compute()\n            for var in variables[1:]:\n                equals = getattr(out, compat)(var)\n                if not equals:\n                    break\n    if not equals:\n        raise MergeError(f\"conflicting values for variable {name!r} on objects to be combined. You can skip this check by specifying compat='override'.\")\n    if combine_method:\n        for var in variables[1:]:\n            out = getattr(out, combine_method)(var)\n    return out",
    ".xarray.core.dataarray.py@@DataArray.__init__": "def __init__(self, data: Any=dtypes.NA, coords: Sequence[Sequence[Any] | pd.Index | DataArray] | Mapping[Any, Any] | None=None, dims: Hashable | Sequence[Hashable] | None=None, name: Hashable=None, attrs: Mapping=None, indexes: dict[Hashable, Index]=None, fastpath: bool=False) -> None:\n    if fastpath:\n        variable = data\n        assert dims is None\n        assert attrs is None\n        assert indexes is not None\n    else:\n        if indexes is not None:\n            raise ValueError('Providing explicit indexes is not supported yet')\n        if coords is None:\n            if isinstance(data, DataArray):\n                coords = data.coords\n            elif isinstance(data, pd.Series):\n                coords = [data.index]\n            elif isinstance(data, pd.DataFrame):\n                coords = [data.index, data.columns]\n            elif isinstance(data, (pd.Index, IndexVariable)):\n                coords = [data]\n        if dims is None:\n            dims = getattr(data, 'dims', getattr(coords, 'dims', None))\n        if name is None:\n            name = getattr(data, 'name', None)\n        if attrs is None and (not isinstance(data, PANDAS_TYPES)):\n            attrs = getattr(data, 'attrs', None)\n        data = _check_data_shape(data, coords, dims)\n        data = as_compatible_data(data)\n        coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n        variable = Variable(dims, data, attrs, fastpath=True)\n        indexes, coords = _create_indexes_from_coords(coords)\n    self._variable = variable\n    assert isinstance(coords, dict)\n    self._coords = coords\n    self._name = name\n    self._indexes = indexes\n    self._close = None",
    ".xarray.core.variable.py@@IndexVariable.copy": "def copy(self, deep: bool=True, data: ArrayLike | None=None):\n    if data is None:\n        ndata = self._data.copy(deep=deep)\n    else:\n        ndata = as_compatible_data(data)\n        if self.shape != ndata.shape:\n            raise ValueError('Data shape {} must match shape of object {}'.format(ndata.shape, self.shape))\n    attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n    encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)\n    return self._replace(data=ndata, attrs=attrs, encoding=encoding)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.copy": "def copy(self, deep: bool=True) -> PandasIndexingAdapter:\n    array = self.array.copy(deep=True) if deep else self.array\n    return type(self)(array, self._dtype)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__init__": "def __init__(self, array: pd.Index, dtype: DTypeLike=None):\n    self.array = safe_cast_to_index(array)\n    if dtype is None:\n        self._dtype = get_valid_numpy_dtype(array)\n    else:\n        self._dtype = np.dtype(dtype)",
    ".xarray.core.utils.py@@safe_cast_to_index": "def safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    elif hasattr(array, 'to_pandas_index'):\n        index = array.to_pandas_index()\n    elif hasattr(array, 'array') and isinstance(array.array, pd.Index):\n        index = array.array\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)",
    ".xarray.core.utils.py@@_maybe_cast_to_cftimeindex": "def _maybe_cast_to_cftimeindex(index: pd.Index) -> pd.Index:\n    from ..coding.cftimeindex import CFTimeIndex\n    if len(index) > 0 and index.dtype == 'O':\n        try:\n            return CFTimeIndex(index)\n        except (ImportError, TypeError):\n            return index\n    else:\n        return index",
    ".xarray.core.variable.py@@IndexVariable.__init__": "def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n    super().__init__(dims, data, attrs, encoding, fastpath)\n    if self.ndim != 1:\n        raise ValueError(f'{type(self).__name__} objects must be 1-dimensional')\n    if not isinstance(self._data, PandasIndexingAdapter):\n        self._data = PandasIndexingAdapter(self._data)",
    ".xarray.core.variable.py@@IndexVariable.to_index_variable": "def to_index_variable(self) -> IndexVariable:\n    return self.copy()",
    ".xarray.core.indexes.py@@create_default_index_implicit": "def create_default_index_implicit(dim_variable: Variable, all_variables: Mapping | Iterable[Hashable] | None=None) -> tuple[PandasIndex, IndexVars]:\n    if all_variables is None:\n        all_variables = {}\n    if not isinstance(all_variables, Mapping):\n        all_variables = {k: None for k in all_variables}\n    name = dim_variable.dims[0]\n    array = getattr(dim_variable._data, 'array', None)\n    index: PandasIndex\n    if isinstance(array, pd.MultiIndex):\n        index = PandasMultiIndex(array, name)\n        index_vars = index.create_variables()\n        duplicate_names = [k for k in index_vars if k in all_variables and k != name]\n        if duplicate_names:\n            if len(duplicate_names) < len(index.index.names):\n                conflict = True\n            else:\n                duplicate_vars = [all_variables[k] for k in duplicate_names]\n                conflict = any((v is None or not dim_variable.equals(v) for v in duplicate_vars))\n            if conflict:\n                conflict_str = '\\n'.join(duplicate_names)\n                raise ValueError(f'conflicting MultiIndex level / variable name(s):\\n{conflict_str}')\n    else:\n        dim_var = {name: dim_variable}\n        index = PandasIndex.from_variables(dim_var, options={})\n        index_vars = index.create_variables(dim_var)\n    return (index, index_vars)",
    ".xarray.core.indexes.py@@PandasIndex.from_variables": "def from_variables(cls, variables: Mapping[Any, Variable], *, options: Mapping[str, Any]) -> PandasIndex:\n    if len(variables) != 1:\n        raise ValueError(f'PandasIndex only accepts one variable, found {len(variables)} variables')\n    name, var = next(iter(variables.items()))\n    if var.ndim != 1:\n        raise ValueError(f'PandasIndex only accepts a 1-dimensional variable, variable {name!r} has {var.ndim} dimensions')\n    dim = var.dims[0]\n    data = getattr(var._data, 'array', var.data)\n    if isinstance(var._data, PandasMultiIndexingAdapter):\n        level = var._data.level\n        if level is not None:\n            data = var._data.array.get_level_values(level)\n    obj = cls(data, dim, coord_dtype=var.dtype)\n    assert not isinstance(obj.index, pd.MultiIndex)\n    obj.index.name = name\n    return obj",
    ".xarray.core.variable.py@@Variable.data": "def data(self) -> Any:\n    if is_duck_array(self._data):\n        return self._data\n    else:\n        return self.values",
    ".xarray.core.utils.py@@is_duck_array": "def is_duck_array(value: Any) -> bool:\n    if isinstance(value, np.ndarray):\n        return True\n    return hasattr(value, 'ndim') and hasattr(value, 'shape') and hasattr(value, 'dtype') and (hasattr(value, '__array_function__') and hasattr(value, '__array_ufunc__') or hasattr(value, '__array_namespace__'))",
    ".xarray.core.indexes.py@@PandasIndex.__init__": "def __init__(self, array: Any, dim: Hashable, coord_dtype: Any=None):\n    index = utils.safe_cast_to_index(array).copy()\n    if index.name is None:\n        index.name = dim\n    self.index = index\n    self.dim = dim\n    if coord_dtype is None:\n        coord_dtype = get_valid_numpy_dtype(index)\n    self.coord_dtype = coord_dtype",
    ".xarray.core.indexes.py@@PandasIndex.create_variables": "def create_variables(self, variables: Mapping[Any, Variable] | None=None) -> IndexVars:\n    from .variable import IndexVariable\n    name = self.index.name\n    attrs: Mapping[Hashable, Any] | None\n    encoding: Mapping[Hashable, Any] | None\n    if variables is not None and name in variables:\n        var = variables[name]\n        attrs = var.attrs\n        encoding = var.encoding\n    else:\n        attrs = None\n        encoding = None\n    data = PandasIndexingAdapter(self.index, dtype=self.coord_dtype)\n    var = IndexVariable(self.dim, data, attrs=attrs, encoding=encoding)\n    return {name: var}",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> DataArray:\n    return self._data._getitem_coord(key)",
    ".xarray.core.dataarray.py@@DataArray._getitem_coord": "def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:\n    from .dataset import _get_virtual_variable\n    try:\n        var = self._coords[key]\n    except KeyError:\n        dim_sizes = dict(zip(self.dims, self.shape))\n        _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n    return self._replace_maybe_drop_dims(var, name=key)",
    ".xarray.core.dataarray.py@@DataArray._replace_maybe_drop_dims": "def _replace_maybe_drop_dims(self: T_DataArray, variable: Variable, name: Hashable | None | Default=_default) -> T_DataArray:\n    if variable.dims == self.dims and variable.shape == self.shape:\n        coords = self._coords.copy()\n        indexes = self._indexes\n    elif variable.dims == self.dims:\n        new_sizes = dict(zip(self.dims, variable.shape))\n        coords = {k: v for k, v in self._coords.items() if v.shape == tuple((new_sizes[d] for d in v.dims))}\n        indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    else:\n        allowed_dims = set(variable.dims)\n        coords = {k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims}\n        indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.indexes.py@@filter_indexes_from_coords": "def filter_indexes_from_coords(indexes: Mapping[Any, Index], filtered_coord_names: set) -> dict[Hashable, Index]:\n    filtered_indexes: dict[Any, Index] = dict(**indexes)\n    index_coord_names: dict[Hashable, set[Hashable]] = defaultdict(set)\n    for name, idx in indexes.items():\n        index_coord_names[id(idx)].add(name)\n    for idx_coord_names in index_coord_names.values():\n        if not idx_coord_names <= filtered_coord_names:\n            for k in idx_coord_names:\n                del filtered_indexes[k]\n    return filtered_indexes",
    ".xarray.core.dataarray.py@@DataArray._replace": "def _replace(self: T_DataArray, variable: Variable=None, coords=None, name: Hashable | None | Default=_default, indexes=None) -> T_DataArray:\n    if variable is None:\n        variable = self.variable\n    if coords is None:\n        coords = self._coords\n    if indexes is None:\n        indexes = self._indexes\n    if name is _default:\n        name = self.name\n    return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.common.py@@AttrAccessMixin.__setattr__": "def __setattr__(self, name: str, value: Any) -> None:\n    try:\n        object.__setattr__(self, name, value)\n    except AttributeError as e:\n        if str(e) != '{!r} object has no attribute {!r}'.format(type(self).__name__, name):\n            raise\n        raise AttributeError(f\"cannot set attribute {name!r} on a {type(self).__name__!r} object. Use __setitem__ styleassignment (e.g., `ds['name'] = ...`) instead of assigning variables.\") from e",
    ".xarray.core.formatting.py@@summarize_variable": "def summarize_variable(name: Hashable, var, col_width: int, max_width: int=None, is_index: bool=False):\n    variable = getattr(var, 'variable', var)\n    if max_width is None:\n        max_width_options = OPTIONS['display_width']\n        if not isinstance(max_width_options, int):\n            raise TypeError(f'`max_width` value of `{max_width}` is not a valid int')\n        else:\n            max_width = max_width_options\n    marker = '*' if is_index else ' '\n    first_col = pretty_print(f'  {marker} {name} ', col_width)\n    if variable.dims:\n        dims_str = '({}) '.format(', '.join(map(str, variable.dims)))\n    else:\n        dims_str = ''\n    front_str = f'{first_col}{dims_str}{variable.dtype} '\n    values_width = max_width - len(front_str)\n    values_str = inline_variable_array_repr(variable, values_width)\n    return front_str + values_str",
    ".xarray.core.formatting.py@@pretty_print": "def pretty_print(x, numchars: int):\n    s = maybe_truncate(x, numchars)\n    return s + ' ' * max(numchars - len(s), 0)",
    ".xarray.core.formatting.py@@maybe_truncate": "def maybe_truncate(obj, maxlen=500):\n    s = str(obj)\n    if len(s) > maxlen:\n        s = s[:maxlen - 3] + '...'\n    return s",
    ".xarray.core.formatting.py@@inline_variable_array_repr": "def inline_variable_array_repr(var, max_width):\n    if hasattr(var._data, '_repr_inline_'):\n        return var._data._repr_inline_(max_width)\n    elif var._in_memory:\n        return format_array_flat(var, max_width)\n    elif isinstance(var._data, dask_array_type):\n        return inline_dask_repr(var.data)\n    elif isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    elif hasattr(var._data, '__array_function__'):\n        return maybe_truncate(repr(var._data).replace('\\n', ' '), max_width)\n    else:\n        return '...'",
    ".xarray.core.formatting.py@@format_array_flat": "def format_array_flat(array, max_width: int):\n    max_possibly_relevant = min(max(array.size, 1), max(math.ceil(max_width / 2.0), 2))\n    relevant_front_items = format_items(first_n_items(array, (max_possibly_relevant + 1) // 2))\n    relevant_back_items = format_items(last_n_items(array, max_possibly_relevant // 2))\n    relevant_items = sum(zip_longest(relevant_front_items, reversed(relevant_back_items)), ())[:max_possibly_relevant]\n    cum_len = np.cumsum([len(s) + 1 for s in relevant_items]) - 1\n    if array.size > 2 and (max_possibly_relevant < array.size or (cum_len > max_width).any()):\n        padding = ' ... '\n        max_len = max(int(np.argmax(cum_len + len(padding) - 1 > max_width)), 2)\n        count = min(array.size, max_len)\n    else:\n        count = array.size\n        padding = '' if count <= 1 else ' '\n    num_front = (count + 1) // 2\n    num_back = count - num_front\n    pprint_str = ''.join([' '.join(relevant_front_items[:num_front]), padding, ' '.join(relevant_back_items[-num_back:])])\n    if len(pprint_str) > max_width:\n        pprint_str = pprint_str[:max(max_width - 3, 0)] + '...'\n    return pprint_str",
    ".xarray.core.formatting.py@@first_n_items": "def first_n_items(array, n_desired):\n    if n_desired < 1:\n        raise ValueError('must request at least one item')\n    if array.size == 0:\n        return []\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=False)\n        array = array[indexer]\n    return np.asarray(array).flat[:n_desired]",
    ".xarray.core.formatting.py@@last_n_items": "def last_n_items(array, n_desired):\n    if n_desired == 0 or array.size == 0:\n        return []\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=True)\n        array = array[indexer]\n    return np.asarray(array).flat[-n_desired:]",
    ".xarray.core.formatting.py@@_element_formatter": "def _element_formatter(elements: Collection[Hashable], col_width: int, max_rows: int | None=None, delimiter: str=', ') -> str:\n    elements_len = len(elements)\n    out = ['']\n    length_row = 0\n    for i, v in enumerate(elements):\n        delim = delimiter if i < elements_len - 1 else ''\n        v_delim = f'{v}{delim}'\n        length_element = len(v_delim)\n        length_row += length_element\n        if col_width + length_row > OPTIONS['display_width']:\n            out[-1] = out[-1].rstrip()\n            out.append('\\n' + pretty_print('', col_width) + v_delim)\n            length_row = length_element\n        else:\n            out[-1] += v_delim\n    if max_rows and len(out) > max_rows:\n        first_rows = calc_max_rows_first(max_rows)\n        last_rows = calc_max_rows_last(max_rows)\n        out = out[:first_rows] + ['\\n' + pretty_print('', col_width) + '...'] + (out[-last_rows:] if max_rows > 1 else [])\n    return ''.join(out)",
    ".xarray.core.common.py@@AbstractArray.__repr__": "def __repr__(self) -> str:\n    return formatting.array_repr(self)",
    ".xarray.core.formatting.py@@array_repr": "def array_repr(arr):\n    from .variable import Variable\n    max_rows = OPTIONS['display_max_rows']\n    if hasattr(arr, 'name') and arr.name is not None:\n        name_str = f'{arr.name!r} '\n    else:\n        name_str = ''\n    if isinstance(arr, Variable) or _get_boolean_with_default('display_expand_data', default=True) or isinstance(arr.variable._data, MemoryCachedArray):\n        data_repr = short_data_repr(arr)\n    else:\n        data_repr = inline_variable_array_repr(arr.variable, OPTIONS['display_width'])\n    start = f'<xarray.{type(arr).__name__} {name_str}'\n    dims = dim_summary_limited(arr, col_width=len(start) + 1, max_rows=max_rows)\n    summary = [f'{start}({dims})>', data_repr]\n    if hasattr(arr, 'coords'):\n        if arr.coords:\n            col_width = _calculate_col_width(arr.coords)\n            summary.append(coords_repr(arr.coords, col_width=col_width, max_rows=max_rows))\n        unindexed_dims_str = unindexed_dims_repr(arr.dims, arr.coords, max_rows=max_rows)\n        if unindexed_dims_str:\n            summary.append(unindexed_dims_str)\n    if arr.attrs:\n        summary.append(attrs_repr(arr.attrs))\n    return '\\n'.join(summary)",
    ".xarray.core.dataarray.py@@DataArray.name": "def name(self) -> Hashable | None:\n    return self._name",
    ".xarray.core.options.py@@_get_boolean_with_default": "def _get_boolean_with_default(option: Options, default: bool) -> bool:\n    global_choice = OPTIONS[option]\n    if global_choice == 'default':\n        return default\n    elif isinstance(global_choice, bool):\n        return global_choice\n    else:\n        raise ValueError(f\"The global option {option} must be one of True, False or 'default'.\")",
    ".xarray.core.formatting.py@@short_data_repr": "def short_data_repr(array):\n    internal_data = getattr(array, 'variable', array)._data\n    if isinstance(array, np.ndarray):\n        return short_numpy_repr(array)\n    elif is_duck_array(internal_data):\n        return limit_lines(repr(array.data), limit=40)\n    elif array._in_memory or array.size < 100000.0:\n        return short_numpy_repr(array)\n    else:\n        return f'[{array.size} values with dtype={array.dtype}]'",
    ".xarray.core.dataarray.py@@DataArray.data": "def data(self) -> Any:\n    return self.variable.data",
    ".xarray.core.formatting.py@@limit_lines": "def limit_lines(string: str, *, limit: int):\n    lines = string.splitlines()\n    if len(lines) > limit:\n        string = '\\n'.join(chain(lines[:limit // 2], ['...'], lines[-limit // 2:]))\n    return string",
    ".xarray.core.formatting.py@@dim_summary_limited": "def dim_summary_limited(obj, col_width: int, max_rows: int | None=None) -> str:\n    elements = [f'{k}: {v}' for k, v in obj.sizes.items()]\n    return _element_formatter(elements, col_width, max_rows)",
    ".xarray.core.common.py@@AbstractArray.sizes": "def sizes(self: Any) -> Frozen[Hashable, int]:\n    return Frozen(dict(zip(self.dims, self.shape)))",
    ".xarray.core.dataarray.py@@DataArray.shape": "def shape(self) -> tuple[int, ...]:\n    return self.variable.shape",
    ".xarray.core.utils.py@@Frozen.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.coordinates.py@@Coordinates.__len__": "def __len__(self) -> int:\n    return len(self._names)",
    ".xarray.core.formatting.py@@_calculate_col_width": "def _calculate_col_width(col_items):\n    max_name_length = max((len(str(s)) for s in col_items)) if col_items else 0\n    col_width = max(max_name_length, 7) + 6\n    return col_width",
    ".xarray.core.coordinates.py@@Coordinates.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    for k in self.variables:\n        if k in self._names:\n            yield k",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.variables": "def variables(self):\n    return Frozen(self._data._coords)",
    ".xarray.core.formatting.py@@coords_repr": "def coords_repr(coords, col_width=None, max_rows=None):\n    if col_width is None:\n        col_width = _calculate_col_width(coords)\n    return _mapping_repr(coords, title='Coordinates', summarizer=summarize_variable, expand_option_name='display_expand_coords', col_width=col_width, indexes=coords.xindexes, max_rows=max_rows)",
    ".xarray.core.coordinates.py@@Coordinates.xindexes": "def xindexes(self) -> Indexes[Index]:\n    return self._data.xindexes",
    ".xarray.core.dataarray.py@@DataArray.xindexes": "def xindexes(self) -> Indexes:\n    return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})",
    ".xarray.core.formatting.py@@_mapping_repr": "def _mapping_repr(mapping, title, summarizer, expand_option_name, col_width=None, max_rows=None, indexes=None):\n    if col_width is None:\n        col_width = _calculate_col_width(mapping)\n    summarizer_kwargs = defaultdict(dict)\n    if indexes is not None:\n        summarizer_kwargs = {k: {'is_index': k in indexes} for k in mapping}\n    summary = [f'{title}:']\n    if mapping:\n        len_mapping = len(mapping)\n        if not _get_boolean_with_default(expand_option_name, default=True):\n            summary = [f'{summary[0]} ({len_mapping})']\n        elif max_rows is not None and len_mapping > max_rows:\n            summary = [f'{summary[0]} ({max_rows}/{len_mapping})']\n            first_rows = calc_max_rows_first(max_rows)\n            keys = list(mapping.keys())\n            summary += [summarizer(k, mapping[k], col_width, **summarizer_kwargs[k]) for k in keys[:first_rows]]\n            if max_rows > 1:\n                last_rows = calc_max_rows_last(max_rows)\n                summary += [pretty_print('    ...', col_width) + ' ...']\n                summary += [summarizer(k, mapping[k], col_width, **summarizer_kwargs[k]) for k in keys[-last_rows:]]\n        else:\n            summary += [summarizer(k, v, col_width, **summarizer_kwargs[k]) for k, v in mapping.items()]\n    else:\n        summary += [EMPTY_REPR]\n    return '\\n'.join(summary)",
    ".xarray.core.indexing.py@@PandasMultiIndexingAdapter._repr_inline_": "def _repr_inline_(self, max_width: int) -> str:\n    from .formatting import format_array_flat\n    if self.level is None:\n        return 'MultiIndex'\n    else:\n        return format_array_flat(self._get_array_subset(), max_width)",
    ".xarray.core.indexing.py@@PandasMultiIndexingAdapter._get_array_subset": "def _get_array_subset(self) -> np.ndarray:\n    threshold = max(100, OPTIONS['display_values_threshold'] + 2)\n    if self.size > threshold:\n        pos = threshold // 2\n        indices = np.concatenate([np.arange(0, pos), np.arange(-pos, 0)])\n        subset = self[OuterIndexer((indices,))]\n    else:\n        subset = self\n    return np.asarray(subset)",
    ".xarray.core.indexing.py@@PandasMultiIndexingAdapter.__array__": "def __array__(self, dtype: DTypeLike=None) -> np.ndarray:\n    if self.level is not None:\n        return self.array.get_level_values(self.level).values\n    else:\n        return super().__array__(dtype)",
    ".xarray.core.formatting.py@@_get_indexer_at_least_n_items": "def _get_indexer_at_least_n_items(shape, n_desired, from_end):\n    assert 0 < n_desired <= math.prod(shape)\n    cum_items = np.cumprod(shape[::-1])\n    n_steps = np.argmax(cum_items >= n_desired)\n    stop = math.ceil(float(n_desired) / np.r_[1, cum_items][n_steps])\n    indexer = (-1 if from_end else 0,) * (len(shape) - 1 - n_steps) + (slice(-stop, None) if from_end else slice(stop),) + (slice(None),) * n_steps\n    return indexer",
    ".xarray.testing.py@@wrapper": "def wrapper(*args, **kwargs):\n    __tracebackhide__ = True\n    with warnings.catch_warnings():\n        warnings.filters = [f for f in warnings.filters if f[0] != 'error']\n        return func(*args, **kwargs)",
    ".xarray.testing.py@@assert_equal": "def assert_equal(a, b):\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    if isinstance(a, (Variable, DataArray)):\n        assert a.equals(b), formatting.diff_array_repr(a, b, 'equals')\n    elif isinstance(a, Dataset):\n        assert a.equals(b), formatting.diff_dataset_repr(a, b, 'equals')\n    else:\n        raise TypeError(f'{type(a)} not supported by assertion comparison')",
    ".xarray.testing.py@@_assert_internal_invariants": "def _assert_internal_invariants(xarray_obj: Union[DataArray, Dataset, Variable], check_default_indexes: bool):\n    if isinstance(xarray_obj, Variable):\n        _assert_variable_invariants(xarray_obj)\n    elif isinstance(xarray_obj, DataArray):\n        _assert_dataarray_invariants(xarray_obj, check_default_indexes=check_default_indexes)\n    elif isinstance(xarray_obj, Dataset):\n        _assert_dataset_invariants(xarray_obj, check_default_indexes=check_default_indexes)\n    else:\n        raise TypeError('{} is not a supported type for xarray invariant checks'.format(type(xarray_obj)))",
    ".xarray.testing.py@@_assert_variable_invariants": "def _assert_variable_invariants(var: Variable, name: Hashable=None):\n    if name is None:\n        name_or_empty: tuple = ()\n    else:\n        name_or_empty = (name,)\n    assert isinstance(var._dims, tuple), name_or_empty + (var._dims,)\n    assert len(var._dims) == len(var._data.shape), name_or_empty + (var._dims, var._data.shape)\n    assert isinstance(var._encoding, (type(None), dict)), name_or_empty + (var._encoding,)\n    assert isinstance(var._attrs, (type(None), dict)), name_or_empty + (var._attrs,)",
    ".xarray.core.dataarray.py@@DataArray.values": "def values(self) -> np.ndarray:\n    return self.variable.values",
    ".xarray.core.dataarray.py@@DataArray.dtype": "def dtype(self) -> np.dtype:\n    return self.variable.dtype",
    ".xarray.core.dataarray.py@@DataArray.size": "def size(self) -> int:\n    return self.variable.size",
    ".xarray.core.dataarray.py@@DataArray.nbytes": "def nbytes(self) -> int:\n    return self.variable.nbytes",
    ".xarray.core.dataarray.py@@DataArray.ndim": "def ndim(self) -> int:\n    return self.variable.ndim",
    ".xarray.core.dataarray.py@@_check_data_shape": "def _check_data_shape(data, coords, dims):\n    if data is dtypes.NA:\n        data = np.nan\n    if coords is not None and utils.is_scalar(data, include_0d=False):\n        if utils.is_dict_like(coords):\n            if dims is None:\n                return data\n            else:\n                data_shape = tuple((as_variable(coords[k], k).size if k in coords.keys() else 1 for k in dims))\n        else:\n            data_shape = tuple((as_variable(coord, 'foo').size for coord in coords))\n        data = np.full(data_shape, data)\n    return data",
    ".xarray.core.utils.py@@is_scalar": "def is_scalar(value: Any, include_0d: bool=True) -> TypeGuard[Hashable]:\n    return _is_scalar(value, include_0d)",
    ".xarray.core.utils.py@@_is_scalar": "def _is_scalar(value, include_0d):\n    from .variable import NON_NUMPY_SUPPORTED_ARRAY_TYPES\n    if include_0d:\n        include_0d = getattr(value, 'ndim', None) == 0\n    return include_0d or isinstance(value, (str, bytes)) or (not (isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES) or hasattr(value, '__array_function__') or hasattr(value, '__array_namespace__')))",
    ".xarray.core.dataarray.py@@_infer_coords_and_dims": "def _infer_coords_and_dims(shape, coords, dims) -> tuple[dict[Hashable, Variable], tuple[Hashable, ...]]:\n    if coords is not None and (not utils.is_dict_like(coords)) and (len(coords) != len(shape)):\n        raise ValueError(f'coords is not dict-like, but it has {len(coords)} items, which does not match the {len(shape)} dimensions of the data')\n    if isinstance(dims, str):\n        dims = (dims,)\n    if dims is None:\n        dims = [f'dim_{n}' for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            if utils.is_dict_like(coords):\n                dims = list(coords.keys())\n            else:\n                for n, (dim, coord) in enumerate(zip(dims, coords)):\n                    coord = as_variable(coord, name=dims[n]).to_index_variable()\n                    dims[n] = coord.name\n        dims = tuple(dims)\n    elif len(dims) != len(shape):\n        raise ValueError(f'different number of dimensions on data and dims: {len(shape)} vs {len(dims)}')\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError(f'dimension {d} is not a string')\n    new_coords: dict[Hashable, Variable] = {}\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var.to_index_variable()\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any((d not in dims for d in v.dims)):\n            raise ValueError(f'coordinate {k} has dimensions {v.dims}, but these are not a subset of the DataArray dimensions {dims}')\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError(f'conflicting sizes for dimension {d!r}: length {sizes[d]} on the data but length {s} on coordinate {k!r}')\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError(f'coordinate {k!r} is a DataArray dimension, but it has shape {v.shape!r} rather than expected shape {sizes[k]!r} matching the dimension size')\n    return (new_coords, dims)",
    ".xarray.core.utils.py@@get_valid_numpy_dtype": "def get_valid_numpy_dtype(array: np.ndarray | pd.Index):\n    if isinstance(array, pd.PeriodIndex):\n        dtype = np.dtype('O')\n    elif hasattr(array, 'categories'):\n        dtype = array.categories.dtype\n    elif not is_valid_numpy_dtype(array.dtype):\n        dtype = np.dtype('O')\n    else:\n        dtype = array.dtype\n    return dtype",
    ".xarray.core.utils.py@@is_valid_numpy_dtype": "def is_valid_numpy_dtype(dtype: Any) -> bool:\n    try:\n        np.dtype(dtype)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
    ".xarray.core.variable.py@@IndexVariable.name": "def name(self) -> Hashable:\n    return self.dims[0]",
    ".xarray.coding.cftimeindex.py@@CFTimeIndex.__new__": "def __new__(cls, data, name=None, **kwargs):\n    assert_all_valid_date_type(data)\n    if name is None and hasattr(data, 'name'):\n        name = data.name\n    result = object.__new__(cls)\n    result._data = np.array(data, dtype='O')\n    result.name = name\n    result._cache = {}\n    return result",
    ".xarray.coding.cftimeindex.py@@assert_all_valid_date_type": "def assert_all_valid_date_type(data):\n    if cftime is None:\n        raise ModuleNotFoundError(\"No module named 'cftime'\")\n    if len(data) > 0:\n        sample = data[0]\n        date_type = type(sample)\n        if not isinstance(sample, cftime.datetime):\n            raise TypeError('CFTimeIndex requires cftime.datetime objects. Got object of {}.'.format(date_type))\n        if not all((isinstance(value, date_type) for value in data)):\n            raise TypeError('CFTimeIndex requires using datetime objects of all the same type.  Got\\n{}.'.format(data))",
    ".xarray.core.alignment.py@@Aligner._get_dim_pos_indexers": "def _get_dim_pos_indexers(self, matching_indexes: dict[MatchingIndexKey, Index]) -> dict[Hashable, Any]:\n    dim_pos_indexers = {}\n    for key, aligned_idx in self.aligned_indexes.items():\n        obj_idx = matching_indexes.get(key)\n        if obj_idx is not None:\n            if self.reindex[key]:\n                indexers = obj_idx.reindex_like(aligned_idx, **self.reindex_kwargs)\n                dim_pos_indexers.update(indexers)\n    return dim_pos_indexers",
    ".xarray.core.dataarray.py@@DataArray._reindex_callback": "def _reindex_callback(self: T_DataArray, aligner: alignment.Aligner, dim_pos_indexers: dict[Hashable, Any], variables: dict[Hashable, Variable], indexes: dict[Hashable, Index], fill_value: Any, exclude_dims: frozenset[Hashable], exclude_vars: frozenset[Hashable]) -> T_DataArray:\n    if isinstance(fill_value, dict):\n        fill_value = fill_value.copy()\n        sentinel = object()\n        value = fill_value.pop(self.name, sentinel)\n        if value is not sentinel:\n            fill_value[_THIS_ARRAY] = value\n    ds = self._to_temp_dataset()\n    reindexed = ds._reindex_callback(aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\n    return self._from_temp_dataset(reindexed)",
    ".xarray.core.dataarray.py@@DataArray._to_temp_dataset": "def _to_temp_dataset(self) -> Dataset:\n    return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)",
    ".xarray.core.dataarray.py@@DataArray._to_dataset_whole": "def _to_dataset_whole(self, name: Hashable=None, shallow_copy: bool=True) -> Dataset:\n    if name is None:\n        name = self.name\n    if name is None:\n        raise ValueError('unable to convert unnamed DataArray to a Dataset without providing an explicit name')\n    if name in self.coords:\n        raise ValueError('cannot create a Dataset from a DataArray with the same name as one of its coordinates')\n    variables = self._coords.copy()\n    variables[name] = self.variable\n    if shallow_copy:\n        for k in variables:\n            variables[k] = variables[k].copy(deep=False)\n    indexes = self._indexes\n    coord_names = set(self._coords)\n    return Dataset._construct_direct(variables, coord_names, indexes=indexes)",
    ".xarray.core.utils.py@@ReprObject.__hash__": "def __hash__(self) -> int:\n    return hash((type(self), self._value))",
    ".xarray.core.dataset.py@@Dataset._construct_direct": "def _construct_direct(cls: type[T_Dataset], variables: dict[Any, Variable], coord_names: set[Hashable], dims: dict[Any, int] | None=None, attrs: dict | None=None, indexes: dict[Any, Index] | None=None, encoding: dict | None=None, close: Callable[[], None] | None=None) -> T_Dataset:\n    if dims is None:\n        dims = calculate_dimensions(variables)\n    if indexes is None:\n        indexes = {}\n    obj = object.__new__(cls)\n    obj._variables = variables\n    obj._coord_names = coord_names\n    obj._dims = dims\n    obj._indexes = indexes\n    obj._attrs = attrs\n    obj._close = close\n    obj._encoding = encoding\n    return obj",
    ".xarray.core.variable.py@@calculate_dimensions": "def calculate_dimensions(variables: Mapping[Any, Variable]) -> dict[Hashable, int]:\n    dims: dict[Hashable, int] = {}\n    last_used = {}\n    scalar_vars = {k for k, v in variables.items() if not v.dims}\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError(f'dimension {dim!r} already exists as a scalar variable')\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError(f'conflicting sizes for dimension {dim!r}: length {size} on {k!r} and length {dims[dim]} on {last_used!r}')\n    return dims",
    ".xarray.core.dataset.py@@Dataset._reindex_callback": "def _reindex_callback(self, aligner: alignment.Aligner, dim_pos_indexers: dict[Hashable, Any], variables: dict[Hashable, Variable], indexes: dict[Hashable, Index], fill_value: Any, exclude_dims: frozenset[Hashable], exclude_vars: frozenset[Hashable]) -> Dataset:\n    new_variables = variables.copy()\n    new_indexes = indexes.copy()\n    for name, new_var in new_variables.items():\n        var = self._variables.get(name)\n        if var is not None:\n            new_var.attrs = var.attrs\n            new_var.encoding = var.encoding\n    for name, idx in self._indexes.items():\n        var = self._variables[name]\n        if set(var.dims) <= exclude_dims:\n            new_indexes[name] = idx\n            new_variables[name] = var\n    if not dim_pos_indexers:\n        if set(new_indexes) - set(self._indexes):\n            reindexed = self._overwrite_indexes(new_indexes, new_variables)\n        else:\n            reindexed = self.copy(deep=aligner.copy)\n    else:\n        to_reindex = {k: v for k, v in self.variables.items() if k not in variables and k not in exclude_vars}\n        reindexed_vars = alignment.reindex_variables(to_reindex, dim_pos_indexers, copy=aligner.copy, fill_value=fill_value, sparse=aligner.sparse)\n        new_variables.update(reindexed_vars)\n        new_coord_names = self._coord_names | set(new_indexes)\n        reindexed = self._replace_with_new_dims(new_variables, new_coord_names, indexes=new_indexes)\n    return reindexed",
    ".xarray.core.dataset.py@@Dataset.copy": "def copy(self: T_Dataset, deep: bool=False, data: Mapping[Any, ArrayLike] | None=None) -> T_Dataset:\n    if data is None:\n        data = {}\n    elif not utils.is_dict_like(data):\n        raise ValueError('Data must be dict-like')\n    if data:\n        var_keys = set(self.data_vars.keys())\n        data_keys = set(data.keys())\n        keys_not_in_vars = data_keys - var_keys\n        if keys_not_in_vars:\n            raise ValueError('Data must only contain variables in original dataset. Extra variables: {}'.format(keys_not_in_vars))\n        keys_missing_from_data = var_keys - data_keys\n        if keys_missing_from_data:\n            raise ValueError('Data must contain all variables in original dataset. Data is missing {}'.format(keys_missing_from_data))\n    indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n    variables = {}\n    for k, v in self._variables.items():\n        if k in index_vars:\n            variables[k] = index_vars[k]\n        else:\n            variables[k] = v.copy(deep=deep, data=data.get(k))\n    attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n    encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)\n    return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)",
    ".xarray.core.dataset.py@@Dataset.xindexes": "def xindexes(self) -> Indexes[Index]:\n    return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})",
    ".xarray.core.indexes.py@@Indexes.copy_indexes": "def copy_indexes(self, deep: bool=True) -> tuple[dict[Hashable, T_PandasOrXarrayIndex], dict[Hashable, Variable]]:\n    new_indexes = {}\n    new_index_vars = {}\n    for idx, coords in self.group_by_index():\n        if isinstance(idx, pd.Index):\n            convert_new_idx = True\n            dim = next(iter(coords.values())).dims[0]\n            if isinstance(idx, pd.MultiIndex):\n                idx = PandasMultiIndex(idx, dim)\n            else:\n                idx = PandasIndex(idx, dim)\n        else:\n            convert_new_idx = False\n        new_idx = idx.copy(deep=deep)\n        idx_vars = idx.create_variables(coords)\n        if convert_new_idx:\n            new_idx = cast(PandasIndex, new_idx).index\n        new_indexes.update({k: new_idx for k in coords})\n        new_index_vars.update(idx_vars)\n    return (new_indexes, new_index_vars)",
    ".xarray.core.indexes.py@@Indexes._id_coord_names": "def _id_coord_names(self) -> dict[int, tuple[Hashable, ...]]:\n    if self.__id_coord_names is None:\n        id_coord_names: Mapping[int, list[Hashable]] = defaultdict(list)\n        for k, v in self._coord_name_id.items():\n            id_coord_names[v].append(k)\n        self.__id_coord_names = {k: tuple(v) for k, v in id_coord_names.items()}\n    return self.__id_coord_names",
    ".xarray.core.indexes.py@@Indexes._coord_name_id": "def _coord_name_id(self) -> dict[Any, int]:\n    if self.__coord_name_id is None:\n        self.__coord_name_id = {k: id(idx) for k, idx in self._indexes.items()}\n    return self.__coord_name_id",
    ".xarray.core.indexes.py@@PandasIndex.copy": "def copy(self, deep=True):\n    if deep:\n        index = self.index.copy(deep=True)\n    else:\n        index = self.index\n    return self._replace(index)",
    ".xarray.core.indexes.py@@PandasIndex._replace": "def _replace(self, index, dim=None, coord_dtype=None):\n    if dim is None:\n        dim = self.dim\n    if coord_dtype is None:\n        coord_dtype = self.coord_dtype\n    return type(self)(index, dim, coord_dtype)",
    ".xarray.core.dataset.py@@Dataset._replace": "def _replace(self: T_Dataset, variables: dict[Hashable, Variable]=None, coord_names: set[Hashable] | None=None, dims: dict[Any, int] | None=None, attrs: dict[Hashable, Any] | None | Default=_default, indexes: dict[Hashable, Index] | None=None, encoding: dict | None | Default=_default, inplace: bool=False) -> T_Dataset:\n    if inplace:\n        if variables is not None:\n            self._variables = variables\n        if coord_names is not None:\n            self._coord_names = coord_names\n        if dims is not None:\n            self._dims = dims\n        if attrs is not _default:\n            self._attrs = attrs\n        if indexes is not None:\n            self._indexes = indexes\n        if encoding is not _default:\n            self._encoding = encoding\n        obj = self\n    else:\n        if variables is None:\n            variables = self._variables.copy()\n        if coord_names is None:\n            coord_names = self._coord_names.copy()\n        if dims is None:\n            dims = self._dims.copy()\n        if attrs is _default:\n            attrs = copy.copy(self._attrs)\n        if indexes is None:\n            indexes = self._indexes.copy()\n        if encoding is _default:\n            encoding = copy.copy(self._encoding)\n        obj = self._construct_direct(variables, coord_names, dims, attrs, indexes, encoding)\n    return obj",
    ".xarray.core.dataarray.py@@DataArray._from_temp_dataset": "def _from_temp_dataset(self: T_DataArray, dataset: Dataset, name: Hashable | None | Default=_default) -> T_DataArray:\n    variable = dataset._variables.pop(_THIS_ARRAY)\n    coords = dataset._variables\n    indexes = dataset._indexes\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.encoding": "def encoding(self) -> dict[Any, Any]:\n    return self.variable.encoding",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__sub__": "def __sub__(self, other):\n    return self._binary_op(other, operator.sub)",
    ".xarray.core.variable.py@@_broadcast_compat_data": "def _broadcast_compat_data(self, other):\n    if all((hasattr(other, attr) for attr in ['dims', 'data', 'shape', 'encoding'])):\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return (self_data, other_data, dims)",
    ".xarray.core.variable.py@@_broadcast_compat_variables": "def _broadcast_compat_variables(*variables):\n    dims = tuple(_unified_dims(variables))\n    return tuple((var.set_dims(dims) if var.dims != dims else var for var in variables))",
    ".xarray.core.variable.py@@_unified_dims": "def _unified_dims(variables):\n    all_dims = {}\n    for var in variables:\n        var_dims = var.dims\n        if len(set(var_dims)) < len(var_dims):\n            raise ValueError(f'broadcasting cannot handle duplicate dimensions: {list(var_dims)!r}')\n        for d, s in zip(var_dims, var.shape):\n            if d not in all_dims:\n                all_dims[d] = s\n            elif all_dims[d] != s:\n                raise ValueError(f'operands cannot be broadcast together with mismatched lengths for dimension {d!r}: {(all_dims[d], s)}')\n    return all_dims",
    ".xarray.core.options.py@@_get_keep_attrs": "def _get_keep_attrs(default: bool) -> bool:\n    return _get_boolean_with_default('keep_attrs', default)",
    ".xarray.core.coordinates.py@@Coordinates._merge_raw": "def _merge_raw(self, other, reflexive):\n    if other is None:\n        variables = dict(self.variables)\n        indexes = dict(self.xindexes)\n    else:\n        coord_list = [self, other] if not reflexive else [other, self]\n        variables, indexes = merge_coordinates_without_align(coord_list)\n    return (variables, indexes)",
    ".xarray.core.merge.py@@merge_coordinates_without_align": "def merge_coordinates_without_align(objects: list[Coordinates], prioritized: Mapping[Any, MergeElement]=None, exclude_dims: AbstractSet=frozenset(), combine_attrs: CombineAttrsOptions='override') -> tuple[dict[Hashable, Variable], dict[Hashable, Index]]:\n    collected = collect_from_coordinates(objects)\n    if exclude_dims:\n        filtered: dict[Hashable, list[MergeElement]] = {}\n        for name, elements in collected.items():\n            new_elements = [(variable, index) for variable, index in elements if exclude_dims.isdisjoint(variable.dims)]\n            if new_elements:\n                filtered[name] = new_elements\n    else:\n        filtered = collected\n    merged_coords, merged_indexes = merge_collected(filtered, prioritized, combine_attrs=combine_attrs)\n    merged_indexes = filter_indexes_from_coords(merged_indexes, set(merged_coords))\n    return (merged_coords, merged_indexes)",
    ".xarray.core.merge.py@@collect_from_coordinates": "def collect_from_coordinates(list_of_coords: list[Coordinates]) -> dict[Hashable, list[MergeElement]]:\n    grouped: dict[Hashable, list[MergeElement]] = defaultdict(list)\n    for coords in list_of_coords:\n        variables = coords.variables\n        indexes = coords.xindexes\n        for name, variable in variables.items():\n            grouped[name].append((variable, indexes.get(name)))\n    return grouped",
    ".xarray.core.indexes.py@@Indexes.__getitem__": "def __getitem__(self, key) -> T_PandasOrXarrayIndex:\n    return self._indexes[key]",
    ".xarray.core.indexes.py@@indexes_equal": "def indexes_equal(index: Index, other_index: Index, variable: Variable, other_variable: Variable, cache: dict[tuple[int, int], bool | None]=None) -> bool:\n    if cache is None:\n        cache = {}\n    key = (id(index), id(other_index))\n    equal: bool | None = None\n    if key not in cache:\n        if type(index) is type(other_index):\n            try:\n                equal = index.equals(other_index)\n            except NotImplementedError:\n                equal = None\n            else:\n                cache[key] = equal\n        else:\n            equal = None\n    else:\n        equal = cache[key]\n    if equal is None:\n        equal = variable.equals(other_variable)\n    return cast(bool, equal)",
    ".xarray.core.indexes.py@@PandasIndex.equals": "def equals(self, other: Index):\n    if not isinstance(other, PandasIndex):\n        return False\n    return self.index.equals(other.index) and self.dim == other.dim",
    ".xarray.core.dataarray.py@@DataArray._result_name": "def _result_name(self, other: Any=None) -> Hashable | None:\n    other_name = getattr(other, 'name', _default)\n    if other_name is _default or other_name == self.name:\n        return self.name\n    else:\n        return None",
    ".xarray.testing.py@@assert_identical": "def assert_identical(a, b):\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    if isinstance(a, Variable):\n        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')\n    elif isinstance(a, DataArray):\n        assert a.name == b.name\n        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')\n    elif isinstance(a, (Dataset, Variable)):\n        assert a.identical(b), formatting.diff_dataset_repr(a, b, 'identical')\n    else:\n        raise TypeError(f'{type(a)} not supported by assertion comparison')",
    ".xarray.core.dataarray.py@@DataArray.identical": "def identical(self: T_DataArray, other: T_DataArray) -> bool:\n    try:\n        return self.name == other.name and self._all_compat(other, 'identical')\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.dataarray.py@@DataArray._all_compat": "def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:\n\n    def compat(x, y):\n        return getattr(x.variable, compat_str)(y.variable)\n    return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(self, other)",
    ".xarray.core.utils.py@@dict_equiv": "def dict_equiv(first: Mapping[K, V], second: Mapping[K, V], compat: Callable[[V, V], bool]=equivalent) -> bool:\n    for k in first:\n        if k not in second or not compat(first[k], second[k]):\n            return False\n    return all((k in first for k in second))",
    ".xarray.core.dataarray.py@@DataArray.compat": "def compat(x, y):\n    return getattr(x.variable, compat_str)(y.variable)",
    ".xarray.testing.py@@_assert_dataarray_invariants": "def _assert_dataarray_invariants(da: DataArray, check_default_indexes: bool):\n    assert isinstance(da._variable, Variable), da._variable\n    _assert_variable_invariants(da._variable)\n    assert isinstance(da._coords, dict), da._coords\n    assert all((isinstance(v, Variable) for v in da._coords.values())), da._coords\n    assert all((set(v.dims) <= set(da.dims) for v in da._coords.values())), (da.dims, {k: v.dims for k, v in da._coords.items()})\n    assert all((isinstance(v, IndexVariable) for k, v in da._coords.items() if v.dims == (k,))), {k: type(v) for k, v in da._coords.items()}\n    for k, v in da._coords.items():\n        _assert_variable_invariants(v, k)\n    if da._indexes is not None:\n        _assert_indexes_invariants_checks(da._indexes, da._coords, da.dims, check_default=check_default_indexes)",
    ".xarray.testing.py@@_assert_indexes_invariants_checks": "def _assert_indexes_invariants_checks(indexes, possible_coord_variables, dims, check_default=True):\n    assert isinstance(indexes, dict), indexes\n    assert all((isinstance(v, Index) for v in indexes.values())), {k: type(v) for k, v in indexes.items()}\n    index_vars = {k for k, v in possible_coord_variables.items() if isinstance(v, IndexVariable)}\n    assert indexes.keys() <= index_vars, (set(indexes), index_vars)\n    for k, index in indexes.items():\n        if isinstance(index, PandasIndex):\n            pd_index = index.index\n            var = possible_coord_variables[k]\n            assert (index.dim,) == var.dims, (pd_index, var)\n            if k == index.dim:\n                assert index.coord_dtype == var.dtype, (index.coord_dtype, var.dtype)\n            assert isinstance(var._data.array, pd.Index), var._data.array\n            assert pd_index.equals(var._data.array), (pd_index, var)\n        if isinstance(index, PandasMultiIndex):\n            pd_index = index.index\n            for name in index.index.names:\n                assert name in possible_coord_variables, (pd_index, index_vars)\n                var = possible_coord_variables[name]\n                assert (index.dim,) == var.dims, (pd_index, var)\n                assert index.level_coords_dtype[name] == var.dtype, (index.level_coords_dtype[name], var.dtype)\n                assert isinstance(var._data.array, pd.MultiIndex), var._data.array\n                assert pd_index.equals(var._data.array), (pd_index, var)\n                assert name in indexes, (name, set(indexes))\n                assert index is indexes[name], (pd_index, indexes[name].index)\n    if check_default:\n        defaults = default_indexes(possible_coord_variables, dims)\n        assert indexes.keys() == defaults.keys(), (set(indexes), set(defaults))\n        assert all((v.equals(defaults[k]) for k, v in indexes.items())), (indexes, defaults)",
    ".xarray.core.indexes.py@@default_indexes": "def default_indexes(coords: Mapping[Any, Variable], dims: Iterable) -> dict[Hashable, Index]:\n    indexes: dict[Hashable, Index] = {}\n    coord_names = set(coords)\n    for name, var in coords.items():\n        if name in dims:\n            index, index_vars = create_default_index_implicit(var, coords)\n            if set(index_vars) <= coord_names:\n                indexes.update({k: index for k in index_vars})\n    return indexes",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__sub__": "def __sub__(self, other):\n    return self._binary_op(other, operator.sub)",
    ".xarray.core.dataarray.py@@DataArray._binary_op": "def _binary_op(self: T_DataArray, other: Any, f: Callable, reflexive: bool=False) -> T_DataArray:\n    from .groupby import GroupBy\n    if isinstance(other, (Dataset, GroupBy)):\n        return NotImplemented\n    if isinstance(other, DataArray):\n        align_type = OPTIONS['arithmetic_join']\n        self, other = align(self, other, join=align_type, copy=False)\n    other_variable = getattr(other, 'variable', other)\n    other_coords = getattr(other, 'coords', None)\n    variable = f(self.variable, other_variable) if not reflexive else f(other_variable, self.variable)\n    coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n    name = self._result_name(other)\n    return self._replace(variable, coords, name, indexes=indexes)",
    ".xarray.core.indexes.py@@Indexes.dims": "def dims(self) -> Mapping[Hashable, int]:\n    from .variable import calculate_dimensions\n    if self._dims is None:\n        self._dims = calculate_dimensions(self._variables)\n    return Frozen(self._dims)",
    ".xarray.core.alignment.py@@Aligner._need_reindex": "def _need_reindex(self, dims, cmp_indexes) -> bool:\n    has_unindexed_dims = any((dim in self.unindexed_dim_sizes for dim in dims))\n    return not indexes_all_equal(cmp_indexes) or has_unindexed_dims",
    ".xarray.core.indexes.py@@indexes_all_equal": "def indexes_all_equal(elements: Sequence[tuple[Index, dict[Hashable, Variable]]]) -> bool:\n\n    def check_variables():\n        variables = [e[1] for e in elements]\n        return any((not variables[0][k].equals(other_vars[k]) for other_vars in variables[1:] for k in variables[0]))\n    indexes = [e[0] for e in elements]\n    same_type = all((type(indexes[0]) is type(other_idx) for other_idx in indexes[1:]))\n    if same_type:\n        try:\n            not_equal = any((not indexes[0].equals(other_idx) for other_idx in indexes[1:]))\n        except NotImplementedError:\n            not_equal = check_variables()\n    else:\n        not_equal = check_variables()\n    return not not_equal",
    ".xarray.core.alignment.py@@Aligner._get_index_joiner": "def _get_index_joiner(self, index_cls) -> Callable:\n    if self.join in ['outer', 'inner']:\n        return functools.partial(functools.reduce, functools.partial(index_cls.join, how=self.join))\n    elif self.join == 'left':\n        return operator.itemgetter(0)\n    elif self.join == 'right':\n        return operator.itemgetter(-1)\n    elif self.join == 'override':\n        return operator.itemgetter(0)\n    else:\n        return lambda _: None",
    ".xarray.core.indexes.py@@PandasIndex.join": "def join(self: PandasIndex, other: PandasIndex, how: str='inner') -> PandasIndex:\n    if how == 'outer':\n        index = self.index.union(other.index)\n    else:\n        index = self.index.intersection(other.index)\n    coord_dtype = np.result_type(self.coord_dtype, other.coord_dtype)\n    return type(self)(index, self.dim, coord_dtype=coord_dtype)",
    ".xarray.core.alignment.py@@Aligner._reindex_one": "def _reindex_one(self, obj: DataAlignable, matching_indexes: dict[MatchingIndexKey, Index]) -> DataAlignable:\n    new_indexes, new_variables = self._get_indexes_and_vars(obj, matching_indexes)\n    dim_pos_indexers = self._get_dim_pos_indexers(matching_indexes)\n    new_obj = obj._reindex_callback(self, dim_pos_indexers, new_variables, new_indexes, self.fill_value, self.exclude_dims, self.exclude_vars)\n    new_obj.encoding = obj.encoding\n    return new_obj",
    ".xarray.core.alignment.py@@Aligner._get_indexes_and_vars": "def _get_indexes_and_vars(self, obj: DataAlignable, matching_indexes: dict[MatchingIndexKey, Index]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    new_indexes = {}\n    new_variables = {}\n    for key, aligned_idx in self.aligned_indexes.items():\n        index_vars = self.aligned_index_vars[key]\n        obj_idx = matching_indexes.get(key)\n        if obj_idx is None:\n            index_vars_dims = {d for var in index_vars.values() for d in var.dims}\n            if index_vars_dims <= set(obj.dims):\n                obj_idx = aligned_idx\n        if obj_idx is not None:\n            for name, var in index_vars.items():\n                new_indexes[name] = aligned_idx\n                new_variables[name] = var.copy()\n    return (new_indexes, new_variables)",
    ".xarray.core.indexes.py@@PandasIndex.reindex_like": "def reindex_like(self, other: PandasIndex, method=None, tolerance=None) -> dict[Hashable, Any]:\n    if not self.index.is_unique:\n        raise ValueError(f'cannot reindex or align along dimension {self.dim!r} because the (pandas) index has duplicate values')\n    return {self.dim: get_indexer_nd(self.index, other.index, method, tolerance)}",
    ".xarray.core.indexes.py@@get_indexer_nd": "def get_indexer_nd(index, labels, method=None, tolerance=None):\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer",
    ".xarray.core.dataset.py@@Dataset.variables": "def variables(self) -> Frozen[Hashable, Variable]:\n    return Frozen(self._variables)",
    ".xarray.core.alignment.py@@reindex_variables": "def reindex_variables(variables: Mapping[Any, Variable], dim_pos_indexers: Mapping[Any, Any], copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False) -> dict[Hashable, Variable]:\n    new_variables = {}\n    dim_sizes = calculate_dimensions(variables)\n    masked_dims = set()\n    unchanged_dims = set()\n    for dim, indxr in dim_pos_indexers.items():\n        if (indxr < 0).any():\n            masked_dims.add(dim)\n        elif np.array_equal(indxr, np.arange(dim_sizes.get(dim, 0))):\n            unchanged_dims.add(dim)\n    for name, var in variables.items():\n        if isinstance(fill_value, dict):\n            fill_value_ = fill_value.get(name, dtypes.NA)\n        else:\n            fill_value_ = fill_value\n        if sparse:\n            var = var._as_sparse(fill_value=fill_value_)\n        indxr = tuple((slice(None) if d in unchanged_dims else dim_pos_indexers.get(d, slice(None)) for d in var.dims))\n        needs_masking = any((d in masked_dims for d in var.dims))\n        if needs_masking:\n            new_var = var._getitem_with_mask(indxr, fill_value=fill_value_)\n        elif all((is_full_slice(k) for k in indxr)):\n            new_var = var.copy(deep=copy)\n        else:\n            new_var = var[indxr]\n        new_variables[name] = new_var\n    return new_variables",
    ".xarray.core.utils.py@@is_full_slice": "def is_full_slice(value: Any) -> bool:\n    return isinstance(value, slice) and value == slice(None)",
    ".xarray.core.indexing.py@@expanded_indexer": "def expanded_indexer(key, ndim):\n    if not isinstance(key, tuple):\n        key = (key,)\n    new_key = []\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)",
    ".xarray.core.indexing.py@@OuterIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if k.ndim != 1:\n                raise TypeError(f'invalid indexer array for {type(self).__name__}; must have exactly 1 dimension: {k!r}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@ExplicitIndexer.__init__": "def __init__(self, key):\n    if type(self) is ExplicitIndexer:\n        raise TypeError('cannot instantiate base ExplicitIndexer objects')\n    self._key = tuple(key)",
    ".xarray.core.indexing.py@@as_indexable": "def as_indexable(array):\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexingAdapter(array)\n    if is_duck_dask_array(array):\n        return DaskIndexingAdapter(array)\n    if hasattr(array, '__array_function__'):\n        return NdArrayLikeIndexingAdapter(array)\n    if hasattr(array, '__array_namespace__'):\n        return ArrayApiIndexingAdapter(array)\n    raise TypeError(f'Invalid array type: {type(array)}')",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__init__": "def __init__(self, array):\n    if not isinstance(array, np.ndarray):\n        raise TypeError('NumpyIndexingAdapter only wraps np.ndarray. Trying to wrap {}'.format(type(array)))\n    self.array = array",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    array, key = self._indexing_array_and_key(key)\n    return array[key]",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter._indexing_array_and_key": "def _indexing_array_and_key(self, key):\n    if isinstance(key, OuterIndexer):\n        array = self.array\n        key = _outer_to_numpy_indexer(key, self.array.shape)\n    elif isinstance(key, VectorizedIndexer):\n        array = NumpyVIndexAdapter(self.array)\n        key = key.tuple\n    elif isinstance(key, BasicIndexer):\n        array = self.array\n        key = key.tuple + (Ellipsis,)\n    else:\n        raise TypeError(f'unexpected key type: {type(key)}')\n    return (array, key)",
    ".xarray.core.indexing.py@@_outer_to_numpy_indexer": "def _outer_to_numpy_indexer(key, shape):\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple",
    ".xarray.core.indexing.py@@ExplicitIndexer.tuple": "def tuple(self):\n    return self._key",
    ".xarray.core.dataset.py@@Dataset._replace_with_new_dims": "def _replace_with_new_dims(self: T_Dataset, variables: dict[Hashable, Variable], coord_names: set | None=None, attrs: dict[Hashable, Any] | None | Default=_default, indexes: dict[Hashable, Index] | None=None, inplace: bool=False) -> T_Dataset:\n    dims = calculate_dimensions(variables)\n    return self._replace(variables, coord_names, dims, attrs, indexes, inplace=inplace)",
    ".xarray.core.dataarray.py@@DataArray.equals": "def equals(self: T_DataArray, other: T_DataArray) -> bool:\n    try:\n        return self._all_compat(other, 'equals')\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.duck_array_ops.py@@array_equiv": "def array_equiv(arr1, arr2):\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', \"In the future, 'NAT == x'\")\n            flag_array = (arr1 == arr2) | isnull(arr1) & isnull(arr2)\n            return bool(flag_array.all())\n    else:\n        return lazy_equiv",
    ".xarray.core.duck_array_ops.py@@asarray": "def asarray(data, xp=np):\n    return data if is_duck_array(data) else xp.asarray(data)",
    ".xarray.core.duck_array_ops.py@@lazy_array_equiv": "def lazy_array_equiv(arr1, arr2):\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_array and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        if tokenize(arr1) == tokenize(arr2):\n            return True\n        else:\n            return None\n    return None",
    ".xarray.core.pycompat.py@@is_duck_dask_array": "def is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
    ".xarray.core.pycompat.py@@is_dask_collection": "def is_dask_collection(x):\n    if dsk.available:\n        from dask.base import is_dask_collection\n        return is_dask_collection(x)\n    else:\n        return False",
    ".xarray.core.duck_array_ops.py@@isnull": "def isnull(data):\n    data = asarray(data)\n    scalar_type = data.dtype.type\n    if issubclass(scalar_type, (np.datetime64, np.timedelta64)):\n        return isnat(data)\n    elif issubclass(scalar_type, np.inexact):\n        return isnan(data)\n    elif issubclass(scalar_type, (np.bool_, np.integer, np.character, np.void)):\n        return zeros_like(data, dtype=bool)\n    elif isinstance(data, np.ndarray):\n        return pandas_isnull(data)\n    else:\n        return data != data",
    ".xarray.core.merge.py@@determine_coords": "def determine_coords(list_of_mappings: Iterable[DatasetLike]) -> tuple[set[Hashable], set[Hashable]]:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    coord_names: set[Hashable] = set()\n    noncoord_names: set[Hashable] = set()\n    for mapping in list_of_mappings:\n        if isinstance(mapping, Dataset):\n            coord_names.update(mapping.coords)\n            noncoord_names.update(mapping.data_vars)\n        else:\n            for name, var in mapping.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)\n                    coords.discard(name)\n                    coord_names.update(coords)\n    return (coord_names, noncoord_names)",
    ".xarray.core.merge.py@@assert_valid_explicit_coords": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError(f'coordinate {coord_name} shares a name with a dataset dimension, but is not a 1D variable along that dimension. This is disallowed by the xarray data model.')",
    ".xarray.core.merge.py@@merge_attrs": "def merge_attrs(variable_attrs, combine_attrs, context=None):\n    if not variable_attrs:\n        return None\n    if callable(combine_attrs):\n        return combine_attrs(variable_attrs, context=context)\n    elif combine_attrs == 'drop':\n        return {}\n    elif combine_attrs == 'override':\n        return dict(variable_attrs[0])\n    elif combine_attrs == 'no_conflicts':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            try:\n                result = compat_dict_union(result, attrs)\n            except ValueError as e:\n                raise MergeError(f\"combine_attrs='no_conflicts', but some values are not the same. Merging {str(result)} with {str(attrs)}\") from e\n        return result\n    elif combine_attrs == 'drop_conflicts':\n        result = {}\n        dropped_keys = set()\n        for attrs in variable_attrs:\n            result.update({key: value for key, value in attrs.items() if key not in result and key not in dropped_keys})\n            result = {key: value for key, value in result.items() if key not in attrs or equivalent(attrs[key], value)}\n            dropped_keys |= {key for key in attrs if key not in result}\n        return result\n    elif combine_attrs == 'identical':\n        result = dict(variable_attrs[0])\n        for attrs in variable_attrs[1:]:\n            if not dict_equiv(result, attrs):\n                raise MergeError(f\"combine_attrs='identical', but attrs differ. First is {str(result)} , other is {str(attrs)}.\")\n        return result\n    else:\n        raise ValueError(f'Unrecognised value for combine_attrs={combine_attrs}')",
    ".xarray.core.dataset.py@@Dataset.__getitem__": "def __getitem__(self, key: Hashable) -> DataArray:\n    ...",
    ".xarray.core.utils.py@@hashable": "def hashable(v: Any) -> TypeGuard[Hashable]:\n    try:\n        hash(v)\n    except TypeError:\n        return False\n    return True",
    ".xarray.core.dataset.py@@Dataset._construct_dataarray": "def _construct_dataarray(self, name: Hashable) -> DataArray:\n    from .dataarray import DataArray\n    try:\n        variable = self._variables[name]\n    except KeyError:\n        _, name, variable = _get_virtual_variable(self._variables, name, self.dims)\n    needed_dims = set(variable.dims)\n    coords: dict[Hashable, Variable] = {}\n    for k in self._variables:\n        if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n            coords[k] = self.variables[k]\n    indexes = filter_indexes_from_coords(self._indexes, set(coords))\n    return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)",
    ".xarray.core.merge.py@@append_all": "def append_all(variables, indexes):\n    for name, variable in variables.items():\n        append(name, variable, indexes.get(name))",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.dims": "def dims(self) -> tuple[Hashable, ...]:\n    return self._data.dims",
    ".xarray.core.variable.py@@IndexVariable.equals": "def equals(self, other, equiv=None):\n    if equiv is not None:\n        return super().equals(other, equiv)\n    other = getattr(other, 'variable', other)\n    try:\n        return self.dims == other.dims and self._data_equals(other)\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.variable.py@@IndexVariable._data_equals": "def _data_equals(self, other):\n    return self.to_index().equals(other.to_index())",
    ".xarray.core.variable.py@@IndexVariable.to_index": "def to_index(self) -> pd.Index:\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or f'{self.dims[0]}_level_{i}' for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index",
    ".xarray.core.indexes.py@@isel_indexes": "def isel_indexes(indexes: Indexes[Index], indexers: Mapping[Any, Any]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    return _apply_indexes(indexes, indexers, 'isel')",
    ".xarray.core.indexes.py@@_apply_indexes": "def _apply_indexes(indexes: Indexes[Index], args: Mapping[Any, Any], func: str) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    new_indexes: dict[Hashable, Index] = {k: v for k, v in indexes.items()}\n    new_index_variables: dict[Hashable, Variable] = {}\n    for index, index_vars in indexes.group_by_index():\n        index_dims = {d for var in index_vars.values() for d in var.dims}\n        index_args = {k: v for k, v in args.items() if k in index_dims}\n        if index_args:\n            new_index = getattr(index, func)(index_args)\n            if new_index is not None:\n                new_indexes.update({k: new_index for k in index_vars})\n                new_index_vars = new_index.create_variables(index_vars)\n                new_index_variables.update(new_index_vars)\n            else:\n                for k in index_vars:\n                    new_indexes.pop(k, None)\n    return (new_indexes, new_index_variables)",
    ".xarray.core.indexes.py@@PandasIndex.isel": "def isel(self, indexers: Mapping[Any, int | slice | np.ndarray | Variable]) -> PandasIndex | None:\n    from .variable import Variable\n    indxr = indexers[self.dim]\n    if isinstance(indxr, Variable):\n        if indxr.dims != (self.dim,):\n            return None\n        else:\n            indxr = indxr.data\n    if not isinstance(indxr, slice) and is_scalar(indxr):\n        return None\n    return self._replace(self.index[indxr])",
    ".xarray.core.dataset.py@@Dataset.reset_coords": "def reset_coords(self: T_Dataset, names: Dims=None, drop: bool=False) -> T_Dataset:\n    if names is None:\n        names = self._coord_names - set(self._indexes)\n    else:\n        if isinstance(names, str) or not isinstance(names, Iterable):\n            names = [names]\n        else:\n            names = list(names)\n        self._assert_all_in_dataset(names)\n        bad_coords = set(names) & set(self._indexes)\n        if bad_coords:\n            raise ValueError(f'cannot remove index coordinates with reset_coords: {bad_coords}')\n    obj = self.copy()\n    obj._coord_names.difference_update(names)\n    if drop:\n        for name in names:\n            del obj._variables[name]\n    return obj",
    ".xarray.core.dataset.py@@Dataset._assert_all_in_dataset": "def _assert_all_in_dataset(self, names: Iterable[Hashable], virtual_okay: bool=False) -> None:\n    bad_names = set(names) - set(self._variables)\n    if virtual_okay:\n        bad_names -= self.virtual_variables\n    if bad_names:\n        raise ValueError('One or more of the specified variables cannot be found in this dataset')",
    ".xarray.core.dataarray.py@@DataArray.copy": "def copy(self: T_DataArray, deep: bool=True, data: Any=None) -> T_DataArray:\n    variable = self.variable.copy(deep=deep, data=data)\n    indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n    coords = {}\n    for k, v in self._coords.items():\n        if k in index_vars:\n            coords[k] = index_vars[k]\n        else:\n            coords[k] = v.copy(deep=deep)\n    return self._replace(variable, coords, indexes=indexes)",
    ".xarray.core.dataset.py@@Dataset._rename_dims": "def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:\n    return {name_dict.get(k, k): v for k, v in self.dims.items()}",
    ".xarray.core.dataset.py@@Dataset.dims": "def dims(self) -> Frozen[Hashable, int]:\n    return Frozen(self._dims)",
    ".xarray.core.dataset.py@@Dataset._rename_indexes": "def _rename_indexes(self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    if not self._indexes:\n        return ({}, {})\n    indexes = {}\n    variables = {}\n    for index, coord_names in self.xindexes.group_by_index():\n        new_index = index.rename(name_dict, dims_dict)\n        new_coord_names = [name_dict.get(k, k) for k in coord_names]\n        indexes.update({k: new_index for k in new_coord_names})\n        new_index_vars = new_index.create_variables({new: self._variables[old] for old, new in zip(coord_names, new_coord_names)})\n        variables.update(new_index_vars)\n    return (indexes, variables)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._update_coords": "def _update_coords(self, coords: dict[Hashable, Variable], indexes: Mapping[Any, Index]) -> None:\n    coords_plus_data = coords.copy()\n    coords_plus_data[_THIS_ARRAY] = self._data.variable\n    dims = calculate_dimensions(coords_plus_data)\n    if not set(dims) <= set(self.dims):\n        raise ValueError('cannot add coordinates with new dimensions to a DataArray')\n    self._data._coords = coords\n    original_indexes = dict(self._data.xindexes)\n    original_indexes.update(indexes)\n    self._data._indexes = original_indexes",
    ".xarray.core.dataarray.py@@DataArray._item_key_to_dict": "def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n    if utils.is_dict_like(key):\n        return key\n    key = indexing.expanded_indexer(key, self.ndim)\n    return dict(zip(self.dims, key))",
    ".xarray.core.indexing.py@@BasicIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    for k in key:\n        if isinstance(k, integer_types):\n            k = int(k)\n        elif isinstance(k, slice):\n            k = as_integer_slice(k)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.indexing.py@@as_integer_slice": "def as_integer_slice(value):\n    start = as_integer_or_none(value.start)\n    stop = as_integer_or_none(value.stop)\n    step = as_integer_or_none(value.step)\n    return slice(start, stop, step)",
    ".xarray.core.indexing.py@@as_integer_or_none": "def as_integer_or_none(value):\n    return None if value is None else operator.index(value)",
    ".xarray.core.utils.py@@infix_dims": "def infix_dims(dims_supplied: Collection, dims_all: Collection, missing_dims: ErrorOptionsWithWarn='raise') -> Iterator:\n    if ... in dims_supplied:\n        if len(set(dims_all)) != len(dims_all):\n            raise ValueError('Cannot use ellipsis with repeated dims')\n        if list(dims_supplied).count(...) > 1:\n            raise ValueError('More than one ellipsis supplied')\n        other_dims = [d for d in dims_all if d not in dims_supplied]\n        existing_dims = drop_missing_dims(dims_supplied, dims_all, missing_dims)\n        for d in existing_dims:\n            if d is ...:\n                yield from other_dims\n            else:\n                yield d\n    else:\n        existing_dims = drop_missing_dims(dims_supplied, dims_all, missing_dims)\n        if set(existing_dims) ^ set(dims_all):\n            raise ValueError(f'{dims_supplied} must be a permuted list of {dims_all}, unless `...` is included')\n        yield from existing_dims",
    ".xarray.core.utils.py@@drop_missing_dims": "def drop_missing_dims(supplied_dims: Collection, dims: Collection, missing_dims: ErrorOptionsWithWarn) -> Collection:\n    if missing_dims == 'raise':\n        supplied_dims_set = {val for val in supplied_dims if val is not ...}\n        invalid = supplied_dims_set - set(dims)\n        if invalid:\n            raise ValueError(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return supplied_dims\n    elif missing_dims == 'warn':\n        invalid = set(supplied_dims) - set(dims)\n        if invalid:\n            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return [val for val in supplied_dims if val in dims or val is ...]\n    elif missing_dims == 'ignore':\n        return [val for val in supplied_dims if val in dims or val is ...]\n    else:\n        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.__setitem__": "def __setitem__(self, key, value):\n    array, key = self._indexing_array_and_key(key)\n    try:\n        array[key] = value\n    except ValueError:\n        if not array.flags.writeable and (not array.flags.owndata):\n            raise ValueError('Assignment destination is a view.  Do you want to .copy() array first?')\n        else:\n            raise",
    ".xarray.core.dataarray.py@@DataArray.broadcast_equals": "def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:\n    try:\n        return self._all_compat(other, 'broadcast_equals')\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.variable.py@@broadcast_variables": "def broadcast_variables(*variables: Variable) -> tuple[Variable, ...]:\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple((var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables))",
    ".xarray.core.dataset.py@@Dataset._get_indexers_coords_and_indexes": "def _get_indexers_coords_and_indexes(self, indexers):\n    from .dataarray import DataArray\n    coords_list = []\n    for k, v in indexers.items():\n        if isinstance(v, DataArray):\n            if v.dtype.kind == 'b':\n                if v.ndim != 1:\n                    raise ValueError('{:d}d-boolean array is used for indexing along dimension {!r}, but only 1d boolean arrays are supported.'.format(v.ndim, k))\n                v_coords = v[v.values.nonzero()[0]].coords\n            else:\n                v_coords = v.coords\n            coords_list.append(v_coords)\n    coords, indexes = merge_coordinates_without_align(coords_list)\n    assert_coordinate_consistent(self, coords)\n    attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n    attached_indexes = {k: v for k, v in indexes.items() if k not in self._variables}\n    return (attached_coords, attached_indexes)",
    ".xarray.core.coordinates.py@@assert_coordinate_consistent": "def assert_coordinate_consistent(obj: DataArray | Dataset, coords: Mapping[Any, Variable]) -> None:\n    for k in obj.dims:\n        if k in coords and k in obj.coords and (not coords[k].equals(obj[k].variable)):\n            raise IndexError(f'dimension coordinate {k!r} conflicts between indexed and indexing objects:\\n{obj[k]}\\nvs.\\n{coords[k]}')",
    ".xarray.core.dataarray.py@@DataArray.__getitem__": "def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:\n    if isinstance(key, str):\n        return self._getitem_coord(key)\n    else:\n        return self.isel(indexers=self._item_key_to_dict(key))",
    ".xarray.core.common.py@@AttrAccessMixin.__getattr__": "def __getattr__(self, name: str) -> Any:\n    if name not in {'__dict__', '__setstate__'}:\n        for source in self._attr_sources:\n            with suppress(KeyError):\n                return source[name]\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {name!r}')",
    ".xarray.core.dataarray.py@@DataArray._attr_sources": "def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield from self._item_sources\n    yield self.attrs",
    ".xarray.core.dataarray.py@@DataArray._item_sources": "def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n    yield HybridMappingProxy(keys=self.dims, mapping={})",
    ".xarray.core.utils.py@@HybridMappingProxy.__init__": "def __init__(self, keys: Collection[K], mapping: Mapping[K, V]):\n    self._keys = keys\n    self.mapping = mapping",
    ".xarray.core.utils.py@@HybridMappingProxy.__getitem__": "def __getitem__(self, key: K) -> V:\n    return self.mapping[key]",
    ".xarray.core.dataset.py@@_get_virtual_variable": "def _get_virtual_variable(variables, key: Hashable, dim_sizes: Mapping=None) -> tuple[Hashable, Hashable, Variable]:\n    from .dataarray import DataArray\n    if dim_sizes is None:\n        dim_sizes = {}\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return (key, key, variable)\n    if not isinstance(key, str):\n        raise KeyError(key)\n    split_key = key.split('.', 1)\n    if len(split_key) != 2:\n        raise KeyError(key)\n    ref_name, var_name = split_key\n    ref_var = variables[ref_name]\n    if _contains_datetime_like_objects(ref_var):\n        ref_var = DataArray(ref_var)\n        data = getattr(ref_var.dt, var_name).data\n    else:\n        data = getattr(ref_var, var_name).data\n    virtual_var = Variable(ref_var.dims, data)\n    return (ref_name, var_name, virtual_var)",
    ".xarray.core.dataarray.py@@DataArray.isel": "def isel(self: T_DataArray, indexers: Mapping[Any, Any] | None=None, drop: bool=False, missing_dims: ErrorOptionsWithWarn='raise', **indexers_kwargs: Any) -> T_DataArray:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    if any((is_fancy_indexer(idx) for idx in indexers.values())):\n        ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n        return self._from_temp_dataset(ds)\n    variable = self._variable.isel(indexers, missing_dims=missing_dims)\n    indexes, index_variables = isel_indexes(self.xindexes, indexers)\n    coords = {}\n    for coord_name, coord_value in self._coords.items():\n        if coord_name in index_variables:\n            coord_value = index_variables[coord_name]\n        else:\n            coord_indexers = {k: v for k, v in indexers.items() if k in coord_value.dims}\n            if coord_indexers:\n                coord_value = coord_value.isel(coord_indexers)\n                if drop and coord_value.ndim == 0:\n                    continue\n        coords[coord_name] = coord_value\n    return self._replace(variable=variable, coords=coords, indexes=indexes)",
    ".xarray.core.utils.py@@either_dict_or_kwargs": "def either_dict_or_kwargs(pos_kwargs: Mapping[Any, T] | None, kw_kwargs: Mapping[str, T], func_name: str) -> Mapping[Hashable, T]:\n    if pos_kwargs is None or pos_kwargs == {}:\n        return cast(Mapping[Hashable, T], kw_kwargs)\n    if not is_dict_like(pos_kwargs):\n        raise ValueError(f'the first argument to .{func_name} must be a dictionary')\n    if kw_kwargs:\n        raise ValueError(f'cannot specify both keyword and positional arguments to .{func_name}')\n    return pos_kwargs",
    ".xarray.core.indexing.py@@is_fancy_indexer": "def is_fancy_indexer(indexer: Any) -> bool:\n    if isinstance(indexer, (int, slice)):\n        return False\n    if isinstance(indexer, np.ndarray):\n        return indexer.ndim > 1\n    if isinstance(indexer, list):\n        return bool(indexer) and (not isinstance(indexer[0], int))\n    return True",
    ".xarray.core.utils.py@@drop_dims_from_indexers": "def drop_dims_from_indexers(indexers: Mapping[Any, Any], dims: Iterable[Hashable] | Mapping[Any, int], missing_dims: ErrorOptionsWithWarn) -> Mapping[Hashable, Any]:\n    if missing_dims == 'raise':\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            raise ValueError(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        return indexers\n    elif missing_dims == 'warn':\n        indexers = dict(indexers)\n        invalid = indexers.keys() - set(dims)\n        if invalid:\n            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')\n        for key in invalid:\n            indexers.pop(key)\n        return indexers\n    elif missing_dims == 'ignore':\n        return {key: val for key, val in indexers.items() if key in dims}\n    else:\n        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')",
    ".xarray.core.utils.py@@OrderedSet.__init__": "def __init__(self, values: Iterable[T]=None):\n    self._d = {}\n    if values is not None:\n        self.update(values)",
    ".xarray.core.utils.py@@OrderedSet.update": "def update(self, values: Iterable[T]) -> None:\n    for v in values:\n        self._d[v] = None",
    ".xarray.core.utils.py@@OrderedSet.add": "def add(self, value: T) -> None:\n    self._d[value] = None",
    ".xarray.core.utils.py@@OrderedSet.__iter__": "def __iter__(self) -> Iterator[T]:\n    return iter(self._d)",
    ".xarray.core.utils.py@@OrderedSet.__len__": "def __len__(self) -> int:\n    return len(self._d)",
    ".xarray.core.indexing.py@@VectorizedIndexer.__init__": "def __init__(self, key):\n    if not isinstance(key, tuple):\n        raise TypeError(f'key must be a tuple: {key!r}')\n    new_key = []\n    ndim = None\n    for k in key:\n        if isinstance(k, slice):\n            k = as_integer_slice(k)\n        elif isinstance(k, np.ndarray):\n            if not np.issubdtype(k.dtype, np.integer):\n                raise TypeError(f'invalid indexer array, does not have integer dtype: {k!r}')\n            if ndim is None:\n                ndim = k.ndim\n            elif ndim != k.ndim:\n                ndims = [k.ndim for k in key if isinstance(k, np.ndarray)]\n                raise ValueError(f'invalid indexer key: ndarray arguments have different numbers of dimensions: {ndims}')\n            k = np.asarray(k, dtype=np.int64)\n        else:\n            raise TypeError(f'unexpected indexer type for {type(self).__name__}: {k!r}')\n        new_key.append(k)\n    super().__init__(new_key)",
    ".xarray.core.nputils.py@@NumpyVIndexAdapter.__init__": "def __init__(self, array):\n    self._array = array",
    ".xarray.core.nputils.py@@NumpyVIndexAdapter.__getitem__": "def __getitem__(self, key):\n    mixed_positions, vindex_positions = _advanced_indexer_subspaces(key)\n    return np.moveaxis(self._array[key], mixed_positions, vindex_positions)",
    ".xarray.core.nputils.py@@_advanced_indexer_subspaces": "def _advanced_indexer_subspaces(key):\n    if not isinstance(key, tuple):\n        key = (key,)\n    advanced_index_positions = [i for i, k in enumerate(key) if not isinstance(k, slice)]\n    if not advanced_index_positions or not _is_contiguous(advanced_index_positions):\n        return ((), ())\n    non_slices = [k for k in key if not isinstance(k, slice)]\n    ndim = len(np.broadcast(*non_slices).shape)\n    mixed_positions = advanced_index_positions[0] + np.arange(ndim)\n    vindex_positions = np.arange(ndim)\n    return (mixed_positions, vindex_positions)",
    ".xarray.core.nputils.py@@_is_contiguous": "def _is_contiguous(positions):\n    previous = positions[0]\n    for current in positions[1:]:\n        if current != previous + 1:\n            return False\n        previous = current\n    return True",
    ".xarray.core.dataset.py@@Dataset.coords": "def coords(self) -> DatasetCoordinates:\n    return DatasetCoordinates(self)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__init__": "def __init__(self, dataset: Dataset):\n    self._data = dataset",
    ".xarray.core.coordinates.py@@DatasetCoordinates._names": "def _names(self) -> set[Hashable]:\n    return self._data._coord_names",
    ".xarray.core.common.py@@AbstractArray.get_axis_num": "def get_axis_num(self, dim: Hashable | Iterable[Hashable]) -> int | tuple[int, ...]:\n    if isinstance(dim, Iterable) and (not isinstance(dim, str)):\n        return tuple((self._get_axis_num(d) for d in dim))\n    else:\n        return self._get_axis_num(dim)",
    ".xarray.core.common.py@@AbstractArray._get_axis_num": "def _get_axis_num(self: Any, dim: Hashable) -> int:\n    try:\n        return self.dims.index(dim)\n    except ValueError:\n        raise ValueError(f'{dim!r} not found in array dimensions {self.dims!r}')",
    ".xarray.core.dataarray.py@@DataArray.__setitem__": "def __setitem__(self, key: Any, value: Any) -> None:\n    if isinstance(key, str):\n        self.coords[key] = value\n    else:\n        obj = self[key]\n        if isinstance(value, DataArray):\n            assert_coordinate_consistent(value, obj.coords.variables)\n        key = {k: v.variable if isinstance(v, DataArray) else v for k, v in self._item_key_to_dict(key).items()}\n        self.variable[key] = value",
    ".xarray.core.dataset.py@@Dataset._isel_fancy": "def _isel_fancy(self: T_Dataset, indexers: Mapping[Any, Any], *, drop: bool, missing_dims: ErrorOptionsWithWarn='raise') -> T_Dataset:\n    valid_indexers = dict(self._validate_indexers(indexers, missing_dims))\n    variables: dict[Hashable, Variable] = {}\n    indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)\n    for name, var in self.variables.items():\n        if name in index_variables:\n            new_var = index_variables[name]\n        else:\n            var_indexers = {k: v for k, v in valid_indexers.items() if k in var.dims}\n            if var_indexers:\n                new_var = var.isel(indexers=var_indexers)\n                if name in self.coords and drop and (new_var.ndim == 0):\n                    continue\n            else:\n                new_var = var.copy(deep=False)\n            if name not in indexes:\n                new_var = new_var.to_base_variable()\n        variables[name] = new_var\n    coord_names = self._coord_names & variables.keys()\n    selected = self._replace_with_new_dims(variables, coord_names, indexes)\n    coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n    variables.update(coord_vars)\n    indexes.update(new_indexes)\n    coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n    return self._replace_with_new_dims(variables, coord_names, indexes=indexes)",
    ".xarray.core.dataset.py@@Dataset._validate_indexers": "def _validate_indexers(self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn='raise') -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:\n    from ..coding.cftimeindex import CFTimeIndex\n    from .dataarray import DataArray\n    indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n    for k, v in indexers.items():\n        if isinstance(v, (int, slice, Variable)):\n            yield (k, v)\n        elif isinstance(v, DataArray):\n            yield (k, v.variable)\n        elif isinstance(v, tuple):\n            yield (k, as_variable(v))\n        elif isinstance(v, Dataset):\n            raise TypeError('cannot use a Dataset as an indexer')\n        elif isinstance(v, Sequence) and len(v) == 0:\n            yield (k, np.empty((0,), dtype='int64'))\n        else:\n            v = np.asarray(v)\n            if v.dtype.kind in 'US':\n                index = self._indexes[k].to_pandas_index()\n                if isinstance(index, pd.DatetimeIndex):\n                    v = v.astype('datetime64[ns]')\n                elif isinstance(index, CFTimeIndex):\n                    v = _parse_array_of_cftime_strings(v, index.date_type)\n            if v.ndim > 1:\n                raise IndexError('Unlabeled multi-dimensional array cannot be used for indexing: {}'.format(k))\n            yield (k, v)",
    ".xarray.core.nputils.py@@NumpyVIndexAdapter.__setitem__": "def __setitem__(self, key, value):\n    mixed_positions, vindex_positions = _advanced_indexer_subspaces(key)\n    self._array[key] = np.moveaxis(value, vindex_positions, mixed_positions)",
    ".xarray.core.common.py@@AbstractArray.__format__": "def __format__(self: Any, format_spec: str='') -> str:\n    if format_spec != '':\n        if self.shape == ():\n            return self.data.__format__(format_spec)\n        else:\n            raise NotImplementedError(f'Using format_spec is only supported when shape is (). Got shape = {self.shape}.')\n    else:\n        return self.__repr__()",
    ".xarray.core.dataarray.py@@DataArray._in_memory": "def _in_memory(self) -> bool:\n    return self.variable._in_memory",
    ".xarray.core.formatting.py@@short_numpy_repr": "def short_numpy_repr(array):\n    array = np.asarray(array)\n    options = {'precision': 6, 'linewidth': OPTIONS['display_width'], 'threshold': OPTIONS['display_values_threshold']}\n    if array.ndim < 3:\n        edgeitems = 3\n    elif array.ndim == 3:\n        edgeitems = 2\n    else:\n        edgeitems = 1\n    options['edgeitems'] = edgeitems\n    with set_numpy_options(**options):\n        return repr(array)",
    ".xarray.core.dataset.py@@Dataset.encoding": "def encoding(self) -> dict[Any, Any]:\n    if self._encoding is None:\n        self._encoding = {}\n    return self._encoding",
    ".xarray.core.dataset.py@@Dataset.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return iter(self.data_vars)",
    ".xarray.core.dataset.py@@Dataset.data_vars": "def data_vars(self) -> DataVariables:\n    return DataVariables(self)",
    ".xarray.core.dataset.py@@DataVariables.__init__": "def __init__(self, dataset: Dataset):\n    self._dataset = dataset",
    ".xarray.core.dataset.py@@DataVariables.__iter__": "def __iter__(self) -> Iterator[Hashable]:\n    return (key for key in self._dataset._variables if key not in self._dataset._coord_names)",
    ".xarray.core.formatting.py@@set_numpy_options": "def set_numpy_options(*args, **kwargs):\n    original = np.get_printoptions()\n    np.set_printoptions(*args, **kwargs)\n    try:\n        yield\n    finally:\n        np.set_printoptions(**original)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    return Frozen({k: v for k, v in self._data.variables.items() if k in self._names})",
    ".xarray.core.dataset.py@@Dataset.attrs": "def attrs(self) -> dict[Any, Any]:\n    if self._attrs is None:\n        self._attrs = {}\n    return self._attrs",
    ".xarray.core.dataarray.py@@DataArray.sel": "def sel(self: T_DataArray, indexers: Mapping[Any, Any]=None, method: str=None, tolerance=None, drop: bool=False, **indexers_kwargs: Any) -> T_DataArray:\n    ds = self._to_temp_dataset().sel(indexers=indexers, drop=drop, method=method, tolerance=tolerance, **indexers_kwargs)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.sel": "def sel(self: T_Dataset, indexers: Mapping[Any, Any]=None, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, drop: bool=False, **indexers_kwargs: Any) -> T_Dataset:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n    query_results = map_index_queries(self, indexers=indexers, method=method, tolerance=tolerance)\n    if drop:\n        no_scalar_variables = {}\n        for k, v in query_results.variables.items():\n            if v.dims:\n                no_scalar_variables[k] = v\n            elif k in self._coord_names:\n                query_results.drop_coords.append(k)\n        query_results.variables = no_scalar_variables\n    result = self.isel(indexers=query_results.dim_indexers, drop=drop)\n    return result._overwrite_indexes(*query_results.as_tuple()[1:])",
    ".xarray.core.indexing.py@@map_index_queries": "def map_index_queries(obj: T_Xarray, indexers: Mapping[Any, Any], method=None, tolerance=None, **indexers_kwargs: Any) -> IndexSelResult:\n    from .dataarray import DataArray\n    if method is None and tolerance is None:\n        options = {}\n    else:\n        options = {'method': method, 'tolerance': tolerance}\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'map_index_queries')\n    grouped_indexers = group_indexers_by_index(obj, indexers, options)\n    results = []\n    for index, labels in grouped_indexers:\n        if index is None:\n            results.append(IndexSelResult(labels))\n        else:\n            results.append(index.sel(labels, **options))\n    merged = merge_sel_results(results)\n    for k, v in merged.dim_indexers.items():\n        if isinstance(v, DataArray):\n            if k in v._indexes:\n                v = v.reset_index(k)\n            drop_coords = [name for name in v._coords if name in merged.dim_indexers]\n            merged.dim_indexers[k] = v.drop_vars(drop_coords)\n    return merged",
    ".xarray.core.indexing.py@@group_indexers_by_index": "def group_indexers_by_index(obj: T_Xarray, indexers: Mapping[Any, Any], options: Mapping[str, Any]) -> list[tuple[Index, dict[Any, Any]]]:\n    unique_indexes = {}\n    grouped_indexers: Mapping[int | None, dict] = defaultdict(dict)\n    for key, label in indexers.items():\n        index: Index = obj.xindexes.get(key, None)\n        if index is not None:\n            index_id = id(index)\n            unique_indexes[index_id] = index\n            grouped_indexers[index_id][key] = label\n        elif key in obj.coords:\n            raise KeyError(f'no index found for coordinate {key!r}')\n        elif key not in obj.dims:\n            raise KeyError(f'{key!r} is not a valid dimension or coordinate')\n        elif len(options):\n            raise ValueError(f'cannot supply selection options {options!r} for dimension {key!r}that has no associated coordinate or index')\n        else:\n            unique_indexes[None] = None\n            grouped_indexers[None][key] = label\n    return [(unique_indexes[k], grouped_indexers[k]) for k in unique_indexes]",
    ".xarray.core.indexes.py@@PandasIndex.sel": "def sel(self, labels: dict[Any, Any], method=None, tolerance=None) -> IndexSelResult:\n    from .dataarray import DataArray\n    from .variable import Variable\n    if method is not None and (not isinstance(method, str)):\n        raise TypeError('``method`` must be a string')\n    assert len(labels) == 1\n    coord_name, label = next(iter(labels.items()))\n    if isinstance(label, slice):\n        indexer = _query_slice(self.index, label, coord_name, method, tolerance)\n    elif is_dict_like(label):\n        raise ValueError('cannot use a dict-like object for selection on a dimension that does not have a MultiIndex')\n    else:\n        label_array = normalize_label(label, dtype=self.coord_dtype)\n        if label_array.ndim == 0:\n            label_value = as_scalar(label_array)\n            if isinstance(self.index, pd.CategoricalIndex):\n                if method is not None:\n                    raise ValueError(\"'method' is not supported when indexing using a CategoricalIndex.\")\n                if tolerance is not None:\n                    raise ValueError(\"'tolerance' is not supported when indexing using a CategoricalIndex.\")\n                indexer = self.index.get_loc(label_value)\n            elif method is not None:\n                indexer = get_indexer_nd(self.index, label_array, method, tolerance)\n                if np.any(indexer < 0):\n                    raise KeyError(f'not all values found in index {coord_name!r}')\n            else:\n                try:\n                    indexer = self.index.get_loc(label_value)\n                except KeyError as e:\n                    raise KeyError(f\"not all values found in index {coord_name!r}. Try setting the `method` keyword argument (example: method='nearest').\") from e\n        elif label_array.dtype.kind == 'b':\n            indexer = label_array\n        else:\n            indexer = get_indexer_nd(self.index, label_array, method, tolerance)\n            if np.any(indexer < 0):\n                raise KeyError(f'not all values found in index {coord_name!r}')\n        if isinstance(label, Variable):\n            indexer = Variable(label.dims, indexer)\n        elif isinstance(label, DataArray):\n            indexer = DataArray(indexer, coords=label._coords, dims=label.dims)\n    return IndexSelResult({self.dim: indexer})",
    ".xarray.core.indexes.py@@_query_slice": "def _query_slice(index, label, coord_name='', method=None, tolerance=None):\n    if method is not None or tolerance is not None:\n        raise NotImplementedError('cannot use ``method`` argument if any indexers are slice objects')\n    indexer = index.slice_indexer(_sanitize_slice_element(label.start), _sanitize_slice_element(label.stop), _sanitize_slice_element(label.step))\n    if not isinstance(indexer, slice):\n        raise KeyError(f'cannot represent labeled-based slice indexer for coordinate {coord_name!r} with a slice over integer positions; the index is unsorted or non-unique')\n    return indexer",
    ".xarray.core.indexes.py@@_sanitize_slice_element": "def _sanitize_slice_element(x):\n    from .dataarray import DataArray\n    from .variable import Variable\n    if not isinstance(x, tuple) and len(np.shape(x)) != 0:\n        raise ValueError(f'cannot use non-scalar arrays in a slice for xarray indexing: {x}')\n    if isinstance(x, (Variable, DataArray)):\n        x = x.values\n    if isinstance(x, np.ndarray):\n        x = x[()]\n    return x",
    ".xarray.core.indexing.py@@merge_sel_results": "def merge_sel_results(results: list[IndexSelResult]) -> IndexSelResult:\n    all_dims_count = Counter([dim for res in results for dim in res.dim_indexers])\n    duplicate_dims = {k: v for k, v in all_dims_count.items() if v > 1}\n    if duplicate_dims:\n        fmt_dims = [f'{dim!r}: {count} indexes involved' for dim, count in duplicate_dims.items()]\n        raise ValueError('Xarray does not support label-based selection with more than one index over the following dimension(s):\\n' + '\\n'.join(fmt_dims) + '\\nSuggestion: use a multi-index for each of those dimension(s).')\n    dim_indexers = {}\n    indexes = {}\n    variables = {}\n    drop_coords = []\n    drop_indexes = []\n    rename_dims = {}\n    for res in results:\n        dim_indexers.update(res.dim_indexers)\n        indexes.update(res.indexes)\n        variables.update(res.variables)\n        drop_coords += res.drop_coords\n        drop_indexes += res.drop_indexes\n        rename_dims.update(res.rename_dims)\n    return IndexSelResult(dim_indexers, indexes, variables, drop_coords, drop_indexes, rename_dims)",
    ".xarray.core.dataset.py@@Dataset.isel": "def isel(self: T_Dataset, indexers: Mapping[Any, Any] | None=None, drop: bool=False, missing_dims: ErrorOptionsWithWarn='raise', **indexers_kwargs: Any) -> T_Dataset:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n    if any((is_fancy_indexer(idx) for idx in indexers.values())):\n        return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n    indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n    variables = {}\n    dims: dict[Hashable, int] = {}\n    coord_names = self._coord_names.copy()\n    indexes, index_variables = isel_indexes(self.xindexes, indexers)\n    for name, var in self._variables.items():\n        if name in index_variables:\n            var = index_variables[name]\n        else:\n            var_indexers = {k: v for k, v in indexers.items() if k in var.dims}\n            if var_indexers:\n                var = var.isel(var_indexers)\n                if drop and var.ndim == 0 and (name in coord_names):\n                    coord_names.remove(name)\n                    continue\n        variables[name] = var\n        dims.update(zip(var.dims, var.shape))\n    return self._construct_direct(variables=variables, coord_names=coord_names, dims=dims, attrs=self._attrs, indexes=indexes, encoding=self._encoding, close=self._close)",
    ".xarray.core.indexing.py@@IndexSelResult.as_tuple": "def as_tuple(self):\n    return (self.dim_indexers, self.indexes, self.variables, self.drop_coords, self.drop_indexes, self.rename_dims)",
    ".xarray.core.dataset.py@@Dataset._overwrite_indexes": "def _overwrite_indexes(self: T_Dataset, indexes: Mapping[Hashable, Index], variables: Mapping[Hashable, Variable] | None=None, drop_variables: list[Hashable] | None=None, drop_indexes: list[Hashable] | None=None, rename_dims: Mapping[Hashable, Hashable] | None=None) -> T_Dataset:\n    if not indexes:\n        return self\n    if variables is None:\n        variables = {}\n    if drop_variables is None:\n        drop_variables = []\n    if drop_indexes is None:\n        drop_indexes = []\n    new_variables = self._variables.copy()\n    new_coord_names = self._coord_names.copy()\n    new_indexes = dict(self._indexes)\n    index_variables = {}\n    no_index_variables = {}\n    for name, var in variables.items():\n        old_var = self._variables.get(name)\n        if old_var is not None:\n            var.attrs.update(old_var.attrs)\n            var.encoding.update(old_var.encoding)\n        if name in indexes:\n            index_variables[name] = var\n        else:\n            no_index_variables[name] = var\n    for name in indexes:\n        new_indexes[name] = indexes[name]\n    for name, var in index_variables.items():\n        new_coord_names.add(name)\n        new_variables[name] = var\n    for k in no_index_variables:\n        new_variables.pop(k)\n    new_variables.update(no_index_variables)\n    for name in drop_indexes:\n        new_indexes.pop(name)\n    for name in drop_variables:\n        new_variables.pop(name)\n        new_indexes.pop(name, None)\n        new_coord_names.remove(name)\n    replaced = self._replace(variables=new_variables, coord_names=new_coord_names, indexes=new_indexes)\n    if rename_dims:\n        dims = replaced._rename_dims(rename_dims)\n        new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)\n        return replaced._replace(variables=new_variables, coord_names=new_coord_names, dims=dims)\n    else:\n        return replaced",
    ".xarray.core.indexes.py@@normalize_label": "def normalize_label(value, dtype=None) -> np.ndarray:\n    if getattr(value, 'ndim', 1) <= 1:\n        value = _asarray_tuplesafe(value)\n    if dtype is not None and dtype.kind == 'f' and (value.dtype.kind != 'b'):\n        value = np.asarray(value, dtype=dtype)\n    return value",
    ".xarray.core.indexes.py@@_asarray_tuplesafe": "def _asarray_tuplesafe(values):\n    if isinstance(values, tuple):\n        result = utils.to_0d_object_array(values)\n    else:\n        result = np.asarray(values)\n        if result.ndim == 2:\n            result = np.empty(len(values), dtype=object)\n            result[:] = values\n    return result",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__lt__": "def __lt__(self, other):\n    return self._binary_op(other, operator.lt)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__lt__": "def __lt__(self, other):\n    return self._binary_op(other, operator.lt)",
    ".xarray.core.dataarray.py@@DataArray.drop_vars": "def drop_vars(self, names: Hashable | Iterable[Hashable], *, errors: ErrorOptions='raise') -> DataArray:\n    ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.drop_vars": "def drop_vars(self: T_Dataset, names: Hashable | Iterable[Hashable], *, errors: ErrorOptions='raise') -> T_Dataset:\n    if is_scalar(names) or not isinstance(names, Iterable):\n        names = {names}\n    else:\n        names = set(names)\n    if errors == 'raise':\n        self._assert_all_in_dataset(names)\n    other_names = set()\n    for var in names:\n        maybe_midx = self._indexes.get(var, None)\n        if isinstance(maybe_midx, PandasMultiIndex):\n            idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])\n            idx_other_names = idx_coord_names - set(names)\n            other_names.update(idx_other_names)\n    if other_names:\n        names |= set(other_names)\n        warnings.warn(f'Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. Please also drop the following variables: {other_names!r} to avoid an error in the future.', DeprecationWarning, stacklevel=2)\n    assert_no_index_corrupted(self.xindexes, names)\n    variables = {k: v for k, v in self._variables.items() if k not in names}\n    coord_names = {k for k in self._coord_names if k in variables}\n    indexes = {k: v for k, v in self._indexes.items() if k not in names}\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.indexes.py@@assert_no_index_corrupted": "def assert_no_index_corrupted(indexes: Indexes[Index], coord_names: set[Hashable], action: str='remove coordinate(s)') -> None:\n    for index, index_coords in indexes.group_by_index():\n        common_names = set(index_coords) & coord_names\n        if common_names and len(common_names) != len(index_coords):\n            common_names_str = ', '.join((f'{k!r}' for k in common_names))\n            index_names_str = ', '.join((f'{k!r}' for k in index_coords))\n            raise ValueError(f'cannot {action} {common_names_str}, which would corrupt the following index built from coordinates {index_names_str}:\\n{index}')",
    ".xarray.core.utils.py@@NdimSizeLenMixin.__len__": "def __len__(self: Any) -> int:\n    try:\n        return self.shape[0]\n    except IndexError:\n        raise TypeError('len() of unsized object')",
    ".xarray.core.indexes.py@@as_scalar": "def as_scalar(value: np.ndarray):\n    return value[()] if value.dtype.kind in 'mM' else value.item()",
    ".xarray.core.dataarray.py@@DataArray.loc": "def loc(self) -> _LocIndexer:\n    return _LocIndexer(self)",
    ".xarray.core.dataarray.py@@_LocIndexer.__init__": "def __init__(self, data_array: DataArray):\n    self.data_array = data_array",
    ".xarray.core.dataarray.py@@_LocIndexer.__getitem__": "def __getitem__(self, key) -> DataArray:\n    if not utils.is_dict_like(key):\n        labels = indexing.expanded_indexer(key, self.data_array.ndim)\n        key = dict(zip(self.data_array.dims, labels))\n    return self.data_array.sel(key)",
    ".xarray.core.variable.py@@_possibly_convert_objects": "def _possibly_convert_objects(values):\n    return np.asarray(pd.Series(values.ravel())).reshape(values.shape)",
    ".xarray.core.dataarray.py@@_LocIndexer.__setitem__": "def __setitem__(self, key, value) -> None:\n    if not utils.is_dict_like(key):\n        labels = indexing.expanded_indexer(key, self.data_array.ndim)\n        key = dict(zip(self.data_array.dims, labels))\n    dim_indexers = map_index_queries(self.data_array, key).dim_indexers\n    self.data_array[dim_indexers] = value",
    ".xarray.core.indexing.py@@_outer_to_vectorized_indexer": "def _outer_to_vectorized_indexer(key, shape):\n    key = key.tuple\n    n_dim = len([k for k in key if not isinstance(k, integer_types)])\n    i_dim = 0\n    new_key = []\n    for k, size in zip(key, shape):\n        if isinstance(k, integer_types):\n            new_key.append(np.array(k).reshape((1,) * n_dim))\n        else:\n            if isinstance(k, slice):\n                k = np.arange(*k.indices(size))\n            assert k.dtype.kind in {'i', 'u'}\n            shape = [(1,) * i_dim + (k.size,) + (1,) * (n_dim - i_dim - 1)]\n            new_key.append(k.reshape(*shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))",
    ".xarray.core.indexes.py@@PandasMultiIndex._replace": "def _replace(self, index, dim=None, level_coords_dtype=None) -> PandasMultiIndex:\n    if dim is None:\n        dim = self.dim\n    index.name = dim\n    if level_coords_dtype is None:\n        level_coords_dtype = self.level_coords_dtype\n    return type(self)(index, dim, level_coords_dtype)",
    ".xarray.core.indexes.py@@PandasMultiIndex.__init__": "def __init__(self, array: Any, dim: Hashable, level_coords_dtype: Any=None):\n    super().__init__(array, dim)\n    names = []\n    for i, idx in enumerate(self.index.levels):\n        name = idx.name or f'{dim}_level_{i}'\n        if name == dim:\n            raise ValueError(f'conflicting multi-index level name {name!r} with dimension {dim!r}')\n        names.append(name)\n    self.index.names = names\n    if level_coords_dtype is None:\n        level_coords_dtype = {idx.name: get_valid_numpy_dtype(idx) for idx in self.index.levels}\n    self.level_coords_dtype = level_coords_dtype",
    ".xarray.core.indexes.py@@PandasMultiIndex.create_variables": "def create_variables(self, variables: Mapping[Any, Variable] | None=None) -> IndexVars:\n    from .variable import IndexVariable\n    if variables is None:\n        variables = {}\n    index_vars: IndexVars = {}\n    for name in (self.dim,) + self.index.names:\n        if name == self.dim:\n            level = None\n            dtype = None\n        else:\n            level = name\n            dtype = self.level_coords_dtype[name]\n        var = variables.get(name, None)\n        if var is not None:\n            attrs = var.attrs\n            encoding = var.encoding\n        else:\n            attrs = {}\n            encoding = {}\n        data = PandasMultiIndexingAdapter(self.index, dtype=dtype, level=level)\n        index_vars[name] = IndexVariable(self.dim, data, attrs=attrs, encoding=encoding, fastpath=True)\n    return index_vars",
    ".xarray.core.indexes.py@@PandasIndex.rename": "def rename(self, name_dict, dims_dict):\n    if self.index.name not in name_dict and self.dim not in dims_dict:\n        return self\n    new_name = name_dict.get(self.index.name, self.index.name)\n    index = self.index.rename(new_name)\n    new_dim = dims_dict.get(self.dim, self.dim)\n    return self._replace(index, dim=new_dim)",
    ".xarray.core.indexes.py@@PandasMultiIndex.sel": "def sel(self, labels, method=None, tolerance=None) -> IndexSelResult:\n    from .dataarray import DataArray\n    from .variable import Variable\n    if method is not None or tolerance is not None:\n        raise ValueError('multi-index does not support ``method`` and ``tolerance``')\n    new_index = None\n    scalar_coord_values = {}\n    if all([lbl in self.index.names for lbl in labels]):\n        label_values = {}\n        for k, v in labels.items():\n            label_array = normalize_label(v, dtype=self.level_coords_dtype[k])\n            try:\n                label_values[k] = as_scalar(label_array)\n            except ValueError:\n                raise ValueError(f'Vectorized selection is not available along coordinate {k!r} (multi-index level)')\n        has_slice = any([isinstance(v, slice) for v in label_values.values()])\n        if len(label_values) == self.index.nlevels and (not has_slice):\n            indexer = self.index.get_loc(tuple((label_values[k] for k in self.index.names)))\n        else:\n            indexer, new_index = self.index.get_loc_level(tuple(label_values.values()), level=tuple(label_values.keys()))\n            scalar_coord_values.update(label_values)\n            if indexer.dtype.kind == 'b' and indexer.sum() == 0:\n                raise KeyError(f'{labels} not found')\n    else:\n        if len(labels) > 1:\n            coord_name = next(iter(set(labels) - set(self.index.names)))\n            raise ValueError(f'cannot provide labels for both coordinate {coord_name!r} (multi-index array) and one or more coordinates among {self.index.names!r} (multi-index levels)')\n        coord_name, label = next(iter(labels.items()))\n        if is_dict_like(label):\n            invalid_levels = [name for name in label if name not in self.index.names]\n            if invalid_levels:\n                raise ValueError(f'invalid multi-index level names {invalid_levels}')\n            return self.sel(label)\n        elif isinstance(label, slice):\n            indexer = _query_slice(self.index, label, coord_name)\n        elif isinstance(label, tuple):\n            if _is_nested_tuple(label):\n                indexer = self.index.get_locs(label)\n            elif len(label) == self.index.nlevels:\n                indexer = self.index.get_loc(label)\n            else:\n                levels = [self.index.names[i] for i in range(len(label))]\n                indexer, new_index = self.index.get_loc_level(label, level=levels)\n                scalar_coord_values.update({k: v for k, v in zip(levels, label)})\n        else:\n            label_array = normalize_label(label)\n            if label_array.ndim == 0:\n                label_value = as_scalar(label_array)\n                indexer, new_index = self.index.get_loc_level(label_value, level=0)\n                scalar_coord_values[self.index.names[0]] = label_value\n            elif label_array.dtype.kind == 'b':\n                indexer = label_array\n            else:\n                if label_array.ndim > 1:\n                    raise ValueError(f'Vectorized selection is not available along coordinate {coord_name!r} with a multi-index')\n                indexer = get_indexer_nd(self.index, label_array)\n                if np.any(indexer < 0):\n                    raise KeyError(f'not all values found in index {coord_name!r}')\n            if isinstance(label, Variable):\n                indexer = Variable(label.dims, indexer)\n            elif isinstance(label, DataArray):\n                coords = {k: v for k, v in label._coords.items() if k not in self.index.names}\n                indexer = DataArray(indexer, coords=coords, dims=label.dims)\n    if new_index is not None:\n        if isinstance(new_index, pd.MultiIndex):\n            level_coords_dtype = {k: self.level_coords_dtype[k] for k in new_index.names}\n            new_index = self._replace(new_index, level_coords_dtype=level_coords_dtype)\n            dims_dict = {}\n            drop_coords = []\n        else:\n            new_index = PandasIndex(new_index, new_index.name, coord_dtype=self.level_coords_dtype[new_index.name])\n            dims_dict = {self.dim: new_index.index.name}\n            drop_coords = [self.dim]\n        new_vars = new_index.create_variables()\n        indexes = cast(Dict[Any, Index], {k: new_index for k in new_vars})\n        variables = new_vars\n        for name, val in scalar_coord_values.items():\n            variables[name] = Variable([], val)\n        return IndexSelResult({self.dim: indexer}, indexes=indexes, variables=variables, drop_indexes=list(scalar_coord_values), drop_coords=drop_coords, rename_dims=dims_dict)\n    else:\n        return IndexSelResult({self.dim: indexer})",
    ".xarray.core.indexes.py@@_is_nested_tuple": "def _is_nested_tuple(possible_tuple):\n    return isinstance(possible_tuple, tuple) and any((isinstance(value, (tuple, list, slice)) for value in possible_tuple))",
    ".xarray.core.dataarray.py@@DataArray.unstack": "def unstack(self, dim: Dims=None, fill_value: Any=dtypes.NA, sparse: bool=False) -> DataArray:\n    ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.unstack": "def unstack(self: T_Dataset, dim: Dims=None, fill_value: Any=xrdtypes.NA, sparse: bool=False) -> T_Dataset:\n    if dim is None:\n        dims = list(self.dims)\n    else:\n        if isinstance(dim, str) or not isinstance(dim, Iterable):\n            dims = [dim]\n        else:\n            dims = list(dim)\n        missing_dims = [d for d in dims if d not in self.dims]\n        if missing_dims:\n            raise ValueError(f'Dataset does not contain the dimensions: {missing_dims}')\n    stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}\n    for d in dims:\n        idx, idx_vars = self._get_stack_index(d, multi=True)\n        if idx is not None:\n            stacked_indexes[d] = (idx, idx_vars)\n    if dim is None:\n        dims = list(stacked_indexes)\n    else:\n        non_multi_dims = set(dims) - set(stacked_indexes)\n        if non_multi_dims:\n            raise ValueError(f'cannot unstack dimensions that do not have exactly one multi-index: {tuple(non_multi_dims)}')\n    result = self.copy(deep=False)\n    nonindexes = [self.variables[k] for k in set(self.variables) - set(self._indexes)]\n    needs_full_reindex = any((is_duck_dask_array(v.data) or isinstance(v.data, sparse_array_type) or (not isinstance(v.data, np.ndarray)) for v in nonindexes))\n    for d in dims:\n        if needs_full_reindex:\n            result = result._unstack_full_reindex(d, stacked_indexes[d], fill_value, sparse)\n        else:\n            result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)\n    return result",
    ".xarray.core.utils.py@@Frozen.__len__": "def __len__(self) -> int:\n    return len(self.mapping)",
    ".xarray.core.dataset.py@@Dataset._get_stack_index": "def _get_stack_index(self, dim, multi=False, create_index=False) -> tuple[Index | None, dict[Hashable, Variable]]:\n    stack_index: Index | None = None\n    stack_coords: dict[Hashable, Variable] = {}\n    for name, index in self._indexes.items():\n        var = self._variables[name]\n        if var.ndim == 1 and var.dims[0] == dim and (not multi and (not self.xindexes.is_multi(name)) or (multi and type(index).unstack is not Index.unstack)):\n            if stack_index is not None and index is not stack_index:\n                if create_index:\n                    raise ValueError(f'cannot stack dimension {dim!r} with `create_index=True` and with more than one index found along that dimension')\n                return (None, {})\n            stack_index = index\n            stack_coords[name] = var\n    if create_index and stack_index is None:\n        if dim in self._variables:\n            var = self._variables[dim]\n        else:\n            _, _, var = _get_virtual_variable(self._variables, dim, self.dims)\n        stack_index = PandasIndex([0], dim)\n        stack_coords = {dim: var}\n    return (stack_index, stack_coords)",
    ".xarray.core.dataset.py@@Dataset._unstack_once": "def _unstack_once(self: T_Dataset, dim: Hashable, index_and_vars: tuple[Index, dict[Hashable, Variable]], fill_value, sparse: bool=False) -> T_Dataset:\n    index, index_vars = index_and_vars\n    variables: dict[Hashable, Variable] = {}\n    indexes = {k: v for k, v in self._indexes.items() if k != dim}\n    new_indexes, clean_index = index.unstack()\n    indexes.update(new_indexes)\n    for name, idx in new_indexes.items():\n        variables.update(idx.create_variables(index_vars))\n    for name, var in self.variables.items():\n        if name not in index_vars:\n            if dim in var.dims:\n                if isinstance(fill_value, Mapping):\n                    fill_value_ = fill_value[name]\n                else:\n                    fill_value_ = fill_value\n                variables[name] = var._unstack_once(index=clean_index, dim=dim, fill_value=fill_value_, sparse=sparse)\n            else:\n                variables[name] = var\n    coord_names = set(self._coord_names) - {dim} | set(new_indexes)\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.indexes.py@@PandasMultiIndex.unstack": "def unstack(self) -> tuple[dict[Hashable, Index], pd.MultiIndex]:\n    clean_index = remove_unused_levels_categories(self.index)\n    new_indexes: dict[Hashable, Index] = {}\n    for name, lev in zip(clean_index.names, clean_index.levels):\n        idx = PandasIndex(lev.copy(), name, coord_dtype=self.level_coords_dtype[name])\n        new_indexes[name] = idx\n    return (new_indexes, clean_index)",
    ".xarray.core.indexes.py@@remove_unused_levels_categories": "def remove_unused_levels_categories(index: pd.Index) -> pd.Index:\n    if isinstance(index, pd.MultiIndex):\n        index = index.remove_unused_levels()\n        if any((isinstance(lev, pd.CategoricalIndex) for lev in index.levels)):\n            levels = []\n            for i, level in enumerate(index.levels):\n                if isinstance(level, pd.CategoricalIndex):\n                    level = level[index.codes[i]].remove_unused_categories()\n                else:\n                    level = level[index.codes[i]]\n                levels.append(level)\n            index = pd.MultiIndex.from_arrays(levels, names=index.names)\n    elif isinstance(index, pd.CategoricalIndex):\n        index = index.remove_unused_categories()\n    return index",
    ".xarray.core.dtypes.py@@get_fill_value": "def get_fill_value(dtype):\n    _, fill_value = maybe_promote(dtype)\n    return fill_value",
    ".xarray.core.dtypes.py@@maybe_promote": "def maybe_promote(dtype):\n    if np.issubdtype(dtype, np.floating):\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.timedelta64):\n        fill_value = np.timedelta64('NaT')\n    elif np.issubdtype(dtype, np.integer):\n        dtype = np.float32 if dtype.itemsize <= 2 else np.float64\n        fill_value = np.nan\n    elif np.issubdtype(dtype, np.complexfloating):\n        fill_value = np.nan + np.nan * 1j\n    elif np.issubdtype(dtype, np.datetime64):\n        fill_value = np.datetime64('NaT')\n    else:\n        dtype = object\n        fill_value = np.nan\n    return (np.dtype(dtype), fill_value)",
    ".xarray.core.concat.py@@concat": "def concat(objs: Iterable[T_Dataset], dim: Hashable | T_DataArray | pd.Index, data_vars: ConcatOptions | list[Hashable]='all', coords: ConcatOptions | list[Hashable]='different', compat: CompatOptions='equals', positions: Iterable[Iterable[int]] | None=None, fill_value: object=dtypes.NA, join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='override') -> T_Dataset:\n    ...",
    ".xarray.core.utils.py@@peek_at": "def peek_at(iterable: Iterable[T]) -> tuple[T, Iterator[T]]:\n    gen = iter(iterable)\n    peek = next(gen)\n    return (peek, itertools.chain([peek], gen))",
    ".xarray.core.concat.py@@_dataarray_concat": "def _dataarray_concat(arrays: Iterable[T_DataArray], dim: str | T_DataArray | pd.Index, data_vars: str | list[str], coords: str | list[str], compat: CompatOptions, positions: Iterable[Iterable[int]] | None, fill_value: object=dtypes.NA, join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='override') -> T_DataArray:\n    from .dataarray import DataArray\n    arrays = list(arrays)\n    if not all((isinstance(array, DataArray) for array in arrays)):\n        raise TypeError(\"The elements in the input list need to be either all 'Dataset's or all 'DataArray's\")\n    if data_vars != 'all':\n        raise ValueError('data_vars is not a valid argument when concatenating DataArray objects')\n    datasets = []\n    for n, arr in enumerate(arrays):\n        if n == 0:\n            name = arr.name\n        elif name != arr.name:\n            if compat == 'identical':\n                raise ValueError('array names not identical')\n            else:\n                arr = cast(T_DataArray, arr.rename(name))\n        datasets.append(arr._to_temp_dataset())\n    ds = _dataset_concat(datasets, dim, data_vars, coords, compat, positions, fill_value=fill_value, join=join, combine_attrs=combine_attrs)\n    merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)\n    result = arrays[0]._from_temp_dataset(ds, name)\n    result.attrs = merged_attrs\n    return result",
    ".xarray.core.concat.py@@_dataset_concat": "def _dataset_concat(datasets: list[T_Dataset], dim: str | T_DataArray | pd.Index, data_vars: str | list[str], coords: str | list[str], compat: CompatOptions, positions: Iterable[Iterable[int]] | None, fill_value: object=dtypes.NA, join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='override') -> T_Dataset:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    datasets = list(datasets)\n    if not all((isinstance(dataset, Dataset) for dataset in datasets)):\n        raise TypeError(\"The elements in the input list need to be either all 'Dataset's or all 'DataArray's\")\n    if isinstance(dim, DataArray):\n        dim_var = dim.variable\n    elif isinstance(dim, Variable):\n        dim_var = dim\n    else:\n        dim_var = None\n    dim, index = _calc_concat_dim_index(dim)\n    datasets = [ds.copy() for ds in datasets]\n    datasets = list(align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value))\n    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n    dim_names = set(dim_coords)\n    unlabeled_dims = dim_names - coord_names\n    both_data_and_coords = coord_names & data_names\n    if both_data_and_coords:\n        raise ValueError(f'{both_data_and_coords!r} is a coordinate in some datasets but not others.')\n    dim_coords.pop(dim, None)\n    dims_sizes.pop(dim, None)\n    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n        datasets = [cast(T_Dataset, ds.expand_dims(dim)) for ds in datasets]\n    concat_over, equals, concat_dim_lengths = _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat)\n    variables_to_merge = (coord_names | data_names) - concat_over - unlabeled_dims\n    result_vars = {}\n    result_indexes = {}\n    if variables_to_merge:\n        grouped = {k: v for k, v in collect_variables_and_indexes(list(datasets)).items() if k in variables_to_merge}\n        merged_vars, merged_indexes = merge_collected(grouped, compat=compat, equals=equals)\n        result_vars.update(merged_vars)\n        result_indexes.update(merged_indexes)\n    result_vars.update(dim_coords)\n    result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)\n    result_encoding = datasets[0].encoding\n    for ds in datasets[1:]:\n        if compat == 'identical' and (not utils.dict_equiv(ds.attrs, result_attrs)):\n            raise ValueError('Dataset global attributes not equal.')\n\n    def ensure_common_dims(vars):\n        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n        if dim not in common_dims:\n            common_dims = (dim,) + common_dims\n        for var, dim_len in zip(vars, concat_dim_lengths):\n            if var.dims != common_dims:\n                common_shape = tuple((dims_sizes.get(d, dim_len) for d in common_dims))\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    def get_indexes(name):\n        for ds in datasets:\n            if name in ds._indexes:\n                yield ds._indexes[name]\n            elif name == dim:\n                var = ds._variables[name]\n                if not var.dims:\n                    data = var.set_dims(dim).values\n                    yield PandasIndex(data, dim, coord_dtype=var.dtype)\n    for name in datasets[0].variables:\n        if name in concat_over and name not in result_indexes:\n            try:\n                vars = ensure_common_dims([ds[name].variable for ds in datasets])\n            except KeyError:\n                raise ValueError(f'{name!r} is not present in all datasets.')\n            indexes: list[Index] = list(get_indexes(name))\n            if indexes:\n                if len(indexes) < len(datasets):\n                    raise ValueError(f'{name!r} must have either an index or no index in all datasets, found {len(indexes)}/{len(datasets)} datasets with an index.')\n                combined_idx = indexes[0].concat(indexes, dim, positions)\n                if name in datasets[0]._indexes:\n                    idx_vars = datasets[0].xindexes.get_all_coords(name)\n                else:\n                    idx_vars = {name: datasets[0][name].variable}\n                result_indexes.update({k: combined_idx for k in idx_vars})\n                combined_idx_vars = combined_idx.create_variables(idx_vars)\n                for k, v in combined_idx_vars.items():\n                    v.attrs = merge_attrs([ds.variables[k].attrs for ds in datasets], combine_attrs=combine_attrs)\n                    result_vars[k] = v\n            else:\n                combined_var = concat_vars(vars, dim, positions, combine_attrs=combine_attrs)\n                result_vars[name] = combined_var\n        elif name in result_vars:\n            result_vars[name] = result_vars.pop(name)\n    result = type(datasets[0])(result_vars, attrs=result_attrs)\n    absent_coord_names = coord_names - set(result.variables)\n    if absent_coord_names:\n        raise ValueError(f'Variables {absent_coord_names!r} are coordinates in some datasets but not others.')\n    result = result.set_coords(coord_names)\n    result.encoding = result_encoding\n    result = result.drop_vars(unlabeled_dims, errors='ignore')\n    if index is not None:\n        if dim_var is not None:\n            index_vars = index.create_variables({dim: dim_var})\n        else:\n            index_vars = index.create_variables()\n        result[dim] = index_vars[dim]\n        result_indexes[dim] = index\n    result = result._overwrite_indexes(result_indexes)\n    return result",
    ".xarray.core.concat.py@@_calc_concat_dim_index": "def _calc_concat_dim_index(dim_or_data: Hashable | Any) -> tuple[Hashable, PandasIndex | None]:\n    from .dataarray import DataArray\n    dim: Hashable | None\n    if isinstance(dim_or_data, str):\n        dim = dim_or_data\n        index = None\n    else:\n        if not isinstance(dim_or_data, (DataArray, Variable)):\n            dim = getattr(dim_or_data, 'name', None)\n            if dim is None:\n                dim = 'concat_dim'\n        else:\n            dim, = dim_or_data.dims\n        coord_dtype = getattr(dim_or_data, 'dtype', None)\n        index = PandasIndex(dim_or_data, dim, coord_dtype=coord_dtype)\n    return (dim, index)",
    ".xarray.core.concat.py@@_parse_datasets": "def _parse_datasets(datasets: Iterable[T_Dataset]) -> tuple[dict[Hashable, Variable], dict[Hashable, int], set[Hashable], set[Hashable]]:\n    dims: set[Hashable] = set()\n    all_coord_names: set[Hashable] = set()\n    data_vars: set[Hashable] = set()\n    dim_coords: dict[Hashable, Variable] = {}\n    dims_sizes: dict[Hashable, int] = {}\n    for ds in datasets:\n        dims_sizes.update(ds.dims)\n        all_coord_names.update(ds.coords)\n        data_vars.update(ds.data_vars)\n        for dim in ds.dims:\n            if dim in dims:\n                continue\n            if dim not in dim_coords:\n                dim_coords[dim] = ds.coords[dim].variable\n        dims = dims | set(ds.dims)\n    return (dim_coords, dims_sizes, all_coord_names, data_vars)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.__getitem__": "def __getitem__(self, key: Hashable) -> DataArray:\n    if key in self._data.data_vars:\n        raise KeyError(key)\n    return cast('DataArray', self._data[key])",
    ".xarray.core.dataset.py@@DataVariables.__contains__": "def __contains__(self, key: Hashable) -> bool:\n    return key in self._dataset._variables and key not in self._dataset._coord_names",
    ".xarray.core.concat.py@@_calc_concat_over": "def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n    concat_over = set()\n    equals = {}\n    if dim in dim_names:\n        concat_over_existing_dim = True\n        concat_over.add(dim)\n    else:\n        concat_over_existing_dim = False\n    concat_dim_lengths = []\n    for ds in datasets:\n        if concat_over_existing_dim:\n            if dim not in ds.dims:\n                if dim in ds:\n                    ds = ds.set_coords(dim)\n        concat_over.update((k for k, v in ds.variables.items() if dim in v.dims))\n        concat_dim_lengths.append(ds.dims.get(dim, 1))\n\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == 'different':\n                if compat == 'override':\n                    raise ValueError(f\"Cannot specify both {subset}='different' and compat='override'.\")\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        equals[k] = None\n                        variables = [ds.variables[k] for ds in datasets if k in ds.variables]\n                        if len(variables) == 1:\n                            break\n                        elif len(variables) != len(datasets) and opt == 'different':\n                            raise ValueError(f\"{k!r} not present in all datasets and coords='different'. Either add {k!r} to datasets where it is missing or specify coords='minimal'.\")\n                        for var in variables[1:]:\n                            equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)\n                            if equals[k] is not True:\n                                break\n                        if equals[k] is False:\n                            concat_over.add(k)\n                        elif equals[k] is None:\n                            v_lhs = datasets[0].variables[k].load()\n                            computed = []\n                            for ds_rhs in datasets[1:]:\n                                v_rhs = ds_rhs.variables[k].compute()\n                                computed.append(v_rhs)\n                                if not getattr(v_lhs, compat)(v_rhs):\n                                    concat_over.add(k)\n                                    equals[k] = False\n                                    for ds, v in zip(datasets[1:], computed):\n                                        ds.variables[k].data = v.data\n                                    break\n                            else:\n                                equals[k] = True\n            elif opt == 'all':\n                concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n            elif opt == 'minimal':\n                pass\n            else:\n                raise ValueError(f'unexpected value for {subset}: {opt}')\n        else:\n            invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n            if invalid_vars:\n                if subset == 'coords':\n                    raise ValueError(f'some variables in coords are not coordinates on the first dataset: {invalid_vars}')\n                else:\n                    raise ValueError(f'some variables in data_vars are not data variables on the first dataset: {invalid_vars}')\n            concat_over.update(opt)\n    process_subset_opt(data_vars, 'data_vars')\n    process_subset_opt(coords, 'coords')\n    return (concat_over, equals, concat_dim_lengths)",
    ".xarray.core.concat.py@@process_subset_opt": "def process_subset_opt(opt, subset):\n    if isinstance(opt, str):\n        if opt == 'different':\n            if compat == 'override':\n                raise ValueError(f\"Cannot specify both {subset}='different' and compat='override'.\")\n            for k in getattr(datasets[0], subset):\n                if k not in concat_over:\n                    equals[k] = None\n                    variables = [ds.variables[k] for ds in datasets if k in ds.variables]\n                    if len(variables) == 1:\n                        break\n                    elif len(variables) != len(datasets) and opt == 'different':\n                        raise ValueError(f\"{k!r} not present in all datasets and coords='different'. Either add {k!r} to datasets where it is missing or specify coords='minimal'.\")\n                    for var in variables[1:]:\n                        equals[k] = getattr(variables[0], compat)(var, equiv=lazy_array_equiv)\n                        if equals[k] is not True:\n                            break\n                    if equals[k] is False:\n                        concat_over.add(k)\n                    elif equals[k] is None:\n                        v_lhs = datasets[0].variables[k].load()\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not getattr(v_lhs, compat)(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n        elif opt == 'all':\n            concat_over.update(set(getattr(datasets[0], subset)) - set(datasets[0].dims))\n        elif opt == 'minimal':\n            pass\n        else:\n            raise ValueError(f'unexpected value for {subset}: {opt}')\n    else:\n        invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n        if invalid_vars:\n            if subset == 'coords':\n                raise ValueError(f'some variables in coords are not coordinates on the first dataset: {invalid_vars}')\n            else:\n                raise ValueError(f'some variables in data_vars are not data variables on the first dataset: {invalid_vars}')\n        concat_over.update(opt)",
    ".xarray.core.concat.py@@get_indexes": "def get_indexes(name):\n    for ds in datasets:\n        if name in ds._indexes:\n            yield ds._indexes[name]\n        elif name == dim:\n            var = ds._variables[name]\n            if not var.dims:\n                data = var.set_dims(dim).values\n                yield PandasIndex(data, dim, coord_dtype=var.dtype)",
    ".xarray.core.indexes.py@@PandasIndex.concat": "def concat(cls, indexes: Sequence[PandasIndex], dim: Hashable, positions: Iterable[Iterable[int]]=None) -> PandasIndex:\n    new_pd_index = cls._concat_indexes(indexes, dim, positions)\n    if not indexes:\n        coord_dtype = None\n    else:\n        coord_dtype = np.result_type(*[idx.coord_dtype for idx in indexes])\n    return cls(new_pd_index, dim=dim, coord_dtype=coord_dtype)",
    ".xarray.core.indexes.py@@PandasIndex._concat_indexes": "def _concat_indexes(indexes, dim, positions=None) -> pd.Index:\n    new_pd_index: pd.Index\n    if not indexes:\n        new_pd_index = pd.Index([])\n    else:\n        if not all((idx.dim == dim for idx in indexes)):\n            dims = ','.join({f'{idx.dim!r}' for idx in indexes})\n            raise ValueError(f'Cannot concatenate along dimension {dim!r} indexes with dimensions: {dims}')\n        pd_indexes = [idx.index for idx in indexes]\n        new_pd_index = pd_indexes[0].append(pd_indexes[1:])\n        if positions is not None:\n            indices = nputils.inverse_permutation(np.concatenate(positions))\n            new_pd_index = new_pd_index.take(indices)\n    return new_pd_index",
    ".xarray.core.indexes.py@@Indexes.get_all_coords": "def get_all_coords(self, key: Hashable, errors: ErrorOptions='raise') -> dict[Hashable, Variable]:\n    if errors not in ['raise', 'ignore']:\n        raise ValueError('errors must be either \"raise\" or \"ignore\"')\n    if key not in self._indexes:\n        if errors == 'raise':\n            raise ValueError(f'no index found for {key!r} coordinate')\n        else:\n            return {}\n    all_coord_names = self._id_coord_names[self._coord_name_id[key]]\n    return {k: self._variables[k] for k in all_coord_names}",
    ".xarray.core.variable.py@@concat": "def concat(variables, dim='concat_dim', positions=None, shortcut=False, combine_attrs='override'):\n    variables = list(variables)\n    if all((isinstance(v, IndexVariable) for v in variables)):\n        return IndexVariable.concat(variables, dim, positions, shortcut, combine_attrs)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut, combine_attrs)",
    ".xarray.core.concat.py@@ensure_common_dims": "def ensure_common_dims(vars):\n    common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n    if dim not in common_dims:\n        common_dims = (dim,) + common_dims\n    for var, dim_len in zip(vars, concat_dim_lengths):\n        if var.dims != common_dims:\n            common_shape = tuple((dims_sizes.get(d, dim_len) for d in common_dims))\n            var = var.set_dims(common_dims, common_shape)\n        yield var",
    ".xarray.core.duck_array_ops.py@@concatenate": "def concatenate(arrays, axis=0):\n    return _concatenate(as_shared_dtype(arrays), axis=axis)",
    ".xarray.core.duck_array_ops.py@@as_shared_dtype": "def as_shared_dtype(scalars_or_arrays):\n    if any((isinstance(x, cupy_array_type) for x in scalars_or_arrays)):\n        import cupy as cp\n        arrays = [asarray(x, xp=cp) for x in scalars_or_arrays]\n    else:\n        arrays = [asarray(x) for x in scalars_or_arrays]\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]",
    ".xarray.core.dtypes.py@@result_type": "def result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n    for left, right in PROMOTE_TO_OBJECT:\n        if any((issubclass(t, left) for t in types)) and any((issubclass(t, right) for t in types)):\n            return np.dtype(object)\n    return np.result_type(*arrays_and_dtypes)",
    ".xarray.core.utils.py@@ReprObject.__eq__": "def __eq__(self, other) -> bool:\n    if isinstance(other, ReprObject):\n        return self._value == other._value\n    return False",
    ".xarray.core.dataset.py@@Dataset.set_coords": "def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:\n    if isinstance(names, str) or not isinstance(names, Iterable):\n        names = [names]\n    else:\n        names = list(names)\n    self._assert_all_in_dataset(names)\n    obj = self.copy()\n    obj._coord_names.update(names)\n    return obj",
    ".xarray.core.common.py@@_contains_datetime_like_objects": "def _contains_datetime_like_objects(var) -> bool:\n    return is_np_datetime_like(var.dtype) or contains_cftime_datetimes(var)",
    ".xarray.core.common.py@@is_np_datetime_like": "def is_np_datetime_like(dtype: DTypeLike) -> bool:\n    return np.issubdtype(dtype, np.datetime64) or np.issubdtype(dtype, np.timedelta64)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.dtypes": "def dtypes(self) -> Frozen[Hashable, np.dtype]:\n    return Frozen({n: v.dtype for n, v in self._data._coords.items()})",
    ".xarray.core.coordinates.py@@DataArrayCoordinates.__delitem__": "def __delitem__(self, key: Hashable) -> None:\n    if key not in self:\n        raise KeyError(f'{key!r} is not a coordinate variable.')\n    assert_no_index_corrupted(self._data.xindexes, {key})\n    del self._data._coords[key]\n    if self._data._indexes is not None and key in self._data._indexes:\n        del self._data._indexes[key]",
    ".xarray.core.coordinates.py@@Coordinates.to_index": "def to_index(self, ordered_dims: Sequence[Hashable]=None) -> pd.Index:\n    if ordered_dims is None:\n        ordered_dims = list(self.dims)\n    elif set(ordered_dims) != set(self.dims):\n        raise ValueError('ordered_dims must match dims, but does not: {} vs {}'.format(ordered_dims, self.dims))\n    if len(ordered_dims) == 0:\n        raise ValueError('no valid index for a 0-dimensional object')\n    elif len(ordered_dims) == 1:\n        dim, = ordered_dims\n        return self._data.get_index(dim)\n    else:\n        indexes = [self._data.get_index(k) for k in ordered_dims]\n        index_lengths = np.fromiter((len(index) for index in indexes), dtype=np.intp)\n        cumprod_lengths = np.cumproduct(index_lengths)\n        if cumprod_lengths[-1] == 0:\n            repeat_counts = np.zeros_like(cumprod_lengths)\n        else:\n            repeat_counts = cumprod_lengths[-1] / cumprod_lengths\n        tile_counts = np.roll(cumprod_lengths, 1)\n        tile_counts[0] = 1\n        code_list = []\n        level_list = []\n        names = []\n        for i, index in enumerate(indexes):\n            if isinstance(index, pd.MultiIndex):\n                codes, levels = (index.codes, index.levels)\n            else:\n                code, level = pd.factorize(index)\n                codes = [code]\n                levels = [level]\n            code_list += [np.tile(np.repeat(code, repeat_counts[i]), tile_counts[i]) for code in codes]\n            level_list += levels\n            names += index.names\n    return pd.MultiIndex(level_list, code_list, names=names)",
    ".xarray.core.dataset.py@@Dataset.identical": "def identical(self, other: Dataset) -> bool:\n    try:\n        return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(other, 'identical')\n    except (TypeError, AttributeError):\n        return False",
    ".xarray.core.dataset.py@@Dataset._all_compat": "def _all_compat(self, other: Dataset, compat_str: str) -> bool:\n\n    def compat(x: Variable, y: Variable) -> bool:\n        return getattr(x, compat_str)(y)\n    return self._coord_names == other._coord_names and utils.dict_equiv(self._variables, other._variables, compat=compat)",
    ".xarray.core.dataset.py@@Dataset.compat": "def compat(x: Variable, y: Variable) -> bool:\n    return getattr(x, compat_str)(y)",
    ".xarray.testing.py@@_assert_dataset_invariants": "def _assert_dataset_invariants(ds: Dataset, check_default_indexes: bool):\n    assert isinstance(ds._variables, dict), type(ds._variables)\n    assert all((isinstance(v, Variable) for v in ds._variables.values())), ds._variables\n    for k, v in ds._variables.items():\n        _assert_variable_invariants(v, k)\n    assert isinstance(ds._coord_names, set), ds._coord_names\n    assert ds._coord_names <= ds._variables.keys(), (ds._coord_names, set(ds._variables))\n    assert type(ds._dims) is dict, ds._dims\n    assert all((isinstance(v, int) for v in ds._dims.values())), ds._dims\n    var_dims: Set[Hashable] = set()\n    for v in ds._variables.values():\n        var_dims.update(v.dims)\n    assert ds._dims.keys() == var_dims, (set(ds._dims), var_dims)\n    assert all((ds._dims[k] == v.sizes[k] for v in ds._variables.values() for k in v.sizes)), (ds._dims, {k: v.sizes for k, v in ds._variables.items()})\n    assert all((isinstance(v, IndexVariable) for k, v in ds._variables.items() if v.dims == (k,))), {k: type(v) for k, v in ds._variables.items() if v.dims == (k,)}\n    assert all((v.dims == (k,) for k, v in ds._variables.items() if k in ds._dims)), {k: v.dims for k, v in ds._variables.items() if k in ds._dims}\n    if ds._indexes is not None:\n        _assert_indexes_invariants_checks(ds._indexes, ds._variables, ds._dims, check_default=check_default_indexes)\n    assert isinstance(ds._encoding, (type(None), dict))\n    assert isinstance(ds._attrs, (type(None), dict))",
    ".xarray.core.common.py@@DataWithCoords.assign_coords": "def assign_coords(self: T_DataWithCoords, coords: Mapping[Any, Any] | None=None, **coords_kwargs: Any) -> T_DataWithCoords:\n    coords_combined = either_dict_or_kwargs(coords, coords_kwargs, 'assign_coords')\n    data = self.copy(deep=False)\n    results: dict[Hashable, Any] = self._calc_assign_results(coords_combined)\n    data.coords.update(results)\n    return data",
    ".xarray.core.common.py@@DataWithCoords._calc_assign_results": "def _calc_assign_results(self: C, kwargs: Mapping[Any, T | Callable[[C], T]]) -> dict[Hashable, T]:\n    return {k: v(self) if callable(v) else v for k, v in kwargs.items()}",
    ".xarray.core.coordinates.py@@Coordinates.update": "def update(self, other: Mapping[Any, Any]) -> None:\n    other_vars = getattr(other, 'variables', other)\n    self._maybe_drop_multiindex_coords(set(other_vars))\n    coords, indexes = merge_coords([self.variables, other_vars], priority_arg=1, indexes=self.xindexes)\n    self._update_coords(coords, indexes)",
    ".xarray.core.coordinates.py@@DataArrayCoordinates._maybe_drop_multiindex_coords": "def _maybe_drop_multiindex_coords(self, coords: set[Hashable]) -> None:\n    variables, indexes = drop_coords(coords, self._data._coords, self._data.xindexes)\n    self._data._coords = variables\n    self._data._indexes = indexes",
    ".xarray.core.coordinates.py@@drop_coords": "def drop_coords(coords_to_drop: set[Hashable], variables, indexes: Indexes) -> tuple[dict, dict]:\n    new_variables = dict(variables.copy())\n    new_indexes = dict(indexes.copy())\n    for key in coords_to_drop & set(indexes):\n        maybe_midx = indexes[key]\n        idx_coord_names = set(indexes.get_all_coords(key))\n        if isinstance(maybe_midx, PandasMultiIndex) and key == maybe_midx.dim and idx_coord_names - coords_to_drop:\n            warnings.warn(f'Updating MultiIndexed coordinate {key!r} would corrupt indices for other variables: {list(maybe_midx.index.names)!r}. This will raise an error in the future. Use `.drop_vars({idx_coord_names!r})` before assigning new coordinate values.', FutureWarning, stacklevel=4)\n            for k in idx_coord_names:\n                del new_variables[k]\n                del new_indexes[k]\n    return (new_variables, new_indexes)",
    ".xarray.core.indexes.py@@Indexes.copy": "def copy(self) -> Indexes:\n    return type(self)(dict(self._indexes), dict(self._variables))",
    ".xarray.core.merge.py@@merge_coords": "def merge_coords(objects: Iterable[CoercibleMapping], compat: CompatOptions='minimal', join: JoinOptions='outer', priority_arg: int | None=None, indexes: Mapping[Any, Index] | None=None, fill_value: object=dtypes.NA) -> tuple[dict[Hashable, Variable], dict[Hashable, Index]]:\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objects)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value)\n    collected = collect_variables_and_indexes(aligned)\n    prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n    variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n    return (variables, out_indexes)",
    ".xarray.core.coordinates.py@@Coordinates.__setitem__": "def __setitem__(self, key: Hashable, value: Any) -> None:\n    self.update({key: value})",
    ".xarray.core.utils.py@@ReprObject.__repr__": "def __repr__(self) -> str:\n    return self._value",
    ".xarray.core.alignment.py@@broadcast": "def broadcast(*args, exclude=None):\n    if exclude is None:\n        exclude = set()\n    args = align(*args, join='outer', copy=False, exclude=exclude)\n    dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n    result = [_broadcast_helper(arg, exclude, dims_map, common_coords) for arg in args]\n    return tuple(result)",
    ".xarray.core.indexing.py@@create_mask": "def create_mask(indexer, shape, data=None):\n    if isinstance(indexer, OuterIndexer):\n        key = _outer_to_vectorized_indexer(indexer, shape).tuple\n        assert not any((isinstance(k, slice) for k in key))\n        mask = _masked_result_drop_slice(key, data)\n    elif isinstance(indexer, VectorizedIndexer):\n        key = indexer.tuple\n        base_mask = _masked_result_drop_slice(key, data)\n        slice_shape = tuple((np.arange(*k.indices(size)).size for k, size in zip(key, shape) if isinstance(k, slice)))\n        expanded_mask = base_mask[(Ellipsis,) + (np.newaxis,) * len(slice_shape)]\n        mask = duck_array_ops.broadcast_to(expanded_mask, base_mask.shape + slice_shape)\n    elif isinstance(indexer, BasicIndexer):\n        mask = any((k == -1 for k in indexer.tuple))\n    else:\n        raise TypeError(f'unexpected key type: {type(indexer)}')\n    return mask",
    ".xarray.core.indexing.py@@_masked_result_drop_slice": "def _masked_result_drop_slice(key, data=None):\n    key = (k for k in key if not isinstance(k, slice))\n    chunks_hint = getattr(data, 'chunks', None)\n    new_keys = []\n    for k in key:\n        if isinstance(k, np.ndarray):\n            if is_duck_dask_array(data):\n                new_keys.append(_dask_array_with_chunks_hint(k, chunks_hint))\n            elif isinstance(data, sparse_array_type):\n                import sparse\n                new_keys.append(sparse.COO.from_numpy(k))\n            else:\n                new_keys.append(k)\n        else:\n            new_keys.append(k)\n    mask = _logical_any((k == -1 for k in new_keys))\n    return mask",
    ".xarray.core.indexing.py@@_logical_any": "def _logical_any(args):\n    return functools.reduce(operator.or_, args)",
    ".xarray.core.duck_array_ops.py@@where": "def where(condition, x, y):\n    return _where(condition, *as_shared_dtype([x, y]))",
    ".xarray.core.alignment.py@@_get_broadcast_dims_map_common_coords": "def _get_broadcast_dims_map_common_coords(args, exclude):\n    common_coords = {}\n    dims_map = {}\n    for arg in args:\n        for dim in arg.dims:\n            if dim not in common_coords and dim not in exclude:\n                dims_map[dim] = arg.sizes[dim]\n                if dim in arg._indexes:\n                    common_coords.update(arg.xindexes.get_all_coords(dim))\n    return (dims_map, common_coords)",
    ".xarray.core.alignment.py@@_broadcast_helper": "def _broadcast_helper(arg: T_DataArrayOrSet, exclude, dims_map, common_coords) -> T_DataArrayOrSet:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    def _set_dims(var):\n        var_dims_map = dims_map.copy()\n        for dim in exclude:\n            with suppress(ValueError):\n                var_dims_map[dim] = var.shape[var.dims.index(dim)]\n        return var.set_dims(var_dims_map)\n\n    def _broadcast_array(array: T_DataArray) -> T_DataArray:\n        data = _set_dims(array.variable)\n        coords = dict(array.coords)\n        coords.update(common_coords)\n        return array.__class__(data, coords, data.dims, name=array.name, attrs=array.attrs)\n\n    def _broadcast_dataset(ds: T_Dataset) -> T_Dataset:\n        data_vars = {k: _set_dims(ds.variables[k]) for k in ds.data_vars}\n        coords = dict(ds.coords)\n        coords.update(common_coords)\n        return ds.__class__(data_vars, coords, ds.attrs)\n    if isinstance(arg, DataArray):\n        return cast('T_DataArrayOrSet', _broadcast_array(arg))\n    elif isinstance(arg, Dataset):\n        return cast('T_DataArrayOrSet', _broadcast_dataset(arg))\n    else:\n        raise ValueError('all input must be Dataset or DataArray objects')",
    ".xarray.core.alignment.py@@_broadcast_array": "def _broadcast_array(array: T_DataArray) -> T_DataArray:\n    data = _set_dims(array.variable)\n    coords = dict(array.coords)\n    coords.update(common_coords)\n    return array.__class__(data, coords, data.dims, name=array.name, attrs=array.attrs)",
    ".xarray.core.alignment.py@@_set_dims": "def _set_dims(var):\n    var_dims_map = dims_map.copy()\n    for dim in exclude:\n        with suppress(ValueError):\n            var_dims_map[dim] = var.shape[var.dims.index(dim)]\n    return var.set_dims(var_dims_map)",
    ".xarray.core.dataarray.py@@DataArray.broadcast_like": "def broadcast_like(self: T_DataArray, other: DataArray | Dataset, exclude: Iterable[Hashable] | None=None) -> T_DataArray:\n    if exclude is None:\n        exclude = set()\n    else:\n        exclude = set(exclude)\n    args = align(other, self, join='outer', copy=False, exclude=exclude)\n    dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n    return _broadcast_helper(cast('T_DataArray', args[1]), exclude, dims_map, common_coords)",
    ".xarray.core.indexing.py@@NumpyIndexingAdapter.transpose": "def transpose(self, order):\n    return self.array.transpose(order)",
    ".xarray.core.dataarray.py@@DataArray.reindex_like": "def reindex_like(self: T_DataArray, other: DataArray | Dataset, method: ReindexMethodOptions=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value=dtypes.NA) -> T_DataArray:\n    return alignment.reindex_like(self, other=other, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.alignment.py@@reindex_like": "def reindex_like(obj: DataAlignable, other: Dataset | DataArray, method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA) -> DataAlignable:\n    if not other._indexes:\n        for dim in other.dims:\n            if dim in obj.dims:\n                other_size = other.sizes[dim]\n                obj_size = obj.sizes[dim]\n                if other_size != obj_size:\n                    raise ValueError(f'different size for unlabeled dimension on argument {dim!r}: {other_size!r} vs {obj_size!r}')\n    return reindex(obj, indexers=other.xindexes, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.alignment.py@@reindex": "def reindex(obj: DataAlignable, indexers: Mapping[Any, Any], method: str=None, tolerance: int | float | Iterable[int | float] | None=None, copy: bool=True, fill_value: Any=dtypes.NA, sparse: bool=False, exclude_vars: Iterable[Hashable]=frozenset()) -> DataAlignable:\n    aligner = Aligner((obj,), indexes=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value, sparse=sparse, exclude_vars=exclude_vars)\n    aligner.align()\n    return aligner.results[0]",
    ".xarray.core.dataarray.py@@DataArray.reindex": "def reindex(self: T_DataArray, indexers: Mapping[Any, Any]=None, method: ReindexMethodOptions=None, tolerance: float | Iterable[float] | None=None, copy: bool=True, fill_value=dtypes.NA, **indexers_kwargs: Any) -> T_DataArray:\n    indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, 'reindex')\n    return alignment.reindex(self, indexers=indexers, method=method, tolerance=tolerance, copy=copy, fill_value=fill_value)",
    ".xarray.core.dataarray.py@@DataArray.rename": "def rename(self, new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None=None, **names: Hashable) -> DataArray:\n    if new_name_or_name_dict is None and (not names):\n        return self._replace(name=None)\n    if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n        name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, 'rename')\n        dataset = self._to_temp_dataset()._rename(name_dict)\n        return self._from_temp_dataset(dataset)\n    if utils.hashable(new_name_or_name_dict) and names:\n        dataset = self._to_temp_dataset()._rename(names)\n        dataarray = self._from_temp_dataset(dataset)\n        return dataarray._replace(name=new_name_or_name_dict)\n    return self._replace(name=new_name_or_name_dict)",
    ".xarray.core.dataarray.py@@DataArray.expand_dims": "def expand_dims(self, dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any]=None, axis: None | int | Sequence[int]=None, **dim_kwargs: Any) -> DataArray:\n    if isinstance(dim, int):\n        raise TypeError('dim should be Hashable or sequence/mapping of Hashables')\n    elif isinstance(dim, Sequence) and (not isinstance(dim, str)):\n        if len(dim) != len(set(dim)):\n            raise ValueError('dims should not contain duplicate values.')\n        dim = dict.fromkeys(dim, 1)\n    elif dim is not None and (not isinstance(dim, Mapping)):\n        dim = {cast(Hashable, dim): 1}\n    dim = either_dict_or_kwargs(dim, dim_kwargs, 'expand_dims')\n    ds = self._to_temp_dataset().expand_dims(dim, axis)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.expand_dims": "def expand_dims(self, dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any]=None, axis: None | int | Sequence[int]=None, **dim_kwargs: Any) -> Dataset:\n    if dim is None:\n        pass\n    elif isinstance(dim, Mapping):\n        dim = dict(dim)\n    elif isinstance(dim, int):\n        raise TypeError('dim should be hashable or sequence of hashables or mapping')\n    elif isinstance(dim, str) or not isinstance(dim, Sequence):\n        dim = {dim: 1}\n    elif isinstance(dim, Sequence):\n        if len(dim) != len(set(dim)):\n            raise ValueError('dims should not contain duplicate values.')\n        dim = {d: 1 for d in dim}\n    dim = either_dict_or_kwargs(dim, dim_kwargs, 'expand_dims')\n    assert isinstance(dim, MutableMapping)\n    if axis is None:\n        axis = list(range(len(dim)))\n    elif not isinstance(axis, Sequence):\n        axis = [axis]\n    if len(dim) != len(axis):\n        raise ValueError('lengths of dim and axis should be identical.')\n    for d in dim:\n        if d in self.dims:\n            raise ValueError(f'Dimension {d} already exists.')\n        if d in self._variables and (not utils.is_scalar(self._variables[d])):\n            raise ValueError('{dim} already exists as coordinate or variable name.'.format(dim=d))\n    variables: dict[Hashable, Variable] = {}\n    indexes: dict[Hashable, Index] = dict(self._indexes)\n    coord_names = self._coord_names.copy()\n    for k, v in dim.items():\n        if hasattr(v, '__iter__'):\n            index = PandasIndex(v, k)\n            indexes[k] = index\n            variables.update(index.create_variables())\n            coord_names.add(k)\n            dim[k] = variables[k].size\n        elif isinstance(v, int):\n            pass\n        else:\n            raise TypeError('The value of new dimension {k} must be an iterable or an int'.format(k=k))\n    for k, v in self._variables.items():\n        if k not in dim:\n            if k in coord_names:\n                variables[k] = v\n            else:\n                result_ndim = len(v.dims) + len(axis)\n                for a in axis:\n                    if a < -result_ndim or result_ndim - 1 < a:\n                        raise IndexError(f'Axis {a} of variable {k} is out of bounds of the expanded dimension size {result_ndim}')\n                axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n                if len(axis_pos) != len(set(axis_pos)):\n                    raise ValueError('axis should not contain duplicate values')\n                zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n                all_dims = list(zip(v.dims, v.shape))\n                for d, c in zip_axis_dim:\n                    all_dims.insert(d, c)\n                variables[k] = v.set_dims(dict(all_dims))\n        elif k not in variables:\n            index, index_vars = create_default_index_implicit(v.set_dims(k))\n            indexes[k] = index\n            variables.update(index_vars)\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.reset_index": "def reset_index(self, dims_or_levels: Hashable | Sequence[Hashable], drop: bool=False) -> DataArray:\n    ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.reset_index": "def reset_index(self: T_Dataset, dims_or_levels: Hashable | Sequence[Hashable], drop: bool=False) -> T_Dataset:\n    if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n        dims_or_levels = [dims_or_levels]\n    invalid_coords = set(dims_or_levels) - set(self._indexes)\n    if invalid_coords:\n        raise ValueError(f'{tuple(invalid_coords)} are not coordinates with an index')\n    drop_indexes: set[Hashable] = set()\n    drop_variables: set[Hashable] = set()\n    seen: set[Index] = set()\n    new_indexes: dict[Hashable, Index] = {}\n    new_variables: dict[Hashable, Variable] = {}\n\n    def drop_or_convert(var_names):\n        if drop:\n            drop_variables.update(var_names)\n        else:\n            base_vars = {k: self._variables[k].to_base_variable() for k in var_names}\n            new_variables.update(base_vars)\n    for name in dims_or_levels:\n        index = self._indexes[name]\n        if index in seen:\n            continue\n        seen.add(index)\n        idx_var_names = set(self.xindexes.get_all_coords(name))\n        drop_indexes.update(idx_var_names)\n        if isinstance(index, PandasMultiIndex):\n            level_names = index.index.names\n            keep_level_vars = {k: self._variables[k] for k in level_names if k not in dims_or_levels}\n            if index.dim not in dims_or_levels and keep_level_vars:\n                idx = index.keep_levels(keep_level_vars)\n                idx_vars = idx.create_variables(keep_level_vars)\n                new_indexes.update({k: idx for k in idx_vars})\n                new_variables.update(idx_vars)\n                if not isinstance(idx, PandasMultiIndex):\n                    drop_variables.update(keep_level_vars)\n                drop_or_convert([k for k in level_names if k not in keep_level_vars])\n            else:\n                drop_variables.add(index.dim)\n                drop_or_convert(level_names)\n        else:\n            drop_or_convert(idx_var_names)\n    indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n    indexes.update(new_indexes)\n    variables = {k: v for k, v in self._variables.items() if k not in drop_variables}\n    variables.update(new_variables)\n    coord_names = self._coord_names - drop_variables\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.core.dataset.py@@Dataset.drop_or_convert": "def drop_or_convert(var_names):\n    if drop:\n        drop_variables.update(var_names)\n    else:\n        base_vars = {k: self._variables[k].to_base_variable() for k in var_names}\n        new_variables.update(base_vars)",
    ".xarray.core.dataarray.py@@DataArray.reorder_levels": "def reorder_levels(self: T_DataArray, dim_order: Mapping[Any, Sequence[int | Hashable]] | None=None, **dim_order_kwargs: Sequence[int | Hashable]) -> T_DataArray:\n    ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.reorder_levels": "def reorder_levels(self: T_Dataset, dim_order: Mapping[Any, Sequence[int | Hashable]] | None=None, **dim_order_kwargs: Sequence[int | Hashable]) -> T_Dataset:\n    dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, 'reorder_levels')\n    variables = self._variables.copy()\n    indexes = dict(self._indexes)\n    new_indexes: dict[Hashable, Index] = {}\n    new_variables: dict[Hashable, IndexVariable] = {}\n    for dim, order in dim_order.items():\n        index = self._indexes[dim]\n        if not isinstance(index, PandasMultiIndex):\n            raise ValueError(f'coordinate {dim} has no MultiIndex')\n        level_vars = {k: self._variables[k] for k in order}\n        idx = index.reorder_levels(level_vars)\n        idx_vars = idx.create_variables(level_vars)\n        new_indexes.update({k: idx for k in idx_vars})\n        new_variables.update(idx_vars)\n    indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}\n    indexes.update(new_indexes)\n    variables = {k: v for k, v in self._variables.items() if k not in new_variables}\n    variables.update(new_variables)\n    return self._replace(variables, indexes=indexes)",
    ".xarray.core.computation.py@@_UFuncSignature.num_outputs": "def num_outputs(self):\n    return len(self.output_core_dims)",
    ".xarray.core.computation.py@@apply_ufunc": "def apply_ufunc(func: Callable, *args: Any, input_core_dims: Sequence[Sequence]=None, output_core_dims: Sequence[Sequence] | None=((),), exclude_dims: AbstractSet=frozenset(), vectorize: bool=False, join: JoinOptions='exact', dataset_join: str='exact', dataset_fill_value: object=_NO_FILL_VALUE, keep_attrs: bool | str | None=None, kwargs: Mapping | None=None, dask: str='forbidden', output_dtypes: Sequence | None=None, output_sizes: Mapping[Any, int] | None=None, meta: Any=None, dask_gufunc_kwargs: dict[str, Any] | None=None) -> Any:\n    from .dataarray import DataArray\n    from .groupby import GroupBy\n    from .variable import Variable\n    if input_core_dims is None:\n        input_core_dims = ((),) * len(args)\n    elif len(input_core_dims) != len(args):\n        raise ValueError(f'input_core_dims must be None or a tuple with the length same to the number of arguments. Given {len(input_core_dims)} input_core_dims: {input_core_dims},  but number of args is {len(args)}.')\n    if kwargs is None:\n        kwargs = {}\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n    if exclude_dims:\n        if not isinstance(exclude_dims, set):\n            raise TypeError(f\"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead.\")\n        if not exclude_dims <= signature.all_core_dims:\n            raise ValueError(f'each dimension in `exclude_dims` must also be a core dimension in the function signature. Please make {exclude_dims - signature.all_core_dims} a core dimension')\n    if dask == 'parallelized':\n        if dask_gufunc_kwargs is None:\n            dask_gufunc_kwargs = {}\n        else:\n            dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n        if meta is not None:\n            warnings.warn('``meta`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('meta', meta)\n        if output_sizes is not None:\n            warnings.warn('``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.', FutureWarning, stacklevel=2)\n            dask_gufunc_kwargs.setdefault('output_sizes', output_sizes)\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    if isinstance(keep_attrs, bool):\n        keep_attrs = 'override' if keep_attrs else 'drop'\n    variables_vfunc = functools.partial(apply_variable_ufunc, func, signature=signature, exclude_dims=exclude_dims, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n    if any((isinstance(a, GroupBy) for a in args)):\n        this_apply = functools.partial(apply_ufunc, func, input_core_dims=input_core_dims, output_core_dims=output_core_dims, exclude_dims=exclude_dims, join=join, dataset_join=dataset_join, dataset_fill_value=dataset_fill_value, keep_attrs=keep_attrs, dask=dask, vectorize=vectorize, output_dtypes=output_dtypes, dask_gufunc_kwargs=dask_gufunc_kwargs)\n        return apply_groupby_func(this_apply, *args)\n    elif any((is_dict_like(a) for a in args)):\n        return apply_dataset_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, dataset_join=dataset_join, fill_value=dataset_fill_value, keep_attrs=keep_attrs)\n    elif any((isinstance(a, DataArray) for a in args)):\n        return apply_dataarray_vfunc(variables_vfunc, *args, signature=signature, join=join, exclude_dims=exclude_dims, keep_attrs=keep_attrs)\n    elif any((isinstance(a, Variable) for a in args)):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)",
    ".xarray.core.computation.py@@_UFuncSignature.__init__": "def __init__(self, input_core_dims, output_core_dims=((),)):\n    self.input_core_dims = tuple((tuple(a) for a in input_core_dims))\n    self.output_core_dims = tuple((tuple(a) for a in output_core_dims))\n    self._all_input_core_dims = None\n    self._all_output_core_dims = None\n    self._all_core_dims = None",
    ".xarray.core.computation.py@@apply_variable_ufunc": "def apply_variable_ufunc(func, *args, signature: _UFuncSignature, exclude_dims=frozenset(), dask='forbidden', output_dtypes=None, vectorize=False, keep_attrs='override', dask_gufunc_kwargs=None) -> Variable | tuple[Variable, ...]:\n    from .variable import Variable, as_compatible_data\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')), exclude_dims=exclude_dims)\n    broadcast_dims = tuple((dim for dim in dim_sizes if dim not in signature.all_core_dims))\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims) if isinstance(arg, Variable) else arg for arg, core_dims in zip(args, signature.input_core_dims)]\n    if any((is_duck_dask_array(array) for array in input_data)):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an argument, but handling for dask arrays has not been enabled. Either set the ``dask`` argument or load your data into memory first with ``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            numpy_func = func\n            if dask_gufunc_kwargs is None:\n                dask_gufunc_kwargs = {}\n            else:\n                dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n            allow_rechunk = dask_gufunc_kwargs.get('allow_rechunk', None)\n            if allow_rechunk is None:\n                for n, (data, core_dims) in enumerate(zip(input_data, signature.input_core_dims)):\n                    if is_duck_dask_array(data):\n                        for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                            if len(data.chunks[axis]) != 1:\n                                raise ValueError(f\"dimension {dim} on {n}th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.chunk(dict({dim}=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\")\n                dask_gufunc_kwargs['allow_rechunk'] = True\n            output_sizes = dask_gufunc_kwargs.pop('output_sizes', {})\n            if output_sizes:\n                output_sizes_renamed = {}\n                for key, value in output_sizes.items():\n                    if key not in signature.all_output_core_dims:\n                        raise ValueError(f\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\")\n                    output_sizes_renamed[signature.dims_map[key]] = value\n                dask_gufunc_kwargs['output_sizes'] = output_sizes_renamed\n            for key in signature.all_output_core_dims:\n                if key not in signature.all_input_core_dims and key not in output_sizes:\n                    raise ValueError(f\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\")\n\n            def func(*arrays):\n                import dask.array as da\n                res = da.apply_gufunc(numpy_func, signature.to_gufunc_string(exclude_dims), *arrays, vectorize=vectorize, output_dtypes=output_dtypes, **dask_gufunc_kwargs)\n                return res\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in apply_ufunc: {}'.format(dask))\n    elif vectorize:\n        func = _vectorize(func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims)\n    result_data = func(*input_data)\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs:\n        raise ValueError('applied function does not have the number of outputs specified in the ufunc signature. Result is not a tuple of {} elements: {!r}'.format(signature.num_outputs, result_data))\n    objs = _all_of_type(args, Variable)\n    attrs = merge_attrs([obj.attrs for obj in objs], combine_attrs=keep_attrs)\n    output: list[Variable] = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError(f'applied function returned data with unexpected number of dimensions. Received {data.ndim} dimension(s) but expected {len(dims)} dimensions with names: {dims!r}')\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError('size of dimension {!r} on inputs was unexpectedly changed by applied function from {} to {}. Only dimensions specified in ``exclude_dims`` with xarray.apply_ufunc are allowed to change size.'.format(dim, dim_sizes[dim], new_size))\n        var.attrs = attrs\n        output.append(var)\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)",
    ".xarray.core.computation.py@@unified_dim_sizes": "def unified_dim_sizes(variables: Iterable[Variable], exclude_dims: AbstractSet=frozenset()) -> dict[Hashable, int]:\n    dim_sizes: dict[Hashable, int] = {}\n    for var in variables:\n        if len(set(var.dims)) < len(var.dims):\n            raise ValueError(f'broadcasting cannot handle duplicate dimensions on a variable: {list(var.dims)}')\n        for dim, size in zip(var.dims, var.shape):\n            if dim not in exclude_dims:\n                if dim not in dim_sizes:\n                    dim_sizes[dim] = size\n                elif dim_sizes[dim] != size:\n                    raise ValueError(f'operands cannot be broadcast together with mismatched lengths for dimension {dim}: {dim_sizes[dim]} vs {size}')\n    return dim_sizes",
    ".xarray.core.computation.py@@_UFuncSignature.all_core_dims": "def all_core_dims(self):\n    if self._all_core_dims is None:\n        self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims\n    return self._all_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_input_core_dims": "def all_input_core_dims(self):\n    if self._all_input_core_dims is None:\n        self._all_input_core_dims = frozenset((dim for dims in self.input_core_dims for dim in dims))\n    return self._all_input_core_dims",
    ".xarray.core.computation.py@@_UFuncSignature.all_output_core_dims": "def all_output_core_dims(self):\n    if self._all_output_core_dims is None:\n        self._all_output_core_dims = frozenset((dim for dims in self.output_core_dims for dim in dims))\n    return self._all_output_core_dims",
    ".xarray.core.computation.py@@broadcast_compat_data": "def broadcast_compat_data(variable: Variable, broadcast_dims: tuple[Hashable, ...], core_dims: tuple[Hashable, ...]) -> Any:\n    data = variable.data\n    old_dims = variable.dims\n    new_dims = broadcast_dims + core_dims\n    if new_dims == old_dims:\n        return data\n    set_old_dims = set(old_dims)\n    missing_core_dims = [d for d in core_dims if d not in set_old_dims]\n    if missing_core_dims:\n        raise ValueError('operand to apply_ufunc has required core dimensions {}, but some of these dimensions are absent on an input variable: {}'.format(list(core_dims), missing_core_dims))\n    set_new_dims = set(new_dims)\n    unexpected_dims = [d for d in old_dims if d not in set_new_dims]\n    if unexpected_dims:\n        raise ValueError(f'operand to apply_ufunc encountered unexpected dimensions {unexpected_dims!r} on an input variable: these are core dimensions on other input or output variables')\n    old_broadcast_dims = tuple((d for d in broadcast_dims if d in set_old_dims))\n    reordered_dims = old_broadcast_dims + core_dims\n    if reordered_dims != old_dims:\n        order = tuple((old_dims.index(d) for d in reordered_dims))\n        data = duck_array_ops.transpose(data, order)\n    if new_dims != reordered_dims:\n        key_parts: list[slice | None] = []\n        for dim in new_dims:\n            if dim in set_old_dims:\n                key_parts.append(SLICE_NONE)\n            elif key_parts:\n                key_parts.append(np.newaxis)\n        data = data[tuple(key_parts)]\n    return data",
    ".xarray.core.duck_array_ops.py@@astype": "def astype(data, dtype, **kwargs):\n    return data.astype(dtype, **kwargs)",
    ".xarray.core.computation.py@@_all_of_type": "def _all_of_type(args, kind):\n    return [arg for arg in args if isinstance(arg, kind)]",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.argsort": "def argsort(self, *args, **kwargs):\n    return self._unary_op(ops.argsort, *args, **kwargs)",
    ".xarray.core.ops.py@@func": "def func(self, *args, **kwargs):\n    try:\n        return getattr(self, name)(*args, **kwargs)\n    except AttributeError:\n        return f(self, *args, **kwargs)",
    ".xarray.core.ops.py@@_call_possibly_missing_method": "def _call_possibly_missing_method(arg, name, args, kwargs):\n    try:\n        method = getattr(arg, name)\n    except AttributeError:\n        duck_array_ops.fail_on_dask_array_input(arg, func_name=name)\n        if hasattr(arg, 'data'):\n            duck_array_ops.fail_on_dask_array_input(arg.data, func_name=name)\n        raise\n    else:\n        return method(*args, **kwargs)",
    ".xarray.core.arithmetic.py@@SupportsArithmetic.__array_ufunc__": "def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n    from .computation import apply_ufunc\n    out = kwargs.get('out', ())\n    for x in inputs + out:\n        if not is_duck_array(x) and (not isinstance(x, self._HANDLED_TYPES + (SupportsArithmetic,))):\n            return NotImplemented\n    if ufunc.signature is not None:\n        raise NotImplementedError('{} not supported: xarray objects do not directly implement generalized ufuncs. Instead, use xarray.apply_ufunc or explicitly convert to xarray objects to NumPy arrays (e.g., with `.values`).'.format(ufunc))\n    if method != '__call__':\n        raise NotImplementedError('{} method for ufunc {} is not implemented on xarray objects, which currently only support the __call__ method. As an alternative, consider explicitly converting xarray objects to NumPy arrays (e.g., with `.values`).'.format(method, ufunc))\n    if any((isinstance(o, SupportsArithmetic) for o in out)):\n        raise NotImplementedError('xarray objects are not yet supported in the `out` argument for ufuncs. As an alternative, consider explicitly converting xarray objects to NumPy arrays (e.g., with `.values`).')\n    join = dataset_join = OPTIONS['arithmetic_join']\n    return apply_ufunc(ufunc, *inputs, input_core_dims=((),) * ufunc.nin, output_core_dims=((),) * ufunc.nout, join=join, dataset_join=dataset_join, dataset_fill_value=np.nan, kwargs=kwargs, dask='allowed', keep_attrs=_get_keep_attrs(default=True))",
    ".xarray.core.computation.py@@build_output_coords_and_indexes": "def build_output_coords_and_indexes(args: Iterable[Any], signature: _UFuncSignature, exclude_dims: AbstractSet=frozenset(), combine_attrs: CombineAttrsOptions='override') -> tuple[list[dict[Any, Variable]], list[dict[Any, Index]]]:\n    coords_list = _get_coords_list(args)\n    if len(coords_list) == 1 and (not exclude_dims):\n        unpacked_coords, = coords_list\n        merged_vars = dict(unpacked_coords.variables)\n        merged_indexes = dict(unpacked_coords.xindexes)\n    else:\n        merged_vars, merged_indexes = merge_coordinates_without_align(coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs)\n    output_coords = []\n    output_indexes = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered_coords = {k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)}\n            filtered_indexes = filter_indexes_from_coords(merged_indexes, set(filtered_coords))\n        else:\n            filtered_coords = merged_vars\n            filtered_indexes = merged_indexes\n        output_coords.append(filtered_coords)\n        output_indexes.append(filtered_indexes)\n    return (output_coords, output_indexes)",
    ".xarray.core.computation.py@@_get_coords_list": "def _get_coords_list(args: Iterable[Any]) -> list[Coordinates]:\n    coords_list = []\n    for arg in args:\n        try:\n            coords = arg.coords\n        except AttributeError:\n            pass\n        else:\n            coords_list.append(coords)\n    return coords_list",
    ".xarray.core.computation.py@@apply_dict_of_variables_vfunc": "def apply_dict_of_variables_vfunc(func, *args, signature: _UFuncSignature, join='inner', fill_value=None):\n    args = tuple((_as_variables_or_variable(arg) for arg in args))\n    names = join_dict_keys(args, how=join)\n    grouped_by_name = collect_dict_values(args, names, fill_value)\n    result_vars = {}\n    for name, variable_args in zip(names, grouped_by_name):\n        result_vars[name] = func(*variable_args)\n    if signature.num_outputs > 1:\n        return _unpack_dict_tuples(result_vars, signature.num_outputs)\n    else:\n        return result_vars",
    ".xarray.core.computation.py@@_as_variables_or_variable": "def _as_variables_or_variable(arg):\n    try:\n        return arg.variables\n    except AttributeError:\n        try:\n            return arg.variable\n        except AttributeError:\n            return arg",
    ".xarray.core.dataset.py@@DataVariables.variables": "def variables(self) -> Mapping[Hashable, Variable]:\n    all_variables = self._dataset.variables\n    return Frozen({k: all_variables[k] for k in self})",
    ".xarray.core.computation.py@@join_dict_keys": "def join_dict_keys(objects: Iterable[Mapping | Any], how: str='inner') -> Iterable:\n    joiner = _JOINERS[how]\n    all_keys = [obj.keys() for obj in objects if hasattr(obj, 'keys')]\n    return joiner(all_keys)",
    ".xarray.core.computation.py@@assert_and_return_exact_match": "def assert_and_return_exact_match(all_keys):\n    first_keys = all_keys[0]\n    for keys in all_keys[1:]:\n        if keys != first_keys:\n            raise ValueError(f'exact match required for all data variable names, but {keys!r} != {first_keys!r}')\n    return first_keys",
    ".xarray.core.computation.py@@collect_dict_values": "def collect_dict_values(objects: Iterable[Mapping | Any], keys: Iterable, fill_value: object=None) -> list[list]:\n    return [[obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects] for key in keys]",
    ".xarray.core.common.py@@DataWithCoords.astype": "def astype(self: T_DataWithCoords, dtype, *, order=None, casting=None, subok=None, copy=None, keep_attrs=True) -> T_DataWithCoords:\n    from .computation import apply_ufunc\n    kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n    return apply_ufunc(duck_array_ops.astype, self, dtype, kwargs=kwargs, keep_attrs=keep_attrs, dask='allowed')",
    ".xarray.core.common.py@@DataWithCoords.isnull": "def isnull(self: T_DataWithCoords, keep_attrs: bool | None=None) -> T_DataWithCoords:\n    from .computation import apply_ufunc\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    return apply_ufunc(duck_array_ops.isnull, self, dask='allowed', keep_attrs=keep_attrs)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__invert__": "def __invert__(self):\n    return self._unary_op(operator.invert)",
    ".xarray.core.dataarray.py@@DataArray._unary_op": "def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:\n    keep_attrs = kwargs.pop('keep_attrs', None)\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=True)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        with np.errstate(all='ignore'):\n            da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n        if keep_attrs:\n            da.attrs = self.attrs\n        return da",
    ".xarray.core.dataarray.py@@DataArray.__array_wrap__": "def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:\n    new_var = self.variable.__array_wrap__(obj, context)\n    return self._replace(new_var)",
    ".xarray.core.common.py@@DataWithCoords.notnull": "def notnull(self: T_DataWithCoords, keep_attrs: bool | None=None) -> T_DataWithCoords:\n    from .computation import apply_ufunc\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    return apply_ufunc(duck_array_ops.notnull, self, dask='allowed', keep_attrs=keep_attrs)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__pos__": "def __pos__(self):\n    return self._unary_op(operator.pos)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__add__": "def __add__(self, other):\n    return self._binary_op(other, operator.add)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__add__": "def __add__(self, other):\n    return self._binary_op(other, operator.add)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__radd__": "def __radd__(self, other):\n    return self._binary_op(other, operator.add, reflexive=True)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__radd__": "def __radd__(self, other):\n    return self._binary_op(other, operator.add, reflexive=True)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__rmul__": "def __rmul__(self, other):\n    return self._binary_op(other, operator.mul, reflexive=True)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__iadd__": "def __iadd__(self, other):\n    return self._inplace_binary_op(other, operator.iadd)",
    ".xarray.core.dataarray.py@@DataArray._inplace_binary_op": "def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:\n    from .groupby import GroupBy\n    if isinstance(other, GroupBy):\n        raise TypeError('in-place operations between a DataArray and a grouped object are not permitted')\n    other_coords = getattr(other, 'coords', None)\n    other_variable = getattr(other, 'variable', other)\n    try:\n        with self.coords._merge_inplace(other_coords):\n            f(self.variable, other_variable)\n    except MergeError as exc:\n        raise MergeError('Automatic alignment is not supported for in-place operations.\\nConsider aligning the indices manually or using a not-in-place operation.\\nSee https://github.com/pydata/xarray/issues/3910 for more explanations.') from exc\n    return self",
    ".xarray.core.coordinates.py@@Coordinates._merge_inplace": "def _merge_inplace(self, other):\n    if other is None:\n        yield\n    else:\n        prioritized = {k: (v, None) for k, v in self.variables.items() if k not in self.xindexes}\n        variables, indexes = merge_coordinates_without_align([self, other], prioritized)\n        yield\n        self._update_coords(variables, indexes)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__iadd__": "def __iadd__(self, other):\n    return self._inplace_binary_op(other, operator.iadd)",
    ".xarray.core.variable.py@@IndexVariable._inplace_binary_op": "def _inplace_binary_op(self, other, f):\n    raise TypeError('Values of an IndexVariable are immutable and can not be modified inplace')",
    ".xarray.core.merge.py@@broadcast_dimension_size": "def broadcast_dimension_size(variables: list[Variable]) -> dict[Hashable, int]:\n    dims: dict[Hashable, int] = {}\n    for var in variables:\n        for dim, size in zip(var.dims, var.shape):\n            if dim in dims and size != dims[dim]:\n                raise ValueError(f'index {dim!r} not aligned')\n            dims[dim] = size\n    return dims",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__neg__": "def __neg__(self):\n    return self._unary_op(operator.neg)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__isub__": "def __isub__(self, other):\n    return self._inplace_binary_op(other, operator.isub)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__isub__": "def __isub__(self, other):\n    return self._inplace_binary_op(other, operator.isub)",
    ".xarray.core.dataarray.py@@DataArray.drop_sel": "def drop_sel(self: T_DataArray, labels: Mapping[Any, Any] | None=None, *, errors: ErrorOptions='raise', **labels_kwargs) -> T_DataArray:\n    if labels_kwargs or isinstance(labels, dict):\n        labels = either_dict_or_kwargs(labels, labels_kwargs, 'drop')\n    ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.drop_sel": "def drop_sel(self: T_Dataset, labels=None, *, errors: ErrorOptions='raise', **labels_kwargs) -> T_Dataset:\n    if errors not in ['raise', 'ignore']:\n        raise ValueError('errors must be either \"raise\" or \"ignore\"')\n    labels = either_dict_or_kwargs(labels, labels_kwargs, 'drop_sel')\n    ds = self\n    for dim, labels_for_dim in labels.items():\n        if utils.is_scalar(labels_for_dim):\n            labels_for_dim = [labels_for_dim]\n        labels_for_dim = np.asarray(labels_for_dim)\n        try:\n            index = self.get_index(dim)\n        except KeyError:\n            raise ValueError(f'dimension {dim!r} does not have coordinate labels')\n        new_index = index.drop(labels_for_dim, errors=errors)\n        ds = ds.loc[{dim: new_index}]\n    return ds",
    ".xarray.core.common.py@@DataWithCoords.get_index": "def get_index(self, key: Hashable) -> pd.Index:\n    if key not in self.dims:\n        raise KeyError(key)\n    try:\n        return self._indexes[key].to_pandas_index()\n    except KeyError:\n        return pd.Index(range(self.sizes[key]), name=key)",
    ".xarray.core.indexes.py@@PandasIndex.to_pandas_index": "def to_pandas_index(self) -> pd.Index:\n    return self.index",
    ".xarray.core.dataset.py@@Dataset.loc": "def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:\n    return _LocIndexer(self)",
    ".xarray.core.dataset.py@@_LocIndexer.__init__": "def __init__(self, dataset: T_Dataset):\n    self.dataset = dataset",
    ".xarray.core.dataset.py@@_LocIndexer.__getitem__": "def __getitem__(self, key: Mapping[Any, Any]) -> T_Dataset:\n    if not utils.is_dict_like(key):\n        raise TypeError('can only lookup dictionaries from Dataset.loc')\n    return self.dataset.sel(key)",
    ".xarray.core.dataarray.py@@DataArray.drop": "def drop(self: T_DataArray, labels: Mapping[Any, Any] | None=None, dim: Hashable | None=None, *, errors: ErrorOptions='raise', **labels_kwargs) -> T_DataArray:\n    ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.drop": "def drop(self: T_Dataset, labels=None, dim=None, *, errors: ErrorOptions='raise', **labels_kwargs) -> T_Dataset:\n    if errors not in ['raise', 'ignore']:\n        raise ValueError('errors must be either \"raise\" or \"ignore\"')\n    if is_dict_like(labels) and (not isinstance(labels, dict)):\n        warnings.warn('dropping coordinates using `drop` is be deprecated; use drop_vars.', FutureWarning, stacklevel=2)\n        return self.drop_vars(labels, errors=errors)\n    if labels_kwargs or isinstance(labels, dict):\n        if dim is not None:\n            raise ValueError('cannot specify dim and dict-like arguments.')\n        labels = either_dict_or_kwargs(labels, labels_kwargs, 'drop')\n    if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n        warnings.warn('dropping variables using `drop` will be deprecated; using drop_vars is encouraged.', PendingDeprecationWarning, stacklevel=2)\n        return self.drop_vars(labels, errors=errors)\n    if dim is not None:\n        warnings.warn('dropping labels using list-like labels is deprecated; using dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).', DeprecationWarning, stacklevel=2)\n        return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n    warnings.warn('dropping labels using `drop` will be deprecated; using drop_sel is encouraged.', PendingDeprecationWarning, stacklevel=2)\n    return self.drop_sel(labels, errors=errors)",
    ".xarray.core.dataarray.py@@DataArray.drop_isel": "def drop_isel(self: T_DataArray, indexers: Mapping[Any, Any] | None=None, **indexers_kwargs) -> T_DataArray:\n    dataset = self._to_temp_dataset()\n    dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)\n    return self._from_temp_dataset(dataset)",
    ".xarray.core.dataset.py@@Dataset.drop_isel": "def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:\n    indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'drop_isel')\n    ds = self\n    dimension_index = {}\n    for dim, pos_for_dim in indexers.items():\n        if utils.is_scalar(pos_for_dim):\n            pos_for_dim = [pos_for_dim]\n        pos_for_dim = np.asarray(pos_for_dim)\n        index = self.get_index(dim)\n        new_index = index.delete(pos_for_dim)\n        dimension_index[dim] = new_index\n    ds = ds.loc[dimension_index]\n    return ds",
    ".xarray.core.dataset.py@@Dataset.sizes": "def sizes(self) -> Frozen[Hashable, int]:\n    return self.dims",
    ".xarray.core.dataarray.py@@DataArray.drop_indexes": "def drop_indexes(self: T_DataArray, coord_names: Hashable | Iterable[Hashable], *, errors: ErrorOptions='raise') -> T_DataArray:\n    ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.drop_indexes": "def drop_indexes(self: T_Dataset, coord_names: Hashable | Iterable[Hashable], *, errors: ErrorOptions='raise') -> T_Dataset:\n    if is_scalar(coord_names) or not isinstance(coord_names, Iterable):\n        coord_names = {coord_names}\n    else:\n        coord_names = set(coord_names)\n    if errors == 'raise':\n        invalid_coords = coord_names - self._coord_names\n        if invalid_coords:\n            raise ValueError(f\"those coordinates don't exist: {invalid_coords}\")\n        unindexed_coords = set(coord_names) - set(self._indexes)\n        if unindexed_coords:\n            raise ValueError(f'those coordinates do not have an index: {unindexed_coords}')\n    assert_no_index_corrupted(self.xindexes, coord_names, action='remove index(es)')\n    variables = {}\n    for name, var in self._variables.items():\n        if name in coord_names:\n            variables[name] = var.to_base_variable()\n        else:\n            variables[name] = var\n    indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}\n    return self._replace(variables=variables, indexes=indexes)",
    ".xarray.core.dataarray.py@@DataArray.dropna": "def dropna(self: T_DataArray, dim: Hashable, how: Literal['any', 'all']='any', thresh: int | None=None) -> T_DataArray:\n    ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.dropna": "def dropna(self: T_Dataset, dim: Hashable, how: Literal['any', 'all']='any', thresh: int | None=None, subset: Iterable[Hashable] | None=None) -> T_Dataset:\n    if dim not in self.dims:\n        raise ValueError(f'{dim} must be a single dataset dimension')\n    if subset is None:\n        subset = iter(self.data_vars)\n    count = np.zeros(self.dims[dim], dtype=np.int64)\n    size = np.int_(0)\n    for k in subset:\n        array = self._variables[k]\n        if dim in array.dims:\n            dims = [d for d in array.dims if d != dim]\n            count += np.asarray(array.count(dims))\n            size += math.prod([self.dims[d] for d in dims])\n    if thresh is not None:\n        mask = count >= thresh\n    elif how == 'any':\n        mask = count == size\n    elif how == 'all':\n        mask = count > 0\n    elif how is not None:\n        raise ValueError(f'invalid how option: {how}')\n    else:\n        raise TypeError('must specify how or thresh')\n    return self.isel({dim: mask})",
    ".xarray.core.common.py@@ImplementsArrayReduce.wrapped_func": "def wrapped_func(self, dim=None, axis=None, **kwargs):\n    return self.reduce(func=func, dim=dim, axis=axis, **kwargs)",
    ".xarray.core.duck_array_ops.py@@count": "def count(data, axis=None):\n    return np.sum(np.logical_not(isnull(data)), axis=axis)",
    ".xarray.core.common.py@@DataWithCoords.where": "def where(self: T_DataWithCoords, cond: Any, other: Any=dtypes.NA, drop: bool=False) -> T_DataWithCoords:\n    from .alignment import align\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if callable(cond):\n        cond = cond(self)\n    if drop:\n        if not isinstance(cond, (Dataset, DataArray)):\n            raise TypeError(f'cond argument is {cond!r} but must be a {Dataset!r} or {DataArray!r}')\n        self, cond = align(self, cond)\n\n        def _dataarray_indexer(dim: Hashable) -> DataArray:\n            return cond.any(dim=(d for d in cond.dims if d != dim))\n\n        def _dataset_indexer(dim: Hashable) -> DataArray:\n            cond_wdim = cond.drop_vars((var for var in cond if dim not in cond[var].dims))\n            keepany = cond_wdim.any(dim=(d for d in cond.dims.keys() if d != dim))\n            return keepany.to_array().any('variable')\n        _get_indexer = _dataarray_indexer if isinstance(cond, DataArray) else _dataset_indexer\n        indexers = {}\n        for dim in cond.sizes.keys():\n            indexers[dim] = _get_indexer(dim)\n        self = self.isel(**indexers)\n        cond = cond.isel(**indexers)\n    return ops.where_method(self, cond, other)",
    ".xarray.core.common.py@@DataWithCoords._dataarray_indexer": "def _dataarray_indexer(dim: Hashable) -> DataArray:\n    return cond.any(dim=(d for d in cond.dims if d != dim))",
    ".xarray.core._reductions.py@@DataArrayReductions.any": "def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.array_any, dim=dim, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.dataarray.py@@DataArray.reduce": "def reduce(self: T_DataArray, func: Callable[..., Any], dim: Dims | ellipsis=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> T_DataArray:\n    var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n    return self._replace_maybe_drop_dims(var)",
    ".xarray.core.ops.py@@where_method": "def where_method(self, cond, other=dtypes.NA):\n    from .computation import apply_ufunc\n    join = 'inner' if other is dtypes.NA else 'exact'\n    return apply_ufunc(duck_array_ops.where_method, self, cond, other, join=join, dataset_join=join, dask='allowed', keep_attrs=True)",
    ".xarray.core.duck_array_ops.py@@cumsum": "def cumsum(array, axis=None, **kwargs):\n    return _nd_cum_func(cumsum_1d, array, axis, **kwargs)",
    ".xarray.core.duck_array_ops.py@@_nd_cum_func": "def _nd_cum_func(cum_func, array, axis, **kwargs):\n    array = asarray(array)\n    if axis is None:\n        axis = tuple(range(array.ndim))\n    if isinstance(axis, int):\n        axis = (axis,)\n    out = array\n    for ax in axis:\n        out = cum_func(out, axis=ax, **kwargs)\n    return out",
    ".xarray.core.duck_array_ops.py@@f": "def f(values, axis=None, skipna=None, **kwargs):\n    if kwargs.pop('out', None) is not None:\n        raise TypeError(f'`out` is not valid for {name}')\n    if invariant_0d and axis == ():\n        return values\n    values = asarray(values)\n    if coerce_strings and values.dtype.kind in 'SU':\n        values = values.astype(object)\n    func = None\n    if skipna or (skipna is None and values.dtype.kind in 'cfO'):\n        nanname = 'nan' + name\n        func = getattr(nanops, nanname)\n    else:\n        if name in ['sum', 'prod']:\n            kwargs.pop('min_count', None)\n        if hasattr(values, '__array_namespace__'):\n            xp = values.__array_namespace__()\n            func = getattr(xp, name)\n        else:\n            func = getattr(np, name)\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', 'All-NaN slice encountered')\n            return func(values, axis=axis, **kwargs)\n    except AttributeError:\n        if not is_duck_dask_array(values):\n            raise\n        try:\n            return func(values, axis=axis, dtype=values.dtype, **kwargs)\n        except (AttributeError, TypeError):\n            raise NotImplementedError(f'{name} is not yet implemented on dask arrays')",
    ".xarray.core.duck_array_ops.py@@cumprod": "def cumprod(array, axis=None, **kwargs):\n    return _nd_cum_func(cumprod_1d, array, axis, **kwargs)",
    ".xarray.core._reductions.py@@DataArrayReductions.mean": "def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.duck_array_ops.py@@mean": "def mean(array, axis=None, skipna=None, **kwargs):\n    from .common import _contains_cftime_datetimes\n    array = asarray(array)\n    if array.dtype.kind in 'Mm':\n        offset = _datetime_nanmin(array)\n        dtype = 'timedelta64[ns]'\n        return _mean(datetime_to_numeric(array, offset), axis=axis, skipna=skipna, **kwargs).astype(dtype) + offset\n    elif _contains_cftime_datetimes(array):\n        offset = min(array)\n        timedeltas = datetime_to_numeric(array, offset, datetime_unit='us')\n        mean_timedeltas = _mean(timedeltas, axis=axis, skipna=skipna, **kwargs)\n        return _to_pytimedelta(mean_timedeltas, unit='us') + offset\n    else:\n        return _mean(array, axis=axis, skipna=skipna, **kwargs)",
    ".xarray.core.common.py@@_contains_cftime_datetimes": "def _contains_cftime_datetimes(array) -> bool:\n    if cftime is None:\n        return False\n    elif array.dtype == np.dtype('O') and array.size > 0:\n        sample = np.asarray(array).flat[0]\n        if is_duck_dask_array(sample):\n            sample = sample.compute()\n            if isinstance(sample, np.ndarray):\n                sample = sample.item()\n        return isinstance(sample, cftime.datetime)\n    else:\n        return False",
    ".xarray.core._reductions.py@@DataArrayReductions.count": "def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.count, dim=dim, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.nanops.py@@nanmean": "def nanmean(a, axis=None, dtype=None, out=None):\n    if a.dtype.kind == 'O':\n        return _nanmean_ddof_object(0, a, axis=axis, dtype=dtype)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'Mean of empty slice', category=RuntimeWarning)\n        return np.nanmean(a, axis=axis, dtype=dtype)",
    ".xarray.core.dataarray.py@@DataArray.quantile": "def quantile(self: T_DataArray, q: ArrayLike, dim: Dims=None, method: QUANTILE_METHODS='linear', keep_attrs: bool | None=None, skipna: bool | None=None, interpolation: QUANTILE_METHODS=None) -> T_DataArray:\n    ds = self._to_temp_dataset().quantile(q, dim=dim, keep_attrs=keep_attrs, method=method, skipna=skipna, interpolation=interpolation)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.quantile": "def quantile(self: T_Dataset, q: ArrayLike, dim: Dims=None, method: QUANTILE_METHODS='linear', numeric_only: bool=False, keep_attrs: bool=None, skipna: bool=None, interpolation: QUANTILE_METHODS=None) -> T_Dataset:\n    if interpolation is not None:\n        warnings.warn('The `interpolation` argument to quantile was renamed to `method`.', FutureWarning)\n        if method != 'linear':\n            raise TypeError('Cannot pass interpolation and method keywords!')\n        method = interpolation\n    dims: set[Hashable]\n    if isinstance(dim, str):\n        dims = {dim}\n    elif dim is None or dim is ...:\n        dims = set(self.dims)\n    else:\n        dims = set(dim)\n    _assert_empty(tuple((d for d in dims if d not in self.dims)), 'Dataset does not contain the dimensions: %s')\n    q = np.asarray(q, dtype=np.float64)\n    variables = {}\n    for name, var in self.variables.items():\n        reduce_dims = [d for d in var.dims if d in dims]\n        if reduce_dims or not var.dims:\n            if name not in self.coords:\n                if not numeric_only or np.issubdtype(var.dtype, np.number) or var.dtype == np.bool_:\n                    variables[name] = var.quantile(q, dim=reduce_dims, method=method, keep_attrs=keep_attrs, skipna=skipna)\n        else:\n            variables[name] = var\n    coord_names = {k for k in self.coords if k in variables}\n    indexes = {k: v for k, v in self._indexes.items() if k in variables}\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=False)\n    attrs = self.attrs if keep_attrs else None\n    new = self._replace_with_new_dims(variables, coord_names=coord_names, attrs=attrs, indexes=indexes)\n    return new.assign_coords(quantile=q)",
    ".xarray.core.dataset.py@@_assert_empty": "def _assert_empty(args: tuple, msg: str='%s') -> None:\n    if args:\n        raise ValueError(msg % args)",
    ".xarray.core.coordinates.py@@DatasetCoordinates._maybe_drop_multiindex_coords": "def _maybe_drop_multiindex_coords(self, coords: set[Hashable]) -> None:\n    assert self._data.xindexes is not None\n    variables, indexes = drop_coords(coords, self._data._variables, self._data.xindexes)\n    self._data._coord_names.intersection_update(variables)\n    self._data._variables = variables\n    self._data._indexes = indexes",
    ".xarray.core.coordinates.py@@DatasetCoordinates._update_coords": "def _update_coords(self, coords: dict[Hashable, Variable], indexes: Mapping[Any, Index]) -> None:\n    variables = self._data._variables.copy()\n    variables.update(coords)\n    dims = calculate_dimensions(variables)\n    new_coord_names = set(coords)\n    for dim, size in dims.items():\n        if dim in variables:\n            new_coord_names.add(dim)\n    self._data._variables = variables\n    self._data._coord_names.update(new_coord_names)\n    self._data._dims = dims\n    original_indexes = dict(self._data.xindexes)\n    original_indexes.update(indexes)\n    self._data._indexes = original_indexes",
    ".xarray.core.common.py@@DataWithCoords.clip": "def clip(self: T_DataWithCoords, min: ScalarOrArray | None=None, max: ScalarOrArray | None=None, *, keep_attrs: bool | None=None) -> T_DataWithCoords:\n    from .computation import apply_ufunc\n    if keep_attrs is None:\n        keep_attrs = _get_keep_attrs(default=True)\n    return apply_ufunc(np.clip, self, min, max, keep_attrs=keep_attrs, dask='allowed')",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__abs__": "def __abs__(self):\n    return self._unary_op(operator.abs)",
    ".xarray.core.dataarray.py@@DataArray.fillna": "def fillna(self: T_DataArray, value: Any) -> T_DataArray:\n    if utils.is_dict_like(value):\n        raise TypeError('cannot provide fill value as a dictionary with fillna on a DataArray')\n    out = ops.fillna(self, value)\n    return out",
    ".xarray.core.ops.py@@fillna": "def fillna(data, other, join='left', dataset_join='left'):\n    from .computation import apply_ufunc\n    return apply_ufunc(duck_array_ops.fillna, data, other, join=join, dask='allowed', dataset_join=dataset_join, dataset_fill_value=np.nan, keep_attrs=True)",
    ".xarray.core.duck_array_ops.py@@fillna": "def fillna(data, other):\n    return where(notnull(data), data, other)",
    ".xarray.core.duck_array_ops.py@@notnull": "def notnull(data):\n    return ~isnull(data)",
    ".xarray.core.dataarray.py@@DataArray._overwrite_indexes": "def _overwrite_indexes(self: T_DataArray, indexes: Mapping[Any, Index], coords: Mapping[Any, Variable]=None, drop_coords: list[Hashable]=None, rename_dims: Mapping[Any, Any]=None) -> T_DataArray:\n    if not indexes:\n        return self\n    if coords is None:\n        coords = {}\n    if drop_coords is None:\n        drop_coords = []\n    new_variable = self.variable.copy()\n    new_coords = self._coords.copy()\n    new_indexes = dict(self._indexes)\n    for name in indexes:\n        new_coords[name] = coords[name]\n        new_indexes[name] = indexes[name]\n    for name in drop_coords:\n        new_coords.pop(name)\n        new_indexes.pop(name)\n    if rename_dims:\n        new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]\n    return self._replace(variable=new_variable, coords=new_coords, indexes=new_indexes)",
    ".xarray.core.dataarray.py@@DataArray.T": "def T(self: T_DataArray) -> T_DataArray:\n    return self.transpose()",
    ".xarray.core.dataarray.py@@DataArray.transpose": "def transpose(self: T_DataArray, *dims: Hashable, transpose_coords: bool=True, missing_dims: ErrorOptionsWithWarn='raise') -> T_DataArray:\n    if dims:\n        dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))\n    variable = self.variable.transpose(*dims)\n    if transpose_coords:\n        coords: dict[Hashable, Variable] = {}\n        for name, coord in self.coords.items():\n            coord_dims = tuple((dim for dim in dims if dim in coord.dims))\n            coords[name] = coord.variable.transpose(*coord_dims)\n        return self._replace(variable, coords)\n    else:\n        return self._replace(variable)",
    ".xarray.core.dataset.py@@Dataset._attr_sources": "def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield from self._item_sources\n    yield self.attrs",
    ".xarray.core.dataset.py@@Dataset._item_sources": "def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n    yield self.data_vars\n    yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)\n    yield HybridMappingProxy(keys=self.dims, mapping=self)",
    ".xarray.core.dataset.py@@DataVariables.__getitem__": "def __getitem__(self, key: Hashable) -> DataArray:\n    if key not in self._dataset._coord_names:\n        return cast('DataArray', self._dataset[key])\n    raise KeyError(key)",
    ".xarray.core.dataarray.py@@DataArray.to_pandas": "def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:\n    constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n    try:\n        constructor = constructors[self.ndim]\n    except KeyError:\n        raise ValueError(f'Cannot convert arrays with {self.ndim} dimensions into pandas objects. Requires 2 or fewer dimensions.')\n    indexes = [self.get_index(dim) for dim in self.dims]\n    return constructor(self.values, *indexes)",
    ".xarray.core.coordinates.py@@DatasetCoordinates.dims": "def dims(self) -> Mapping[Hashable, int]:\n    return self._data.dims",
    ".xarray.core.dataarray.py@@DataArray.to_series": "def to_series(self) -> pd.Series:\n    index = self.coords.to_index()\n    return pd.Series(self.values.reshape(-1), index=index, name=self.name)",
    ".xarray.core.dataarray.py@@DataArray.from_series": "def from_series(cls, series: pd.Series, sparse: bool=False) -> DataArray:\n    temp_name = '__temporary_name'\n    df = pd.DataFrame({temp_name: series})\n    ds = Dataset.from_dataframe(df, sparse=sparse)\n    result = cast(DataArray, ds[temp_name])\n    result.name = series.name\n    return result",
    ".xarray.core.dataset.py@@Dataset.from_dataframe": "def from_dataframe(cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool=False) -> T_Dataset:\n    if not dataframe.columns.is_unique:\n        raise ValueError('cannot convert DataFrame with non-unique columns')\n    idx = remove_unused_levels_categories(dataframe.index)\n    if isinstance(idx, pd.MultiIndex) and (not idx.is_unique):\n        raise ValueError('cannot convert a DataFrame with a non-unique MultiIndex into xarray')\n    arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]\n    indexes: dict[Hashable, Index] = {}\n    index_vars: dict[Hashable, Variable] = {}\n    if isinstance(idx, pd.MultiIndex):\n        dims = tuple((name if name is not None else 'level_%i' % n for n, name in enumerate(idx.names)))\n        for dim, lev in zip(dims, idx.levels):\n            xr_idx = PandasIndex(lev, dim)\n            indexes[dim] = xr_idx\n            index_vars.update(xr_idx.create_variables())\n    else:\n        index_name = idx.name if idx.name is not None else 'index'\n        dims = (index_name,)\n        xr_idx = PandasIndex(idx, index_name)\n        indexes[index_name] = xr_idx\n        index_vars.update(xr_idx.create_variables())\n    obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)\n    if sparse:\n        obj._set_sparse_data_from_dataframe(idx, arrays, dims)\n    else:\n        obj._set_numpy_data_from_dataframe(idx, arrays, dims)\n    return obj",
    ".xarray.core.dataset.py@@Dataset._set_numpy_data_from_dataframe": "def _set_numpy_data_from_dataframe(self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple) -> None:\n    if not isinstance(idx, pd.MultiIndex):\n        for name, values in arrays:\n            self[name] = (dims, values)\n        return\n    shape = tuple((lev.size for lev in idx.levels))\n    indexer = tuple(idx.codes)\n    missing_values = math.prod(shape) > idx.shape[0]\n    for name, values in arrays:\n        if missing_values:\n            dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n            data = np.full(shape, fill_value, dtype)\n        else:\n            data = np.zeros(shape, values.dtype)\n        data[indexer] = values\n        self[name] = (dims, data)",
    ".xarray.core.dataset.py@@Dataset.__setitem__": "def __setitem__(self, key: Hashable | Iterable[Hashable] | Mapping, value: Any) -> None:\n    from .dataarray import DataArray\n    if utils.is_dict_like(key):\n        value = self._setitem_check(key, value)\n        processed = []\n        for name, var in self.items():\n            try:\n                var[key] = value[name]\n                processed.append(name)\n            except Exception as e:\n                if processed:\n                    raise RuntimeError(f\"An error occurred while setting values of the variable '{name}'. The following variables have been successfully updated:\\n{processed}\") from e\n                else:\n                    raise e\n    elif utils.hashable(key):\n        if isinstance(value, Dataset):\n            raise TypeError('Cannot assign a Dataset to a single key - only a DataArray or Variable object can be stored under a single key.')\n        self.update({key: value})\n    elif utils.iterable_of_hashable(key):\n        keylist = list(key)\n        if len(keylist) == 0:\n            raise ValueError('Empty list of variables to be set')\n        if len(keylist) == 1:\n            self.update({keylist[0]: value})\n        else:\n            if len(keylist) != len(value):\n                raise ValueError(f'Different lengths of variables to be set ({len(keylist)}) and data used as input for setting ({len(value)})')\n            if isinstance(value, Dataset):\n                self.update(dict(zip(keylist, value.data_vars.values())))\n            elif isinstance(value, DataArray):\n                raise ValueError('Cannot assign single DataArray to multiple keys')\n            else:\n                self.update(dict(zip(keylist, value)))\n    else:\n        raise ValueError(f'Unsupported key-type {type(key)}')",
    ".xarray.core.dataset.py@@Dataset.update": "def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:\n    merge_result = dataset_update_method(self, other)\n    return self._replace(inplace=True, **merge_result._asdict())",
    ".xarray.core.merge.py@@dataset_update_method": "def dataset_update_method(dataset: Dataset, other: CoercibleMapping) -> _MergeResult:\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    if not isinstance(other, Dataset):\n        other = dict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                coord_names = [c for c in value.coords if c not in value.dims and c in dataset.coords]\n                if coord_names:\n                    other[key] = value.drop_vars(coord_names)\n    return merge_core([dataset, other], priority_arg=1, indexes=dataset.xindexes, combine_attrs='override')",
    ".xarray.core.dataarray.py@@DataArray.to_dict": "def to_dict(self, data: bool=True, encoding: bool=False) -> dict[str, Any]:\n    d = self.variable.to_dict(data=data)\n    d.update({'coords': {}, 'name': self.name})\n    for k in self.coords:\n        d['coords'][k] = self.coords[k].variable.to_dict(data=data)\n    if encoding:\n        d['encoding'] = dict(self.encoding)\n    return d",
    ".xarray.core.dataarray.py@@DataArray.from_dict": "def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:\n    coords = None\n    if 'coords' in d:\n        try:\n            coords = {k: (v['dims'], v['data'], v.get('attrs')) for k, v in d['coords'].items()}\n        except KeyError as e:\n            raise ValueError(\"cannot convert dict when coords are missing the key '{dims_data}'\".format(dims_data=str(e.args[0])))\n    try:\n        data = d['data']\n    except KeyError:\n        raise ValueError(\"cannot convert dict without the key 'data''\")\n    else:\n        obj = cls(data, coords, d.get('dims'), d.get('name'), d.get('attrs'))\n    obj.encoding.update(d.get('encoding', {}))\n    return obj",
    ".xarray.core.dataarray.py@@DataArray.to_cdms2": "def to_cdms2(self) -> cdms2_Variable:\n    from ..convert import to_cdms2\n    return to_cdms2(self)",
    ".xarray.convert.py@@to_cdms2": "def to_cdms2(dataarray, copy=True):\n    import cdms2\n\n    def set_cdms2_attrs(var, attrs):\n        for k, v in attrs.items():\n            setattr(var, k, v)\n    axes = []\n    for dim in dataarray.dims:\n        coord = encode(dataarray.coords[dim])\n        axis = cdms2.createAxis(coord.values, id=dim)\n        set_cdms2_attrs(axis, coord.attrs)\n        axes.append(axis)\n    var = encode(dataarray)\n    cdms2_var = cdms2.createVariable(var.values, axes=axes, id=dataarray.name, mask=pd.isnull(var.values), copy=copy)\n    set_cdms2_attrs(cdms2_var, var.attrs)\n    if dataarray.name not in dataarray.coords:\n        cdms2_axes = {}\n        for coord_name in set(dataarray.coords.keys()) - set(dataarray.dims):\n            coord_array = dataarray.coords[coord_name].to_cdms2()\n            cdms2_axis_cls = cdms2.coord.TransientAxis2D if coord_array.ndim else cdms2.auxcoord.TransientAuxAxis1D\n            cdms2_axis = cdms2_axis_cls(coord_array)\n            if cdms2_axis.isLongitude():\n                cdms2_axes['lon'] = cdms2_axis\n            elif cdms2_axis.isLatitude():\n                cdms2_axes['lat'] = cdms2_axis\n        if 'lon' in cdms2_axes and 'lat' in cdms2_axes:\n            if len(cdms2_axes['lon'].shape) == 2:\n                cdms2_grid = cdms2.hgrid.TransientCurveGrid(cdms2_axes['lat'], cdms2_axes['lon'])\n            else:\n                cdms2_grid = cdms2.gengrid.AbstractGenericGrid(cdms2_axes['lat'], cdms2_axes['lon'])\n            for axis in cdms2_grid.getAxisList():\n                cdms2_var.setAxis(cdms2_var.getAxisIds().index(axis.id), axis)\n            cdms2_var.setGrid(cdms2_grid)\n    return cdms2_var",
    ".xarray.convert.py@@encode": "def encode(var):\n    return CFTimedeltaCoder().encode(CFDatetimeCoder().encode(var.variable))",
    ".xarray.coding.times.py@@CFDatetimeCoder.__init__": "def __init__(self, use_cftime=None):\n    self.use_cftime = use_cftime",
    ".xarray.coding.times.py@@CFDatetimeCoder.encode": "def encode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_encoding(variable)\n    if np.issubdtype(data.dtype, np.datetime64) or contains_cftime_datetimes(variable):\n        data, units, calendar = encode_cf_datetime(data, encoding.pop('units', None), encoding.pop('calendar', None))\n        safe_setitem(attrs, 'units', units, name=name)\n        safe_setitem(attrs, 'calendar', calendar, name=name)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.variables.py@@unpack_for_encoding": "def unpack_for_encoding(var):\n    return (var.dims, var.data, var.attrs.copy(), var.encoding.copy())",
    ".xarray.coding.times.py@@CFTimedeltaCoder.encode": "def encode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_encoding(variable)\n    if np.issubdtype(data.dtype, np.timedelta64):\n        data, units = encode_cf_timedelta(data, encoding.pop('units', None))\n        safe_setitem(attrs, 'units', units, name=name)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.conventions.py@@decode_cf": "def decode_cf(obj, concat_characters=True, mask_and_scale=True, decode_times=True, decode_coords=True, drop_variables=None, use_cftime=None, decode_timedelta=None):\n    from .backends.common import AbstractDataStore\n    from .core.dataset import Dataset\n    if isinstance(obj, Dataset):\n        vars = obj._variables\n        attrs = obj.attrs\n        extra_coords = set(obj.coords)\n        close = obj._close\n        encoding = obj.encoding\n    elif isinstance(obj, AbstractDataStore):\n        vars, attrs = obj.load()\n        extra_coords = set()\n        close = obj.close\n        encoding = obj.get_encoding()\n    else:\n        raise TypeError('can only decode Dataset or DataStore objects')\n    vars, attrs, coord_names = decode_cf_variables(vars, attrs, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables=drop_variables, use_cftime=use_cftime, decode_timedelta=decode_timedelta)\n    ds = Dataset(vars, attrs=attrs)\n    ds = ds.set_coords(coord_names.union(extra_coords).intersection(vars))\n    ds.set_close(close)\n    ds.encoding = encoding\n    return ds",
    ".xarray.conventions.py@@decode_cf_variables": "def decode_cf_variables(variables, attributes, concat_characters=True, mask_and_scale=True, decode_times=True, decode_coords=True, drop_variables=None, use_cftime=None, decode_timedelta=None):\n    dimensions_used_by = defaultdict(list)\n    for v in variables.values():\n        for d in v.dims:\n            dimensions_used_by[d].append(v)\n\n    def stackable(dim):\n        if dim in variables:\n            return False\n        for v in dimensions_used_by[dim]:\n            if v.dtype.kind != 'S' or dim != v.dims[-1]:\n                return False\n        return True\n    coord_names = set()\n    if isinstance(drop_variables, str):\n        drop_variables = [drop_variables]\n    elif drop_variables is None:\n        drop_variables = []\n    drop_variables = set(drop_variables)\n    if decode_times:\n        _update_bounds_attributes(variables)\n    new_vars = {}\n    for k, v in variables.items():\n        if k in drop_variables:\n            continue\n        stack_char_dim = concat_characters and v.dtype == 'S1' and (v.ndim > 0) and stackable(v.dims[-1])\n        new_vars[k] = decode_cf_variable(k, v, concat_characters=concat_characters, mask_and_scale=mask_and_scale, decode_times=decode_times, stack_char_dim=stack_char_dim, use_cftime=use_cftime, decode_timedelta=decode_timedelta)\n        if decode_coords in [True, 'coordinates', 'all']:\n            var_attrs = new_vars[k].attrs\n            if 'coordinates' in var_attrs:\n                coord_str = var_attrs['coordinates']\n                var_coord_names = coord_str.split()\n                if all((k in variables for k in var_coord_names)):\n                    new_vars[k].encoding['coordinates'] = coord_str\n                    del var_attrs['coordinates']\n                    coord_names.update(var_coord_names)\n        if decode_coords == 'all':\n            for attr_name in CF_RELATED_DATA:\n                if attr_name in var_attrs:\n                    attr_val = var_attrs[attr_name]\n                    if attr_name not in CF_RELATED_DATA_NEEDS_PARSING:\n                        var_names = attr_val.split()\n                    else:\n                        roles_and_names = [role_or_name for part in attr_val.split(':') for role_or_name in part.split()]\n                        if len(roles_and_names) % 2 == 1:\n                            warnings.warn(f'Attribute {attr_name:s} malformed', stacklevel=5)\n                        var_names = roles_and_names[1::2]\n                    if all((var_name in variables for var_name in var_names)):\n                        new_vars[k].encoding[attr_name] = attr_val\n                        coord_names.update(var_names)\n                    else:\n                        referenced_vars_not_in_variables = [proj_name for proj_name in var_names if proj_name not in variables]\n                        warnings.warn(f'Variable(s) referenced in {attr_name:s} not in variables: {referenced_vars_not_in_variables!s}', stacklevel=5)\n                    del var_attrs[attr_name]\n    if decode_coords and 'coordinates' in attributes:\n        attributes = dict(attributes)\n        coord_names.update(attributes.pop('coordinates').split())\n    return (new_vars, attributes, coord_names)",
    ".xarray.conventions.py@@_update_bounds_attributes": "def _update_bounds_attributes(variables):\n    for v in variables.values():\n        attrs = v.attrs\n        units = attrs.get('units')\n        has_date_units = isinstance(units, str) and 'since' in units\n        if has_date_units and 'bounds' in attrs:\n            if attrs['bounds'] in variables:\n                bounds_attrs = variables[attrs['bounds']].attrs\n                bounds_attrs.setdefault('units', attrs['units'])\n                if 'calendar' in attrs:\n                    bounds_attrs.setdefault('calendar', attrs['calendar'])",
    ".xarray.core.common.py@@contains_cftime_datetimes": "def contains_cftime_datetimes(var) -> bool:\n    if var.dtype == np.dtype('O') and var.size > 0:\n        return _contains_cftime_datetimes(var.data)\n    else:\n        return False",
    ".xarray.coding.strings.py@@EncodedStringCoder.__init__": "def __init__(self, allows_unicode=True):\n    self.allows_unicode = allows_unicode",
    ".xarray.coding.strings.py@@EncodedStringCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    if '_Encoding' in attrs:\n        string_encoding = pop_to(attrs, encoding, '_Encoding')\n        func = partial(decode_bytes_array, encoding=string_encoding)\n        data = lazy_elemwise_func(data, func, np.dtype(object))\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.variables.py@@unpack_for_decoding": "def unpack_for_decoding(var):\n    return (var.dims, var._data, var.attrs.copy(), var.encoding.copy())",
    ".xarray.coding.variables.py@@UnsignedIntegerCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    if '_Unsigned' in attrs:\n        unsigned = pop_to(attrs, encoding, '_Unsigned')\n        if data.dtype.kind == 'i':\n            if unsigned == 'true':\n                unsigned_dtype = np.dtype(f'u{data.dtype.itemsize}')\n                transform = partial(np.asarray, dtype=unsigned_dtype)\n                data = lazy_elemwise_func(data, transform, unsigned_dtype)\n                if '_FillValue' in attrs:\n                    new_fill = unsigned_dtype.type(attrs['_FillValue'])\n                    attrs['_FillValue'] = new_fill\n        elif data.dtype.kind == 'u':\n            if unsigned == 'false':\n                signed_dtype = np.dtype(f'i{data.dtype.itemsize}')\n                transform = partial(np.asarray, dtype=signed_dtype)\n                data = lazy_elemwise_func(data, transform, signed_dtype)\n                if '_FillValue' in attrs:\n                    new_fill = signed_dtype.type(attrs['_FillValue'])\n                    attrs['_FillValue'] = new_fill\n        else:\n            warnings.warn(f'variable {name!r} has _Unsigned attribute but is not of integer type. Ignoring attribute.', SerializationWarning, stacklevel=3)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.variables.py@@CFMaskCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    raw_fill_values = [pop_to(attrs, encoding, attr, name=name) for attr in ('missing_value', '_FillValue')]\n    if raw_fill_values:\n        encoded_fill_values = {fv for option in raw_fill_values for fv in np.ravel(option) if not pd.isnull(fv)}\n        if len(encoded_fill_values) > 1:\n            warnings.warn('variable {!r} has multiple fill values {}, decoding all values to NaN.'.format(name, encoded_fill_values), SerializationWarning, stacklevel=3)\n        dtype, decoded_fill_value = dtypes.maybe_promote(data.dtype)\n        if encoded_fill_values:\n            transform = partial(_apply_mask, encoded_fill_values=encoded_fill_values, decoded_fill_value=decoded_fill_value, dtype=dtype)\n            data = lazy_elemwise_func(data, transform, dtype)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.variables.py@@CFScaleOffsetCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    if 'scale_factor' in attrs or 'add_offset' in attrs:\n        scale_factor = pop_to(attrs, encoding, 'scale_factor', name=name)\n        add_offset = pop_to(attrs, encoding, 'add_offset', name=name)\n        dtype = _choose_float_dtype(data.dtype, 'add_offset' in encoding)\n        if np.ndim(scale_factor) > 0:\n            scale_factor = np.asarray(scale_factor).item()\n        if np.ndim(add_offset) > 0:\n            add_offset = np.asarray(add_offset).item()\n        transform = partial(_scale_offset_decoding, scale_factor=scale_factor, add_offset=add_offset, dtype=dtype)\n        data = lazy_elemwise_func(data, transform, dtype)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.times.py@@CFTimedeltaCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    units = attrs.get('units')\n    if isinstance(units, str) and units in TIME_UNITS:\n        units = pop_to(attrs, encoding, 'units')\n        transform = partial(decode_cf_timedelta, units=units)\n        dtype = np.dtype('timedelta64[ns]')\n        data = lazy_elemwise_func(data, transform, dtype=dtype)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.coding.times.py@@CFDatetimeCoder.decode": "def decode(self, variable, name=None):\n    dims, data, attrs, encoding = unpack_for_decoding(variable)\n    units = attrs.get('units')\n    if isinstance(units, str) and 'since' in units:\n        units = pop_to(attrs, encoding, 'units')\n        calendar = pop_to(attrs, encoding, 'calendar')\n        dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n        transform = partial(decode_cf_datetime, units=units, calendar=calendar, use_cftime=self.use_cftime)\n        data = lazy_elemwise_func(data, transform, dtype)\n    return Variable(dims, data, attrs, encoding)",
    ".xarray.core.indexing.py@@LazilyIndexedArray.shape": "def shape(self) -> tuple[int, ...]:\n    shape = []\n    for size, k in zip(self.array.shape, self.key.tuple):\n        if isinstance(k, slice):\n            shape.append(len(range(*k.indices(size))))\n        elif isinstance(k, np.ndarray):\n            shape.append(k.size)\n    return tuple(shape)",
    ".xarray.core.utils.py@@NDArrayMixin.dtype": "def dtype(self: Any) -> np.dtype:\n    return self.array.dtype",
    ".xarray.core.indexing.py@@LazilyIndexedArray.__array__": "def __array__(self, dtype=None):\n    array = as_indexable(self.array)\n    return np.asarray(array[self.key], dtype=None)",
    ".xarray.core.indexing.py@@PandasIndexingAdapter.__getitem__": "def __getitem__(self, indexer) -> PandasIndexingAdapter | NumpyIndexingAdapter | np.ndarray | np.datetime64 | np.timedelta64:\n    key = indexer.tuple\n    if isinstance(key, tuple) and len(key) == 1:\n        key, = key\n    if getattr(key, 'ndim', 0) > 1:\n        return NumpyIndexingAdapter(np.asarray(self))[indexer]\n    result = self.array[key]\n    if isinstance(result, pd.Index):\n        return type(self)(result, dtype=self.dtype)\n    else:\n        return self._convert_scalar(result)",
    ".xarray.core.utils.py@@NDArrayMixin.shape": "def shape(self: Any) -> tuple[int, ...]:\n    return self.array.shape",
    ".xarray.coding.variables.py@@_ElementwiseFunctionArray.dtype": "def dtype(self):\n    return np.dtype(self._dtype)",
    ".xarray.coding.variables.py@@_ElementwiseFunctionArray.__getitem__": "def __getitem__(self, key):\n    return type(self)(self.array[key], self.func, self.dtype)",
    ".xarray.coding.variables.py@@_ElementwiseFunctionArray.__init__": "def __init__(self, array, func, dtype):\n    assert not is_duck_dask_array(array)\n    self.array = indexing.as_indexable(array)\n    self.func = func\n    self._dtype = dtype",
    ".xarray.coding.variables.py@@_ElementwiseFunctionArray.__array__": "def __array__(self, dtype=None):\n    return self.func(self.array)",
    ".xarray.coding.times.py@@decode_cf_datetime": "def decode_cf_datetime(num_dates, units, calendar=None, use_cftime=None):\n    num_dates = np.asarray(num_dates)\n    flat_num_dates = num_dates.ravel()\n    if calendar is None:\n        calendar = 'standard'\n    if use_cftime is None:\n        try:\n            dates = _decode_datetime_with_pandas(flat_num_dates, units, calendar)\n        except (KeyError, OutOfBoundsDatetime, OutOfBoundsTimedelta, OverflowError):\n            dates = _decode_datetime_with_cftime(flat_num_dates.astype(float), units, calendar)\n            if dates[np.nanargmin(num_dates)].year < 1678 or dates[np.nanargmax(num_dates)].year >= 2262:\n                if _is_standard_calendar(calendar):\n                    warnings.warn('Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range', SerializationWarning, stacklevel=3)\n            elif _is_standard_calendar(calendar):\n                dates = cftime_to_nptime(dates)\n    elif use_cftime:\n        dates = _decode_datetime_with_cftime(flat_num_dates, units, calendar)\n    else:\n        dates = _decode_datetime_with_pandas(flat_num_dates, units, calendar)\n    return dates.reshape(num_dates.shape)",
    ".xarray.coding.times.py@@_decode_datetime_with_pandas": "def _decode_datetime_with_pandas(flat_num_dates, units, calendar):\n    if not _is_standard_calendar(calendar):\n        raise OutOfBoundsDatetime('Cannot decode times from a non-standard calendar, {!r}, using pandas.'.format(calendar))\n    delta, ref_date = _unpack_netcdf_time_units(units)\n    delta = _netcdf_to_numpy_timeunit(delta)\n    try:\n        ref_date = pd.Timestamp(ref_date)\n    except ValueError:\n        raise OutOfBoundsDatetime\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'invalid value encountered', RuntimeWarning)\n        if flat_num_dates.size > 0:\n            pd.to_timedelta(flat_num_dates.min(), delta) + ref_date\n            pd.to_timedelta(flat_num_dates.max(), delta) + ref_date\n    if flat_num_dates.dtype.kind in 'iu':\n        flat_num_dates = flat_num_dates.astype(np.int64)\n    flat_num_dates_ns_int = (flat_num_dates * _NS_PER_TIME_DELTA[delta]).astype(np.int64)\n    return (pd.to_timedelta(flat_num_dates_ns_int, 'ns') + ref_date).values",
    ".xarray.coding.times.py@@_is_standard_calendar": "def _is_standard_calendar(calendar):\n    return calendar.lower() in _STANDARD_CALENDARS",
    ".xarray.coding.times.py@@_unpack_netcdf_time_units": "def _unpack_netcdf_time_units(units):\n    matches = re.match('(.+) since (.+)', units)\n    if not matches:\n        raise ValueError(f'invalid time units: {units}')\n    delta_units, ref_date = (s.strip() for s in matches.groups())\n    ref_date = _ensure_padded_year(ref_date)\n    return (delta_units, ref_date)",
    ".xarray.coding.times.py@@_ensure_padded_year": "def _ensure_padded_year(ref_date):\n    matches_year = re.match('.*\\\\d{4}.*', ref_date)\n    if matches_year:\n        return ref_date\n    matches_start_digits = re.match('(\\\\d+)(.*)', ref_date)\n    if not matches_start_digits:\n        raise ValueError(f'invalid reference date for time units: {ref_date}')\n    ref_year, everything_else = (s for s in matches_start_digits.groups())\n    ref_date_padded = f'{int(ref_year):04d}{everything_else}'\n    warning_msg = f'Ambiguous reference date string: {ref_date}. The first value is assumed to be the year hence will be padded with zeros to remove the ambiguity (the padded reference date string is: {ref_date_padded}). To remove this message, remove the ambiguity by padding your reference date strings with zeros.'\n    warnings.warn(warning_msg, SerializationWarning)\n    return ref_date_padded",
    ".xarray.coding.times.py@@_netcdf_to_numpy_timeunit": "def _netcdf_to_numpy_timeunit(units):\n    units = units.lower()\n    if not units.endswith('s'):\n        units = f'{units}s'\n    return {'nanoseconds': 'ns', 'microseconds': 'us', 'milliseconds': 'ms', 'seconds': 's', 'minutes': 'm', 'hours': 'h', 'days': 'D'}[units]",
    ".xarray.core.common.py@@DataWithCoords.set_close": "def set_close(self, close: Callable[[], None] | None) -> None:\n    self._close = close",
    ".xarray.core.dataarray.py@@DataArray.to_dataset": "def to_dataset(self, dim: Hashable=None, *, name: Hashable=None, promote_attrs: bool=False) -> Dataset:\n    if dim is not None and dim not in self.dims:\n        raise TypeError(f'{dim} is not a dim. If supplying a ``name``, pass as a kwarg.')\n    if dim is not None:\n        if name is not None:\n            raise TypeError('cannot supply both dim and name arguments')\n        result = self._to_dataset_split(dim)\n    else:\n        result = self._to_dataset_whole(name)\n    if promote_attrs:\n        result.attrs = dict(self.attrs)\n    return result",
    ".xarray.core.utils.py@@equivalent": "def equivalent(first: T, second: T) -> bool:\n    from . import duck_array_ops\n    if isinstance(first, np.ndarray) or isinstance(second, np.ndarray):\n        return duck_array_ops.array_equiv(first, second)\n    elif isinstance(first, list) or isinstance(second, list):\n        return list_equiv(first, second)\n    else:\n        return first is second or first == second or (pd.isnull(first) and pd.isnull(second))",
    ".xarray.core.dataset.py@@DataVariables.__len__": "def __len__(self) -> int:\n    return len(self._dataset._variables) - len(self._dataset._coord_names)",
    ".xarray.core.dataarray.py@@DataArray._construct_direct": "def _construct_direct(cls: type[T_DataArray], variable: Variable, coords: dict[Any, Variable], name: Hashable, indexes: dict[Hashable, Index]) -> T_DataArray:\n    obj = object.__new__(cls)\n    obj._variable = variable\n    obj._coords = coords\n    obj._name = name\n    obj._indexes = indexes\n    obj._close = None\n    return obj",
    ".xarray.core.dataarray.py@@DataArray._title_for_slice": "def _title_for_slice(self, truncate: int=50) -> str:\n    one_dims = []\n    for dim, coord in self.coords.items():\n        if coord.size == 1:\n            one_dims.append('{dim} = {v}{unit}'.format(dim=dim, v=format_item(coord.values), unit=_get_units_from_attrs(coord)))\n    title = ', '.join(one_dims)\n    if len(title) > truncate:\n        title = title[:truncate - 3] + '...'\n    return title",
    ".xarray.plot.utils.py@@_get_units_from_attrs": "def _get_units_from_attrs(da) -> str:\n    pint_array_type = DuckArrayModule('pint').type\n    units = ' [{}]'\n    if isinstance(da.data, pint_array_type):\n        units = units.format(str(da.data.units))\n    elif da.attrs.get('units'):\n        units = units.format(da.attrs['units'])\n    elif da.attrs.get('unit'):\n        units = units.format(da.attrs['unit'])\n    else:\n        units = ''\n    return units",
    ".xarray.core.pycompat.py@@DuckArrayModule.__init__": "def __init__(self, mod):\n    try:\n        duck_array_module = import_module(mod)\n        duck_array_version = Version(duck_array_module.__version__)\n        if mod == 'dask':\n            duck_array_type = (import_module('dask.array').Array,)\n        elif mod == 'pint':\n            duck_array_type = (duck_array_module.Quantity,)\n        elif mod == 'cupy':\n            duck_array_type = (duck_array_module.ndarray,)\n        elif mod == 'sparse':\n            duck_array_type = (duck_array_module.SparseArray,)\n        else:\n            raise NotImplementedError\n    except ImportError:\n        duck_array_module = None\n        duck_array_version = Version('0.0.0')\n        duck_array_type = ()\n    self.module = duck_array_module\n    self.version = duck_array_version\n    self.type = duck_array_type\n    self.available = duck_array_module is not None",
    ".xarray.core.dataarray.py@@DataArray.diff": "def diff(self: T_DataArray, dim: Hashable, n: int=1, label: Literal['upper', 'lower']='upper') -> T_DataArray:\n    ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.diff": "def diff(self: T_Dataset, dim: Hashable, n: int=1, label: Literal['upper', 'lower']='upper') -> T_Dataset:\n    if n == 0:\n        return self\n    if n < 0:\n        raise ValueError(f'order `n` must be non-negative but got {n}')\n    slice_start = {dim: slice(None, -1)}\n    slice_end = {dim: slice(1, None)}\n    if label == 'upper':\n        slice_new = slice_end\n    elif label == 'lower':\n        slice_new = slice_start\n    else:\n        raise ValueError(\"The 'label' argument has to be either 'upper' or 'lower'\")\n    indexes, index_vars = isel_indexes(self.xindexes, slice_new)\n    variables = {}\n    for name, var in self.variables.items():\n        if name in index_vars:\n            variables[name] = index_vars[name]\n        elif dim in var.dims:\n            if name in self.data_vars:\n                variables[name] = var.isel(slice_end) - var.isel(slice_start)\n            else:\n                variables[name] = var.isel(slice_new)\n        else:\n            variables[name] = var\n    difference = self._replace_with_new_dims(variables, indexes=indexes)\n    if n > 1:\n        return difference.diff(dim, n - 1)\n    else:\n        return difference",
    ".xarray.core.dataarray.py@@DataArray.shift": "def shift(self: T_DataArray, shifts: Mapping[Any, int] | None=None, fill_value: Any=dtypes.NA, **shifts_kwargs: int) -> T_DataArray:\n    variable = self.variable.shift(shifts=shifts, fill_value=fill_value, **shifts_kwargs)\n    return self._replace(variable=variable)",
    ".xarray.core.dataarray.py@@DataArray.roll": "def roll(self: T_DataArray, shifts: Mapping[Hashable, int] | None=None, roll_coords: bool=False, **shifts_kwargs: int) -> T_DataArray:\n    ds = self._to_temp_dataset().roll(shifts=shifts, roll_coords=roll_coords, **shifts_kwargs)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.roll": "def roll(self: T_Dataset, shifts: Mapping[Any, int] | None=None, roll_coords: bool=False, **shifts_kwargs: int) -> T_Dataset:\n    shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'roll')\n    invalid = [k for k in shifts if k not in self.dims]\n    if invalid:\n        raise ValueError(f'dimensions {invalid!r} do not exist')\n    unrolled_vars: tuple[Hashable, ...]\n    if roll_coords:\n        indexes, index_vars = roll_indexes(self.xindexes, shifts)\n        unrolled_vars = ()\n    else:\n        indexes = dict(self._indexes)\n        index_vars = dict(self.xindexes.variables)\n        unrolled_vars = tuple(self.coords)\n    variables = {}\n    for k, var in self.variables.items():\n        if k in index_vars:\n            variables[k] = index_vars[k]\n        elif k not in unrolled_vars:\n            variables[k] = var.roll(shifts={k: s for k, s in shifts.items() if k in var.dims})\n        else:\n            variables[k] = var\n    return self._replace(variables, indexes=indexes)",
    ".xarray.core.indexes.py@@roll_indexes": "def roll_indexes(indexes: Indexes[Index], shifts: Mapping[Any, int]) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n    return _apply_indexes(indexes, shifts, 'roll')",
    ".xarray.core.indexes.py@@PandasIndex.roll": "def roll(self, shifts: Mapping[Any, int]) -> PandasIndex:\n    shift = shifts[self.dim] % self.index.shape[0]\n    if shift != 0:\n        new_pd_idx = self.index[-shift:].append(self.index[:-shift])\n    else:\n        new_pd_idx = self.index[:]\n    return self._replace(new_pd_idx)",
    ".xarray.core.dataarray.py@@DataArray.real": "def real(self: T_DataArray) -> T_DataArray:\n    return self._replace(self.variable.real)",
    ".xarray.core.common.py@@full_like": "def full_like(other: DataArray, fill_value: Any, dtype: DTypeLikeSave=None) -> DataArray:\n    ...",
    ".xarray.core.common.py@@_full_like_variable": "def _full_like_variable(other: Variable, fill_value: Any, dtype: DTypeLike=None) -> Variable:\n    from .variable import Variable\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(dtype if dtype is not None else other.dtype)\n    if is_duck_dask_array(other.data):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype, chunks=other.data.chunks)\n    else:\n        data = np.full_like(other.data, fill_value, dtype=dtype)\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)",
    ".xarray.core.dataarray.py@@DataArray.dot": "def dot(self: T_DataArray, other: T_DataArray, dims: Dims | ellipsis=None) -> T_DataArray:\n    if isinstance(other, Dataset):\n        raise NotImplementedError('dot products are not yet supported with Dataset objects.')\n    if not isinstance(other, DataArray):\n        raise TypeError('dot only operates on DataArrays.')\n    return computation.dot(self, other, dims=dims)",
    ".xarray.core.computation.py@@dot": "def dot(*arrays, dims: str | Iterable[Hashable] | ellipsis | None=None, **kwargs: Any):\n    from .dataarray import DataArray\n    from .variable import Variable\n    if any((not isinstance(arr, (Variable, DataArray)) for arr in arrays)):\n        raise TypeError('Only xr.DataArray and xr.Variable are supported.Given {}.'.format([type(arr) for arr in arrays]))\n    if len(arrays) == 0:\n        raise TypeError('At least one array should be given.')\n    common_dims: set[Hashable] = set.intersection(*(set(arr.dims) for arr in arrays))\n    all_dims = []\n    for arr in arrays:\n        all_dims += [d for d in arr.dims if d not in all_dims]\n    einsum_axes = 'abcdefghijklmnopqrstuvwxyz'\n    dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}\n    if dims is ...:\n        dims = all_dims\n    elif isinstance(dims, str):\n        dims = (dims,)\n    elif dims is None:\n        dim_counts: Counter = Counter()\n        for arr in arrays:\n            dim_counts.update(arr.dims)\n        dims = tuple((d for d, c in dim_counts.items() if c > 1))\n    dot_dims: set[Hashable] = set(dims)\n    broadcast_dims = common_dims - dot_dims\n    input_core_dims = [[d for d in arr.dims if d not in broadcast_dims] for arr in arrays]\n    output_core_dims = [[d for d in all_dims if d not in dot_dims and d not in broadcast_dims]]\n    subscripts_list = ['...' + ''.join((dim_map[d] for d in ds)) for ds in input_core_dims]\n    subscripts = ','.join(subscripts_list)\n    subscripts += '->...' + ''.join((dim_map[d] for d in output_core_dims[0]))\n    join = OPTIONS['arithmetic_join']\n    if join != 'exact':\n        join = 'inner'\n    func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)\n    result = apply_ufunc(func, *arrays, input_core_dims=input_core_dims, output_core_dims=output_core_dims, join=join, dask='allowed')\n    return result.transpose(*all_dims, missing_dims='ignore')",
    ".xarray.core.options.py@@set_options.__init__": "def __init__(self, **kwargs):\n    self.old = {}\n    for k, v in kwargs.items():\n        if k not in OPTIONS:\n            raise ValueError(f'argument name {k!r} is not in the set of valid options {set(OPTIONS)!r}')\n        if k in _VALIDATORS and (not _VALIDATORS[k](v)):\n            if k == 'arithmetic_join':\n                expected = f'Expected one of {_JOIN_OPTIONS!r}'\n            elif k == 'display_style':\n                expected = f'Expected one of {_DISPLAY_OPTIONS!r}'\n            else:\n                expected = ''\n            raise ValueError(f'option {k!r} given an invalid value: {v!r}. ' + expected)\n        self.old[k] = OPTIONS[k]\n    self._apply_update(kwargs)",
    ".xarray.core.options.py@@set_options._apply_update": "def _apply_update(self, options_dict):\n    for k, v in options_dict.items():\n        if k in _SETTERS:\n            _SETTERS[k](v)\n    OPTIONS.update(options_dict)",
    ".xarray.core.options.py@@set_options.__enter__": "def __enter__(self):\n    return",
    ".xarray.core.dataarray.py@@DataArray.__matmul__": "def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:\n    return self.dot(obj)",
    ".xarray.core.options.py@@set_options.__exit__": "def __exit__(self, type, value, traceback):\n    self._apply_update(self.old)",
    ".xarray.core.dataarray.py@@DataArray.combine_first": "def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n    return ops.fillna(self, other, join='outer')",
    ".xarray.core.missing.py@@get_clean_interp_index": "def get_clean_interp_index(arr, dim: Hashable, use_coordinate: str | bool=True, strict: bool=True):\n    from xarray.coding.cftimeindex import CFTimeIndex\n    if use_coordinate is False:\n        axis = arr.get_axis_num(dim)\n        return np.arange(arr.shape[axis], dtype=np.float64)\n    if use_coordinate is True:\n        index = arr.get_index(dim)\n    else:\n        index = arr.coords[use_coordinate]\n        if index.ndim != 1:\n            raise ValueError(f'Coordinates used for interpolation must be 1D, {use_coordinate} is {index.ndim}D.')\n        index = index.to_index()\n    if isinstance(index, pd.MultiIndex):\n        index.name = dim\n    if strict:\n        if not index.is_monotonic_increasing:\n            raise ValueError(f'Index {index.name!r} must be monotonically increasing')\n        if not index.is_unique:\n            raise ValueError(f'Index {index.name!r} has duplicate values')\n    if isinstance(index, (CFTimeIndex, pd.DatetimeIndex)):\n        offset = type(index[0])(1970, 1, 1)\n        if isinstance(index, CFTimeIndex):\n            index = index.values\n        index = Variable(data=datetime_to_numeric(index, offset=offset, datetime_unit='ns'), dims=(dim,))\n    try:\n        index = index.values.astype(np.float64)\n    except (TypeError, ValueError):\n        raise TypeError(f'Index {index.name!r} must be castable to float64 to support interpolation or curve fitting, got {type(index).__name__}.')\n    return index",
    ".xarray.core.duck_array_ops.py@@datetime_to_numeric": "def datetime_to_numeric(array, offset=None, datetime_unit=None, dtype=float):\n    if offset is None:\n        if array.dtype.kind in 'Mm':\n            offset = _datetime_nanmin(array)\n        else:\n            offset = min(array)\n    if is_duck_dask_array(array) and np.issubdtype(array.dtype, object):\n        array = array.map_blocks(lambda a, b: a - b, offset, meta=array._meta)\n    else:\n        array = array - offset\n    if not hasattr(array, 'dtype'):\n        array = np.array(array)\n    if array.dtype.kind in 'O':\n        return py_timedelta_to_float(array, datetime_unit or 'ns').astype(dtype)\n    elif array.dtype.kind in 'mM':\n        if datetime_unit:\n            array = array / np.timedelta64(1, datetime_unit)\n        return np.where(isnull(array), np.nan, array.astype(dtype))",
    ".xarray.core.dataset.py@@Dataset._unstack_full_reindex": "def _unstack_full_reindex(self: T_Dataset, dim: Hashable, index_and_vars: tuple[Index, dict[Hashable, Variable]], fill_value, sparse: bool) -> T_Dataset:\n    index, index_vars = index_and_vars\n    variables: dict[Hashable, Variable] = {}\n    indexes = {k: v for k, v in self._indexes.items() if k != dim}\n    new_indexes, clean_index = index.unstack()\n    indexes.update(new_indexes)\n    new_index_variables = {}\n    for name, idx in new_indexes.items():\n        new_index_variables.update(idx.create_variables(index_vars))\n    new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}\n    variables.update(new_index_variables)\n    full_idx = pd.MultiIndex.from_product(clean_index.levels, names=clean_index.names)\n    if clean_index.equals(full_idx):\n        obj = self\n    else:\n        xr_full_idx = PandasMultiIndex(full_idx, dim)\n        indexers = Indexes({k: xr_full_idx for k in index_vars}, xr_full_idx.create_variables(index_vars))\n        obj = self._reindex(indexers, copy=False, fill_value=fill_value, sparse=sparse)\n    for name, var in obj.variables.items():\n        if name not in index_vars:\n            if dim in var.dims:\n                variables[name] = var.unstack({dim: new_dim_sizes})\n            else:\n                variables[name] = var\n    coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)\n    return self._replace_with_new_dims(variables, coord_names=coord_names, indexes=indexes)",
    ".xarray.testing.py@@assert_allclose": "def assert_allclose(a, b, rtol=1e-05, atol=1e-08, decode_bytes=True):\n    __tracebackhide__ = True\n    assert type(a) == type(b)\n    equiv = functools.partial(_data_allclose_or_equiv, rtol=rtol, atol=atol, decode_bytes=decode_bytes)\n    equiv.__name__ = 'allclose'\n\n    def compat_variable(a, b):\n        a = getattr(a, 'variable', a)\n        b = getattr(b, 'variable', b)\n        return a.dims == b.dims and (a._data is b._data or equiv(a.data, b.data))\n    if isinstance(a, Variable):\n        allclose = compat_variable(a, b)\n        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)\n    elif isinstance(a, DataArray):\n        allclose = utils.dict_equiv(a.coords, b.coords, compat=compat_variable) and compat_variable(a.variable, b.variable)\n        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)\n    elif isinstance(a, Dataset):\n        allclose = a._coord_names == b._coord_names and utils.dict_equiv(a.variables, b.variables, compat=compat_variable)\n        assert allclose, formatting.diff_dataset_repr(a, b, compat=equiv)\n    else:\n        raise TypeError(f'{type(a)} not supported by assertion comparison')",
    ".xarray.testing.py@@compat_variable": "def compat_variable(a, b):\n    a = getattr(a, 'variable', a)\n    b = getattr(b, 'variable', b)\n    return a.dims == b.dims and (a._data is b._data or equiv(a.data, b.data))",
    ".xarray.testing.py@@_data_allclose_or_equiv": "def _data_allclose_or_equiv(arr1, arr2, rtol=1e-05, atol=1e-08, decode_bytes=True):\n    if any((arr.dtype.kind == 'S' for arr in [arr1, arr2])) and decode_bytes:\n        arr1 = _decode_string_data(arr1)\n        arr2 = _decode_string_data(arr2)\n    exact_dtypes = ['M', 'm', 'O', 'S', 'U']\n    if any((arr.dtype.kind in exact_dtypes for arr in [arr1, arr2])):\n        return duck_array_ops.array_equiv(arr1, arr2)\n    else:\n        return duck_array_ops.allclose_or_equiv(arr1, arr2, rtol=rtol, atol=atol)",
    ".xarray.core.duck_array_ops.py@@allclose_or_equiv": "def allclose_or_equiv(arr1, arr2, rtol=1e-05, atol=1e-08):\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n            return bool(isclose(arr1, arr2, rtol=rtol, atol=atol, equal_nan=True).all())\n    else:\n        return lazy_equiv",
    ".xarray.core.nputils.py@@_nanpolyfit_1d": "def _nanpolyfit_1d(arr, x, rcond=None):\n    out = np.full((x.shape[1] + 1,), np.nan)\n    mask = np.isnan(arr)\n    if not np.all(mask):\n        out[:-1], resid, rank, _ = np.linalg.lstsq(x[~mask, :], arr[~mask], rcond=rcond)\n        out[-1] = resid if resid.size > 0 else np.nan\n        warn_on_deficient_rank(rank, x.shape[1])\n    return out",
    ".xarray.core.nputils.py@@warn_on_deficient_rank": "def warn_on_deficient_rank(rank, order):\n    if rank != order:\n        warnings.warn('Polyfit may be poorly conditioned', np.RankWarning, stacklevel=2)",
    ".xarray.core.dataarray.py@@DataArray.polyfit": "def polyfit(self, dim: Hashable, deg: int, skipna: bool | None=None, rcond: float | None=None, w: Hashable | Any | None=None, full: bool=False, cov: bool | Literal['unscaled']=False) -> Dataset:\n    return self._to_temp_dataset().polyfit(dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov)",
    ".xarray.core.dataset.py@@Dataset.polyfit": "def polyfit(self: T_Dataset, dim: Hashable, deg: int, skipna: bool | None=None, rcond: float | None=None, w: Hashable | Any=None, full: bool=False, cov: bool | Literal['unscaled']=False) -> T_Dataset:\n    from .dataarray import DataArray\n    variables = {}\n    skipna_da = skipna\n    x = get_clean_interp_index(self, dim, strict=False)\n    xname = f'{self[dim].name}_'\n    order = int(deg) + 1\n    lhs = np.vander(x, order)\n    if rcond is None:\n        rcond = x.shape[0] * np.core.finfo(x.dtype).eps\n    if w is not None:\n        if isinstance(w, Hashable):\n            w = self.coords[w]\n        w = np.asarray(w)\n        if w.ndim != 1:\n            raise TypeError('Expected a 1-d array for weights.')\n        if w.shape[0] != lhs.shape[0]:\n            raise TypeError(f'Expected w and {dim} to have the same length')\n        lhs *= w[:, np.newaxis]\n    scale = np.sqrt((lhs * lhs).sum(axis=0))\n    lhs /= scale\n    degree_dim = utils.get_temp_dimname(self.dims, 'degree')\n    rank = np.linalg.matrix_rank(lhs)\n    if full:\n        rank = DataArray(rank, name=xname + 'matrix_rank')\n        variables[rank.name] = rank\n        _sing = np.linalg.svd(lhs, compute_uv=False)\n        sing = DataArray(_sing, dims=(degree_dim,), coords={degree_dim: np.arange(rank - 1, -1, -1)}, name=xname + 'singular_values')\n        variables[sing.name] = sing\n    for name, da in self.data_vars.items():\n        if dim not in da.dims:\n            continue\n        if is_duck_dask_array(da.data) and (rank != order or full or skipna is None):\n            skipna_da = True\n        elif skipna is None:\n            skipna_da = bool(np.any(da.isnull()))\n        dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n        stacked_coords: dict[Hashable, DataArray] = {}\n        if dims_to_stack:\n            stacked_dim = utils.get_temp_dimname(dims_to_stack, 'stacked')\n            rhs = da.transpose(dim, *dims_to_stack).stack({stacked_dim: dims_to_stack})\n            stacked_coords = {stacked_dim: rhs[stacked_dim]}\n            scale_da = scale[:, np.newaxis]\n        else:\n            rhs = da\n            scale_da = scale\n        if w is not None:\n            rhs *= w[:, np.newaxis]\n        with warnings.catch_warnings():\n            if full:\n                warnings.simplefilter('ignore', np.RankWarning)\n            else:\n                warnings.simplefilter('once', np.RankWarning)\n            coeffs, residuals = duck_array_ops.least_squares(lhs, rhs.data, rcond=rcond, skipna=skipna_da)\n        if isinstance(name, str):\n            name = f'{name}_'\n        else:\n            name = ''\n        coeffs = DataArray(coeffs / scale_da, dims=[degree_dim] + list(stacked_coords.keys()), coords={degree_dim: np.arange(order)[::-1], **stacked_coords}, name=name + 'polyfit_coefficients')\n        if dims_to_stack:\n            coeffs = coeffs.unstack(stacked_dim)\n        variables[coeffs.name] = coeffs\n        if full or cov is True:\n            residuals = DataArray(residuals if dims_to_stack else residuals.squeeze(), dims=list(stacked_coords.keys()), coords=stacked_coords, name=name + 'polyfit_residuals')\n            if dims_to_stack:\n                residuals = residuals.unstack(stacked_dim)\n            variables[residuals.name] = residuals\n        if cov:\n            Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n            Vbase /= np.outer(scale, scale)\n            if cov == 'unscaled':\n                fac = 1\n            else:\n                if x.shape[0] <= order:\n                    raise ValueError('The number of data points must exceed order to scale the covariance matrix.')\n                fac = residuals / (x.shape[0] - order)\n            covariance = DataArray(Vbase, dims=('cov_i', 'cov_j')) * fac\n            variables[name + 'polyfit_covariance'] = covariance\n    return type(self)(data_vars=variables, attrs=self.attrs.copy())",
    ".xarray.core.utils.py@@get_temp_dimname": "def get_temp_dimname(dims: Container[Hashable], new_dim: Hashable) -> Hashable:\n    while new_dim in dims:\n        new_dim = '_' + str(new_dim)\n    return new_dim",
    ".xarray.core._reductions.py@@DataArrayReductions.all": "def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.array_all, dim=dim, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.common.py@@AbstractArray.__bool__": "def __bool__(self: Any) -> bool:\n    return bool(self.values)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__mul__": "def __mul__(self, other):\n    return self._binary_op(other, operator.mul)",
    ".xarray.core.dataset.py@@Dataset.__contains__": "def __contains__(self, key: object) -> bool:\n    return key in self._variables",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__eq__": "def __eq__(self, other):\n    return self._binary_op(other, nputils.array_eq)",
    ".xarray.core.nputils.py@@array_eq": "def array_eq(self, other):\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'elementwise comparison failed')\n        return _ensure_bool_is_ndarray(self == other, self, other)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__eq__": "def __eq__(self, other):\n    return self._binary_op(other, nputils.array_eq)",
    ".xarray.core.nputils.py@@_ensure_bool_is_ndarray": "def _ensure_bool_is_ndarray(result, *args):\n    if isinstance(result, bool):\n        shape = np.broadcast(*args).shape\n        constructor = np.ones if result else np.zeros\n        result = constructor(shape, dtype=bool)\n    return result",
    ".xarray.core.dataarray.py@@DataArray.pad": "def pad(self: T_DataArray, pad_width: Mapping[Any, int | tuple[int, int]] | None=None, mode: PadModeOptions='constant', stat_length: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None=None, constant_values: float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None=None, end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None=None, reflect_type: PadReflectOptions=None, **pad_width_kwargs: Any) -> T_DataArray:\n    ds = self._to_temp_dataset().pad(pad_width=pad_width, mode=mode, stat_length=stat_length, constant_values=constant_values, end_values=end_values, reflect_type=reflect_type, **pad_width_kwargs)\n    return self._from_temp_dataset(ds)",
    ".xarray.core.dataset.py@@Dataset.pad": "def pad(self: T_Dataset, pad_width: Mapping[Any, int | tuple[int, int]]=None, mode: PadModeOptions='constant', stat_length: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None=None, constant_values: float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None=None, end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None=None, reflect_type: PadReflectOptions=None, **pad_width_kwargs: Any) -> T_Dataset:\n    pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, 'pad')\n    if mode in ('edge', 'reflect', 'symmetric', 'wrap'):\n        coord_pad_mode = mode\n        coord_pad_options = {'stat_length': stat_length, 'constant_values': constant_values, 'end_values': end_values, 'reflect_type': reflect_type}\n    else:\n        coord_pad_mode = 'constant'\n        coord_pad_options = {}\n    variables = {}\n    xindexes = self.xindexes\n    pad_dims = set(pad_width)\n    indexes = {}\n    for k, idx in xindexes.items():\n        if not pad_dims.intersection(xindexes.get_all_dims(k)):\n            indexes[k] = idx\n    for name, var in self.variables.items():\n        var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n        if not var_pad_width:\n            variables[name] = var\n        elif name in self.data_vars:\n            variables[name] = var.pad(pad_width=var_pad_width, mode=mode, stat_length=stat_length, constant_values=constant_values, end_values=end_values, reflect_type=reflect_type)\n        else:\n            variables[name] = var.pad(pad_width=var_pad_width, mode=coord_pad_mode, **coord_pad_options)\n            if (name,) == var.dims:\n                dim_var = {name: variables[name]}\n                index = PandasIndex.from_variables(dim_var, options={})\n                index_vars = index.create_variables(dim_var)\n                indexes[name] = index\n                variables[name] = index_vars[name]\n    return self._replace_with_new_dims(variables, indexes=indexes)",
    ".xarray.core.indexes.py@@Indexes.get_all_dims": "def get_all_dims(self, key: Hashable, errors: ErrorOptions='raise') -> Mapping[Hashable, int]:\n    from .variable import calculate_dimensions\n    return calculate_dimensions(self.get_all_coords(key, errors=errors))",
    ".xarray.core.dataarray.py@@DataArray.query": "def query(self, queries: Mapping[Any, Any] | None=None, parser: QueryParserOptions='pandas', engine: QueryEngineOptions=None, missing_dims: ErrorOptionsWithWarn='raise', **queries_kwargs: Any) -> DataArray:\n    ds = self._to_dataset_whole(shallow_copy=True)\n    ds = ds.query(queries=queries, parser=parser, engine=engine, missing_dims=missing_dims, **queries_kwargs)\n    return ds[self.name]",
    ".xarray.core.dataset.py@@Dataset.query": "def query(self: T_Dataset, queries: Mapping[Any, Any] | None=None, parser: QueryParserOptions='pandas', engine: QueryEngineOptions=None, missing_dims: ErrorOptionsWithWarn='raise', **queries_kwargs: Any) -> T_Dataset:\n    queries = either_dict_or_kwargs(queries, queries_kwargs, 'query')\n    for dim, expr in queries.items():\n        if not isinstance(expr, str):\n            msg = f'expr for dim {dim} must be a string to be evaluated, {type(expr)} given'\n            raise ValueError(msg)\n    indexers = {dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine) for dim, expr in queries.items()}\n    return self.isel(indexers, missing_dims=missing_dims)",
    ".xarray.core.common.py@@DataWithCoords.isin": "def isin(self: T_DataWithCoords, test_elements: Any) -> T_DataWithCoords:\n    from .computation import apply_ufunc\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    from .variable import Variable\n    if isinstance(test_elements, Dataset):\n        raise TypeError('isin() argument must be convertible to an array: {}'.format(test_elements))\n    elif isinstance(test_elements, (Variable, DataArray)):\n        test_elements = test_elements.data\n    return apply_ufunc(duck_array_ops.isin, self, kwargs=dict(test_elements=test_elements), dask='allowed')",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__init__": "def __init__(self, array):\n    self.array = array",
    ".xarray.core.indexing.py@@DaskIndexingAdapter.__getitem__": "def __getitem__(self, key):\n    if not isinstance(key, VectorizedIndexer):\n        rewritten_indexer = False\n        new_indexer = []\n        for idim, k in enumerate(key.tuple):\n            if isinstance(k, Iterable) and duck_array_ops.array_equiv(k, np.arange(self.array.shape[idim])):\n                new_indexer.append(slice(None))\n                rewritten_indexer = True\n            else:\n                new_indexer.append(k)\n        if rewritten_indexer:\n            key = type(key)(tuple(new_indexer))\n    if isinstance(key, BasicIndexer):\n        return self.array[key.tuple]\n    elif isinstance(key, VectorizedIndexer):\n        return self.array.vindex[key.tuple]\n    else:\n        assert isinstance(key, OuterIndexer)\n        key = key.tuple\n        try:\n            return self.array[key]\n        except NotImplementedError:\n            value = self.array\n            for axis, subkey in reversed(list(enumerate(key))):\n                value = value[(slice(None),) * axis + (subkey,)]\n            return value",
    ".xarray.core.dataset.py@@_get_func_args": "def _get_func_args(func, param_names):\n    try:\n        func_args = inspect.signature(func).parameters\n    except ValueError:\n        func_args = {}\n        if not param_names:\n            raise ValueError('Unable to inspect `func` signature, and `param_names` was not provided.')\n    if param_names:\n        params = param_names\n    else:\n        params = list(func_args)[1:]\n        if any([p.kind in [p.VAR_POSITIONAL, p.VAR_KEYWORD] for p in func_args.values()]):\n            raise ValueError('`param_names` must be provided because `func` takes variable length arguments.')\n    return (params, func_args)",
    ".xarray.core.dataset.py@@_initialize_curvefit_params": "def _initialize_curvefit_params(params, p0, bounds, func_args):\n\n    def _initialize_feasible(lb, ub):\n        lb_finite = np.isfinite(lb)\n        ub_finite = np.isfinite(ub)\n        p0 = np.nansum([0.5 * (lb + ub) * int(lb_finite & ub_finite), (lb + 1) * int(lb_finite & ~ub_finite), (ub - 1) * int(~lb_finite & ub_finite)])\n        return p0\n    param_defaults = {p: 1 for p in params}\n    bounds_defaults = {p: (-np.inf, np.inf) for p in params}\n    for p in params:\n        if p in func_args and func_args[p].default is not func_args[p].empty:\n            param_defaults[p] = func_args[p].default\n        if p in bounds:\n            bounds_defaults[p] = tuple(bounds[p])\n            if param_defaults[p] < bounds[p][0] or param_defaults[p] > bounds[p][1]:\n                param_defaults[p] = _initialize_feasible(bounds[p][0], bounds[p][1])\n        if p in p0:\n            param_defaults[p] = p0[p]\n    return (param_defaults, bounds_defaults)",
    ".xarray.core.dataset.py@@_initialize_feasible": "def _initialize_feasible(lb, ub):\n    lb_finite = np.isfinite(lb)\n    ub_finite = np.isfinite(ub)\n    p0 = np.nansum([0.5 * (lb + ub) * int(lb_finite & ub_finite), (lb + 1) * int(lb_finite & ~ub_finite), (ub - 1) * int(~lb_finite & ub_finite)])\n    return p0",
    ".xarray.core.computation.py@@_UFuncSignature.dims_map": "def dims_map(self):\n    return {core_dim: f'dim{n}' for n, core_dim in enumerate(sorted(self.all_core_dims))}",
    ".xarray.core.computation.py@@func": "def func(*arrays):\n    import dask.array as da\n    res = da.apply_gufunc(numpy_func, signature.to_gufunc_string(exclude_dims), *arrays, vectorize=vectorize, output_dtypes=output_dtypes, **dask_gufunc_kwargs)\n    return res",
    ".xarray.core.computation.py@@_UFuncSignature.to_gufunc_string": "def to_gufunc_string(self, exclude_dims=frozenset()):\n    input_core_dims = [[self.dims_map[dim] for dim in core_dims] for core_dims in self.input_core_dims]\n    output_core_dims = [[self.dims_map[dim] for dim in core_dims] for core_dims in self.output_core_dims]\n    if exclude_dims:\n        exclude_dims = [self.dims_map[dim] for dim in exclude_dims]\n        counter = Counter()\n\n        def _enumerate(dim):\n            if dim in exclude_dims:\n                n = counter[dim]\n                counter.update([dim])\n                dim = f'{dim}_{n}'\n            return dim\n        input_core_dims = [[_enumerate(dim) for dim in arg] for arg in input_core_dims]\n    alt_signature = type(self)(input_core_dims, output_core_dims)\n    return str(alt_signature)",
    ".xarray.core.computation.py@@_UFuncSignature._enumerate": "def _enumerate(dim):\n    if dim in exclude_dims:\n        n = counter[dim]\n        counter.update([dim])\n        dim = f'{dim}_{n}'\n    return dim",
    ".xarray.core.computation.py@@_UFuncSignature.__str__": "def __str__(self):\n    lhs = ','.join(('({})'.format(','.join(dims)) for dims in self.input_core_dims))\n    rhs = ','.join(('({})'.format(','.join(dims)) for dims in self.output_core_dims))\n    return f'{lhs}->{rhs}'",
    ".xarray.core.dataset.py@@Dataset._wrapper": "def _wrapper(Y, *coords_, **kwargs):\n    x = np.vstack([c.ravel() for c in coords_])\n    y = Y.ravel()\n    if skipna:\n        mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)\n        x = x[:, mask]\n        y = y[mask]\n        if not len(y):\n            popt = np.full([n_params], np.nan)\n            pcov = np.full([n_params, n_params], np.nan)\n            return (popt, pcov)\n    x = np.squeeze(x)\n    popt, pcov = curve_fit(func, x, y, **kwargs)\n    return (popt, pcov)",
    ".xarray.core.computation.py@@_vectorize": "def _vectorize(func, signature, output_dtypes, exclude_dims):\n    if signature.all_core_dims:\n        func = np.vectorize(func, otypes=output_dtypes, signature=signature.to_gufunc_string(exclude_dims))\n    else:\n        func = np.vectorize(func, otypes=output_dtypes)\n    return func",
    ".xarray.core.dataarray.py@@DataArray.argmin": "def argmin(self, dim: Dims | ellipsis=None, axis: int | None=None, keep_attrs: bool | None=None, skipna: bool | None=None) -> DataArray | dict[Hashable, DataArray]:\n    result = self.variable.argmin(dim, axis, keep_attrs, skipna)\n    if isinstance(result, dict):\n        return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n    else:\n        return self._replace_maybe_drop_dims(result)",
    ".xarray.core.nanops.py@@nanargmin": "def nanargmin(a, axis=None):\n    if a.dtype.kind == 'O':\n        fill_value = dtypes.get_pos_infinity(a.dtype)\n        return _nan_argminmax_object('argmin', fill_value, a, axis=axis)\n    return nputils.nanargmin(a, axis=axis)",
    ".xarray.core.nputils.py@@f": "def f(values, axis=None, **kwargs):\n    dtype = kwargs.get('dtype', None)\n    bn_func = getattr(bn, name, None)\n    if _USE_BOTTLENECK and OPTIONS['use_bottleneck'] and isinstance(values, np.ndarray) and (bn_func is not None) and (not isinstance(axis, tuple)) and (values.dtype.kind in 'uifc') and values.dtype.isnative and (dtype is None or np.dtype(dtype) == values.dtype):\n        kwargs.pop('dtype', None)\n        result = bn_func(values, axis=axis, **kwargs)\n    else:\n        result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n    return result",
    ".xarray.core.dtypes.py@@get_pos_infinity": "def get_pos_infinity(dtype, max_for_int=False):\n    if issubclass(dtype.type, np.floating):\n        return np.inf\n    if issubclass(dtype.type, np.integer):\n        if max_for_int:\n            return np.iinfo(dtype).max\n        else:\n            return np.inf\n    if issubclass(dtype.type, np.complexfloating):\n        return np.inf + 1j * np.inf\n    return INF",
    ".xarray.core.nanops.py@@_nan_argminmax_object": "def _nan_argminmax_object(func, fill_value, value, axis=None, **kwargs):\n    valid_count = count(value, axis=axis)\n    value = fillna(value, fill_value)\n    data = getattr(np, func)(value, axis=axis, **kwargs)\n    if (valid_count == 0).any():\n        raise ValueError('All-NaN slice encountered')\n    return data",
    ".xarray.core.dtypes.py@@AlwaysGreaterThan.__gt__": "def __gt__(self, other):\n    return True",
    ".xarray.core.dataarray.py@@DataArray.argmax": "def argmax(self, dim: Dims | ellipsis=None, axis: int | None=None, keep_attrs: bool | None=None, skipna: bool | None=None) -> DataArray | dict[Hashable, DataArray]:\n    result = self.variable.argmax(dim, axis, keep_attrs, skipna)\n    if isinstance(result, dict):\n        return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n    else:\n        return self._replace_maybe_drop_dims(result)",
    ".xarray.core.nanops.py@@nanargmax": "def nanargmax(a, axis=None):\n    if a.dtype.kind == 'O':\n        fill_value = dtypes.get_neg_infinity(a.dtype)\n        return _nan_argminmax_object('argmax', fill_value, a, axis=axis)\n    return nputils.nanargmax(a, axis=axis)",
    ".xarray.core.dtypes.py@@get_neg_infinity": "def get_neg_infinity(dtype, min_for_int=False):\n    if issubclass(dtype.type, np.floating):\n        return -np.inf\n    if issubclass(dtype.type, np.integer):\n        if min_for_int:\n            return np.iinfo(dtype).min\n        else:\n            return -np.inf\n    if issubclass(dtype.type, np.complexfloating):\n        return -np.inf - 1j * np.inf\n    return NINF",
    ".xarray.core.dtypes.py@@AlwaysLessThan.__lt__": "def __lt__(self, other):\n    return True",
    ".xarray.core.dataarray.py@@DataArray.idxmin": "def idxmin(self, dim: Hashable | None=None, skipna: bool | None=None, fill_value: Any=dtypes.NA, keep_attrs: bool | None=None) -> DataArray:\n    return computation._calc_idxminmax(array=self, func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs), dim=dim, skipna=skipna, fill_value=fill_value, keep_attrs=keep_attrs)",
    ".xarray.core.computation.py@@_calc_idxminmax": "def _calc_idxminmax(*, array, func: Callable, dim: Hashable=None, skipna: bool=None, fill_value: Any=dtypes.NA, keep_attrs: bool=None):\n    if not array.ndim:\n        raise ValueError('This function does not apply for scalars')\n    if dim is not None:\n        pass\n    elif array.ndim == 1:\n        dim = array.dims[0]\n    else:\n        raise ValueError(\"Must supply 'dim' argument for multidimensional arrays\")\n    if dim not in array.dims:\n        raise KeyError(f'Dimension \"{dim}\" not in dimension')\n    if dim not in array.coords:\n        raise KeyError(f'Dimension \"{dim}\" does not have coordinates')\n    na_dtypes = 'cfO'\n    if skipna or (skipna is None and array.dtype.kind in na_dtypes):\n        allna = array.isnull().all(dim)\n        array = array.where(~allna, 0)\n    indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)\n    if is_duck_dask_array(array.data):\n        import dask.array\n        chunks = dict(zip(array.dims, array.chunks))\n        dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])\n        res = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))\n        res.name = dim\n    else:\n        res = array[dim][indx,]\n        del res.coords[dim]\n    if skipna or (skipna is None and array.dtype.kind in na_dtypes):\n        res = res.where(~allna, fill_value)\n    res.attrs = indx.attrs\n    return res",
    ".xarray.core.dataarray.py@@DataArray.chunks": "def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n    return self.variable.chunks",
    ".xarray.core.duck_array_ops.py@@where_method": "def where_method(data, cond, other=dtypes.NA):\n    if other is dtypes.NA:\n        other = dtypes.get_fill_value(data.dtype)\n    return where(cond, data, other)",
    ".xarray.core.dataarray.py@@DataArray.idxmax": "def idxmax(self, dim: Hashable=None, skipna: bool | None=None, fill_value: Any=dtypes.NA, keep_attrs: bool | None=None) -> DataArray:\n    return computation._calc_idxminmax(array=self, func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs), dim=dim, skipna=skipna, fill_value=fill_value, keep_attrs=keep_attrs)",
    ".xarray.core._reductions.py@@DataArrayReductions.min": "def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.nanops.py@@nanmin": "def nanmin(a, axis=None, out=None):\n    if a.dtype.kind == 'O':\n        return _nan_minmax_object('min', dtypes.get_pos_infinity(a.dtype), a, axis)\n    return nputils.nanmin(a, axis=axis)",
    ".xarray.core.nanops.py@@_nan_minmax_object": "def _nan_minmax_object(func, fill_value, value, axis=None, **kwargs):\n    valid_count = count(value, axis=axis)\n    filled_value = fillna(value, fill_value)\n    data = getattr(np, func)(filled_value, axis=axis, **kwargs)\n    if not hasattr(data, 'dtype'):\n        data = fill_value if valid_count == 0 else data\n        return utils.to_0d_object_array(data)\n    return where_method(data, valid_count != 0)",
    ".xarray.core._reductions.py@@DataArrayReductions.max": "def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:\n    return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)",
    ".xarray.core.nanops.py@@nanmax": "def nanmax(a, axis=None, out=None):\n    if a.dtype.kind == 'O':\n        return _nan_minmax_object('max', dtypes.get_neg_infinity(a.dtype), a, axis)\n    return nputils.nanmax(a, axis=axis)",
    ".conftest.py@@pytest_runtest_setup": "def pytest_runtest_setup(item):\n    if 'flaky' in item.keywords and (not item.config.getoption('--run-flaky')):\n        pytest.skip('set --run-flaky option to run flaky tests')\n    if 'network' in item.keywords and (not item.config.getoption('--run-network-tests')):\n        pytest.skip('set --run-network-tests to run test requiring an internet connection')",
    ".conftest.py@@add_standard_imports": "def add_standard_imports(doctest_namespace, tmpdir):\n    import numpy as np\n    import pandas as pd\n    import xarray as xr\n    doctest_namespace['np'] = np\n    doctest_namespace['pd'] = pd\n    doctest_namespace['xr'] = xr\n    np.random.seed(0)\n    tmpdir.chdir()",
    ".xarray.core.dataset.py@@_maybe_chunk": "def _maybe_chunk(name, var, chunks, token=None, lock=None, name_prefix='xarray-', overwrite_encoded_chunks=False, inline_array=False):\n    from dask.base import tokenize\n    if chunks is not None:\n        chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}\n    if var.ndim:\n        token2 = tokenize(name, token if token else var._data, chunks)\n        name2 = f'{name_prefix}{name}-{token2}'\n        var = var.chunk(chunks, name=name2, lock=lock, inline_array=inline_array)\n        if overwrite_encoded_chunks and var.chunks is not None:\n            var.encoding['chunks'] = tuple((x[0] for x in var.chunks))\n        return var\n    else:\n        return var",
    ".xarray.core.utils.py@@ReprObject.__dask_tokenize__": "def __dask_tokenize__(self):\n    from dask.base import normalize_token\n    return normalize_token((type(self), self._value))",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__gt__": "def __gt__(self, other):\n    return self._binary_op(other, operator.gt)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__gt__": "def __gt__(self, other):\n    return self._binary_op(other, operator.gt)",
    ".xarray.core.dataarray.py@@DataArray.to_iris": "def to_iris(self) -> iris_Cube:\n    from ..convert import to_iris\n    return to_iris(self)",
    ".xarray.convert.py@@to_iris": "def to_iris(dataarray):\n    import iris\n    from iris.fileformats.netcdf import parse_cell_methods\n    dim_coords = []\n    aux_coords = []\n    for coord_name in dataarray.coords:\n        coord = encode(dataarray.coords[coord_name])\n        coord_args = _get_iris_args(coord.attrs)\n        coord_args['var_name'] = coord_name\n        axis = None\n        if coord.dims:\n            axis = dataarray.get_axis_num(coord.dims)\n        if coord_name in dataarray.dims:\n            try:\n                iris_coord = iris.coords.DimCoord(coord.values, **coord_args)\n                dim_coords.append((iris_coord, axis))\n            except ValueError:\n                iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n                aux_coords.append((iris_coord, axis))\n        else:\n            iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n            aux_coords.append((iris_coord, axis))\n    args = _get_iris_args(dataarray.attrs)\n    args['var_name'] = dataarray.name\n    args['dim_coords_and_dims'] = dim_coords\n    args['aux_coords_and_dims'] = aux_coords\n    if 'cell_methods' in dataarray.attrs:\n        args['cell_methods'] = parse_cell_methods(dataarray.attrs['cell_methods'])\n    masked_data = duck_array_ops.masked_invalid(dataarray.data)\n    cube = iris.cube.Cube(masked_data, **args)\n    return cube",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__ge__": "def __ge__(self, other):\n    return self._binary_op(other, operator.ge)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__ge__": "def __ge__(self, other):\n    return self._binary_op(other, operator.ge)",
    ".xarray.core._typed_ops.py@@DataArrayOpsMixin.__le__": "def __le__(self, other):\n    return self._binary_op(other, operator.le)",
    ".xarray.core._typed_ops.py@@VariableOpsMixin.__le__": "def __le__(self, other):\n    return self._binary_op(other, operator.le)",
    ".xarray.core.dataarray.py@@DataArray.drop_duplicates": "def drop_duplicates(self: T_DataArray, dim: Hashable | Iterable[Hashable], keep: Literal['first', 'last', False]='first') -> T_DataArray:\n    deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)\n    return self._from_temp_dataset(deduplicated)",
    ".xarray.core.dataset.py@@Dataset.drop_duplicates": "def drop_duplicates(self: T_Dataset, dim: Hashable | Iterable[Hashable], keep: Literal['first', 'last', False]='first') -> T_Dataset:\n    if isinstance(dim, str):\n        dims: Iterable = (dim,)\n    elif dim is ...:\n        dims = self.dims\n    elif not isinstance(dim, Iterable):\n        dims = [dim]\n    else:\n        dims = dim\n    missing_dims = set(dims) - set(self.dims)\n    if missing_dims:\n        raise ValueError(f\"'{missing_dims}' not found in dimensions\")\n    indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}\n    return self.isel(indexes)",
    ".xarray.core.dataarray.py@@DataArray.as_numpy": "def as_numpy(self: T_DataArray) -> T_DataArray:\n    coords = {k: v.as_numpy() for k, v in self._coords.items()}\n    return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)"
}