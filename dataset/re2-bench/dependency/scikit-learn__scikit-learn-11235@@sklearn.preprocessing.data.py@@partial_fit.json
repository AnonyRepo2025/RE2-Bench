{
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    array_orig = array\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of strings will be interpreted as decimal numbers if parameter 'dtype' is 'numeric'. It is recommended that you convert the array to type np.float64 before passing it to check_array.\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(X.sum()):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.validation.py@@_shape_repr": "def _shape_repr(shape):\n    if len(shape) == 0:\n        return '()'\n    joined = ', '.join(('%d' % e for e in shape))\n    if len(shape) == 1:\n        joined += ','\n    return '(%s)' % joined",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        return x.shape[0]\n    else:\n        return len(x)",
    ".sklearn.preprocessing.data.py@@_handle_zeros_in_scale": "def _handle_zeros_in_scale(scale, copy=True):\n    if np.isscalar(scale):\n        if scale == 0.0:\n            scale = 1.0\n        return scale\n    elif isinstance(scale, np.ndarray):\n        if copy:\n            scale = scale.copy()\n        scale[scale == 0.0] = 1.0\n        return scale",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, six.string_types):\n        accept_sparse = [accept_sparse]\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.sparsefuncs.py@@min_max_axis": "def min_max_axis(X, axis, ignore_nan=False):\n    if isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix):\n        if ignore_nan:\n            return _sparse_nan_min_max(X, axis=axis)\n        else:\n            return _sparse_min_max(X, axis=axis)\n    else:\n        _raise_typeerror(X)",
    ".sklearn.utils.sparsefuncs.py@@_sparse_min_max": "def _sparse_min_max(X, axis):\n    return (_sparse_min_or_max(X, axis, np.minimum), _sparse_min_or_max(X, axis, np.maximum))",
    ".sklearn.utils.sparsefuncs.py@@_sparse_min_or_max": "def _sparse_min_or_max(X, axis, min_or_max):\n    if axis is None:\n        if 0 in X.shape:\n            raise ValueError('zero-size array to reduction operation')\n        zero = X.dtype.type(0)\n        if X.nnz == 0:\n            return zero\n        m = min_or_max.reduce(X.data.ravel())\n        if X.nnz != np.product(X.shape):\n            m = min_or_max(zero, m)\n        return m\n    if axis < 0:\n        axis += 2\n    if axis == 0 or axis == 1:\n        return _min_or_max_axis(X, axis, min_or_max)\n    else:\n        raise ValueError('invalid axis, use 0 for rows, or 1 for columns')",
    ".sklearn.utils.sparsefuncs.py@@_min_or_max_axis": "def _min_or_max_axis(X, axis, min_or_max):\n    N = X.shape[axis]\n    if N == 0:\n        raise ValueError('zero-size array to reduction operation')\n    M = X.shape[1 - axis]\n    mat = X.tocsc() if axis == 0 else X.tocsr()\n    mat.sum_duplicates()\n    major_index, value = _minor_reduce(mat, min_or_max)\n    not_full = np.diff(mat.indptr)[major_index] < N\n    value[not_full] = min_or_max(value[not_full], 0)\n    mask = value != 0\n    major_index = np.compress(mask, major_index)\n    value = np.compress(mask, value)\n    if axis == 0:\n        res = sp.coo_matrix((value, (np.zeros(len(value)), major_index)), dtype=X.dtype, shape=(1, M))\n    else:\n        res = sp.coo_matrix((value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1))\n    return res.A.ravel()",
    ".sklearn.utils.sparsefuncs.py@@_minor_reduce": "def _minor_reduce(X, ufunc):\n    major_index = np.flatnonzero(np.diff(X.indptr))\n    value = ufunc.reduceat(X.data, X.indptr[major_index])\n    return (major_index, value)"
}