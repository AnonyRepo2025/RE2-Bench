{
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.feature_extraction.text.py@@CountVectorizer.transform": "def transform(self, raw_documents):\n    if isinstance(raw_documents, str):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    if not hasattr(self, 'vocabulary_'):\n        self._validate_vocabulary()\n    self._check_vocabulary()\n    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n    if self.binary:\n        X.data.fill(1)\n    return X",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._check_vocabulary": "def _check_vocabulary(self):\n    msg = \"%(name)s - Vocabulary wasn't fitted.\"\n    (check_is_fitted(self, 'vocabulary_', msg=msg),)\n    if len(self.vocabulary_) == 0:\n        raise ValueError('Vocabulary is empty')",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._count_vocab": "def _count_vocab(self, raw_documents, fixed_vocab):\n    if fixed_vocab:\n        vocabulary = self.vocabulary_\n    else:\n        vocabulary = defaultdict()\n        vocabulary.default_factory = vocabulary.__len__\n    analyze = self.build_analyzer()\n    j_indices = []\n    indptr = []\n    values = _make_int_array()\n    indptr.append(0)\n    for doc in raw_documents:\n        feature_counter = {}\n        for feature in analyze(doc):\n            try:\n                feature_idx = vocabulary[feature]\n                if feature_idx not in feature_counter:\n                    feature_counter[feature_idx] = 1\n                else:\n                    feature_counter[feature_idx] += 1\n            except KeyError:\n                continue\n        j_indices.extend(feature_counter.keys())\n        values.extend(feature_counter.values())\n        indptr.append(len(j_indices))\n    if not fixed_vocab:\n        vocabulary = dict(vocabulary)\n        if not vocabulary:\n            raise ValueError('empty vocabulary; perhaps the documents only contain stop words')\n    if indptr[-1] > 2147483648:\n        if _IS_32BIT:\n            raise ValueError('sparse CSR array has {} non-zero elements and requires 64 bit indexing, which is unsupported with 32 bit Python.'.format(indptr[-1]))\n        indices_dtype = np.int64\n    else:\n        indices_dtype = np.int32\n    j_indices = np.asarray(j_indices, dtype=indices_dtype)\n    indptr = np.asarray(indptr, dtype=indices_dtype)\n    values = np.frombuffer(values, dtype=np.intc)\n    X = sp.csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, len(vocabulary)), dtype=self.dtype)\n    X.sort_indices()\n    return (vocabulary, X)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_analyzer": "def build_analyzer(self):\n    if callable(self.analyzer):\n        if self.input in ['file', 'filename']:\n            self._validate_custom_analyzer()\n        return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n    preprocess = self.build_preprocessor()\n    if self.analyzer == 'char':\n        return partial(_analyze, ngrams=self._char_ngrams, preprocessor=preprocess, decoder=self.decode)\n    elif self.analyzer == 'char_wb':\n        return partial(_analyze, ngrams=self._char_wb_ngrams, preprocessor=preprocess, decoder=self.decode)\n    elif self.analyzer == 'word':\n        stop_words = self.get_stop_words()\n        tokenize = self.build_tokenizer()\n        self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n        return partial(_analyze, ngrams=self._word_ngrams, tokenizer=tokenize, preprocessor=preprocess, decoder=self.decode, stop_words=stop_words)\n    else:\n        raise ValueError('%s is not a valid tokenization scheme/analyzer' % self.analyzer)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_preprocessor": "def build_preprocessor(self):\n    if self.preprocessor is not None:\n        return self.preprocessor\n    if not self.strip_accents:\n        strip_accents = None\n    elif callable(self.strip_accents):\n        strip_accents = self.strip_accents\n    elif self.strip_accents == 'ascii':\n        strip_accents = strip_accents_ascii\n    elif self.strip_accents == 'unicode':\n        strip_accents = strip_accents_unicode\n    else:\n        raise ValueError('Invalid value for \"strip_accents\": %s' % self.strip_accents)\n    return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.get_stop_words": "def get_stop_words(self):\n    return _check_stop_list(self.stop_words)",
    ".sklearn.feature_extraction.text.py@@_check_stop_list": "def _check_stop_list(stop):\n    if stop == 'english':\n        return ENGLISH_STOP_WORDS\n    elif isinstance(stop, str):\n        raise ValueError('not a built-in stop list: %s' % stop)\n    elif stop is None:\n        return None\n    else:\n        return frozenset(stop)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_tokenizer": "def build_tokenizer(self):\n    if self.tokenizer is not None:\n        return self.tokenizer\n    token_pattern = re.compile(self.token_pattern)\n    return token_pattern.findall",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._check_stop_words_consistency": "def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n    if id(self.stop_words) == getattr(self, '_stop_words_id', None):\n        return None\n    try:\n        inconsistent = set()\n        for w in stop_words or ():\n            tokens = list(tokenize(preprocess(w)))\n            for token in tokens:\n                if token not in stop_words:\n                    inconsistent.add(token)\n        self._stop_words_id = id(self.stop_words)\n        if inconsistent:\n            warnings.warn('Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens %r not in stop_words.' % sorted(inconsistent))\n        return not inconsistent\n    except Exception:\n        self._stop_words_id = id(self.stop_words)\n        return 'error'",
    ".sklearn.feature_extraction.text.py@@_make_int_array": "def _make_int_array():\n    return array.array(str('i'))",
    ".sklearn.feature_extraction.text.py@@_analyze": "def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None, preprocessor=None, decoder=None, stop_words=None):\n    if decoder is not None:\n        doc = decoder(doc)\n    if analyzer is not None:\n        doc = analyzer(doc)\n    else:\n        if preprocessor is not None:\n            doc = preprocessor(doc)\n        if tokenizer is not None:\n            doc = tokenizer(doc)\n        if ngrams is not None:\n            if stop_words is not None:\n                doc = ngrams(doc, stop_words)\n            else:\n                doc = ngrams(doc)\n    return doc",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.decode": "def decode(self, doc):\n    if self.input == 'filename':\n        with open(doc, 'rb') as fh:\n            doc = fh.read()\n    elif self.input == 'file':\n        doc = doc.read()\n    if isinstance(doc, bytes):\n        doc = doc.decode(self.encoding, self.decode_error)\n    if doc is np.nan:\n        raise ValueError('np.nan is an invalid document, expected byte or unicode string.')\n    return doc",
    ".sklearn.feature_extraction.text.py@@_preprocess": "def _preprocess(doc, accent_function=None, lower=False):\n    if lower:\n        doc = doc.lower()\n    if accent_function is not None:\n        doc = accent_function(doc)\n    return doc",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._word_ngrams": "def _word_ngrams(self, tokens, stop_words=None):\n    if stop_words is not None:\n        tokens = [w for w in tokens if w not in stop_words]\n    min_n, max_n = self.ngram_range\n    if max_n != 1:\n        original_tokens = tokens\n        if min_n == 1:\n            tokens = list(original_tokens)\n            min_n += 1\n        else:\n            tokens = []\n        n_original_tokens = len(original_tokens)\n        tokens_append = tokens.append\n        space_join = ' '.join\n        for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n            for i in range(n_original_tokens - n + 1):\n                tokens_append(space_join(original_tokens[i:i + n]))\n    return tokens",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.__init__": "def __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', n_features=2 ** 20, binary=False, norm='l2', alternate_sign=True, dtype=np.float64):\n    self.input = input\n    self.encoding = encoding\n    self.decode_error = decode_error\n    self.strip_accents = strip_accents\n    self.preprocessor = preprocessor\n    self.tokenizer = tokenizer\n    self.analyzer = analyzer\n    self.lowercase = lowercase\n    self.token_pattern = token_pattern\n    self.stop_words = stop_words\n    self.n_features = n_features\n    self.ngram_range = ngram_range\n    self.binary = binary\n    self.norm = norm\n    self.alternate_sign = alternate_sign\n    self.dtype = dtype",
    ".sklearn.utils.testing.py@@assert_raise_message": "def assert_raise_message(exceptions, message, function, *args, **kwargs):\n    try:\n        function(*args, **kwargs)\n    except exceptions as e:\n        error_message = str(e)\n        if message not in error_message:\n            raise AssertionError('Error message does not include the expected string: %r. Observed error message: %r' % (message, error_message))\n    else:\n        if isinstance(exceptions, tuple):\n            names = ' or '.join((e.__name__ for e in exceptions))\n        else:\n            names = exceptions.__name__\n        raise AssertionError('%s not raised by %s' % (names, function.__name__))",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.fit_transform": "def fit_transform(self, X, y=None):\n    return self.fit(X, y).transform(X)",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.fit": "def fit(self, X, y=None):\n    if isinstance(X, str):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    self._get_hasher().fit(X, y=y)\n    return self",
    ".sklearn.feature_extraction.text.py@@HashingVectorizer.transform": "def transform(self, X):\n    if isinstance(X, str):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    analyzer = self.build_analyzer()\n    X = self._get_hasher().transform((analyzer(doc) for doc in X))\n    if self.binary:\n        X.data.fill(1)\n    if self.norm is not None:\n        X = normalize(X, norm=self.norm, copy=False)\n    return X",
    ".sklearn.feature_extraction.text.py@@TfidfTransformer.__init__": "def __init__(self, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False):\n    self.norm = norm\n    self.use_idf = use_idf\n    self.smooth_idf = smooth_idf\n    self.sublinear_tf = sublinear_tf",
    ".sklearn.base.py@@TransformerMixin.fit_transform": "def fit_transform(self, X, y=None, **fit_params):\n    if y is None:\n        return self.fit(X, **fit_params).transform(X)\n    else:\n        return self.fit(X, y, **fit_params).transform(X)",
    ".sklearn.feature_extraction.text.py@@TfidfTransformer.fit": "def fit(self, X, y=None):\n    X = check_array(X, accept_sparse=('csr', 'csc'))\n    if not sp.issparse(X):\n        X = sp.csr_matrix(X)\n    dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n    if self.use_idf:\n        n_samples, n_features = X.shape\n        df = _document_frequency(X)\n        df = df.astype(dtype, **_astype_copy_false(df))\n        df += int(self.smooth_idf)\n        n_samples += int(self.smooth_idf)\n        idf = np.log(n_samples / df) + 1\n        self._idf_diag = sp.diags(idf, offsets=0, shape=(n_features, n_features), format='csr', dtype=dtype)\n    return self",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.feature_extraction.text.py@@_document_frequency": "def _document_frequency(X):\n    if sp.isspmatrix_csr(X):\n        return np.bincount(X.indices, minlength=X.shape[1])\n    else:\n        return np.diff(X.indptr)",
    ".sklearn.utils.fixes.py@@_astype_copy_false": "def _astype_copy_false(X):\n    if sp_version >= (1, 1) or not sp.issparse(X):\n        return {'copy': False}\n    else:\n        return {}",
    ".sklearn.feature_extraction.text.py@@TfidfTransformer.transform": "def transform(self, X, copy=True):\n    X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n    if not sp.issparse(X):\n        X = sp.csr_matrix(X, dtype=np.float64)\n    n_samples, n_features = X.shape\n    if self.sublinear_tf:\n        np.log(X.data, X.data)\n        X.data += 1\n    if self.use_idf:\n        check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n        expected_n_features = self._idf_diag.shape[0]\n        if n_features != expected_n_features:\n            raise ValueError('Input has n_features=%d while the model has been trained with n_features=%d' % (n_features, expected_n_features))\n        X = X * self._idf_diag\n    if self.norm:\n        X = normalize(X, norm=self.norm, copy=False)\n    return X",
    ".sklearn.preprocessing.data.py@@normalize": "def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):\n    if norm not in ('l1', 'l2', 'max'):\n        raise ValueError(\"'%s' is not a supported norm\" % norm)\n    if axis == 0:\n        sparse_format = 'csc'\n    elif axis == 1:\n        sparse_format = 'csr'\n    else:\n        raise ValueError(\"'%d' is not a supported axis\" % axis)\n    X = check_array(X, sparse_format, copy=copy, estimator='the normalize function', dtype=FLOAT_DTYPES)\n    if axis == 0:\n        X = X.T\n    if sparse.issparse(X):\n        if return_norm and norm in ('l1', 'l2'):\n            raise NotImplementedError(\"return_norm=True is not implemented for sparse matrices with norm 'l1' or norm 'l2'\")\n        if norm == 'l1':\n            inplace_csr_row_normalize_l1(X)\n        elif norm == 'l2':\n            inplace_csr_row_normalize_l2(X)\n        elif norm == 'max':\n            _, norms = min_max_axis(X, 1)\n            norms_elementwise = norms.repeat(np.diff(X.indptr))\n            mask = norms_elementwise != 0\n            X.data[mask] /= norms_elementwise[mask]\n    else:\n        if norm == 'l1':\n            norms = np.abs(X).sum(axis=1)\n        elif norm == 'l2':\n            norms = row_norms(X)\n        elif norm == 'max':\n            norms = np.max(X, axis=1)\n        norms = _handle_zeros_in_scale(norms, copy=False)\n        X /= norms[:, np.newaxis]\n    if axis == 0:\n        X = X.T\n    if return_norm:\n        return (X, norms)\n    else:\n        return X"
}