{
    ".sklearn.base.py@@BaseEstimator._get_tags": "def _get_tags(self):\n    collected_tags = {}\n    for base_class in reversed(inspect.getmro(self.__class__)):\n        if hasattr(base_class, '_more_tags'):\n            more_tags = base_class._more_tags(self)\n            collected_tags.update(more_tags)\n    return collected_tags",
    ".sklearn.base.py@@BaseEstimator._more_tags": "def _more_tags(self):\n    return _DEFAULT_TAGS",
    ".sklearn.feature_selection._from_model.py@@SelectFromModel._more_tags": "def _more_tags(self):\n    estimator_tags = self.estimator._get_tags()\n    return {'allow_nan': estimator_tags.get('allow_nan', True)}",
    ".sklearn.base.py@@MultiOutputMixin._more_tags": "def _more_tags(self):\n    return {'multioutput': True}",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", FutureWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n        if all((isinstance(dtype, np.dtype) for dtype in dtypes_orig)):\n            dtype_orig = np.result_type(*array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)\n                    array = array.astype(dtype, casting='unsafe', copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, msg_dtype if msg_dtype is not None else X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.feature_selection._from_model.py@@SelectFromModel._get_support_mask": "def _get_support_mask(self):\n    if self.prefit:\n        estimator = self.estimator\n    elif hasattr(self, 'estimator_'):\n        estimator = self.estimator_\n    else:\n        raise ValueError('Either fit the model before transform or set \"prefit=True\" while passing the fitted estimator to the constructor.')\n    scores = _get_feature_importances(estimator, self.norm_order)\n    threshold = _calculate_threshold(estimator, scores, self.threshold)\n    if self.max_features is not None:\n        mask = np.zeros_like(scores, dtype=bool)\n        candidate_indices = np.argsort(-scores, kind='mergesort')[:self.max_features]\n        mask[candidate_indices] = True\n    else:\n        mask = np.ones_like(scores, dtype=bool)\n    mask[scores < threshold] = False\n    return mask",
    ".sklearn.feature_selection._from_model.py@@_get_feature_importances": "def _get_feature_importances(estimator, norm_order=1):\n    importances = getattr(estimator, 'feature_importances_', None)\n    coef_ = getattr(estimator, 'coef_', None)\n    if importances is None and coef_ is not None:\n        if estimator.coef_.ndim == 1:\n            importances = np.abs(coef_)\n        else:\n            importances = np.linalg.norm(coef_, axis=0, ord=norm_order)\n    elif importances is None:\n        raise ValueError('The underlying estimator %s has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.' % estimator.__class__.__name__)\n    return importances",
    ".sklearn.ensemble._forest.py@@BaseForest.feature_importances_": "def feature_importances_(self):\n    check_is_fitted(self)\n    all_importances = Parallel(n_jobs=self.n_jobs, **_joblib_parallel_args(prefer='threads'))((delayed(getattr)(tree, 'feature_importances_') for tree in self.estimators_ if tree.tree_.node_count > 1))\n    if not all_importances:\n        return np.zeros(self.n_features_, dtype=np.float64)\n    all_importances = np.mean(all_importances, axis=0, dtype=np.float64)\n    return all_importances / np.sum(all_importances)",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes='deprecated', msg=None, all_or_any='deprecated'):\n    if attributes != 'deprecated':\n        warnings.warn('Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.', FutureWarning)\n    if all_or_any != 'deprecated':\n        warnings.warn('Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.', FutureWarning)\n    if isclass(estimator):\n        raise TypeError('{} is a class, not an instance.'.format(estimator))\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    attrs = [v for v in vars(estimator) if (v.endswith('_') or v.startswith('_')) and (not v.startswith('__'))]\n    if not attrs:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.utils.fixes.py@@_joblib_parallel_args": "def _joblib_parallel_args(**kwargs):\n    import joblib\n    if joblib.__version__ >= LooseVersion('0.12'):\n        return kwargs\n    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})\n    if extra_args:\n        raise NotImplementedError('unhandled arguments %s with joblib %s' % (list(extra_args), joblib.__version__))\n    args = {}\n    if 'prefer' in kwargs:\n        prefer = kwargs['prefer']\n        if prefer not in ['threads', 'processes', None]:\n            raise ValueError('prefer=%s is not supported' % prefer)\n        args['backend'] = {'threads': 'threading', 'processes': 'multiprocessing', None: None}[prefer]\n    if 'require' in kwargs:\n        require = kwargs['require']\n        if require not in [None, 'sharedmem']:\n            raise ValueError('require=%s is not supported' % require)\n        if require == 'sharedmem':\n            args['backend'] = 'threading'\n    return args",
    ".sklearn.tree._classes.py@@BaseDecisionTree.feature_importances_": "def feature_importances_(self):\n    check_is_fitted(self)\n    return self.tree_.compute_feature_importances()",
    ".sklearn.feature_selection._from_model.py@@_calculate_threshold": "def _calculate_threshold(estimator, importances, threshold):\n    if threshold is None:\n        est_name = estimator.__class__.__name__\n        if hasattr(estimator, 'penalty') and estimator.penalty == 'l1' or 'Lasso' in est_name:\n            threshold = 1e-05\n        else:\n            threshold = 'mean'\n    if isinstance(threshold, str):\n        if '*' in threshold:\n            scale, reference = threshold.split('*')\n            scale = float(scale.strip())\n            reference = reference.strip()\n            if reference == 'median':\n                reference = np.median(importances)\n            elif reference == 'mean':\n                reference = np.mean(importances)\n            else:\n                raise ValueError('Unknown reference: ' + reference)\n            threshold = scale * reference\n        elif threshold == 'median':\n            threshold = np.median(importances)\n        elif threshold == 'mean':\n            threshold = np.mean(importances)\n        else:\n            raise ValueError(\"Expected threshold='mean' or threshold='median' got %s\" % threshold)\n    else:\n        threshold = float(threshold)\n    return threshold",
    ".sklearn.feature_selection._rfe.py@@RFE._more_tags": "def _more_tags(self):\n    estimator_tags = self.estimator._get_tags()\n    return {'poor_score': True, 'allow_nan': estimator_tags.get('allow_nan', True)}",
    ".sklearn.feature_selection._rfe.py@@RFE._get_support_mask": "def _get_support_mask(self):\n    check_is_fitted(self)\n    return self.support_",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.feature_selection._variance_threshold.py@@VarianceThreshold._more_tags": "def _more_tags(self):\n    return {'allow_nan': True}",
    ".sklearn.feature_selection._variance_threshold.py@@VarianceThreshold._get_support_mask": "def _get_support_mask(self):\n    check_is_fitted(self)\n    return self.variances_ > self.threshold"
}