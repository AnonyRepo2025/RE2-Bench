{
    ".requests.cookies.py@@MockRequest.get_origin_req_host": "def get_origin_req_host(self):\n    return self.get_host()",
    ".requests.cookies.py@@MockRequest.get_host": "def get_host(self):\n    return urlparse(self._r.url).netloc",
    ".requests.cookies.py@@MockRequest.get_full_url": "def get_full_url(self):\n    if not self._r.headers.get('Host'):\n        return self._r.url\n    host = self._r.headers['Host']\n    parsed = urlparse(self._r.url)\n    return urlunparse([parsed.scheme, host, parsed.path, parsed.params, parsed.query, parsed.fragment])",
    ".requests.structures.py@@CaseInsensitiveDict.__getitem__": "def __getitem__(self, key):\n    return self._store[key.lower()][1]",
    ".requests.cookies.py@@MockResponse.__init__": "def __init__(self, headers):\n    self._headers = headers",
    ".requests.cookies.py@@MockResponse.info": "def info(self):\n    return self._headers",
    ".requests.cookies.py@@RequestsCookieJar.set_cookie": "def set_cookie(self, cookie, *args, **kwargs):\n    if hasattr(cookie.value, 'startswith') and cookie.value.startswith('\"') and cookie.value.endswith('\"'):\n        cookie.value = cookie.value.replace('\\\\\"', '')\n    return super(RequestsCookieJar, self).set_cookie(cookie, *args, **kwargs)",
    ".requests.cookies.py@@MockRequest.origin_req_host": "def origin_req_host(self):\n    return self.get_origin_req_host()",
    ".requests.structures.py@@CaseInsensitiveDict.__setitem__": "def __setitem__(self, key, value):\n    self._store[key.lower()] = (key, value)",
    ".requests.packages.urllib3._collections.py@@HTTPHeaderDict.__getitem__": "def __getitem__(self, key):\n    val = _dict_getitem(self, key.lower())\n    return ', '.join(val[1:])",
    ".requests.models.py@@PreparedRequest.copy": "def copy(self):\n    p = PreparedRequest()\n    p.method = self.method\n    p.url = self.url\n    p.headers = self.headers.copy() if self.headers is not None else None\n    p._cookies = _copy_cookie_jar(self._cookies)\n    p.body = self.body\n    p.hooks = self.hooks\n    return p",
    ".requests.models.py@@PreparedRequest.__init__": "def __init__(self):\n    self.method = None\n    self.url = None\n    self.headers = None\n    self._cookies = None\n    self.body = None\n    self.hooks = default_hooks()",
    ".requests.cookies.py@@MockRequest.unverifiable": "def unverifiable(self):\n    return self.is_unverifiable()",
    ".requests.cookies.py@@MockRequest.is_unverifiable": "def is_unverifiable(self):\n    return True",
    ".requests.packages.urllib3.util.timeout.py@@Timeout.__init__": "def __init__(self, total=None, connect=_Default, read=_Default):\n    self._connect = self._validate_timeout(connect, 'connect')\n    self._read = self._validate_timeout(read, 'read')\n    self.total = self._validate_timeout(total, 'total')\n    self._start_connect = None",
    ".requests.packages.urllib3.util.timeout.py@@Timeout._validate_timeout": "def _validate_timeout(cls, value, name):\n    if value is _Default:\n        return cls.DEFAULT_TIMEOUT\n    if value is None or value is cls.DEFAULT_TIMEOUT:\n        return value\n    try:\n        float(value)\n    except (TypeError, ValueError):\n        raise ValueError('Timeout value %s was %s, but it must be an int or float.' % (name, value))\n    try:\n        if value < 0:\n            raise ValueError('Attempted to set %s timeout to %s, but the timeout cannot be set to a value less than 0.' % (name, value))\n    except TypeError:\n        raise ValueError('Timeout value %s was %s, but it must be an int or float.' % (name, value))\n    return value",
    ".requests.packages.urllib3._collections.py@@HTTPHeaderDict.add": "def add(self, key, val):\n    key_lower = key.lower()\n    new_vals = (key, val)\n    vals = _dict_setdefault(self, key_lower, new_vals)\n    if new_vals is not vals:\n        if isinstance(vals, list):\n            vals.append(val)\n        else:\n            _dict_setitem(self, key_lower, [vals[0], vals[1], val])",
    ".requests.packages.urllib3.util.response.py@@is_fp_closed": "def is_fp_closed(obj):\n    try:\n        return obj.closed\n    except AttributeError:\n        pass\n    try:\n        return obj.fp is None\n    except AttributeError:\n        pass\n    raise ValueError('Unable to determine whether fp is closed.')",
    ".requests.models.py@@Response.iter_content": "def iter_content(self, chunk_size=1, decode_unicode=False):\n\n    def generate():\n        if hasattr(self.raw, 'stream'):\n            try:\n                for chunk in self.raw.stream(chunk_size, decode_content=True):\n                    yield chunk\n            except ProtocolError as e:\n                raise ChunkedEncodingError(e)\n            except DecodeError as e:\n                raise ContentDecodingError(e)\n            except ReadTimeoutError as e:\n                raise ConnectionError(e)\n        else:\n            while True:\n                chunk = self.raw.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n        self._content_consumed = True\n    if self._content_consumed and isinstance(self._content, bool):\n        raise StreamConsumedError()\n    reused_chunks = iter_slices(self._content, chunk_size)\n    stream_chunks = generate()\n    chunks = reused_chunks if self._content_consumed else stream_chunks\n    if decode_unicode:\n        chunks = stream_decode_response_unicode(chunks, self)\n    return chunks",
    ".requests.packages.urllib3.response.py@@HTTPResponse.read": "def read(self, amt=None, decode_content=None, cache_content=False):\n    self._init_decoder()\n    if decode_content is None:\n        decode_content = self.decode_content\n    if self._fp is None:\n        return\n    flush_decoder = False\n    data = None\n    with self._error_catcher():\n        if amt is None:\n            data = self._fp.read()\n            flush_decoder = True\n        else:\n            cache_content = False\n            data = self._fp.read(amt)\n            if amt != 0 and (not data):\n                self._fp.close()\n                flush_decoder = True\n    if data:\n        self._fp_bytes_read += len(data)\n        data = self._decode(data, decode_content, flush_decoder)\n        if cache_content:\n            self._body = data\n    return data",
    ".requests.packages.urllib3.poolmanager.py@@PoolManager.connection_from_url": "def connection_from_url(self, url):\n    u = parse_url(url)\n    return self.connection_from_host(u.host, port=u.port, scheme=u.scheme)",
    ".requests.adapters.py@@HTTPAdapter.build_response": "def build_response(self, req, resp):\n    response = Response()\n    response.status_code = getattr(resp, 'status', None)\n    response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))\n    response.encoding = get_encoding_from_headers(response.headers)\n    response.raw = resp\n    response.reason = response.raw.reason\n    if isinstance(req.url, bytes):\n        response.url = req.url.decode('utf-8')\n    else:\n        response.url = req.url\n    extract_cookies_to_jar(response.cookies, req, resp)\n    response.request = req\n    response.connection = self\n    return response",
    ".requests.utils.py@@parse_dict_header": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result",
    ".requests.cookies.py@@MockRequest.get_new_headers": "def get_new_headers(self):\n    return self._new_headers",
    ".requests.packages.urllib3.response.py@@HTTPResponse.release_conn": "def release_conn(self):\n    if not self._pool or not self._connection:\n        return\n    self._pool._put_conn(self._connection)\n    self._connection = None",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._put_conn": "def _put_conn(self, conn):\n    try:\n        self.pool.put(conn, block=False)\n        return\n    except AttributeError:\n        pass\n    except Full:\n        log.warning('Connection pool is full, discarding connection: %s' % self.host)\n    if conn:\n        conn.close()",
    ".requests.utils.py@@unquote_header_value": "def unquote_header_value(value, is_filename=False):\n    if value and value[0] == value[-1] == '\"':\n        value = value[1:-1]\n        if not is_filename or value[:2] != '\\\\\\\\':\n            return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n    return value",
    ".requests.hooks.py@@default_hooks": "def default_hooks():\n    hooks = {}\n    for event in HOOKS:\n        hooks[event] = []\n    return hooks",
    ".requests.structures.py@@CaseInsensitiveDict.copy": "def copy(self):\n    return CaseInsensitiveDict(self._store.values())",
    ".requests.structures.py@@CaseInsensitiveDict.__init__": "def __init__(self, data=None, **kwargs):\n    self._store = dict()\n    if data is None:\n        data = {}\n    self.update(data, **kwargs)",
    ".requests.packages.urllib3.util.timeout.py@@Timeout.read_timeout": "def read_timeout(self):\n    if self.total is not None and self.total is not self.DEFAULT_TIMEOUT and (self._read is not None) and (self._read is not self.DEFAULT_TIMEOUT):\n        if self._start_connect is None:\n            return self._read\n        return max(0, min(self.total - self.get_connect_duration(), self._read))\n    elif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:\n        return max(0, self.total - self.get_connect_duration())\n    else:\n        return self._read",
    ".requests.models.py@@Response.content": "def content(self):\n    if self._content is False:\n        try:\n            if self._content_consumed:\n                raise RuntimeError('The content for this response was already consumed')\n            if self.status_code == 0:\n                self._content = None\n            else:\n                self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n        except AttributeError:\n            self._content = None\n    self._content_consumed = True\n    return self._content",
    ".requests.models.py@@Response.generate": "def generate():\n    if hasattr(self.raw, 'stream'):\n        try:\n            for chunk in self.raw.stream(chunk_size, decode_content=True):\n                yield chunk\n        except ProtocolError as e:\n            raise ChunkedEncodingError(e)\n        except DecodeError as e:\n            raise ContentDecodingError(e)\n        except ReadTimeoutError as e:\n            raise ConnectionError(e)\n    else:\n        while True:\n            chunk = self.raw.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk\n    self._content_consumed = True",
    ".requests.models.py@@RequestEncodingMixin.path_url": "def path_url(self):\n    url = []\n    p = urlsplit(self.url)\n    path = p.path\n    if not path:\n        path = '/'\n    url.append(path)\n    query = p.query\n    if query:\n        url.append('?')\n        url.append(query)\n    return ''.join(url)",
    ".requests.adapters.py@@HTTPAdapter.add_headers": "def add_headers(self, request, **kwargs):\n    pass",
    ".requests.models.py@@Response.close": "def close(self):\n    if not self._content_consumed:\n        return self.raw.close()\n    return self.raw.release_conn()",
    ".requests.cookies.py@@MockRequest.__init__": "def __init__(self, request):\n    self._r = request\n    self._new_headers = {}\n    self.type = urlparse(self._r.url).scheme",
    ".requests.cookies.py@@_copy_cookie_jar": "def _copy_cookie_jar(jar):\n    if jar is None:\n        return None\n    if hasattr(jar, 'copy'):\n        return jar.copy()\n    new_jar = copy.copy(jar)\n    new_jar.clear()\n    for cookie in jar:\n        new_jar.set_cookie(copy.copy(cookie))\n    return new_jar",
    ".requests.packages.urllib3._collections.py@@RecentlyUsedContainer.__getitem__": "def __getitem__(self, key):\n    with self.lock:\n        item = self._container.pop(key)\n        self._container[key] = item\n        return item",
    ".requests.adapters.py@@HTTPAdapter.cert_verify": "def cert_verify(self, conn, url, verify, cert):\n    if url.lower().startswith('https') and verify:\n        cert_loc = None\n        if verify is not True:\n            cert_loc = verify\n        if not cert_loc:\n            cert_loc = DEFAULT_CA_BUNDLE_PATH\n        if not cert_loc:\n            raise Exception('Could not find a suitable SSL CA certificate bundle.')\n        conn.cert_reqs = 'CERT_REQUIRED'\n        conn.ca_certs = cert_loc\n    else:\n        conn.cert_reqs = 'CERT_NONE'\n        conn.ca_certs = None\n    if cert:\n        if not isinstance(cert, basestring):\n            conn.cert_file = cert[0]\n            conn.key_file = cert[1]\n        else:\n            conn.cert_file = cert",
    ".requests.adapters.py@@HTTPAdapter.request_url": "def request_url(self, request, proxies):\n    proxies = proxies or {}\n    scheme = urlparse(request.url).scheme\n    proxy = proxies.get(scheme)\n    if proxy and scheme != 'https':\n        url = urldefragauth(request.url)\n    else:\n        url = request.path_url\n    return url",
    ".requests.cookies.py@@RequestsCookieJar.copy": "def copy(self):\n    new_cj = RequestsCookieJar()\n    new_cj.update(self)\n    return new_cj",
    ".requests.cookies.py@@RequestsCookieJar.update": "def update(self, other):\n    if isinstance(other, cookielib.CookieJar):\n        for cookie in other:\n            self.set_cookie(copy.copy(cookie))\n    else:\n        super(RequestsCookieJar, self).update(other)",
    ".requests.packages.urllib3.util.url.py@@split_first": "def split_first(s, delims):\n    min_idx = None\n    min_delim = None\n    for d in delims:\n        idx = s.find(d)\n        if idx < 0:\n            continue\n        if min_idx is None or idx < min_idx:\n            min_idx = idx\n            min_delim = d\n    if min_idx is None or min_idx < 0:\n        return (s, '', None)\n    return (s[:min_idx], s[min_idx + 1:], min_delim)",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool.urlopen": "def urlopen(self, method, url, body=None, headers=None, retries=None, redirect=True, assert_same_host=True, timeout=_Default, pool_timeout=None, release_conn=None, **response_kw):\n    if headers is None:\n        headers = self.headers\n    if not isinstance(retries, Retry):\n        retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n    if release_conn is None:\n        release_conn = response_kw.get('preload_content', True)\n    if assert_same_host and (not self.is_same_host(url)):\n        raise HostChangedError(self, url, retries)\n    conn = None\n    if self.scheme == 'http':\n        headers = headers.copy()\n        headers.update(self.proxy_headers)\n    err = None\n    try:\n        timeout_obj = self._get_timeout(timeout)\n        conn = self._get_conn(timeout=pool_timeout)\n        conn.timeout = timeout_obj.connect_timeout\n        is_new_proxy_conn = self.proxy is not None and (not getattr(conn, 'sock', None))\n        if is_new_proxy_conn:\n            self._prepare_proxy(conn)\n        httplib_response = self._make_request(conn, method, url, timeout=timeout_obj, body=body, headers=headers)\n        response_conn = not release_conn and conn\n        response = HTTPResponse.from_httplib(httplib_response, pool=self, connection=response_conn, **response_kw)\n    except Empty:\n        raise EmptyPoolError(self, 'No pool connections are available.')\n    except (BaseSSLError, CertificateError) as e:\n        conn = conn and conn.close()\n        release_conn = True\n        raise SSLError(e)\n    except SSLError:\n        conn = conn and conn.close()\n        release_conn = True\n        raise\n    except (TimeoutError, HTTPException, SocketError, ConnectionError) as e:\n        conn = conn and conn.close()\n        release_conn = True\n        if isinstance(e, SocketError) and self.proxy:\n            e = ProxyError('Cannot connect to proxy.', e)\n        elif isinstance(e, (SocketError, HTTPException)):\n            e = ProtocolError('Connection aborted.', e)\n        retries = retries.increment(method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2])\n        retries.sleep()\n        err = e\n    finally:\n        if release_conn:\n            self._put_conn(conn)\n    if not conn:\n        log.warning(\"Retrying (%r) after connection broken by '%r': %s\" % (retries, err, url))\n        return self.urlopen(method, url, body, headers, retries, redirect, assert_same_host, timeout=timeout, pool_timeout=pool_timeout, release_conn=release_conn, **response_kw)\n    redirect_location = redirect and response.get_redirect_location()\n    if redirect_location:\n        if response.status == 303:\n            method = 'GET'\n        try:\n            retries = retries.increment(method, url, response=response, _pool=self)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                response.release_conn()\n                raise\n            return response\n        log.info('Redirecting %s -> %s' % (url, redirect_location))\n        return self.urlopen(method, redirect_location, body, headers, retries=retries, redirect=redirect, assert_same_host=assert_same_host, timeout=timeout, pool_timeout=pool_timeout, release_conn=release_conn, **response_kw)\n    if retries.is_forced_retry(method, status_code=response.status):\n        retries = retries.increment(method, url, response=response, _pool=self)\n        retries.sleep()\n        log.info('Forced retry: %s' % url)\n        return self.urlopen(method, url, body, headers, retries=retries, redirect=redirect, assert_same_host=assert_same_host, timeout=timeout, pool_timeout=pool_timeout, release_conn=release_conn, **response_kw)\n    return response",
    ".requests.packages.urllib3.util.url.py@@Url.__new__": "def __new__(cls, scheme=None, auth=None, host=None, port=None, path=None, query=None, fragment=None):\n    if path and (not path.startswith('/')):\n        path = '/' + path\n    return super(Url, cls).__new__(cls, scheme, auth, host, port, path, query, fragment)",
    ".requests.cookies.py@@MockRequest.has_header": "def has_header(self, name):\n    return name in self._r.headers or name in self._new_headers",
    ".requests.packages.urllib3.util.timeout.py@@Timeout.clone": "def clone(self):\n    return Timeout(connect=self._connect, read=self._read, total=self.total)",
    ".requests.models.py@@Response.__init__": "def __init__(self):\n    super(Response, self).__init__()\n    self._content = False\n    self._content_consumed = False\n    self.status_code = None\n    self.headers = CaseInsensitiveDict()\n    self.raw = None\n    self.url = None\n    self.encoding = None\n    self.history = []\n    self.reason = None\n    self.cookies = cookiejar_from_dict({})\n    self.elapsed = datetime.timedelta(0)\n    self.request = None",
    ".requests.packages.urllib3.response.py@@HTTPResponse.__init__": "def __init__(self, body='', headers=None, status=0, version=0, reason=None, strict=0, preload_content=True, decode_content=True, original_response=None, pool=None, connection=None):\n    if isinstance(headers, HTTPHeaderDict):\n        self.headers = headers\n    else:\n        self.headers = HTTPHeaderDict(headers)\n    self.status = status\n    self.version = version\n    self.reason = reason\n    self.strict = strict\n    self.decode_content = decode_content\n    self._decoder = None\n    self._body = None\n    self._fp = None\n    self._original_response = original_response\n    self._fp_bytes_read = 0\n    if body and isinstance(body, (basestring, binary_type)):\n        self._body = body\n    self._pool = pool\n    self._connection = connection\n    if hasattr(body, 'read'):\n        self._fp = body\n    self.chunked = False\n    self.chunk_left = None\n    tr_enc = self.headers.get('transfer-encoding', '').lower()\n    encodings = (enc.strip() for enc in tr_enc.split(','))\n    if 'chunked' in encodings:\n        self.chunked = True\n    if not self.chunked and preload_content and (not self._body):\n        self._body = self.read(decode_content=decode_content)",
    ".requests.adapters.py@@HTTPAdapter.send": "def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None):\n    conn = self.get_connection(request.url, proxies)\n    self.cert_verify(conn, request.url, verify, cert)\n    url = self.request_url(request, proxies)\n    self.add_headers(request)\n    chunked = not (request.body is None or 'Content-Length' in request.headers)\n    if isinstance(timeout, tuple):\n        try:\n            connect, read = timeout\n            timeout = TimeoutSauce(connect=connect, read=read)\n        except ValueError as e:\n            err = 'Invalid timeout {0}. Pass a (connect, read) timeout tuple, or a single float to set both timeouts to the same value'.format(timeout)\n            raise ValueError(err)\n    else:\n        timeout = TimeoutSauce(connect=timeout, read=timeout)\n    try:\n        if not chunked:\n            resp = conn.urlopen(method=request.method, url=url, body=request.body, headers=request.headers, redirect=False, assert_same_host=False, preload_content=False, decode_content=False, retries=self.max_retries, timeout=timeout)\n        else:\n            if hasattr(conn, 'proxy_pool'):\n                conn = conn.proxy_pool\n            low_conn = conn._get_conn(timeout=DEFAULT_POOL_TIMEOUT)\n            try:\n                low_conn.putrequest(request.method, url, skip_accept_encoding=True)\n                for header, value in request.headers.items():\n                    low_conn.putheader(header, value)\n                low_conn.endheaders()\n                for i in request.body:\n                    low_conn.send(hex(len(i))[2:].encode('utf-8'))\n                    low_conn.send(b'\\r\\n')\n                    low_conn.send(i)\n                    low_conn.send(b'\\r\\n')\n                low_conn.send(b'0\\r\\n\\r\\n')\n                r = low_conn.getresponse()\n                resp = HTTPResponse.from_httplib(r, pool=conn, connection=low_conn, preload_content=False, decode_content=False)\n            except:\n                low_conn.close()\n                raise\n    except (ProtocolError, socket.error) as err:\n        raise ConnectionError(err, request=request)\n    except MaxRetryError as e:\n        if isinstance(e.reason, ConnectTimeoutError):\n            raise ConnectTimeout(e, request=request)\n        if isinstance(e.reason, ResponseError):\n            raise RetryError(e, request=request)\n        raise ConnectionError(e, request=request)\n    except ClosedPoolError as e:\n        raise ConnectionError(e, request=request)\n    except _ProxyError as e:\n        raise ProxyError(e)\n    except (_SSLError, _HTTPError) as e:\n        if isinstance(e, _SSLError):\n            raise SSLError(e, request=request)\n        elif isinstance(e, ReadTimeoutError):\n            raise ReadTimeout(e, request=request)\n        else:\n            raise\n    return self.build_response(request, resp)",
    ".requests.cookies.py@@MockRequest.add_unredirected_header": "def add_unredirected_header(self, name, value):\n    self._new_headers[name] = value",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._get_timeout": "def _get_timeout(self, timeout):\n    if timeout is _Default:\n        return self.timeout.clone()\n    if isinstance(timeout, Timeout):\n        return timeout.clone()\n    else:\n        return Timeout.from_float(timeout)",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._get_conn": "def _get_conn(self, timeout=None):\n    conn = None\n    try:\n        conn = self.pool.get(block=self.block, timeout=timeout)\n    except AttributeError:\n        raise ClosedPoolError(self, 'Pool is closed.')\n    except Empty:\n        if self.block:\n            raise EmptyPoolError(self, 'Pool reached maximum size and no more connections are allowed.')\n        pass\n    if conn and is_connection_dropped(conn):\n        log.info('Resetting dropped connection: %s' % self.host)\n        conn.close()\n        if getattr(conn, 'auto_open', 1) == 0:\n            conn = None\n    return conn or self._new_conn()",
    ".requests.packages.urllib3.util.connection.py@@is_connection_dropped": "def is_connection_dropped(conn):\n    sock = getattr(conn, 'sock', False)\n    if sock is False:\n        return False\n    if sock is None:\n        return True\n    if not poll:\n        if not select:\n            return False\n        try:\n            return select([sock], [], [], 0.0)[0]\n        except socket.error:\n            return True\n    p = poll()\n    p.register(sock, POLLIN)\n    for fno, ev in p.poll(0.0):\n        if fno == sock.fileno():\n            return True",
    ".requests.utils.py@@get_encoding_from_headers": "def get_encoding_from_headers(headers):\n    content_type = headers.get('content-type')\n    if not content_type:\n        return None\n    content_type, params = cgi.parse_header(content_type)\n    if 'charset' in params:\n        return params['charset'].strip('\\'\"')\n    if 'text' in content_type:\n        return 'ISO-8859-1'",
    ".requests.cookies.py@@extract_cookies_to_jar": "def extract_cookies_to_jar(jar, request, response):\n    if not (hasattr(response, '_original_response') and response._original_response):\n        return\n    req = MockRequest(request)\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)",
    ".requests.packages.urllib3.util.url.py@@parse_url": "def parse_url(url):\n    if not url:\n        return Url()\n    scheme = None\n    auth = None\n    host = None\n    port = None\n    path = None\n    fragment = None\n    query = None\n    if '://' in url:\n        scheme, url = url.split('://', 1)\n    url, path_, delim = split_first(url, ['/', '?', '#'])\n    if delim:\n        path = delim + path_\n    if '@' in url:\n        auth, url = url.rsplit('@', 1)\n    if url and url[0] == '[':\n        host, url = url.split(']', 1)\n        host += ']'\n    if ':' in url:\n        _host, port = url.split(':', 1)\n        if not host:\n            host = _host\n        if port:\n            if not port.isdigit():\n                raise LocationParseError(url)\n            port = int(port)\n        else:\n            port = None\n    elif not host and url:\n        host = url\n    if not path:\n        return Url(scheme, auth, host, port, path, query, fragment)\n    if '#' in path:\n        path, fragment = path.split('#', 1)\n    if '?' in path:\n        path, query = path.split('?', 1)\n    return Url(scheme, auth, host, port, path, query, fragment)",
    ".requests.models.py@@PreparedRequest.prepare_cookies": "def prepare_cookies(self, cookies):\n    if isinstance(cookies, cookielib.CookieJar):\n        self._cookies = cookies\n    else:\n        self._cookies = cookiejar_from_dict(cookies)\n    cookie_header = get_cookie_header(self._cookies, self)\n    if cookie_header is not None:\n        self.headers['Cookie'] = cookie_header",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._make_request": "def _make_request(self, conn, method, url, timeout=_Default, **httplib_request_kw):\n    self.num_requests += 1\n    timeout_obj = self._get_timeout(timeout)\n    timeout_obj.start_connect()\n    conn.timeout = timeout_obj.connect_timeout\n    try:\n        self._validate_conn(conn)\n    except (SocketTimeout, BaseSSLError) as e:\n        self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n        raise\n    conn.request(method, url, **httplib_request_kw)\n    read_timeout = timeout_obj.read_timeout\n    if getattr(conn, 'sock', None):\n        if read_timeout == 0:\n            raise ReadTimeoutError(self, url, 'Read timed out. (read timeout=%s)' % read_timeout)\n        if read_timeout is Timeout.DEFAULT_TIMEOUT:\n            conn.sock.settimeout(socket.getdefaulttimeout())\n        else:\n            conn.sock.settimeout(read_timeout)\n    try:\n        try:\n            httplib_response = conn.getresponse(buffering=True)\n        except TypeError:\n            httplib_response = conn.getresponse()\n    except (SocketTimeout, BaseSSLError, SocketError) as e:\n        self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n        raise\n    http_version = getattr(conn, '_http_vsn_str', 'HTTP/?')\n    log.debug('\"%s %s %s\" %s %s' % (method, url, http_version, httplib_response.status, httplib_response.length))\n    try:\n        assert_header_parsing(httplib_response.msg)\n    except HeaderParsingError as hpe:\n        log.warning('Failed to parse headers (url=%s): %s', self._absolute_url(url), hpe, exc_info=True)\n    return httplib_response",
    ".requests.packages.urllib3.util.timeout.py@@Timeout.connect_timeout": "def connect_timeout(self):\n    if self.total is None:\n        return self._connect\n    if self._connect is None or self._connect is self.DEFAULT_TIMEOUT:\n        return self.total\n    return min(self._connect, self.total)",
    ".requests.cookies.py@@get_cookie_header": "def get_cookie_header(jar, request):\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get('Cookie')",
    ".requests.packages.urllib3.util.timeout.py@@current_time": "def current_time():\n    return time.time()",
    ".requests.structures.py@@CaseInsensitiveDict.__iter__": "def __iter__(self):\n    return (casedkey for casedkey, mappedvalue in self._store.values())",
    ".requests.adapters.py@@HTTPAdapter.get_connection": "def get_connection(self, url, proxies=None):\n    proxies = proxies or {}\n    proxy = proxies.get(urlparse(url.lower()).scheme)\n    if proxy:\n        proxy = prepend_scheme_if_needed(proxy, 'http')\n        proxy_manager = self.proxy_manager_for(proxy)\n        conn = proxy_manager.connection_from_url(url)\n    else:\n        parsed = urlparse(url)\n        url = parsed.geturl()\n        conn = self.poolmanager.connection_from_url(url)\n    return conn",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._validate_conn": "def _validate_conn(self, conn):\n    pass",
    ".requests.packages.urllib3.util.timeout.py@@Timeout.start_connect": "def start_connect(self):\n    if self._start_connect is not None:\n        raise TimeoutStateError('Timeout timer has already been started.')\n    self._start_connect = current_time()\n    return self._start_connect",
    ".requests.packages.urllib3.poolmanager.py@@PoolManager.connection_from_host": "def connection_from_host(self, host, port=None, scheme='http'):\n    if not host:\n        raise LocationValueError('No host specified.')\n    scheme = scheme or 'http'\n    port = port or port_by_scheme.get(scheme, 80)\n    pool_key = (scheme, host, port)\n    with self.pools.lock:\n        pool = self.pools.get(pool_key)\n        if pool:\n            return pool\n        pool = self._new_pool(scheme, host, port)\n        self.pools[pool_key] = pool\n    return pool",
    ".requests.packages.urllib3._collections.py@@HTTPHeaderDict.extend": "def extend(self, *args, **kwargs):\n    if len(args) > 1:\n        raise TypeError('extend() takes at most 1 positional arguments ({} given)'.format(len(args)))\n    other = args[0] if len(args) >= 1 else ()\n    if isinstance(other, HTTPHeaderDict):\n        for key, val in other.iteritems():\n            self.add(key, val)\n    elif isinstance(other, Mapping):\n        for key in other:\n            self.add(key, other[key])\n    elif hasattr(other, 'keys'):\n        for key in other.keys():\n            self.add(key, other[key])\n    else:\n        for key, value in other:\n            self.add(key, value)\n    for key, value in kwargs.items():\n        self.add(key, value)",
    ".requests.packages.urllib3.response.py@@HTTPResponse.from_httplib": "def from_httplib(ResponseCls, r, **response_kw):\n    headers = r.msg\n    if not isinstance(headers, HTTPHeaderDict):\n        if PY3:\n            headers = HTTPHeaderDict(headers.items())\n        else:\n            headers = HTTPHeaderDict.from_httplib(headers)\n    strict = getattr(r, 'strict', 0)\n    resp = ResponseCls(body=r, headers=headers, status=r.status, version=r.version, reason=r.reason, strict=strict, original_response=r, **response_kw)\n    return resp",
    ".requests.packages.urllib3.connection.py@@HTTPConnection.__init__": "def __init__(self, *args, **kw):\n    if six.PY3:\n        kw.pop('strict', None)\n    self.source_address = kw.get('source_address')\n    if sys.version_info < (2, 7):\n        kw.pop('source_address', None)\n    self.socket_options = kw.pop('socket_options', self.default_socket_options)\n    _HTTPConnection.__init__(self, *args, **kw)",
    ".requests.packages.urllib3.connection.py@@HTTPConnection._prepare_conn": "def _prepare_conn(self, conn):\n    self.sock = conn\n    if getattr(self, '_tunnel_host', None):\n        self._tunnel()\n        self.auto_open = 0",
    ".requests.packages.urllib3.connection.py@@HTTPConnection.connect": "def connect(self):\n    conn = self._new_conn()\n    self._prepare_conn(conn)",
    ".requests.packages.urllib3.util.retry.py@@Retry.is_forced_retry": "def is_forced_retry(self, method, status_code):\n    if self.method_whitelist and method.upper() not in self.method_whitelist:\n        return False\n    return self.status_forcelist and status_code in self.status_forcelist",
    ".requests.packages.urllib3.connection.py@@HTTPConnection._new_conn": "def _new_conn(self):\n    extra_kw = {}\n    if self.source_address:\n        extra_kw['source_address'] = self.source_address\n    if self.socket_options:\n        extra_kw['socket_options'] = self.socket_options\n    try:\n        conn = connection.create_connection((self.host, self.port), self.timeout, **extra_kw)\n    except SocketTimeout:\n        raise ConnectTimeoutError(self, 'Connection to %s timed out. (connect timeout=%s)' % (self.host, self.timeout))\n    return conn",
    ".requests.packages.urllib3.util.connection.py@@create_connection": "def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None, socket_options=None):\n    host, port = address\n    if host.startswith('['):\n        host = host.strip('[]')\n    err = None\n    for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n            _set_socket_options(sock, socket_options)\n            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n        except socket.error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n                sock = None\n    if err is not None:\n        raise err\n    else:\n        raise socket.error('getaddrinfo returns an empty list')",
    ".requests.cookies.py@@cookiejar_from_dict": "def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or name not in names_from_jar:\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n    return cookiejar",
    ".requests.packages.urllib3.util.response.py@@assert_header_parsing": "def assert_header_parsing(headers):\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError('expected httplib.Message, got {}.'.format(type(headers)))\n    defects = getattr(headers, 'defects', None)\n    get_payload = getattr(headers, 'get_payload', None)\n    unparsed_data = None\n    if get_payload:\n        unparsed_data = get_payload()\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    ".requests.packages.urllib3._collections.py@@HTTPHeaderDict.__init__": "def __init__(self, headers=None, **kwargs):\n    dict.__init__(self)\n    if headers is not None:\n        if isinstance(headers, HTTPHeaderDict):\n            self._copy_from(headers)\n        else:\n            self.extend(headers)\n    if kwargs:\n        self.extend(kwargs)",
    ".requests.packages.urllib3.util.connection.py@@_set_socket_options": "def _set_socket_options(sock, options):\n    if options is None:\n        return\n    for opt in options:\n        sock.setsockopt(*opt)",
    ".requests.models.py@@Response.is_redirect": "def is_redirect(self):\n    return 'location' in self.headers and self.status_code in REDIRECT_STATI",
    ".requests.packages.urllib3.connectionpool.py@@HTTPConnectionPool._new_conn": "def _new_conn(self):\n    self.num_connections += 1\n    log.info('Starting new HTTP connection (%d): %s' % (self.num_connections, self.host))\n    conn = self.ConnectionCls(host=self.host, port=self.port, timeout=self.timeout.connect_timeout, strict=self.strict, **self.conn_kw)\n    return conn",
    ".requests.packages.urllib3.response.py@@HTTPResponse.stream": "def stream(self, amt=2 ** 16, decode_content=None):\n    if self.chunked:\n        for line in self.read_chunked(amt, decode_content=decode_content):\n            yield line\n    else:\n        while not is_fp_closed(self._fp):\n            data = self.read(amt=amt, decode_content=decode_content)\n            if data:\n                yield data",
    ".requests.models.py@@PreparedRequest.prepare": "def prepare(self, method=None, url=None, headers=None, files=None, data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n    self.prepare_method(method)\n    self.prepare_url(url, params)\n    self.prepare_headers(headers)\n    self.prepare_cookies(cookies)\n    self.prepare_body(data, files, json)\n    self.prepare_auth(auth, url)\n    self.prepare_hooks(hooks)",
    ".requests.models.py@@PreparedRequest.prepare_method": "def prepare_method(self, method):\n    self.method = method\n    if self.method is not None:\n        self.method = self.method.upper()",
    ".requests.models.py@@PreparedRequest.prepare_url": "def prepare_url(self, url, params):\n    if isinstance(url, bytes):\n        url = url.decode('utf8')\n    else:\n        url = unicode(url) if is_py2 else str(url)\n    if ':' in url and (not url.lower().startswith('http')):\n        self.url = url\n        return\n    try:\n        scheme, auth, host, port, path, query, fragment = parse_url(url)\n    except LocationParseError as e:\n        raise InvalidURL(*e.args)\n    if not scheme:\n        error = 'Invalid URL {0!r}: No schema supplied. Perhaps you meant http://{0}?'\n        error = error.format(to_native_string(url, 'utf8'))\n        raise MissingSchema(error)\n    if not host:\n        raise InvalidURL('Invalid URL %r: No host supplied' % url)\n    try:\n        host = host.encode('idna').decode('utf-8')\n    except UnicodeError:\n        raise InvalidURL('URL has an invalid label.')\n    netloc = auth or ''\n    if netloc:\n        netloc += '@'\n    netloc += host\n    if port:\n        netloc += ':' + str(port)\n    if not path:\n        path = '/'\n    if is_py2:\n        if isinstance(scheme, str):\n            scheme = scheme.encode('utf-8')\n        if isinstance(netloc, str):\n            netloc = netloc.encode('utf-8')\n        if isinstance(path, str):\n            path = path.encode('utf-8')\n        if isinstance(query, str):\n            query = query.encode('utf-8')\n        if isinstance(fragment, str):\n            fragment = fragment.encode('utf-8')\n    enc_params = self._encode_params(params)\n    if enc_params:\n        if query:\n            query = '%s&%s' % (query, enc_params)\n        else:\n            query = enc_params\n    url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n    self.url = url",
    ".requests.sessions.py@@Session.get": "def get(self, url, **kwargs):\n    kwargs.setdefault('allow_redirects', True)\n    return self.request('GET', url, **kwargs)",
    ".requests.sessions.py@@Session.request": "def request(self, method, url, params=None, data=None, headers=None, cookies=None, files=None, auth=None, timeout=None, allow_redirects=True, proxies=None, hooks=None, stream=None, verify=None, cert=None, json=None):\n    method = to_native_string(method)\n    req = Request(method=method.upper(), url=url, headers=headers, files=files, data=data or {}, json=json, params=params or {}, auth=auth, cookies=cookies, hooks=hooks)\n    prep = self.prepare_request(req)\n    proxies = proxies or {}\n    settings = self.merge_environment_settings(prep.url, proxies, stream, verify, cert)\n    send_kwargs = {'timeout': timeout, 'allow_redirects': allow_redirects}\n    send_kwargs.update(settings)\n    resp = self.send(prep, **send_kwargs)\n    return resp",
    ".requests.sessions.py@@Session.merge_environment_settings": "def merge_environment_settings(self, url, proxies, stream, verify, cert):\n    if self.trust_env:\n        env_proxies = get_environ_proxies(url) or {}\n        for k, v in env_proxies.items():\n            proxies.setdefault(k, v)\n        if verify is True or verify is None:\n            verify = os.environ.get('REQUESTS_CA_BUNDLE') or os.environ.get('CURL_CA_BUNDLE')\n    proxies = merge_setting(proxies, self.proxies)\n    stream = merge_setting(stream, self.stream)\n    verify = merge_setting(verify, self.verify)\n    cert = merge_setting(cert, self.cert)\n    return {'verify': verify, 'proxies': proxies, 'stream': stream, 'cert': cert}",
    ".requests.utils.py@@get_environ_proxies": "def get_environ_proxies(url):\n    if should_bypass_proxies(url):\n        return {}\n    else:\n        return getproxies()",
    ".requests.utils.py@@should_bypass_proxies": "def should_bypass_proxies(url):\n    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())\n    no_proxy = get_proxy('no_proxy')\n    netloc = urlparse(url).netloc\n    if no_proxy:\n        no_proxy = no_proxy.replace(' ', '').split(',')\n        ip = netloc.split(':')[0]\n        if is_ipv4_address(ip):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(ip, proxy_ip):\n                        return True\n        else:\n            for host in no_proxy:\n                if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n                    return True\n    try:\n        bypass = proxy_bypass(netloc)\n    except (TypeError, socket.gaierror):\n        bypass = False\n    if bypass:\n        return True\n    return False",
    ".requests.sessions.py@@merge_setting": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n    for k, v in merged_setting.items():\n        if v is None:\n            del merged_setting[k]\n    return merged_setting",
    ".requests.sessions.py@@Session.prepare_request": "def prepare_request(self, request):\n    cookies = request.cookies or {}\n    if not isinstance(cookies, cookielib.CookieJar):\n        cookies = cookiejar_from_dict(cookies)\n    merged_cookies = merge_cookies(merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n    auth = request.auth\n    if self.trust_env and (not auth) and (not self.auth):\n        auth = get_netrc_auth(request.url)\n    p = PreparedRequest()\n    p.prepare(method=request.method.upper(), url=request.url, files=request.files, data=request.data, json=request.json, headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict), params=merge_setting(request.params, self.params), auth=merge_setting(auth, self.auth), cookies=merged_cookies, hooks=merge_hooks(request.hooks, self.hooks))\n    return p",
    ".requests.utils.py@@to_key_val_list": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    if isinstance(value, collections.Mapping):\n        value = value.items()\n    return list(value)",
    ".requests.cookies.py@@merge_cookies": "def merge_cookies(cookiejar, cookies):\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError('You can only merge into CookieJar')\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n    return cookiejar",
    ".requests.models.py@@RequestEncodingMixin._encode_params": "def _encode_params(data):\n    if isinstance(data, (str, bytes)):\n        return data\n    elif hasattr(data, 'read'):\n        return data\n    elif hasattr(data, '__iter__'):\n        result = []\n        for k, vs in to_key_val_list(data):\n            if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\n                vs = [vs]\n            for v in vs:\n                if v is not None:\n                    result.append((k.encode('utf-8') if isinstance(k, str) else k, v.encode('utf-8') if isinstance(v, str) else v))\n        return urlencode(result, doseq=True)\n    else:\n        return data",
    ".requests.utils.py@@requote_uri": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)",
    ".requests.packages.urllib3.response.py@@HTTPResponse._decode": "def _decode(self, data, decode_content, flush_decoder):\n    try:\n        if decode_content and self._decoder:\n            data = self._decoder.decompress(data)\n    except (IOError, zlib.error) as e:\n        content_encoding = self.headers.get('content-encoding', '').lower()\n        raise DecodeError('Received response with content-encoding: %s, but failed to decode it.' % content_encoding, e)\n    if flush_decoder and decode_content and self._decoder:\n        buf = self._decoder.decompress(binary_type())\n        data += buf + self._decoder.flush()\n    return data",
    ".requests.utils.py@@to_native_string": "def to_native_string(string, encoding='ascii'):\n    out = None\n    if isinstance(string, builtin_str):\n        out = string\n    elif is_py2:\n        out = string.encode(encoding)\n    else:\n        out = string.decode(encoding)\n    return out",
    ".requests.structures.py@@CaseInsensitiveDict.__len__": "def __len__(self):\n    return len(self._store)",
    ".requests.sessions.py@@SessionRedirectMixin.resolve_redirects": "def resolve_redirects(self, resp, req, stream=False, timeout=None, verify=True, cert=None, proxies=None, **adapter_kwargs):\n    i = 0\n    hist = []\n    while resp.is_redirect:\n        prepared_request = req.copy()\n        if i > 0:\n            hist.append(resp)\n            new_hist = list(hist)\n            resp.history = new_hist\n        try:\n            resp.content\n        except (ChunkedEncodingError, ContentDecodingError, RuntimeError):\n            resp.raw.read(decode_content=False)\n        if i >= self.max_redirects:\n            raise TooManyRedirects('Exceeded %s redirects.' % self.max_redirects)\n        resp.close()\n        url = resp.headers['location']\n        method = req.method\n        if url.startswith('//'):\n            parsed_rurl = urlparse(resp.url)\n            url = '%s:%s' % (parsed_rurl.scheme, url)\n        parsed = urlparse(url)\n        url = parsed.geturl()\n        if not parsed.netloc:\n            url = urljoin(resp.url, requote_uri(url))\n        else:\n            url = requote_uri(url)\n        prepared_request.url = to_native_string(url)\n        if resp.is_permanent_redirect and req.url != prepared_request.url:\n            self.redirect_cache[req.url] = prepared_request.url\n        if resp.status_code == codes.see_other and method != 'HEAD':\n            method = 'GET'\n        if resp.status_code == codes.found and method != 'HEAD':\n            method = 'GET'\n        if resp.status_code == codes.moved and method == 'POST':\n            method = 'GET'\n        prepared_request.method = method\n        if resp.status_code not in (codes.temporary_redirect, codes.permanent_redirect):\n            if 'Content-Length' in prepared_request.headers:\n                del prepared_request.headers['Content-Length']\n            prepared_request.body = None\n        headers = prepared_request.headers\n        try:\n            del headers['Cookie']\n        except KeyError:\n            pass\n        extract_cookies_to_jar(prepared_request._cookies, req, resp.raw)\n        prepared_request._cookies.update(self.cookies)\n        prepared_request.prepare_cookies(prepared_request._cookies)\n        proxies = self.rebuild_proxies(prepared_request, proxies)\n        self.rebuild_auth(prepared_request, resp)\n        req = prepared_request\n        resp = self.send(req, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies, allow_redirects=False, **adapter_kwargs)\n        extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n        i += 1\n        yield resp",
    ".requests.packages.urllib3.response.py@@HTTPResponse._error_catcher": "def _error_catcher(self):\n    try:\n        try:\n            yield\n        except SocketTimeout:\n            raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n        except BaseSSLError as e:\n            if 'read operation timed out' not in str(e):\n                raise\n            raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n        except HTTPException as e:\n            raise ProtocolError('Connection broken: %r' % e, e)\n    except Exception:\n        if self._original_response and (not self._original_response.isclosed()):\n            self._original_response.close()\n        raise\n    finally:\n        if self._original_response and self._original_response.isclosed():\n            self.release_conn()",
    ".requests.sessions.py@@Session.send": "def send(self, request, **kwargs):\n    kwargs.setdefault('stream', self.stream)\n    kwargs.setdefault('verify', self.verify)\n    kwargs.setdefault('cert', self.cert)\n    kwargs.setdefault('proxies', self.proxies)\n    if not isinstance(request, PreparedRequest):\n        raise ValueError('You can only send PreparedRequests.')\n    checked_urls = set()\n    while request.url in self.redirect_cache:\n        checked_urls.add(request.url)\n        new_url = self.redirect_cache.get(request.url)\n        if new_url in checked_urls:\n            break\n        request.url = new_url\n    allow_redirects = kwargs.pop('allow_redirects', True)\n    stream = kwargs.get('stream')\n    hooks = request.hooks\n    adapter = self.get_adapter(url=request.url)\n    start = datetime.utcnow()\n    r = adapter.send(request, **kwargs)\n    r.elapsed = datetime.utcnow() - start\n    r = dispatch_hook('response', hooks, r, **kwargs)\n    if r.history:\n        for resp in r.history:\n            extract_cookies_to_jar(self.cookies, resp.request, resp.raw)\n    extract_cookies_to_jar(self.cookies, request, r.raw)\n    gen = self.resolve_redirects(r, request, **kwargs)\n    history = [resp for resp in gen] if allow_redirects else []\n    if history:\n        history.insert(0, r)\n        r = history.pop()\n        r.history = history\n    if not stream:\n        r.content\n    return r",
    ".requests.sessions.py@@merge_hooks": "def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get('response') == []:\n        return request_hooks\n    if request_hooks is None or request_hooks.get('response') == []:\n        return session_hooks\n    return merge_setting(request_hooks, session_hooks, dict_class)",
    ".requests.packages.urllib3.response.py@@HTTPResponse._init_decoder": "def _init_decoder(self):\n    content_encoding = self.headers.get('content-encoding', '').lower()\n    if self._decoder is None and content_encoding in self.CONTENT_DECODERS:\n        self._decoder = _get_decoder(content_encoding)",
    ".requests.packages.urllib3.response.py@@HTTPResponse.closed": "def closed(self):\n    if self._fp is None:\n        return True\n    elif hasattr(self._fp, 'closed'):\n        return self._fp.closed\n    elif hasattr(self._fp, 'isclosed'):\n        return self._fp.isclosed()\n    else:\n        return True",
    ".requests.models.py@@PreparedRequest.prepare_content_length": "def prepare_content_length(self, body):\n    if hasattr(body, 'seek') and hasattr(body, 'tell'):\n        body.seek(0, 2)\n        self.headers['Content-Length'] = builtin_str(body.tell())\n        body.seek(0, 0)\n    elif body is not None:\n        l = super_len(body)\n        if l:\n            self.headers['Content-Length'] = builtin_str(l)\n    elif self.method not in ('GET', 'HEAD') and self.headers.get('Content-Length') is None:\n        self.headers['Content-Length'] = '0'",
    ".requests.models.py@@PreparedRequest.prepare_hooks": "def prepare_hooks(self, hooks):\n    hooks = hooks or []\n    for event in hooks:\n        self.register_hook(event, hooks[event])",
    ".requests.models.py@@RequestHooksMixin.register_hook": "def register_hook(self, event, hook):\n    if event not in self.hooks:\n        raise ValueError('Unsupported event specified, with event name \"%s\"' % event)\n    if isinstance(hook, collections.Callable):\n        self.hooks[event].append(hook)\n    elif hasattr(hook, '__iter__'):\n        self.hooks[event].extend((h for h in hook if isinstance(h, collections.Callable)))",
    ".requests.utils.py@@unquote_unreserved": "def unquote_unreserved(uri):\n    parts = uri.split('%')\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(\"Invalid percent-escape sequence: '%s'\" % h)\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]\n            else:\n                parts[i] = '%' + parts[i]\n        else:\n            parts[i] = '%' + parts[i]\n    return ''.join(parts)",
    ".requests.models.py@@PreparedRequest.prepare_headers": "def prepare_headers(self, headers):\n    if headers:\n        self.headers = CaseInsensitiveDict(((to_native_string(name), value) for name, value in headers.items()))\n    else:\n        self.headers = CaseInsensitiveDict()",
    ".requests.hooks.py@@dispatch_hook": "def dispatch_hook(key, hooks, hook_data, **kwargs):\n    hooks = hooks or dict()\n    if key in hooks:\n        hooks = hooks.get(key)\n        if hasattr(hooks, '__call__'):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data"
}