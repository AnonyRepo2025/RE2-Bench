{
    ".sklearn.decomposition.dict_learning.py@@_check_positive_coding": "def _check_positive_coding(method, positive):\n    if positive and method in ['omp', 'lars']:\n        raise ValueError(\"Positive constraint not supported for '{}' coding method.\".format(method))",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.linear_model.omp.py@@orthogonal_mp_gram": "def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False):\n    Gram = check_array(Gram, order='F', copy=copy_Gram)\n    Xy = np.asarray(Xy)\n    if Xy.ndim > 1 and Xy.shape[1] > 1:\n        copy_Gram = True\n    if Xy.ndim == 1:\n        Xy = Xy[:, np.newaxis]\n        if tol is not None:\n            norms_squared = [norms_squared]\n    if copy_Xy or not Xy.flags.writeable:\n        Xy = Xy.copy()\n    if n_nonzero_coefs is None and tol is None:\n        n_nonzero_coefs = int(0.1 * len(Gram))\n    if tol is not None and norms_squared is None:\n        raise ValueError('Gram OMP needs the precomputed norms in order to evaluate the error sum of squares.')\n    if tol is not None and tol < 0:\n        raise ValueError('Epsilon cannot be negative')\n    if tol is None and n_nonzero_coefs <= 0:\n        raise ValueError('The number of atoms must be positive')\n    if tol is None and n_nonzero_coefs > len(Gram):\n        raise ValueError('The number of atoms cannot be more than the number of features')\n    if return_path:\n        coef = np.zeros((len(Gram), Xy.shape[1], len(Gram)))\n    else:\n        coef = np.zeros((len(Gram), Xy.shape[1]))\n    n_iters = []\n    for k in range(Xy.shape[1]):\n        out = _gram_omp(Gram, Xy[:, k], n_nonzero_coefs, norms_squared[k] if tol is not None else None, tol, copy_Gram=copy_Gram, copy_Xy=False, return_path=return_path)\n        if return_path:\n            _, idx, coefs, n_iter = out\n            coef = coef[:, :, :len(idx)]\n            for n_active, x in enumerate(coefs.T):\n                coef[idx[:n_active + 1], k, n_active] = x[:n_active + 1]\n        else:\n            x, idx, n_iter = out\n            coef[idx, k] = x\n        n_iters.append(n_iter)\n    if Xy.shape[1] == 1:\n        n_iters = n_iters[0]\n    if return_n_iter:\n        return (np.squeeze(coef), n_iters)\n    else:\n        return np.squeeze(coef)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.linear_model.omp.py@@_gram_omp": "def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None, copy_Gram=True, copy_Xy=True, return_path=False):\n    Gram = Gram.copy('F') if copy_Gram else np.asfortranarray(Gram)\n    if copy_Xy or not Xy.flags.writeable:\n        Xy = Xy.copy()\n    min_float = np.finfo(Gram.dtype).eps\n    nrm2, swap = linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))\n    potrs, = get_lapack_funcs(('potrs',), (Gram,))\n    indices = np.arange(len(Gram))\n    alpha = Xy\n    tol_curr = tol_0\n    delta = 0\n    gamma = np.empty(0)\n    n_active = 0\n    max_features = len(Gram) if tol is not None else n_nonzero_coefs\n    L = np.empty((max_features, max_features), dtype=Gram.dtype)\n    L[0, 0] = 1.0\n    if return_path:\n        coefs = np.empty_like(L)\n    while True:\n        lam = np.argmax(np.abs(alpha))\n        if lam < n_active or alpha[lam] ** 2 < min_float:\n            warnings.warn(premature, RuntimeWarning, stacklevel=3)\n            break\n        if n_active > 0:\n            L[n_active, :n_active] = Gram[lam, :n_active]\n            linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, check_finite=False)\n            v = nrm2(L[n_active, :n_active]) ** 2\n            Lkk = Gram[lam, lam] - v\n            if Lkk <= min_float:\n                warnings.warn(premature, RuntimeWarning, stacklevel=3)\n                break\n            L[n_active, n_active] = sqrt(Lkk)\n        else:\n            L[0, 0] = sqrt(Gram[lam, lam])\n        Gram[n_active], Gram[lam] = swap(Gram[n_active], Gram[lam])\n        Gram.T[n_active], Gram.T[lam] = swap(Gram.T[n_active], Gram.T[lam])\n        indices[n_active], indices[lam] = (indices[lam], indices[n_active])\n        Xy[n_active], Xy[lam] = (Xy[lam], Xy[n_active])\n        n_active += 1\n        gamma, _ = potrs(L[:n_active, :n_active], Xy[:n_active], lower=True, overwrite_b=False)\n        if return_path:\n            coefs[:n_active, n_active - 1] = gamma\n        beta = np.dot(Gram[:, :n_active], gamma)\n        alpha = Xy - beta\n        if tol is not None:\n            tol_curr += delta\n            delta = np.inner(gamma, beta[:n_active])\n            tol_curr -= delta\n            if abs(tol_curr) <= tol:\n                break\n        elif n_active == max_features:\n            break\n    if return_path:\n        return (gamma, indices[:n_active], coefs[:, :n_active], n_active)\n    else:\n        return (gamma, indices[:n_active], n_active)",
    ".sklearn.linear_model.least_angle.py@@LassoLars.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=np.finfo(np.float).eps, copy_X=True, fit_path=True, positive=False):\n    self.alpha = alpha\n    self.fit_intercept = fit_intercept\n    self.max_iter = max_iter\n    self.verbose = verbose\n    self.normalize = normalize\n    self.positive = positive\n    self.precompute = precompute\n    self.copy_X = copy_X\n    self.eps = eps\n    self.fit_path = fit_path",
    ".sklearn.linear_model.least_angle.py@@Lars.fit": "def fit(self, X, y, Xy=None):\n    X, y = check_X_y(X, y, y_numeric=True, multi_output=True)\n    alpha = getattr(self, 'alpha', 0.0)\n    if hasattr(self, 'n_nonzero_coefs'):\n        alpha = 0.0\n        max_iter = self.n_nonzero_coefs\n    else:\n        max_iter = self.max_iter\n    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)\n    return self",
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.linear_model.least_angle.py@@Lars._fit": "def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):\n    n_features = X.shape[1]\n    X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n    n_targets = y.shape[1]\n    Gram = self._get_gram(self.precompute, X, y)\n    self.alphas_ = []\n    self.n_iter_ = []\n    self.coef_ = np.empty((n_targets, n_features))\n    if fit_path:\n        self.active_ = []\n        self.coef_path_ = []\n        for k in range(n_targets):\n            this_Xy = None if Xy is None else Xy[:, k]\n            alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)\n            self.alphas_.append(alphas)\n            self.active_.append(active)\n            self.n_iter_.append(n_iter_)\n            self.coef_path_.append(coef_path)\n            self.coef_[k] = coef_path[:, -1]\n        if n_targets == 1:\n            self.alphas_, self.active_, self.coef_path_, self.coef_ = [a[0] for a in (self.alphas_, self.active_, self.coef_path_, self.coef_)]\n            self.n_iter_ = self.n_iter_[0]\n    else:\n        for k in range(n_targets):\n            this_Xy = None if Xy is None else Xy[:, k]\n            alphas, _, self.coef_[k], n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=False, return_n_iter=True, positive=self.positive)\n            self.alphas_.append(alphas)\n            self.n_iter_.append(n_iter_)\n        if n_targets == 1:\n            self.alphas_ = self.alphas_[0]\n            self.n_iter_ = self.n_iter_[0]\n    self._set_intercept(X_offset, y_offset, X_scale)\n    return self",
    ".sklearn.linear_model.base.py@@_preprocess_data": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order='K')\n    y = np.asarray(y, dtype=X.dtype)\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n            if normalize:\n                X_var *= X.shape[0]\n                X_scale = np.sqrt(X_var, X_var)\n                del X_var\n                X_scale[X_scale == 0] = 1\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        else:\n            X_offset = np.average(X, axis=0, weights=sample_weight)\n            X -= X_offset\n            if normalize:\n                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n    return (X, y, X_offset, y_offset, X_scale)",
    ".sklearn.linear_model.least_angle.py@@Lars._get_gram": "def _get_gram(precompute, X, y):\n    if not hasattr(precompute, '__array__') and (precompute is True or (precompute == 'auto' and X.shape[0] > X.shape[1]) or (precompute == 'auto' and y.shape[1] > 1)):\n        precompute = np.dot(X.T, X)\n    return precompute",
    ".sklearn.linear_model.least_angle.py@@lars_path": "def lars_path(X, y, Xy=None, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False):\n    if X is None and Gram is not None:\n        warnings.warn('Use lars_path_gram to avoid passing X and y. The current option will be removed in v0.23.', DeprecationWarning)\n    return _lars_path_solver(X=X, y=y, Xy=Xy, Gram=Gram, n_samples=None, max_iter=max_iter, alpha_min=alpha_min, method=method, copy_X=copy_X, eps=eps, copy_Gram=copy_Gram, verbose=verbose, return_path=return_path, return_n_iter=return_n_iter, positive=positive)",
    ".sklearn.linear_model.least_angle.py@@_lars_path_solver": "def _lars_path_solver(X, y, Xy=None, Gram=None, n_samples=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False):\n    if method == 'lar' and positive:\n        raise ValueError(\"Positive constraint not supported for 'lar' coding method.\")\n    n_samples = n_samples if n_samples is not None else y.size\n    if Xy is None:\n        Cov = np.dot(X.T, y)\n    else:\n        Cov = Xy.copy()\n    if Gram is None or Gram is False:\n        Gram = None\n        if X is None:\n            raise ValueError('X and Gram cannot both be unspecified.')\n        if copy_X:\n            X = X.copy('F')\n    elif isinstance(Gram, str) and Gram == 'auto' or Gram is True:\n        if Gram is True or X.shape[0] > X.shape[1]:\n            Gram = np.dot(X.T, X)\n        else:\n            Gram = None\n    elif copy_Gram:\n        Gram = Gram.copy()\n    if Gram is None:\n        n_features = X.shape[1]\n    else:\n        n_features = Cov.shape[0]\n        if Gram.shape != (n_features, n_features):\n            raise ValueError('The shapes of the inputs Gram and Xy do not match.')\n    max_features = min(max_iter, n_features)\n    if return_path:\n        coefs = np.zeros((max_features + 1, n_features))\n        alphas = np.zeros(max_features + 1)\n    else:\n        coef, prev_coef = (np.zeros(n_features), np.zeros(n_features))\n        alpha, prev_alpha = (np.array([0.0]), np.array([0.0]))\n    n_iter, n_active = (0, 0)\n    active, indices = (list(), np.arange(n_features))\n    sign_active = np.empty(max_features, dtype=np.int8)\n    drop = False\n    if Gram is None:\n        L = np.empty((max_features, max_features), dtype=X.dtype)\n        swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\n    else:\n        L = np.empty((max_features, max_features), dtype=Gram.dtype)\n        swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (Cov,))\n    solve_cholesky, = get_lapack_funcs(('potrs',), (L,))\n    if verbose:\n        if verbose > 1:\n            print('Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC')\n        else:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n    tiny32 = np.finfo(np.float32).tiny\n    equality_tolerance = np.finfo(np.float32).eps\n    if Gram is not None:\n        Gram_copy = Gram.copy()\n        Cov_copy = Cov.copy()\n    while True:\n        if Cov.size:\n            if positive:\n                C_idx = np.argmax(Cov)\n            else:\n                C_idx = np.argmax(np.abs(Cov))\n            C_ = Cov[C_idx]\n            if positive:\n                C = C_\n            else:\n                C = np.fabs(C_)\n        else:\n            C = 0.0\n        if return_path:\n            alpha = alphas[n_iter, np.newaxis]\n            coef = coefs[n_iter]\n            prev_alpha = alphas[n_iter - 1, np.newaxis]\n            prev_coef = coefs[n_iter - 1]\n        alpha[0] = C / n_samples\n        if alpha[0] <= alpha_min + equality_tolerance:\n            if abs(alpha[0] - alpha_min) > equality_tolerance:\n                if n_iter > 0:\n                    ss = (prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])\n                    coef[:] = prev_coef + ss * (coef - prev_coef)\n                alpha[0] = alpha_min\n            if return_path:\n                coefs[n_iter] = coef\n            break\n        if n_iter >= max_iter or n_active >= n_features:\n            break\n        if not drop:\n            if positive:\n                sign_active[n_active] = np.ones_like(C_)\n            else:\n                sign_active[n_active] = np.sign(C_)\n            m, n = (n_active, C_idx + n_active)\n            Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n            indices[n], indices[m] = (indices[m], indices[n])\n            Cov_not_shortened = Cov\n            Cov = Cov[1:]\n            if Gram is None:\n                X.T[n], X.T[m] = swap(X.T[n], X.T[m])\n                c = nrm2(X.T[n_active]) ** 2\n                L[n_active, :n_active] = np.dot(X.T[n_active], X.T[:n_active].T)\n            else:\n                Gram[m], Gram[n] = swap(Gram[m], Gram[n])\n                Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\n                c = Gram[n_active, n_active]\n                L[n_active, :n_active] = Gram[n_active, :n_active]\n            if n_active:\n                linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, **SOLVE_TRIANGULAR_ARGS)\n            v = np.dot(L[n_active, :n_active], L[n_active, :n_active])\n            diag = max(np.sqrt(np.abs(c - v)), eps)\n            L[n_active, n_active] = diag\n            if diag < 1e-07:\n                warnings.warn('Regressors in active set degenerate. Dropping a regressor, after %i iterations, i.e. alpha=%.3e, with an active set of %i regressors, and the smallest cholesky pivot element being %.3e. Reduce max_iter or increase eps parameters.' % (n_iter, alpha, n_active, diag), ConvergenceWarning)\n                Cov = Cov_not_shortened\n                Cov[0] = 0\n                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n                continue\n            active.append(indices[n_active])\n            n_active += 1\n            if verbose > 1:\n                print('%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s' % (n_iter, active[-1], '', n_active, C))\n        if method == 'lasso' and n_iter > 0 and (prev_alpha[0] < alpha[0]):\n            warnings.warn('Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. %i iterations, alpha=%.3e, previous alpha=%.3e, with an active set of %i regressors.' % (n_iter, alpha, prev_alpha, n_active), ConvergenceWarning)\n            break\n        least_squares, _ = solve_cholesky(L[:n_active, :n_active], sign_active[:n_active], lower=True)\n        if least_squares.size == 1 and least_squares == 0:\n            least_squares[...] = 1\n            AA = 1.0\n        else:\n            AA = 1.0 / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\n            if not np.isfinite(AA):\n                i = 0\n                L_ = L[:n_active, :n_active].copy()\n                while not np.isfinite(AA):\n                    L_.flat[::n_active + 1] += 2 ** i * eps\n                    least_squares, _ = solve_cholesky(L_, sign_active[:n_active], lower=True)\n                    tmp = max(np.sum(least_squares * sign_active[:n_active]), eps)\n                    AA = 1.0 / np.sqrt(tmp)\n                    i += 1\n            least_squares *= AA\n        if Gram is None:\n            eq_dir = np.dot(X.T[:n_active].T, least_squares)\n            corr_eq_dir = np.dot(X.T[n_active:], eq_dir)\n        else:\n            corr_eq_dir = np.dot(Gram[:n_active, n_active:].T, least_squares)\n        g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))\n        if positive:\n            gamma_ = min(g1, C / AA)\n        else:\n            g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))\n            gamma_ = min(g1, g2, C / AA)\n        drop = False\n        z = -coef[active] / (least_squares + tiny32)\n        z_pos = arrayfuncs.min_pos(z)\n        if z_pos < gamma_:\n            idx = np.where(z == z_pos)[0][::-1]\n            sign_active[idx] = -sign_active[idx]\n            if method == 'lasso':\n                gamma_ = z_pos\n            drop = True\n        n_iter += 1\n        if return_path:\n            if n_iter >= coefs.shape[0]:\n                del coef, alpha, prev_alpha, prev_coef\n                add_features = 2 * max(1, max_features - n_active)\n                coefs = np.resize(coefs, (n_iter + add_features, n_features))\n                coefs[-add_features:] = 0\n                alphas = np.resize(alphas, n_iter + add_features)\n                alphas[-add_features:] = 0\n            coef = coefs[n_iter]\n            prev_coef = coefs[n_iter - 1]\n        else:\n            prev_coef = coef\n            prev_alpha[0] = alpha[0]\n            coef = np.zeros_like(coef)\n        coef[active] = prev_coef[active] + gamma_ * least_squares\n        Cov -= gamma_ * corr_eq_dir\n        if drop and method == 'lasso':\n            for ii in idx:\n                arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii)\n            n_active -= 1\n            drop_idx = [active.pop(ii) for ii in idx]\n            if Gram is None:\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\n                        indices[i], indices[i + 1] = (indices[i + 1], indices[i])\n                residual = y - np.dot(X[:, :n_active], coef[active])\n                temp = np.dot(X.T[n_active], residual)\n                Cov = np.r_[temp, Cov]\n            else:\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        indices[i], indices[i + 1] = (indices[i + 1], indices[i])\n                        Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])\n                        Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i], Gram[:, i + 1])\n                temp = Cov_copy[drop_idx] - np.dot(Gram_copy[drop_idx], coef)\n                Cov = np.r_[temp, Cov]\n            sign_active = np.delete(sign_active, idx)\n            sign_active = np.append(sign_active, 0.0)\n            if verbose > 1:\n                print('%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s' % (n_iter, '', drop_idx, n_active, abs(temp)))\n    if return_path:\n        alphas = alphas[:n_iter + 1]\n        coefs = coefs[:n_iter + 1]\n        if return_n_iter:\n            return (alphas, active, coefs.T, n_iter)\n        else:\n            return (alphas, active, coefs.T)\n    elif return_n_iter:\n        return (alpha, active, coef, n_iter)\n    else:\n        return (alpha, active, coef)",
    ".sklearn.linear_model.base.py@@LinearModel._set_intercept": "def _set_intercept(self, X_offset, y_offset, X_scale):\n    if self.fit_intercept:\n        self.coef_ = self.coef_ / X_scale\n        self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n    else:\n        self.intercept_ = 0.0",
    ".sklearn.linear_model.coordinate_descent.py@@Lasso.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    super().__init__(alpha=alpha, l1_ratio=1.0, fit_intercept=fit_intercept, normalize=normalize, precompute=precompute, copy_X=copy_X, max_iter=max_iter, tol=tol, warm_start=warm_start, positive=positive, random_state=random_state, selection=selection)",
    ".sklearn.linear_model.coordinate_descent.py@@ElasticNet.__init__": "def __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.precompute = precompute\n    self.max_iter = max_iter\n    self.copy_X = copy_X\n    self.tol = tol\n    self.warm_start = warm_start\n    self.positive = positive\n    self.random_state = random_state\n    self.selection = selection",
    ".sklearn.linear_model.coordinate_descent.py@@ElasticNet.fit": "def fit(self, X, y, check_input=True):\n    if self.alpha == 0:\n        warnings.warn('With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator', stacklevel=2)\n    if isinstance(self.precompute, str):\n        raise ValueError('precompute should be one of True, False or array-like. Got %r' % self.precompute)\n    X_copied = False\n    if check_input:\n        X_copied = self.copy_X and self.fit_intercept\n        X, y = check_X_y(X, y, accept_sparse='csc', order='F', dtype=[np.float64, np.float32], copy=X_copied, multi_output=True, y_numeric=True)\n        y = check_array(y, order='F', copy=False, dtype=X.dtype.type, ensure_2d=False)\n    should_copy = self.copy_X and (not X_copied)\n    X, y, X_offset, y_offset, X_scale, precompute, Xy = _pre_fit(X, y, None, self.precompute, self.normalize, self.fit_intercept, copy=should_copy, check_input=check_input)\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n    if Xy is not None and Xy.ndim == 1:\n        Xy = Xy[:, np.newaxis]\n    n_samples, n_features = X.shape\n    n_targets = y.shape[1]\n    if self.selection not in ['cyclic', 'random']:\n        raise ValueError('selection should be either random or cyclic.')\n    if not self.warm_start or not hasattr(self, 'coef_'):\n        coef_ = np.zeros((n_targets, n_features), dtype=X.dtype, order='F')\n    else:\n        coef_ = self.coef_\n        if coef_.ndim == 1:\n            coef_ = coef_[np.newaxis, :]\n    dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)\n    self.n_iter_ = []\n    for k in range(n_targets):\n        if Xy is not None:\n            this_Xy = Xy[:, k]\n        else:\n            this_Xy = None\n        _, this_coef, this_dual_gap, this_iter = self.path(X, y[:, k], l1_ratio=self.l1_ratio, eps=None, n_alphas=None, alphas=[self.alpha], precompute=precompute, Xy=this_Xy, fit_intercept=False, normalize=False, copy_X=True, verbose=False, tol=self.tol, positive=self.positive, X_offset=X_offset, X_scale=X_scale, return_n_iter=True, coef_init=coef_[k], max_iter=self.max_iter, random_state=self.random_state, selection=self.selection, check_input=False)\n        coef_[k] = this_coef[:, 0]\n        dual_gaps_[k] = this_dual_gap[0]\n        self.n_iter_.append(this_iter[0])\n    if n_targets == 1:\n        self.n_iter_ = self.n_iter_[0]\n        self.coef_ = coef_[0]\n        self.dual_gap_ = dual_gaps_[0]\n    else:\n        self.coef_ = coef_\n        self.dual_gap_ = dual_gaps_\n    self._set_intercept(X_offset, y_offset, X_scale)\n    self.coef_ = np.asarray(self.coef_, dtype=X.dtype)\n    return self",
    ".sklearn.linear_model.base.py@@_pre_fit": "def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy, check_input=True):\n    n_samples, n_features = X.shape\n    if sparse.isspmatrix(X):\n        precompute = False\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, copy=False, return_mean=True, check_input=check_input)\n    else:\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy, check_input=check_input)\n    if hasattr(precompute, '__array__') and (fit_intercept and (not np.allclose(X_offset, np.zeros(n_features))) or (normalize and (not np.allclose(X_scale, np.ones(n_features))))):\n        warnings.warn('Gram matrix was provided but X was centered to fit intercept, or X was normalized : recomputing Gram matrix.', UserWarning)\n        precompute = 'auto'\n        Xy = None\n    if isinstance(precompute, str) and precompute == 'auto':\n        precompute = n_samples > n_features\n    if precompute is True:\n        precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype, order='C')\n        np.dot(X.T, X, out=precompute)\n    if not hasattr(precompute, '__array__'):\n        Xy = None\n    if hasattr(precompute, '__array__') and Xy is None:\n        common_dtype = np.find_common_type([X.dtype, y.dtype], [])\n        if y.ndim == 1:\n            Xy = np.empty(shape=n_features, dtype=common_dtype, order='C')\n            np.dot(X.T, y, out=Xy)\n        else:\n            n_targets = y.shape[1]\n            Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F')\n            np.dot(y.T, X, out=Xy.T)\n    return (X, y, X_offset, y_offset, X_scale, precompute, Xy)",
    ".sklearn.linear_model.coordinate_descent.py@@enet_path": "def enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params):\n    if check_input:\n        X = check_array(X, 'csc', dtype=[np.float64, np.float32], order='F', copy=copy_X)\n        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False, ensure_2d=False)\n        if Xy is not None:\n            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False, ensure_2d=False)\n    n_samples, n_features = X.shape\n    multi_output = False\n    if y.ndim != 1:\n        multi_output = True\n        _, n_outputs = y.shape\n    if multi_output and positive:\n        raise ValueError('positive=True is not allowed for multi-output (y.ndim != 1)')\n    if not multi_output and sparse.isspmatrix(X):\n        if 'X_offset' in params:\n            X_sparse_scaling = params['X_offset'] / params['X_scale']\n            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)\n        else:\n            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)\n    if check_input:\n        X, y, X_offset, y_offset, X_scale, precompute, Xy = _pre_fit(X, y, Xy, precompute, normalize=False, fit_intercept=False, copy=False, check_input=check_input)\n    if alphas is None:\n        alphas = _alpha_grid(X, y, Xy=Xy, l1_ratio=l1_ratio, fit_intercept=False, eps=eps, n_alphas=n_alphas, normalize=False, copy_X=False)\n    else:\n        alphas = np.sort(alphas)[::-1]\n    n_alphas = len(alphas)\n    tol = params.get('tol', 0.0001)\n    max_iter = params.get('max_iter', 1000)\n    dual_gaps = np.empty(n_alphas)\n    n_iters = []\n    rng = check_random_state(params.get('random_state', None))\n    selection = params.get('selection', 'cyclic')\n    if selection not in ['random', 'cyclic']:\n        raise ValueError('selection should be either random or cyclic.')\n    random = selection == 'random'\n    if not multi_output:\n        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)\n    else:\n        coefs = np.empty((n_outputs, n_features, n_alphas), dtype=X.dtype)\n    if coef_init is None:\n        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')\n    else:\n        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)\n    for i, alpha in enumerate(alphas):\n        l1_reg = alpha * l1_ratio * n_samples\n        l2_reg = alpha * (1.0 - l1_ratio) * n_samples\n        if not multi_output and sparse.isspmatrix(X):\n            model = cd_fast.sparse_enet_coordinate_descent(coef_, l1_reg, l2_reg, X.data, X.indices, X.indptr, y, X_sparse_scaling, max_iter, tol, rng, random, positive)\n        elif multi_output:\n            model = cd_fast.enet_coordinate_descent_multi_task(coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random)\n        elif isinstance(precompute, np.ndarray):\n            if check_input:\n                precompute = check_array(precompute, dtype=X.dtype.type, order='C')\n            model = cd_fast.enet_coordinate_descent_gram(coef_, l1_reg, l2_reg, precompute, Xy, y, max_iter, tol, rng, random, positive)\n        elif precompute is False:\n            model = cd_fast.enet_coordinate_descent(coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive)\n        else:\n            raise ValueError(\"Precompute should be one of True, False, 'auto' or array-like. Got %r\" % precompute)\n        coef_, dual_gap_, eps_, n_iter_ = model\n        coefs[..., i] = coef_\n        dual_gaps[i] = dual_gap_\n        n_iters.append(n_iter_)\n        if verbose:\n            if verbose > 2:\n                print(model)\n            elif verbose > 1:\n                print('Path: %03i out of %03i' % (i, n_alphas))\n            else:\n                sys.stderr.write('.')\n    if return_n_iter:\n        return (alphas, coefs, dual_gaps, n_iters)\n    return (alphas, coefs, dual_gaps)",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.linear_model.least_angle.py@@Lars.__init__": "def __init__(self, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n    self.fit_intercept = fit_intercept\n    self.verbose = verbose\n    self.normalize = normalize\n    self.precompute = precompute\n    self.n_nonzero_coefs = n_nonzero_coefs\n    self.eps = eps\n    self.copy_X = copy_X\n    self.fit_path = fit_path",
    ".sklearn.decomposition.dict_learning.py@@_update_dict": "def _update_dict(dictionary, Y, code, verbose=False, return_r2=False, random_state=None, positive=False):\n    n_components = len(code)\n    n_features = Y.shape[0]\n    random_state = check_random_state(random_state)\n    gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))\n    ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))\n    nrm2, = linalg.get_blas_funcs(('nrm2',), (dictionary,))\n    R = gemm(-1.0, dictionary, code, 1.0, Y)\n    for k in range(n_components):\n        R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)\n        dictionary[:, k] = np.dot(R, code[k, :])\n        if positive:\n            np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])\n        atom_norm = nrm2(dictionary[:, k])\n        if atom_norm < 1e-10:\n            if verbose == 1:\n                sys.stdout.write('+')\n                sys.stdout.flush()\n            elif verbose:\n                print('Adding new random atom')\n            dictionary[:, k] = random_state.randn(n_features)\n            if positive:\n                np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])\n            code[k, :] = 0.0\n            atom_norm = nrm2(dictionary[:, k])\n            dictionary[:, k] /= atom_norm\n        else:\n            dictionary[:, k] /= atom_norm\n            R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)\n    if return_r2:\n        R = nrm2(R) ** 2.0\n        return (dictionary, R)\n    return dictionary",
    ".sklearn.decomposition.dict_learning.py@@sparse_encode": "def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None, max_iter=1000, n_jobs=None, check_input=True, verbose=0, positive=False):\n    if check_input:\n        if algorithm == 'lasso_cd':\n            dictionary = check_array(dictionary, order='C', dtype='float64')\n            X = check_array(X, order='C', dtype='float64')\n        else:\n            dictionary = check_array(dictionary)\n            X = check_array(X)\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n    if gram is None and algorithm != 'threshold':\n        gram = np.dot(dictionary, dictionary.T)\n    if cov is None and algorithm != 'lasso_cd':\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n    if algorithm in ('lars', 'omp'):\n        regularization = n_nonzero_coefs\n        if regularization is None:\n            regularization = min(max(n_features / 10, 1), n_components)\n    else:\n        regularization = alpha\n        if regularization is None:\n            regularization = 1.0\n    if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':\n        code = _sparse_encode(X, dictionary, gram, cov=cov, algorithm=algorithm, regularization=regularization, copy_cov=copy_cov, init=init, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive)\n        return code\n    code = np.empty((n_samples, n_components))\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(_sparse_encode)(X[this_slice], dictionary, gram, cov[:, this_slice] if cov is not None else None, algorithm, regularization=regularization, copy_cov=copy_cov, init=init[this_slice] if init is not None else None, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive) for this_slice in slices))\n    for this_slice, this_view in zip(slices, code_views):\n        code[this_slice] = this_view\n    return code",
    ".sklearn.utils.__init__.py@@gen_even_slices": "def gen_even_slices(n, n_packs, n_samples=None):\n    start = 0\n    if n_packs < 1:\n        raise ValueError('gen_even_slices got n_packs=%s, must be >=1' % n_packs)\n    for pack_num in range(n_packs):\n        this_n = n // n_packs\n        if pack_num < n % n_packs:\n            this_n += 1\n        if this_n > 0:\n            end = start + this_n\n            if n_samples is not None:\n                end = min(n_samples, end)\n            yield slice(start, end, None)\n            start = end",
    ".sklearn.decomposition.dict_learning.py@@SparseCodingMixin.transform": "def transform(self, X):\n    check_is_fitted(self, 'components_')\n    X = check_array(X)\n    code = sparse_encode(X, self.components_, algorithm=self.transform_algorithm, n_nonzero_coefs=self.transform_n_nonzero_coefs, alpha=self.transform_alpha, max_iter=self.transform_max_iter, n_jobs=self.n_jobs, positive=self.positive_code)\n    if self.split_sign:\n        n_samples, n_features = code.shape\n        split_code = np.empty((n_samples, 2 * n_features))\n        split_code[:, :n_features] = np.maximum(code, 0)\n        split_code[:, n_features:] = -np.minimum(code, 0)\n        code = split_code\n    return code",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.base.py@@BaseEstimator.set_params": "def set_params(self, **params):\n    if not params:\n        return self\n    valid_params = self.get_params(deep=True)\n    nested_params = defaultdict(dict)\n    for key, value in params.items():\n        key, delim, sub_key = key.partition('__')\n        if key not in valid_params:\n            raise ValueError('Invalid parameter %s for estimator %s. Check the list of available parameters with `estimator.get_params().keys()`.' % (key, self))\n        if delim:\n            nested_params[key][sub_key] = value\n        else:\n            setattr(self, key, value)\n            valid_params[key] = value\n    for key, sub_params in nested_params.items():\n        valid_params[key].set_params(**sub_params)\n    return self",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key, None)\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.utils.testing.py@@TempMemmap.__init__": "def __init__(self, data, mmap_mode='r'):\n    self.mmap_mode = mmap_mode\n    self.data = data",
    ".sklearn.utils.testing.py@@TempMemmap.__enter__": "def __enter__(self):\n    data_read_only, self.temp_folder = create_memmap_backed_data(self.data, mmap_mode=self.mmap_mode, return_folder=True)\n    return data_read_only",
    ".sklearn.utils.testing.py@@create_memmap_backed_data": "def create_memmap_backed_data(data, mmap_mode='r', return_folder=False):\n    temp_folder = tempfile.mkdtemp(prefix='sklearn_testing_')\n    atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))\n    filename = op.join(temp_folder, 'data.pkl')\n    joblib.dump(data, filename)\n    memmap_backed_data = joblib.load(filename, mmap_mode=mmap_mode)\n    result = memmap_backed_data if not return_folder else (memmap_backed_data, temp_folder)\n    return result",
    ".sklearn.decomposition.dict_learning.py@@DictionaryLearning.__init__": "def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-08, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=None, code_init=None, dict_init=None, verbose=False, split_sign=False, random_state=None, positive_code=False, positive_dict=False, transform_max_iter=1000):\n    self._set_sparse_coding_params(n_components, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs, positive_code, transform_max_iter)\n    self.alpha = alpha\n    self.max_iter = max_iter\n    self.tol = tol\n    self.fit_algorithm = fit_algorithm\n    self.code_init = code_init\n    self.dict_init = dict_init\n    self.verbose = verbose\n    self.random_state = random_state\n    self.positive_dict = positive_dict",
    ".sklearn.decomposition.dict_learning.py@@SparseCodingMixin._set_sparse_coding_params": "def _set_sparse_coding_params(self, n_components, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=None, positive_code=False, transform_max_iter=1000):\n    self.n_components = n_components\n    self.transform_algorithm = transform_algorithm\n    self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n    self.transform_alpha = transform_alpha\n    self.transform_max_iter = transform_max_iter\n    self.split_sign = split_sign\n    self.n_jobs = n_jobs\n    self.positive_code = positive_code",
    ".sklearn.utils.testing.py@@ignore_warnings": "def ignore_warnings(obj=None, category=Warning):\n    if isinstance(obj, type) and issubclass(obj, Warning):\n        warning_name = obj.__name__\n        raise ValueError(\"'obj' should be a callable where you want to ignore warnings. You passed a warning class instead: 'obj={warning_name}'. If you want to pass a warning class to ignore_warnings, you should use 'category={warning_name}'\".format(warning_name=warning_name))\n    elif callable(obj):\n        return _IgnoreWarnings(category=category)(obj)\n    else:\n        return _IgnoreWarnings(category=category)",
    ".sklearn.utils.testing.py@@_IgnoreWarnings.__init__": "def __init__(self, category):\n    self._record = True\n    self._module = sys.modules['warnings']\n    self._entered = False\n    self.log = []\n    self.category = category",
    ".sklearn.utils.testing.py@@_IgnoreWarnings.__enter__": "def __enter__(self):\n    if self._entered:\n        raise RuntimeError('Cannot enter %r twice' % self)\n    self._entered = True\n    self._filters = self._module.filters\n    self._module.filters = self._filters[:]\n    self._showwarning = self._module.showwarning\n    warnings.simplefilter('ignore', self.category)",
    ".sklearn.decomposition.dict_learning.py@@DictionaryLearning.fit": "def fit(self, X, y=None):\n    random_state = check_random_state(self.random_state)\n    X = check_array(X)\n    if self.n_components is None:\n        n_components = X.shape[1]\n    else:\n        n_components = self.n_components\n    V, U, E, self.n_iter_ = dict_learning(X, n_components, self.alpha, tol=self.tol, max_iter=self.max_iter, method=self.fit_algorithm, method_max_iter=self.transform_max_iter, n_jobs=self.n_jobs, code_init=self.code_init, dict_init=self.dict_init, verbose=self.verbose, random_state=random_state, return_n_iter=True, positive_dict=self.positive_dict, positive_code=self.positive_code)\n    self.components_ = U\n    self.error_ = E\n    return self",
    ".sklearn.decomposition.dict_learning.py@@dict_learning": "def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-08, method='lars', n_jobs=None, dict_init=None, code_init=None, callback=None, verbose=False, random_state=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000):\n    if method not in ('lars', 'cd'):\n        raise ValueError('Coding method %r not supported as a fit algorithm.' % method)\n    _check_positive_coding(method, positive_code)\n    method = 'lasso_' + method\n    t0 = time.time()\n    alpha = float(alpha)\n    random_state = check_random_state(random_state)\n    if code_init is not None and dict_init is not None:\n        code = np.array(code_init, order='F')\n        dictionary = dict_init\n    else:\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\n        dictionary = S[:, np.newaxis] * dictionary\n    r = len(dictionary)\n    if n_components <= r:\n        code = code[:, :n_components]\n        dictionary = dictionary[:n_components, :]\n    else:\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\n        dictionary = np.r_[dictionary, np.zeros((n_components - r, dictionary.shape[1]))]\n    dictionary = np.array(dictionary, order='F')\n    residuals = 0\n    errors = []\n    current_cost = np.nan\n    if verbose == 1:\n        print('[dict_learning]', end=' ')\n    ii = -1\n    for ii in range(max_iter):\n        dt = time.time() - t0\n        if verbose == 1:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n        elif verbose:\n            print('Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)' % (ii, dt, dt / 60, current_cost))\n        code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha, init=code, n_jobs=n_jobs, positive=positive_code, max_iter=method_max_iter, verbose=verbose)\n        dictionary, residuals = _update_dict(dictionary.T, X.T, code.T, verbose=verbose, return_r2=True, random_state=random_state, positive=positive_dict)\n        dictionary = dictionary.T\n        current_cost = 0.5 * residuals + alpha * np.sum(np.abs(code))\n        errors.append(current_cost)\n        if ii > 0:\n            dE = errors[-2] - errors[-1]\n            if dE < tol * errors[-1]:\n                if verbose == 1:\n                    print('')\n                elif verbose:\n                    print('--- Convergence reached after %d iterations' % ii)\n                break\n        if ii % 5 == 0 and callback is not None:\n            callback(locals())\n    if return_n_iter:\n        return (code, dictionary, errors, ii + 1)\n    else:\n        return (code, dictionary, errors)"
}