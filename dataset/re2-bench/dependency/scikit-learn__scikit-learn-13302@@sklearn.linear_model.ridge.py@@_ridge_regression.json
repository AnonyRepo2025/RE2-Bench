{
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.linear_model.ridge.py@@_solve_svd": "def _solve_svd(X, y, alpha):\n    U, s, Vt = linalg.svd(X, full_matrices=False)\n    idx = s > 1e-15\n    s_nnz = s[idx][:, np.newaxis]\n    UTy = np.dot(U.T, y)\n    d = np.zeros((s.size, alpha.size), dtype=X.dtype)\n    d[idx] = s_nnz / (s_nnz ** 2 + alpha)\n    d_UT_y = d * UTy\n    return np.dot(Vt.T, d_UT_y).T",
    ".sklearn.linear_model.ridge.py@@_solve_sparse_cg": "def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=0.001, verbose=0, X_offset=None, X_scale=None):\n\n    def _get_rescaled_operator(X):\n        X_offset_scale = X_offset / X_scale\n\n        def matvec(b):\n            return X.dot(b) - b.dot(X_offset_scale)\n\n        def rmatvec(b):\n            return X.T.dot(b) - X_offset_scale * np.sum(b)\n        X1 = sparse.linalg.LinearOperator(shape=X.shape, matvec=matvec, rmatvec=rmatvec)\n        return X1\n    n_samples, n_features = X.shape\n    if X_offset is None or X_scale is None:\n        X1 = sp_linalg.aslinearoperator(X)\n    else:\n        X1 = _get_rescaled_operator(X)\n    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)\n    if n_features > n_samples:\n\n        def create_mv(curr_alpha):\n\n            def _mv(x):\n                return X1.matvec(X1.rmatvec(x)) + curr_alpha * x\n            return _mv\n    else:\n\n        def create_mv(curr_alpha):\n\n            def _mv(x):\n                return X1.rmatvec(X1.matvec(x)) + curr_alpha * x\n            return _mv\n    for i in range(y.shape[1]):\n        y_column = y[:, i]\n        mv = create_mv(alpha[i])\n        if n_features > n_samples:\n            C = sp_linalg.LinearOperator((n_samples, n_samples), matvec=mv, dtype=X.dtype)\n            try:\n                coef, info = sp_linalg.cg(C, y_column, tol=tol, atol='legacy')\n            except TypeError:\n                coef, info = sp_linalg.cg(C, y_column, tol=tol)\n            coefs[i] = X1.rmatvec(coef)\n        else:\n            y_column = X1.rmatvec(y_column)\n            C = sp_linalg.LinearOperator((n_features, n_features), matvec=mv, dtype=X.dtype)\n            try:\n                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter, tol=tol, atol='legacy')\n            except TypeError:\n                coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter, tol=tol)\n        if info < 0:\n            raise ValueError('Failed with error code %d' % info)\n        if max_iter is None and info > 0 and verbose:\n            warnings.warn('sparse_cg did not converge after %d iterations.' % info, ConvergenceWarning)\n    return coefs",
    ".sklearn.linear_model.ridge.py@@create_mv": "def create_mv(curr_alpha):\n\n    def _mv(x):\n        return X1.rmatvec(X1.matvec(x)) + curr_alpha * x\n    return _mv",
    ".sklearn.linear_model.ridge.py@@_mv": "def _mv(x):\n    return X1.rmatvec(X1.matvec(x)) + curr_alpha * x",
    ".sklearn.linear_model.ridge.py@@_solve_cholesky": "def _solve_cholesky(X, y, alpha):\n    n_samples, n_features = X.shape\n    n_targets = y.shape[1]\n    A = safe_sparse_dot(X.T, X, dense_output=True)\n    Xy = safe_sparse_dot(X.T, y, dense_output=True)\n    one_alpha = np.array_equal(alpha, len(alpha) * [alpha[0]])\n    if one_alpha:\n        A.flat[::n_features + 1] += alpha[0]\n        return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n    else:\n        coefs = np.empty([n_targets, n_features], dtype=X.dtype)\n        for coef, target, current_alpha in zip(coefs, Xy.T, alpha):\n            A.flat[::n_features + 1] += current_alpha\n            coef[:] = linalg.solve(A, target, sym_pos=True, overwrite_a=False).ravel()\n            A.flat[::n_features + 1] -= current_alpha\n        return coefs",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if sparse.issparse(a) or sparse.issparse(b):\n        ret = a * b\n        if dense_output and hasattr(ret, 'toarray'):\n            ret = ret.toarray()\n        return ret\n    else:\n        return np.dot(a, b)",
    ".sklearn.linear_model.base.py@@_rescale_data": "def _rescale_data(X, y, sample_weight):\n    n_samples = X.shape[0]\n    sample_weight = np.full(n_samples, sample_weight, dtype=np.array(sample_weight).dtype)\n    sample_weight = np.sqrt(sample_weight)\n    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n    X = safe_sparse_dot(sw_matrix, X)\n    y = safe_sparse_dot(sw_matrix, y)\n    return (X, y)",
    ".sklearn.linear_model.ridge.py@@_solve_cholesky_kernel": "def _solve_cholesky_kernel(K, y, alpha, sample_weight=None, copy=False):\n    n_samples = K.shape[0]\n    n_targets = y.shape[1]\n    if copy:\n        K = K.copy()\n    alpha = np.atleast_1d(alpha)\n    one_alpha = (alpha == alpha[0]).all()\n    has_sw = isinstance(sample_weight, np.ndarray) or sample_weight not in [1.0, None]\n    if has_sw:\n        sw = np.sqrt(np.atleast_1d(sample_weight))\n        y = y * sw[:, np.newaxis]\n        K *= np.outer(sw, sw)\n    if one_alpha:\n        K.flat[::n_samples + 1] += alpha[0]\n        try:\n            dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)\n        except np.linalg.LinAlgError:\n            warnings.warn('Singular matrix in solving dual problem. Using least-squares solution instead.')\n            dual_coef = linalg.lstsq(K, y)[0]\n        K.flat[::n_samples + 1] -= alpha[0]\n        if has_sw:\n            dual_coef *= sw[:, np.newaxis]\n        return dual_coef\n    else:\n        dual_coefs = np.empty([n_targets, n_samples], K.dtype)\n        for dual_coef, target, current_alpha in zip(dual_coefs, y.T, alpha):\n            K.flat[::n_samples + 1] += current_alpha\n            dual_coef[:] = linalg.solve(K, target, sym_pos=True, overwrite_a=False).ravel()\n            K.flat[::n_samples + 1] -= current_alpha\n        if has_sw:\n            dual_coefs *= sw[np.newaxis, :]\n        return dual_coefs.T",
    ".sklearn.linear_model.ridge.py@@_solve_lsqr": "def _solve_lsqr(X, y, alpha, max_iter=None, tol=0.001):\n    n_samples, n_features = X.shape\n    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)\n    n_iter = np.empty(y.shape[1], dtype=np.int32)\n    sqrt_alpha = np.sqrt(alpha)\n    for i in range(y.shape[1]):\n        y_column = y[:, i]\n        info = sp_linalg.lsqr(X, y_column, damp=sqrt_alpha[i], atol=tol, btol=tol, iter_lim=max_iter)\n        coefs[i] = info[0]\n        n_iter[i] = info[2]\n    return (coefs, n_iter)",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.linear_model.sag.py@@sag_solver": "def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.0, beta=0.0, max_iter=1000, tol=0.001, verbose=0, random_state=None, check_input=True, max_squared_sum=None, warm_start_mem=None, is_saga=False):\n    if warm_start_mem is None:\n        warm_start_mem = {}\n    if max_iter is None:\n        max_iter = 1000\n    if check_input:\n        _dtype = [np.float64, np.float32]\n        X = check_array(X, dtype=_dtype, accept_sparse='csr', order='C')\n        y = check_array(y, dtype=_dtype, ensure_2d=False, order='C')\n    n_samples, n_features = (X.shape[0], X.shape[1])\n    alpha_scaled = float(alpha) / n_samples\n    beta_scaled = float(beta) / n_samples\n    n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=X.dtype, order='C')\n    if 'coef' in warm_start_mem.keys():\n        coef_init = warm_start_mem['coef']\n    else:\n        coef_init = np.zeros((n_features, n_classes), dtype=X.dtype, order='C')\n    fit_intercept = coef_init.shape[0] == n_features + 1\n    if fit_intercept:\n        intercept_init = coef_init[-1, :]\n        coef_init = coef_init[:-1, :]\n    else:\n        intercept_init = np.zeros(n_classes, dtype=X.dtype)\n    if 'intercept_sum_gradient' in warm_start_mem.keys():\n        intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']\n    else:\n        intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)\n    if 'gradient_memory' in warm_start_mem.keys():\n        gradient_memory_init = warm_start_mem['gradient_memory']\n    else:\n        gradient_memory_init = np.zeros((n_samples, n_classes), dtype=X.dtype, order='C')\n    if 'sum_gradient' in warm_start_mem.keys():\n        sum_gradient_init = warm_start_mem['sum_gradient']\n    else:\n        sum_gradient_init = np.zeros((n_features, n_classes), dtype=X.dtype, order='C')\n    if 'seen' in warm_start_mem.keys():\n        seen_init = warm_start_mem['seen']\n    else:\n        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')\n    if 'num_seen' in warm_start_mem.keys():\n        num_seen_init = warm_start_mem['num_seen']\n    else:\n        num_seen_init = 0\n    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)\n    if max_squared_sum is None:\n        max_squared_sum = row_norms(X, squared=True).max()\n    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=n_samples, is_saga=is_saga)\n    if step_size * alpha_scaled == 1:\n        raise ZeroDivisionError('Current sag implementation does not handle the case step_size * alpha_scaled == 1')\n    sag = sag64 if X.dtype == np.float64 else sag32\n    num_seen, n_iter_ = sag(dataset, coef_init, intercept_init, n_samples, n_features, n_classes, tol, max_iter, loss, step_size, alpha_scaled, beta_scaled, sum_gradient_init, gradient_memory_init, seen_init, num_seen_init, fit_intercept, intercept_sum_gradient, intercept_decay, is_saga, verbose)\n    if n_iter_ == max_iter:\n        warnings.warn('The max_iter was reached which means the coef_ did not converge', ConvergenceWarning)\n    if fit_intercept:\n        coef_init = np.vstack((coef_init, intercept_init))\n    warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init, 'intercept_sum_gradient': intercept_sum_gradient, 'gradient_memory': gradient_memory_init, 'seen': seen_init, 'num_seen': num_seen}\n    if loss == 'multinomial':\n        coef_ = coef_init.T\n    else:\n        coef_ = coef_init[:, 0]\n    return (coef_, n_iter_, warm_start_mem)",
    ".sklearn.linear_model.base.py@@make_dataset": "def make_dataset(X, y, sample_weight, random_state=None):\n    rng = check_random_state(random_state)\n    seed = rng.randint(1, np.iinfo(np.int32).max)\n    if X.dtype == np.float32:\n        CSRData = CSRDataset32\n        ArrayData = ArrayDataset32\n    else:\n        CSRData = CSRDataset64\n        ArrayData = ArrayDataset64\n    if sp.issparse(X):\n        dataset = CSRData(X.data, X.indptr, X.indices, y, sample_weight, seed=seed)\n        intercept_decay = SPARSE_INTERCEPT_DECAY\n    else:\n        dataset = ArrayData(X, y, sample_weight, seed=seed)\n        intercept_decay = 1.0\n    return (dataset, intercept_decay)",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.linear_model.sag.py@@get_auto_step_size": "def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=None, is_saga=False):\n    if loss in ('log', 'multinomial'):\n        L = 0.25 * (max_squared_sum + int(fit_intercept)) + alpha_scaled\n    elif loss == 'squared':\n        L = max_squared_sum + int(fit_intercept) + alpha_scaled\n    else:\n        raise ValueError(\"Unknown loss function for SAG solver, got %s instead of 'log' or 'squared'\" % loss)\n    if is_saga:\n        mun = min(2 * n_samples * alpha_scaled, L)\n        step = 1.0 / (2 * L + mun)\n    else:\n        step = 1.0 / L\n    return step",
    ".sklearn.linear_model.ridge.py@@_get_valid_accept_sparse": "def _get_valid_accept_sparse(is_X_sparse, solver):\n    if is_X_sparse and solver in ['auto', 'sag', 'saga']:\n        return 'csr'\n    else:\n        return ['csr', 'csc', 'coo']",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.linear_model.ridge.py@@_get_rescaled_operator": "def _get_rescaled_operator(X):\n    X_offset_scale = X_offset / X_scale\n\n    def matvec(b):\n        return X.dot(b) - b.dot(X_offset_scale)\n\n    def rmatvec(b):\n        return X.T.dot(b) - X_offset_scale * np.sum(b)\n    X1 = sparse.linalg.LinearOperator(shape=X.shape, matvec=matvec, rmatvec=rmatvec)\n    return X1",
    ".sklearn.linear_model.ridge.py@@matvec": "def matvec(b):\n    return X.dot(b) - b.dot(X_offset_scale)",
    ".sklearn.linear_model.ridge.py@@rmatvec": "def rmatvec(b):\n    return X.T.dot(b) - X_offset_scale * np.sum(b)",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)"
}