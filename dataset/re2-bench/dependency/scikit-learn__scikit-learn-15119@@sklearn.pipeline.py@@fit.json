{
    ".sklearn.utils.metaestimators.py@@_BaseComposition._validate_names": "def _validate_names(self, names):\n    if len(set(names)) != len(names):\n        raise ValueError('Names provided are not unique: {0!r}'.format(list(names)))\n    invalid_names = set(names).intersection(self.get_params(deep=False))\n    if invalid_names:\n        raise ValueError('Estimator names conflict with constructor arguments: {0!r}'.format(sorted(invalid_names)))\n    invalid_names = [name for name in names if '__' in name]\n    if invalid_names:\n        raise ValueError('Estimator names must not contain __: got {0!r}'.format(invalid_names))",
    ".sklearn.utils.metaestimators.py@@_BaseComposition._get_params": "def _get_params(self, attr, deep=True):\n    out = super().get_params(deep=deep)\n    if not deep:\n        return out\n    estimators = getattr(self, attr)\n    out.update(estimators)\n    for name, estimator in estimators:\n        if hasattr(estimator, 'get_params'):\n            for key, value in estimator.get_params(deep=True).items():\n                out['%s__%s' % (name, key)] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        try:\n            value = getattr(self, key)\n        except AttributeError:\n            warnings.warn('From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.', FutureWarning)\n            value = None\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.pipeline.py@@_fit_one": "def _fit_one(transformer, X, y, weight, message_clsname='', message=None, **fit_params):\n    with _print_elapsed_time(message_clsname, message):\n        return transformer.fit(X, y, **fit_params)",
    ".sklearn.utils.__init__.py@@_print_elapsed_time": "def _print_elapsed_time(source, message=None):\n    if message is None:\n        yield\n    else:\n        start = timeit.default_timer()\n        yield\n        print(_message_with_time(source, message, timeit.default_timer() - start))",
    ".sklearn.decomposition.truncated_svd.py@@TruncatedSVD.fit": "def fit(self, X, y=None):\n    self.fit_transform(X)\n    return self",
    ".sklearn.decomposition.truncated_svd.py@@TruncatedSVD.fit_transform": "def fit_transform(self, X, y=None):\n    X = check_array(X, accept_sparse=['csr', 'csc'], ensure_min_features=2)\n    random_state = check_random_state(self.random_state)\n    if self.algorithm == 'arpack':\n        U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n        Sigma = Sigma[::-1]\n        U, VT = svd_flip(U[:, ::-1], VT[::-1])\n    elif self.algorithm == 'randomized':\n        k = self.n_components\n        n_features = X.shape[1]\n        if k >= n_features:\n            raise ValueError('n_components must be < n_features; got %d >= %d' % (k, n_features))\n        U, Sigma, VT = randomized_svd(X, self.n_components, n_iter=self.n_iter, random_state=random_state)\n    else:\n        raise ValueError('unknown algorithm %r' % self.algorithm)\n    self.components_ = VT\n    X_transformed = U * Sigma\n    self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n    if sp.issparse(X):\n        _, full_var = mean_variance_axis(X, axis=0)\n        full_var = full_var.sum()\n    else:\n        full_var = np.var(X, axis=0).sum()\n    self.explained_variance_ratio_ = exp_var / full_var\n    self.singular_values_ = Sigma\n    return X_transformed",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)\n                    array = array.astype(dtype, casting='unsafe', copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, msg_dtype if msg_dtype is not None else X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.utils.extmath.py@@randomized_svd": "def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto', power_iteration_normalizer='auto', transpose='auto', flip_sign=True, random_state=0):\n    if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):\n        warnings.warn('Calculating SVD of a {} is expensive. csr_matrix is more efficient.'.format(type(M).__name__), sparse.SparseEfficiencyWarning)\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n    if n_iter == 'auto':\n        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4\n    if transpose == 'auto':\n        transpose = n_samples < n_features\n    if transpose:\n        M = M.T\n    Q = randomized_range_finder(M, n_random, n_iter, power_iteration_normalizer, random_state)\n    B = safe_sparse_dot(Q.T, M)\n    Uhat, s, V = linalg.svd(B, full_matrices=False)\n    del B\n    U = np.dot(Q, Uhat)\n    if flip_sign:\n        if not transpose:\n            U, V = svd_flip(U, V)\n        else:\n            U, V = svd_flip(U, V, u_based_decision=False)\n    if transpose:\n        return (V[:n_components, :].T, s[:n_components], U[:, :n_components].T)\n    else:\n        return (U[:, :n_components], s[:n_components], V[:n_components, :])",
    ".sklearn.utils.extmath.py@@randomized_range_finder": "def randomized_range_finder(A, size, n_iter, power_iteration_normalizer='auto', random_state=None):\n    random_state = check_random_state(random_state)\n    Q = random_state.normal(size=(A.shape[1], size))\n    if A.dtype.kind == 'f':\n        Q = Q.astype(A.dtype, copy=False)\n    if power_iteration_normalizer == 'auto':\n        if n_iter <= 2:\n            power_iteration_normalizer = 'none'\n        else:\n            power_iteration_normalizer = 'LU'\n    for i in range(n_iter):\n        if power_iteration_normalizer == 'none':\n            Q = safe_sparse_dot(A, Q)\n            Q = safe_sparse_dot(A.T, Q)\n        elif power_iteration_normalizer == 'LU':\n            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n        elif power_iteration_normalizer == 'QR':\n            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    return Q",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n    if sparse.issparse(a) and sparse.issparse(b) and dense_output and hasattr(ret, 'toarray'):\n        return ret.toarray()\n    return ret",
    ".sklearn.utils.extmath.py@@svd_flip": "def svd_flip(u, v, u_based_decision=True):\n    if u_based_decision:\n        max_abs_cols = np.argmax(np.abs(u), axis=0)\n        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    else:\n        max_abs_rows = np.argmax(np.abs(v), axis=1)\n        signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    return (u, v)",
    ".sklearn.feature_selection.univariate_selection.py@@_BaseFilter.fit": "def fit(self, X, y):\n    X, y = check_X_y(X, y, ['csr', 'csc'], multi_output=True)\n    if not callable(self.score_func):\n        raise TypeError('The score function should be a callable, %s (%s) was passed.' % (self.score_func, type(self.score_func)))\n    self._check_params(X, y)\n    score_func_ret = self.score_func(X, y)\n    if isinstance(score_func_ret, (list, tuple)):\n        self.scores_, self.pvalues_ = score_func_ret\n        self.pvalues_ = np.asarray(self.pvalues_)\n    else:\n        self.scores_ = score_func_ret\n        self.pvalues_ = None\n    self.scores_ = np.asarray(self.scores_)\n    return self",
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.feature_selection.univariate_selection.py@@SelectKBest._check_params": "def _check_params(self, X, y):\n    if not (self.k == 'all' or 0 <= self.k <= X.shape[1]):\n        raise ValueError(\"k should be >=0, <= n_features = %d; got %r. Use k='all' to return all features.\" % (X.shape[1], self.k))",
    ".sklearn.feature_selection.univariate_selection.py@@f_classif": "def f_classif(X, y):\n    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'])\n    args = [X[safe_mask(X, y == k)] for k in np.unique(y)]\n    return f_oneway(*args)",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.utils.__init__.py@@safe_mask": "def safe_mask(X, mask):\n    mask = np.asarray(mask)\n    if np.issubdtype(mask.dtype, np.signedinteger):\n        return mask\n    if hasattr(X, 'toarray'):\n        ind = np.arange(mask.shape[0])\n        mask = ind[mask]\n    return mask",
    ".sklearn.feature_selection.univariate_selection.py@@f_oneway": "def f_oneway(*args):\n    n_classes = len(args)\n    args = [as_float_array(a) for a in args]\n    n_samples_per_class = np.array([a.shape[0] for a in args])\n    n_samples = np.sum(n_samples_per_class)\n    ss_alldata = sum((safe_sqr(a).sum(axis=0) for a in args))\n    sums_args = [np.asarray(a.sum(axis=0)) for a in args]\n    square_of_sums_alldata = sum(sums_args) ** 2\n    square_of_sums_args = [s ** 2 for s in sums_args]\n    sstot = ss_alldata - square_of_sums_alldata / float(n_samples)\n    ssbn = 0.0\n    for k, _ in enumerate(args):\n        ssbn += square_of_sums_args[k] / n_samples_per_class[k]\n    ssbn -= square_of_sums_alldata / float(n_samples)\n    sswn = sstot - ssbn\n    dfbn = n_classes - 1\n    dfwn = n_samples - n_classes\n    msb = ssbn / float(dfbn)\n    msw = sswn / float(dfwn)\n    constant_features_idx = np.where(msw == 0.0)[0]\n    if np.nonzero(msb)[0].size != msb.size and constant_features_idx.size:\n        warnings.warn('Features %s are constant.' % constant_features_idx, UserWarning)\n    f = msb / msw\n    f = np.asarray(f).ravel()\n    prob = special.fdtrc(dfbn, dfwn, f)\n    return (f, prob)",
    ".sklearn.utils.validation.py@@as_float_array": "def as_float_array(X, copy=True, force_all_finite=True):\n    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray) and (not sp.issparse(X))):\n        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64, copy=copy, force_all_finite=force_all_finite, ensure_2d=False)\n    elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:\n        return X.copy() if copy else X\n    elif X.dtype in [np.float32, np.float64]:\n        return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X\n    else:\n        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:\n            return_dtype = np.float32\n        else:\n            return_dtype = np.float64\n        return X.astype(return_dtype)",
    ".sklearn.utils.__init__.py@@safe_sqr": "def safe_sqr(X, copy=True):\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False)\n    if issparse(X):\n        if copy:\n            X = X.copy()\n        X.data **= 2\n    elif copy:\n        X = X ** 2\n    else:\n        X **= 2\n    return X",
    ".sklearn.decomposition.pca.py@@PCA.fit": "def fit(self, X, y=None):\n    self._fit(X)\n    return self",
    ".sklearn.decomposition.pca.py@@PCA._fit": "def _fit(self, X):\n    if issparse(X):\n        raise TypeError('PCA does not support sparse input. See TruncatedSVD for a possible alternative.')\n    X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True, copy=self.copy)\n    if self.n_components is None:\n        if self.svd_solver != 'arpack':\n            n_components = min(X.shape)\n        else:\n            n_components = min(X.shape) - 1\n    else:\n        n_components = self.n_components\n    self._fit_svd_solver = self.svd_solver\n    if self._fit_svd_solver == 'auto':\n        if max(X.shape) <= 500 or n_components == 'mle':\n            self._fit_svd_solver = 'full'\n        elif n_components >= 1 and n_components < 0.8 * min(X.shape):\n            self._fit_svd_solver = 'randomized'\n        else:\n            self._fit_svd_solver = 'full'\n    if self._fit_svd_solver == 'full':\n        return self._fit_full(X, n_components)\n    elif self._fit_svd_solver in ['arpack', 'randomized']:\n        return self._fit_truncated(X, n_components, self._fit_svd_solver)\n    else:\n        raise ValueError(\"Unrecognized svd_solver='{0}'\".format(self._fit_svd_solver))",
    ".sklearn.decomposition.pca.py@@PCA._fit_truncated": "def _fit_truncated(self, X, n_components, svd_solver):\n    n_samples, n_features = X.shape\n    if isinstance(n_components, str):\n        raise ValueError(\"n_components=%r cannot be a string with svd_solver='%s'\" % (n_components, svd_solver))\n    elif not 1 <= n_components <= min(n_samples, n_features):\n        raise ValueError(\"n_components=%r must be between 1 and min(n_samples, n_features)=%r with svd_solver='%s'\" % (n_components, min(n_samples, n_features), svd_solver))\n    elif not isinstance(n_components, numbers.Integral):\n        raise ValueError('n_components=%r must be of type int when greater than or equal to 1, was of type=%r' % (n_components, type(n_components)))\n    elif svd_solver == 'arpack' and n_components == min(n_samples, n_features):\n        raise ValueError(\"n_components=%r must be strictly less than min(n_samples, n_features)=%r with svd_solver='%s'\" % (n_components, min(n_samples, n_features), svd_solver))\n    random_state = check_random_state(self.random_state)\n    self.mean_ = np.mean(X, axis=0)\n    X -= self.mean_\n    if svd_solver == 'arpack':\n        v0 = random_state.uniform(-1, 1, size=min(X.shape))\n        U, S, V = svds(X, k=n_components, tol=self.tol, v0=v0)\n        S = S[::-1]\n        U, V = svd_flip(U[:, ::-1], V[::-1])\n    elif svd_solver == 'randomized':\n        U, S, V = randomized_svd(X, n_components=n_components, n_iter=self.iterated_power, flip_sign=True, random_state=random_state)\n    self.n_samples_, self.n_features_ = (n_samples, n_features)\n    self.components_ = V\n    self.n_components_ = n_components\n    self.explained_variance_ = S ** 2 / (n_samples - 1)\n    total_var = np.var(X, ddof=1, axis=0)\n    self.explained_variance_ratio_ = self.explained_variance_ / total_var.sum()\n    self.singular_values_ = S.copy()\n    if self.n_components_ < min(n_features, n_samples):\n        self.noise_variance_ = total_var.sum() - self.explained_variance_.sum()\n        self.noise_variance_ /= min(n_features, n_samples) - n_components\n    else:\n        self.noise_variance_ = 0.0\n    return (U, S, V)",
    ".sklearn.feature_extraction.text.py@@CountVectorizer.fit": "def fit(self, raw_documents, y=None):\n    self._warn_for_unused_params()\n    self.fit_transform(raw_documents)\n    return self",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._warn_for_unused_params": "def _warn_for_unused_params(self):\n    if self.tokenizer is not None and self.token_pattern is not None:\n        warnings.warn(\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\")\n    if self.preprocessor is not None and callable(self.analyzer):\n        warnings.warn(\"The parameter 'preprocessor' will not be used since 'analyzer' is callable'\")\n    if self.ngram_range != (1, 1) and self.ngram_range is not None and callable(self.analyzer):\n        warnings.warn(\"The parameter 'ngram_range' will not be used since 'analyzer' is callable'\")\n    if self.analyzer != 'word' or callable(self.analyzer):\n        if self.stop_words is not None:\n            warnings.warn(\"The parameter 'stop_words' will not be used since 'analyzer' != 'word'\")\n        if self.token_pattern is not None and self.token_pattern != '(?u)\\\\b\\\\w\\\\w+\\\\b':\n            warnings.warn(\"The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\")\n        if self.tokenizer is not None:\n            warnings.warn(\"The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\")",
    ".sklearn.feature_extraction.text.py@@CountVectorizer.fit_transform": "def fit_transform(self, raw_documents, y=None):\n    if isinstance(raw_documents, str):\n        raise ValueError('Iterable over raw text documents expected, string object received.')\n    self._validate_params()\n    self._validate_vocabulary()\n    max_df = self.max_df\n    min_df = self.min_df\n    max_features = self.max_features\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n    if self.binary:\n        X.data.fill(1)\n    if not self.fixed_vocabulary_:\n        X = self._sort_features(X, vocabulary)\n        n_doc = X.shape[0]\n        max_doc_count = max_df if isinstance(max_df, numbers.Integral) else max_df * n_doc\n        min_doc_count = min_df if isinstance(min_df, numbers.Integral) else min_df * n_doc\n        if max_doc_count < min_doc_count:\n            raise ValueError('max_df corresponds to < documents than min_df')\n        X, self.stop_words_ = self._limit_features(X, vocabulary, max_doc_count, min_doc_count, max_features)\n        self.vocabulary_ = vocabulary\n    return X",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._validate_params": "def _validate_params(self):\n    min_n, max_m = self.ngram_range\n    if min_n > max_m:\n        raise ValueError('Invalid value for ngram_range=%s lower boundary larger than the upper boundary.' % str(self.ngram_range))",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._validate_vocabulary": "def _validate_vocabulary(self):\n    vocabulary = self.vocabulary\n    if vocabulary is not None:\n        if isinstance(vocabulary, set):\n            vocabulary = sorted(vocabulary)\n        if not isinstance(vocabulary, Mapping):\n            vocab = {}\n            for i, t in enumerate(vocabulary):\n                if vocab.setdefault(t, i) != i:\n                    msg = 'Duplicate term in vocabulary: %r' % t\n                    raise ValueError(msg)\n            vocabulary = vocab\n        else:\n            indices = set(vocabulary.values())\n            if len(indices) != len(vocabulary):\n                raise ValueError('Vocabulary contains repeated indices.')\n            for i in range(len(vocabulary)):\n                if i not in indices:\n                    msg = \"Vocabulary of size %d doesn't contain index %d.\" % (len(vocabulary), i)\n                    raise ValueError(msg)\n        if not vocabulary:\n            raise ValueError('empty vocabulary passed to fit')\n        self.fixed_vocabulary_ = True\n        self.vocabulary_ = dict(vocabulary)\n    else:\n        self.fixed_vocabulary_ = False",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._count_vocab": "def _count_vocab(self, raw_documents, fixed_vocab):\n    if fixed_vocab:\n        vocabulary = self.vocabulary_\n    else:\n        vocabulary = defaultdict()\n        vocabulary.default_factory = vocabulary.__len__\n    analyze = self.build_analyzer()\n    j_indices = []\n    indptr = []\n    values = _make_int_array()\n    indptr.append(0)\n    for doc in raw_documents:\n        feature_counter = {}\n        for feature in analyze(doc):\n            try:\n                feature_idx = vocabulary[feature]\n                if feature_idx not in feature_counter:\n                    feature_counter[feature_idx] = 1\n                else:\n                    feature_counter[feature_idx] += 1\n            except KeyError:\n                continue\n        j_indices.extend(feature_counter.keys())\n        values.extend(feature_counter.values())\n        indptr.append(len(j_indices))\n    if not fixed_vocab:\n        vocabulary = dict(vocabulary)\n        if not vocabulary:\n            raise ValueError('empty vocabulary; perhaps the documents only contain stop words')\n    if indptr[-1] > 2147483648:\n        if _IS_32BIT:\n            raise ValueError('sparse CSR array has {} non-zero elements and requires 64 bit indexing, which is unsupported with 32 bit Python.'.format(indptr[-1]))\n        indices_dtype = np.int64\n    else:\n        indices_dtype = np.int32\n    j_indices = np.asarray(j_indices, dtype=indices_dtype)\n    indptr = np.asarray(indptr, dtype=indices_dtype)\n    values = np.frombuffer(values, dtype=np.intc)\n    X = sp.csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, len(vocabulary)), dtype=self.dtype)\n    X.sort_indices()\n    return (vocabulary, X)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_analyzer": "def build_analyzer(self):\n    if callable(self.analyzer):\n        if self.input in ['file', 'filename']:\n            self._validate_custom_analyzer()\n        return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n    preprocess = self.build_preprocessor()\n    if self.analyzer == 'char':\n        return partial(_analyze, ngrams=self._char_ngrams, preprocessor=preprocess, decoder=self.decode)\n    elif self.analyzer == 'char_wb':\n        return partial(_analyze, ngrams=self._char_wb_ngrams, preprocessor=preprocess, decoder=self.decode)\n    elif self.analyzer == 'word':\n        stop_words = self.get_stop_words()\n        tokenize = self.build_tokenizer()\n        self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n        return partial(_analyze, ngrams=self._word_ngrams, tokenizer=tokenize, preprocessor=preprocess, decoder=self.decode, stop_words=stop_words)\n    else:\n        raise ValueError('%s is not a valid tokenization scheme/analyzer' % self.analyzer)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_preprocessor": "def build_preprocessor(self):\n    if self.preprocessor is not None:\n        return self.preprocessor\n    if not self.strip_accents:\n        strip_accents = None\n    elif callable(self.strip_accents):\n        strip_accents = self.strip_accents\n    elif self.strip_accents == 'ascii':\n        strip_accents = strip_accents_ascii\n    elif self.strip_accents == 'unicode':\n        strip_accents = strip_accents_unicode\n    else:\n        raise ValueError('Invalid value for \"strip_accents\": %s' % self.strip_accents)\n    return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.get_stop_words": "def get_stop_words(self):\n    return _check_stop_list(self.stop_words)",
    ".sklearn.feature_extraction.text.py@@_check_stop_list": "def _check_stop_list(stop):\n    if stop == 'english':\n        return ENGLISH_STOP_WORDS\n    elif isinstance(stop, str):\n        raise ValueError('not a built-in stop list: %s' % stop)\n    elif stop is None:\n        return None\n    else:\n        return frozenset(stop)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.build_tokenizer": "def build_tokenizer(self):\n    if self.tokenizer is not None:\n        return self.tokenizer\n    token_pattern = re.compile(self.token_pattern)\n    return token_pattern.findall",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._check_stop_words_consistency": "def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n    if id(self.stop_words) == getattr(self, '_stop_words_id', None):\n        return None\n    try:\n        inconsistent = set()\n        for w in stop_words or ():\n            tokens = list(tokenize(preprocess(w)))\n            for token in tokens:\n                if token not in stop_words:\n                    inconsistent.add(token)\n        self._stop_words_id = id(self.stop_words)\n        if inconsistent:\n            warnings.warn('Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens %r not in stop_words.' % sorted(inconsistent))\n        return not inconsistent\n    except Exception:\n        self._stop_words_id = id(self.stop_words)\n        return 'error'",
    ".sklearn.feature_extraction.text.py@@_make_int_array": "def _make_int_array():\n    return array.array(str('i'))",
    ".sklearn.feature_extraction.text.py@@_analyze": "def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None, preprocessor=None, decoder=None, stop_words=None):\n    if decoder is not None:\n        doc = decoder(doc)\n    if analyzer is not None:\n        doc = analyzer(doc)\n    else:\n        if preprocessor is not None:\n            doc = preprocessor(doc)\n        if tokenizer is not None:\n            doc = tokenizer(doc)\n        if ngrams is not None:\n            if stop_words is not None:\n                doc = ngrams(doc, stop_words)\n            else:\n                doc = ngrams(doc)\n    return doc",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin.decode": "def decode(self, doc):\n    if self.input == 'filename':\n        with open(doc, 'rb') as fh:\n            doc = fh.read()\n    elif self.input == 'file':\n        doc = doc.read()\n    if isinstance(doc, bytes):\n        doc = doc.decode(self.encoding, self.decode_error)\n    if doc is np.nan:\n        raise ValueError('np.nan is an invalid document, expected byte or unicode string.')\n    return doc",
    ".sklearn.feature_extraction.text.py@@_preprocess": "def _preprocess(doc, accent_function=None, lower=False):\n    if lower:\n        doc = doc.lower()\n    if accent_function is not None:\n        doc = accent_function(doc)\n    return doc",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._word_ngrams": "def _word_ngrams(self, tokens, stop_words=None):\n    if stop_words is not None:\n        tokens = [w for w in tokens if w not in stop_words]\n    min_n, max_n = self.ngram_range\n    if max_n != 1:\n        original_tokens = tokens\n        if min_n == 1:\n            tokens = list(original_tokens)\n            min_n += 1\n        else:\n            tokens = []\n        n_original_tokens = len(original_tokens)\n        tokens_append = tokens.append\n        space_join = ' '.join\n        for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n            for i in range(n_original_tokens - n + 1):\n                tokens_append(space_join(original_tokens[i:i + n]))\n    return tokens",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._sort_features": "def _sort_features(self, X, vocabulary):\n    sorted_features = sorted(vocabulary.items())\n    map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n    for new_val, (term, old_val) in enumerate(sorted_features):\n        vocabulary[term] = new_val\n        map_index[old_val] = new_val\n    X.indices = map_index.take(X.indices, mode='clip')\n    return X",
    ".sklearn.feature_extraction.text.py@@CountVectorizer._limit_features": "def _limit_features(self, X, vocabulary, high=None, low=None, limit=None):\n    if high is None and low is None and (limit is None):\n        return (X, set())\n    dfs = _document_frequency(X)\n    mask = np.ones(len(dfs), dtype=bool)\n    if high is not None:\n        mask &= dfs <= high\n    if low is not None:\n        mask &= dfs >= low\n    if limit is not None and mask.sum() > limit:\n        tfs = np.asarray(X.sum(axis=0)).ravel()\n        mask_inds = (-tfs[mask]).argsort()[:limit]\n        new_mask = np.zeros(len(dfs), dtype=bool)\n        new_mask[np.where(mask)[0][mask_inds]] = True\n        mask = new_mask\n    new_indices = np.cumsum(mask) - 1\n    removed_terms = set()\n    for term, old_index in list(vocabulary.items()):\n        if mask[old_index]:\n            vocabulary[term] = new_indices[old_index]\n        else:\n            del vocabulary[term]\n            removed_terms.add(term)\n    kept_indices = np.where(mask)[0]\n    if len(kept_indices) == 0:\n        raise ValueError('After pruning, no terms remain. Try a lower min_df or a higher max_df.')\n    return (X[:, kept_indices], removed_terms)",
    ".sklearn.feature_extraction.text.py@@_document_frequency": "def _document_frequency(X):\n    if sp.isspmatrix_csr(X):\n        return np.bincount(X.indices, minlength=X.shape[1])\n    else:\n        return np.diff(X.indptr)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._char_ngrams": "def _char_ngrams(self, text_document):\n    text_document = self._white_spaces.sub(' ', text_document)\n    text_len = len(text_document)\n    min_n, max_n = self.ngram_range\n    if min_n == 1:\n        ngrams = list(text_document)\n        min_n += 1\n    else:\n        ngrams = []\n    ngrams_append = ngrams.append\n    for n in range(min_n, min(max_n + 1, text_len + 1)):\n        for i in range(text_len - n + 1):\n            ngrams_append(text_document[i:i + n])\n    return ngrams",
    ".sklearn.base.py@@BaseEstimator.__getstate__": "def __getstate__(self):\n    try:\n        state = super().__getstate__()\n    except AttributeError:\n        state = self.__dict__.copy()\n    if type(self).__module__.startswith('sklearn.'):\n        return dict(state.items(), _sklearn_version=__version__)\n    else:\n        return state",
    ".sklearn.utils.fixes.py@@_parse_version": "def _parse_version(version_string):\n    version = []\n    for x in version_string.split('.'):\n        try:\n            version.append(int(x))\n        except ValueError:\n            version.append(x)\n    return tuple(version)",
    ".sklearn.utils.deprecation.py@@deprecated.__init__": "def __init__(self, extra=''):\n    self.extra = extra",
    ".sklearn.utils.deprecation.py@@deprecated.__call__": "def __call__(self, obj):\n    if isinstance(obj, type):\n        return self._decorate_class(obj)\n    elif isinstance(obj, property):\n        return self._decorate_property(obj)\n    else:\n        return self._decorate_fun(obj)",
    ".sklearn.utils.deprecation.py@@deprecated._decorate_fun": "def _decorate_fun(self, fun):\n    msg = 'Function %s is deprecated' % fun.__name__\n    if self.extra:\n        msg += '; %s' % self.extra\n\n    @functools.wraps(fun)\n    def wrapped(*args, **kwargs):\n        warnings.warn(msg, category=DeprecationWarning)\n        return fun(*args, **kwargs)\n    wrapped.__doc__ = self._update_doc(wrapped.__doc__)\n    wrapped.__wrapped__ = fun\n    return wrapped",
    ".sklearn.utils.deprecation.py@@deprecated._update_doc": "def _update_doc(self, olddoc):\n    newdoc = 'DEPRECATED'\n    if self.extra:\n        newdoc = '%s: %s' % (newdoc, self.extra)\n    if olddoc:\n        newdoc = '%s\\n\\n%s' % (newdoc, olddoc)\n    return newdoc",
    ".sklearn.utils.deprecation.py@@deprecated._decorate_class": "def _decorate_class(self, cls):\n    msg = 'Class %s is deprecated' % cls.__name__\n    if self.extra:\n        msg += '; %s' % self.extra\n    init = cls.__init__\n\n    def wrapped(*args, **kwargs):\n        warnings.warn(msg, category=DeprecationWarning)\n        return init(*args, **kwargs)\n    cls.__init__ = wrapped\n    wrapped.__name__ = '__init__'\n    wrapped.__doc__ = self._update_doc(init.__doc__)\n    wrapped.deprecated_original = init\n    return cls",
    ".sklearn.utils.metaestimators.py@@if_delegate_has_method": "def if_delegate_has_method(delegate):\n    if isinstance(delegate, list):\n        delegate = tuple(delegate)\n    if not isinstance(delegate, tuple):\n        delegate = (delegate,)\n    return lambda fn: _IffHasAttrDescriptor(fn, delegate, attribute_name=fn.__name__)",
    ".sklearn.utils.metaestimators.py@@_IffHasAttrDescriptor.__init__": "def __init__(self, fn, delegate_names, attribute_name):\n    self.fn = fn\n    self.delegate_names = delegate_names\n    self.attribute_name = attribute_name\n    update_wrapper(self, fn)",
    ".sklearn.base.py@@BaseEstimator.__setstate__": "def __setstate__(self, state):\n    if type(self).__module__.startswith('sklearn.'):\n        pickle_version = state.pop('_sklearn_version', 'pre-0.18')\n        if pickle_version != __version__:\n            warnings.warn('Trying to unpickle estimator {0} from version {1} when using version {2}. This might lead to breaking code or invalid results. Use at your own risk.'.format(self.__class__.__name__, pickle_version, __version__), UserWarning)\n    try:\n        super().__setstate__(state)\n    except AttributeError:\n        self.__dict__.update(state)",
    ".sklearn.feature_extraction.text.py@@VectorizerMixin._char_wb_ngrams": "def _char_wb_ngrams(self, text_document):\n    text_document = self._white_spaces.sub(' ', text_document)\n    min_n, max_n = self.ngram_range\n    ngrams = []\n    ngrams_append = ngrams.append\n    for w in text_document.split():\n        w = ' ' + w + ' '\n        w_len = len(w)\n        for n in range(min_n, max_n + 1):\n            offset = 0\n            ngrams_append(w[offset:offset + n])\n            while offset + n < w_len:\n                offset += 1\n                ngrams_append(w[offset:offset + n])\n            if offset == 0:\n                break\n    return ngrams",
    ".sklearn.utils.testing.py@@assert_raise_message": "def assert_raise_message(exceptions, message, function, *args, **kwargs):\n    try:\n        function(*args, **kwargs)\n    except exceptions as e:\n        error_message = str(e)\n        if message not in error_message:\n            raise AssertionError('Error message does not include the expected string: %r. Observed error message: %r' % (message, error_message))\n    else:\n        if isinstance(exceptions, tuple):\n            names = ' or '.join((e.__name__ for e in exceptions))\n        else:\n            names = exceptions.__name__\n        raise AssertionError('%s not raised by %s' % (names, function.__name__))",
    ".sklearn.dummy.py@@DummyRegressor.__init__": "def __init__(self, strategy='mean', constant=None, quantile=None):\n    self.strategy = strategy\n    self.constant = constant\n    self.quantile = quantile",
    ".sklearn.pipeline.py@@Pipeline.__init__": "def __init__(self, steps, memory=None, verbose=False):\n    self.steps = steps\n    self.memory = memory\n    self.verbose = verbose\n    self._validate_steps()",
    ".sklearn.pipeline.py@@Pipeline._validate_steps": "def _validate_steps(self):\n    names, estimators = zip(*self.steps)\n    self._validate_names(names)\n    transformers = estimators[:-1]\n    estimator = estimators[-1]\n    for t in transformers:\n        if t is None or t == 'passthrough':\n            continue\n        if not (hasattr(t, 'fit') or hasattr(t, 'fit_transform')) or not hasattr(t, 'transform'):\n            raise TypeError(\"All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '%s' (type %s) doesn't\" % (t, type(t)))\n    if estimator is not None and estimator != 'passthrough' and (not hasattr(estimator, 'fit')):\n        raise TypeError(\"Last step of Pipeline should implement fit or be the string 'passthrough'. '%s' (type %s) doesn't\" % (estimator, type(estimator)))",
    ".sklearn.pipeline.py@@Pipeline.get_params": "def get_params(self, deep=True):\n    return self._get_params('steps', deep=deep)",
    ".sklearn.linear_model.coordinate_descent.py@@Lasso.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    super().__init__(alpha=alpha, l1_ratio=1.0, fit_intercept=fit_intercept, normalize=normalize, precompute=precompute, copy_X=copy_X, max_iter=max_iter, tol=tol, warm_start=warm_start, positive=positive, random_state=random_state, selection=selection)",
    ".sklearn.linear_model.coordinate_descent.py@@ElasticNet.__init__": "def __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.precompute = precompute\n    self.max_iter = max_iter\n    self.copy_X = copy_X\n    self.tol = tol\n    self.warm_start = warm_start\n    self.positive = positive\n    self.random_state = random_state\n    self.selection = selection",
    ".sklearn.pipeline.py@@Pipeline.set_params": "def set_params(self, **kwargs):\n    self._set_params('steps', **kwargs)\n    return self",
    ".sklearn.utils.metaestimators.py@@_BaseComposition._set_params": "def _set_params(self, attr, **params):\n    if attr in params:\n        setattr(self, attr, params.pop(attr))\n    items = getattr(self, attr)\n    names = []\n    if items:\n        names, _ = zip(*items)\n    for name in list(params.keys()):\n        if '__' not in name and name in names:\n            self._replace_estimator(attr, name, params.pop(name))\n    super().set_params(**params)\n    return self",
    ".sklearn.base.py@@BaseEstimator.set_params": "def set_params(self, **params):\n    if not params:\n        return self\n    valid_params = self.get_params(deep=True)\n    nested_params = defaultdict(dict)\n    for key, value in params.items():\n        key, delim, sub_key = key.partition('__')\n        if key not in valid_params:\n            raise ValueError('Invalid parameter %s for estimator %s. Check the list of available parameters with `estimator.get_params().keys()`.' % (key, self))\n        if delim:\n            nested_params[key][sub_key] = value\n        else:\n            setattr(self, key, value)\n            valid_params[key] = value\n    for key, sub_params in nested_params.items():\n        valid_params[key].set_params(**sub_params)\n    return self",
    ".sklearn.utils.metaestimators.py@@_BaseComposition._replace_estimator": "def _replace_estimator(self, attr, name, new_val):\n    new_estimators = list(getattr(self, attr))\n    for i, (estimator_name, _) in enumerate(new_estimators):\n        if estimator_name == name:\n            new_estimators[i] = (name, new_val)\n            break\n    setattr(self, attr, new_estimators)",
    ".sklearn.linear_model.logistic.py@@LogisticRegression.__init__": "def __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None):\n    self.penalty = penalty\n    self.dual = dual\n    self.tol = tol\n    self.C = C\n    self.fit_intercept = fit_intercept\n    self.intercept_scaling = intercept_scaling\n    self.class_weight = class_weight\n    self.random_state = random_state\n    self.solver = solver\n    self.max_iter = max_iter\n    self.multi_class = multi_class\n    self.verbose = verbose\n    self.warm_start = warm_start\n    self.n_jobs = n_jobs\n    self.l1_ratio = l1_ratio",
    ".sklearn.utils.__init__.py@@Bunch.__getattr__": "def __getattr__(self, key):\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key)",
    ".sklearn.svm.classes.py@@SVC.__init__": "def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None):\n    super().__init__(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, tol=tol, C=C, nu=0.0, shrinking=shrinking, probability=probability, cache_size=cache_size, class_weight=class_weight, verbose=verbose, max_iter=max_iter, decision_function_shape=decision_function_shape, break_ties=break_ties, random_state=random_state)",
    ".sklearn.svm.base.py@@BaseSVC.__init__": "def __init__(self, kernel, degree, gamma, coef0, tol, C, nu, shrinking, probability, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state, break_ties):\n    self.decision_function_shape = decision_function_shape\n    self.break_ties = break_ties\n    super().__init__(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, tol=tol, C=C, nu=nu, epsilon=0.0, shrinking=shrinking, probability=probability, cache_size=cache_size, class_weight=class_weight, verbose=verbose, max_iter=max_iter, random_state=random_state)",
    ".sklearn.svm.base.py@@BaseLibSVM.__init__": "def __init__(self, kernel, degree, gamma, coef0, tol, C, nu, epsilon, shrinking, probability, cache_size, class_weight, verbose, max_iter, random_state):\n    if self._impl not in LIBSVM_IMPL:\n        raise ValueError('impl should be one of %s, %s was given' % (LIBSVM_IMPL, self._impl))\n    if gamma == 0:\n        msg = \"The gamma value of 0.0 is invalid. Use 'auto' to set gamma to a value of 1 / n_features.\"\n        raise ValueError(msg)\n    self.kernel = kernel\n    self.degree = degree\n    self.gamma = gamma\n    self.coef0 = coef0\n    self.tol = tol\n    self.C = C\n    self.nu = nu\n    self.epsilon = epsilon\n    self.shrinking = shrinking\n    self.probability = probability\n    self.cache_size = cache_size\n    self.class_weight = class_weight\n    self.verbose = verbose\n    self.max_iter = max_iter\n    self.random_state = random_state",
    ".sklearn.utils._unittest_backport.py@@TestCase.assertRaisesRegex": "def assertRaisesRegex(self, expected_exception, expected_regex, *args, **kwargs):\n    context = _AssertRaisesContext(expected_exception, self, expected_regex)\n    return context.handle('assertRaisesRegex', args, kwargs)",
    ".sklearn.utils._unittest_backport.py@@_AssertRaisesBaseContext.__init__": "def __init__(self, expected, test_case, expected_regex=None):\n    _BaseTestCaseContext.__init__(self, test_case)\n    self.expected = expected\n    self.test_case = test_case\n    if expected_regex is not None:\n        expected_regex = re.compile(expected_regex)\n    self.expected_regex = expected_regex\n    self.obj_name = None\n    self.msg = None",
    ".sklearn.utils._unittest_backport.py@@_BaseTestCaseContext.__init__": "def __init__(self, test_case):\n    self.test_case = test_case",
    ".sklearn.utils._unittest_backport.py@@_AssertRaisesBaseContext.handle": "def handle(self, name, args, kwargs):\n    try:\n        if not _is_subtype(self.expected, self._base_type):\n            raise TypeError('%s() arg 1 must be %s' % (name, self._base_type_str))\n        if args and args[0] is None:\n            warnings.warn('callable is None', DeprecationWarning, 3)\n            args = ()\n        if not args:\n            self.msg = kwargs.pop('msg', None)\n            if kwargs:\n                warnings.warn('%r is an invalid keyword argument for this function' % next(iter(kwargs)), DeprecationWarning, 3)\n            return self\n        callable_obj, args = (args[0], args[1:])\n        try:\n            self.obj_name = callable_obj.__name__\n        except AttributeError:\n            self.obj_name = str(callable_obj)\n        with self:\n            callable_obj(*args, **kwargs)\n    finally:\n        self = None",
    ".sklearn.utils._unittest_backport.py@@_is_subtype": "def _is_subtype(expected, basetype):\n    if isinstance(expected, tuple):\n        return all((_is_subtype(e, basetype) for e in expected))\n    return isinstance(expected, type) and issubclass(expected, basetype)",
    ".sklearn.utils._unittest_backport.py@@_AssertRaisesContext.__enter__": "def __enter__(self):\n    return self",
    ".sklearn.pipeline.py@@Pipeline.fit": "def fit(self, X, y=None, **fit_params):\n    Xt, fit_params = self._fit(X, y, **fit_params)\n    with _print_elapsed_time('Pipeline', self._log_message(len(self.steps) - 1)):\n        if self._final_estimator != 'passthrough':\n            self._final_estimator.fit(Xt, y, **fit_params)\n    return self",
    ".sklearn.pipeline.py@@Pipeline._fit": "def _fit(self, X, y=None, **fit_params):\n    self.steps = list(self.steps)\n    self._validate_steps()\n    memory = check_memory(self.memory)\n    fit_transform_one_cached = memory.cache(_fit_transform_one)\n    fit_params_steps = {name: {} for name, step in self.steps if step is not None}\n    for pname, pval in fit_params.items():\n        if '__' not in pname:\n            raise ValueError('Pipeline.fit does not accept the {} parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.'.format(pname))\n        step, param = pname.split('__', 1)\n        fit_params_steps[step][param] = pval\n    for step_idx, name, transformer in self._iter(with_final=False, filter_passthrough=False):\n        if transformer is None or transformer == 'passthrough':\n            with _print_elapsed_time('Pipeline', self._log_message(step_idx)):\n                continue\n        if hasattr(memory, 'location'):\n            if memory.location is None:\n                cloned_transformer = transformer\n            else:\n                cloned_transformer = clone(transformer)\n        elif hasattr(memory, 'cachedir'):\n            if memory.cachedir is None:\n                cloned_transformer = transformer\n            else:\n                cloned_transformer = clone(transformer)\n        else:\n            cloned_transformer = clone(transformer)\n        X, fitted_transformer = fit_transform_one_cached(cloned_transformer, X, y, None, message_clsname='Pipeline', message=self._log_message(step_idx), **fit_params_steps[name])\n        self.steps[step_idx] = (name, fitted_transformer)\n    if self._final_estimator == 'passthrough':\n        return (X, {})\n    return (X, fit_params_steps[self.steps[-1][0]])",
    ".sklearn.utils.validation.py@@check_memory": "def check_memory(memory):\n    if memory is None or isinstance(memory, str):\n        if LooseVersion(joblib.__version__) < '0.12':\n            memory = joblib.Memory(cachedir=memory, verbose=0)\n        else:\n            memory = joblib.Memory(location=memory, verbose=0)\n    elif not hasattr(memory, 'cache'):\n        raise ValueError(\"'memory' should be None, a string or have the same interface as joblib.Memory. Got memory='{}' instead.\".format(memory))\n    return memory",
    ".sklearn.utils._unittest_backport.py@@_AssertRaisesContext.__exit__": "def __exit__(self, exc_type, exc_value, tb):\n    if exc_type is None:\n        try:\n            exc_name = self.expected.__name__\n        except AttributeError:\n            exc_name = str(self.expected)\n        if self.obj_name:\n            self._raiseFailure('{} not raised by {}'.format(exc_name, self.obj_name))\n        else:\n            self._raiseFailure('{} not raised'.format(exc_name))\n    if not issubclass(exc_type, self.expected):\n        return False\n    if self.expected_regex is None:\n        return True\n    expected_regex = self.expected_regex\n    if not expected_regex.search(str(exc_value)):\n        self._raiseFailure('\"{}\" does not match \"{}\"'.format(expected_regex.pattern, str(exc_value)))\n    return True",
    ".sklearn.pipeline.py@@Pipeline._iter": "def _iter(self, with_final=True, filter_passthrough=True):\n    stop = len(self.steps)\n    if not with_final:\n        stop -= 1\n    for idx, (name, trans) in enumerate(islice(self.steps, 0, stop)):\n        if not filter_passthrough:\n            yield (idx, name, trans)\n        elif trans is not None and trans != 'passthrough':\n            yield (idx, name, trans)",
    ".sklearn.base.py@@clone": "def clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\" % (repr(estimator), type(estimator)))\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError('Cannot clone object %s, as the constructor either does not set or modifies parameter %s' % (estimator, name))\n    return new_object",
    ".sklearn.pipeline.py@@Pipeline._log_message": "def _log_message(self, step_idx):\n    if not self.verbose:\n        return None\n    name, step = self.steps[step_idx]\n    return '(step %d of %d) Processing %s' % (step_idx + 1, len(self.steps), name)",
    ".sklearn.pipeline.py@@_fit_transform_one": "def _fit_transform_one(transformer, X, y, weight, message_clsname='', message=None, **fit_params):\n    with _print_elapsed_time(message_clsname, message):\n        if hasattr(transformer, 'fit_transform'):\n            res = transformer.fit_transform(X, y, **fit_params)\n        else:\n            res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return (res, transformer)\n    return (res * weight, transformer)",
    ".sklearn.pipeline.py@@Pipeline._final_estimator": "def _final_estimator(self):\n    estimator = self.steps[-1][1]\n    return 'passthrough' if estimator is None else estimator"
}