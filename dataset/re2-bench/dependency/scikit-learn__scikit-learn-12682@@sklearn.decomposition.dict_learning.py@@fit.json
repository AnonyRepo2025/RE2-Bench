{
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.decomposition.dict_learning.py@@dict_learning_online": "def dict_learning_online(X, n_components=2, alpha=1, n_iter=100, return_code=True, dict_init=None, callback=None, batch_size=3, verbose=False, shuffle=True, n_jobs=None, method='lars', iter_offset=0, random_state=None, return_inner_stats=False, inner_stats=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000):\n    if n_components is None:\n        n_components = X.shape[1]\n    if method not in ('lars', 'cd'):\n        raise ValueError('Coding method not supported as a fit algorithm.')\n    _check_positive_coding(method, positive_code)\n    method = 'lasso_' + method\n    t0 = time.time()\n    n_samples, n_features = X.shape\n    alpha = float(alpha)\n    random_state = check_random_state(random_state)\n    if dict_init is not None:\n        dictionary = dict_init\n    else:\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\n        dictionary = S[:, np.newaxis] * dictionary\n    r = len(dictionary)\n    if n_components <= r:\n        dictionary = dictionary[:n_components, :]\n    else:\n        dictionary = np.r_[dictionary, np.zeros((n_components - r, dictionary.shape[1]))]\n    if verbose == 1:\n        print('[dict_learning]', end=' ')\n    if shuffle:\n        X_train = X.copy()\n        random_state.shuffle(X_train)\n    else:\n        X_train = X\n    dictionary = check_array(dictionary.T, order='F', dtype=np.float64, copy=False)\n    dictionary = np.require(dictionary, requirements='W')\n    X_train = check_array(X_train, order='C', dtype=np.float64, copy=False)\n    batches = gen_batches(n_samples, batch_size)\n    batches = itertools.cycle(batches)\n    if inner_stats is None:\n        A = np.zeros((n_components, n_components))\n        B = np.zeros((n_features, n_components))\n    else:\n        A = inner_stats[0].copy()\n        B = inner_stats[1].copy()\n    ii = iter_offset - 1\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\n        this_X = X_train[batch]\n        dt = time.time() - t0\n        if verbose == 1:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n        elif verbose:\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\n                print('Iteration % 3i (elapsed time: % 3is, % 4.1fmn)' % (ii, dt, dt / 60))\n        this_code = sparse_encode(this_X, dictionary.T, algorithm=method, alpha=alpha, n_jobs=n_jobs, check_input=False, positive=positive_code, max_iter=method_max_iter, verbose=verbose).T\n        if ii < batch_size - 1:\n            theta = float((ii + 1) * batch_size)\n        else:\n            theta = float(batch_size ** 2 + ii + 1 - batch_size)\n        beta = (theta + 1 - batch_size) / (theta + 1)\n        A *= beta\n        A += np.dot(this_code, this_code.T)\n        B *= beta\n        B += np.dot(this_X.T, this_code.T)\n        dictionary = _update_dict(dictionary, B, A, verbose=verbose, random_state=random_state, positive=positive_dict)\n        if callback is not None:\n            callback(locals())\n    if return_inner_stats:\n        if return_n_iter:\n            return (dictionary.T, (A, B), ii - iter_offset + 1)\n        else:\n            return (dictionary.T, (A, B))\n    if return_code:\n        if verbose > 1:\n            print('Learning code...', end=' ')\n        elif verbose == 1:\n            print('|', end=' ')\n        code = sparse_encode(X, dictionary.T, algorithm=method, alpha=alpha, n_jobs=n_jobs, check_input=False, positive=positive_code, max_iter=method_max_iter, verbose=verbose)\n        if verbose > 1:\n            dt = time.time() - t0\n            print('done (total time: % 3is, % 4.1fmn)' % (dt, dt / 60))\n        if return_n_iter:\n            return (code, dictionary.T, ii - iter_offset + 1)\n        else:\n            return (code, dictionary.T)\n    if return_n_iter:\n        return (dictionary.T, ii - iter_offset + 1)\n    else:\n        return dictionary.T",
    ".sklearn.decomposition.dict_learning.py@@_check_positive_coding": "def _check_positive_coding(method, positive):\n    if positive and method in ['omp', 'lars']:\n        raise ValueError(\"Positive constraint not supported for '{}' coding method.\".format(method))",
    ".sklearn.utils.extmath.py@@randomized_svd": "def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto', power_iteration_normalizer='auto', transpose='auto', flip_sign=True, random_state=0):\n    if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):\n        warnings.warn('Calculating SVD of a {} is expensive. csr_matrix is more efficient.'.format(type(M).__name__), sparse.SparseEfficiencyWarning)\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n    if n_iter == 'auto':\n        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4\n    if transpose == 'auto':\n        transpose = n_samples < n_features\n    if transpose:\n        M = M.T\n    Q = randomized_range_finder(M, n_random, n_iter, power_iteration_normalizer, random_state)\n    B = safe_sparse_dot(Q.T, M)\n    Uhat, s, V = linalg.svd(B, full_matrices=False)\n    del B\n    U = np.dot(Q, Uhat)\n    if flip_sign:\n        if not transpose:\n            U, V = svd_flip(U, V)\n        else:\n            U, V = svd_flip(U, V, u_based_decision=False)\n    if transpose:\n        return (V[:n_components, :].T, s[:n_components], U[:, :n_components].T)\n    else:\n        return (U[:, :n_components], s[:n_components], V[:n_components, :])",
    ".sklearn.utils.extmath.py@@randomized_range_finder": "def randomized_range_finder(A, size, n_iter, power_iteration_normalizer='auto', random_state=None):\n    random_state = check_random_state(random_state)\n    Q = random_state.normal(size=(A.shape[1], size))\n    if A.dtype.kind == 'f':\n        Q = Q.astype(A.dtype, copy=False)\n    if power_iteration_normalizer == 'auto':\n        if n_iter <= 2:\n            power_iteration_normalizer = 'none'\n        else:\n            power_iteration_normalizer = 'LU'\n    for i in range(n_iter):\n        if power_iteration_normalizer == 'none':\n            Q = safe_sparse_dot(A, Q)\n            Q = safe_sparse_dot(A.T, Q)\n        elif power_iteration_normalizer == 'LU':\n            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n        elif power_iteration_normalizer == 'QR':\n            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    return Q",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if sparse.issparse(a) or sparse.issparse(b):\n        ret = a * b\n        if dense_output and hasattr(ret, 'toarray'):\n            ret = ret.toarray()\n        return ret\n    else:\n        return np.dot(a, b)",
    ".sklearn.utils.extmath.py@@svd_flip": "def svd_flip(u, v, u_based_decision=True):\n    if u_based_decision:\n        max_abs_cols = np.argmax(np.abs(u), axis=0)\n        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    else:\n        max_abs_rows = np.argmax(np.abs(v), axis=1)\n        signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    return (u, v)",
    ".sklearn.utils.__init__.py@@gen_batches": "def gen_batches(n, batch_size, min_batch_size=0):\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    ".sklearn.decomposition.dict_learning.py@@sparse_encode": "def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None, max_iter=1000, n_jobs=None, check_input=True, verbose=0, positive=False):\n    if check_input:\n        if algorithm == 'lasso_cd':\n            dictionary = check_array(dictionary, order='C', dtype='float64')\n            X = check_array(X, order='C', dtype='float64')\n        else:\n            dictionary = check_array(dictionary)\n            X = check_array(X)\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n    if gram is None and algorithm != 'threshold':\n        gram = np.dot(dictionary, dictionary.T)\n    if cov is None and algorithm != 'lasso_cd':\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n    if algorithm in ('lars', 'omp'):\n        regularization = n_nonzero_coefs\n        if regularization is None:\n            regularization = min(max(n_features / 10, 1), n_components)\n    else:\n        regularization = alpha\n        if regularization is None:\n            regularization = 1.0\n    if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':\n        code = _sparse_encode(X, dictionary, gram, cov=cov, algorithm=algorithm, regularization=regularization, copy_cov=copy_cov, init=init, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive)\n        return code\n    code = np.empty((n_samples, n_components))\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(_sparse_encode)(X[this_slice], dictionary, gram, cov[:, this_slice] if cov is not None else None, algorithm, regularization=regularization, copy_cov=copy_cov, init=init[this_slice] if init is not None else None, max_iter=max_iter, check_input=False, verbose=verbose, positive=positive) for this_slice in slices))\n    for this_slice, this_view in zip(slices, code_views):\n        code[this_slice] = this_view\n    return code",
    ".sklearn.decomposition.dict_learning.py@@_sparse_encode": "def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars', regularization=None, copy_cov=True, init=None, max_iter=1000, check_input=True, verbose=0, positive=False):\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n    if dictionary.shape[1] != X.shape[1]:\n        raise ValueError('Dictionary and X have different numbers of features:dictionary.shape: {} X.shape{}'.format(dictionary.shape, X.shape))\n    if cov is None and algorithm != 'lasso_cd':\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n    _check_positive_coding(algorithm, positive)\n    if algorithm == 'lasso_lars':\n        alpha = float(regularization) / n_features\n        try:\n            err_mgt = np.seterr(all='ignore')\n            lasso_lars = LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False, positive=positive, max_iter=max_iter)\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\n            new_code = lasso_lars.coef_\n        finally:\n            np.seterr(**err_mgt)\n    elif algorithm == 'lasso_cd':\n        alpha = float(regularization) / n_features\n        clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False, precompute=gram, max_iter=max_iter, warm_start=True, positive=positive)\n        if init is not None:\n            clf.coef_ = init\n        clf.fit(dictionary.T, X.T, check_input=check_input)\n        new_code = clf.coef_\n    elif algorithm == 'lars':\n        try:\n            err_mgt = np.seterr(all='ignore')\n            lars = Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False)\n            lars.fit(dictionary.T, X.T, Xy=cov)\n            new_code = lars.coef_\n        finally:\n            np.seterr(**err_mgt)\n    elif algorithm == 'threshold':\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\n        if positive:\n            np.clip(new_code, 0, None, out=new_code)\n    elif algorithm == 'omp':\n        new_code = orthogonal_mp_gram(Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization), tol=None, norms_squared=row_norms(X, squared=True), copy_Xy=copy_cov).T\n    else:\n        raise ValueError('Sparse coding method must be \"lasso_lars\" \"lasso_cd\", \"lasso\", \"threshold\" or \"omp\", got %s.' % algorithm)\n    if new_code.ndim != 2:\n        return new_code.reshape(n_samples, n_components)\n    return new_code",
    ".sklearn.linear_model.coordinate_descent.py@@Lasso.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    super().__init__(alpha=alpha, l1_ratio=1.0, fit_intercept=fit_intercept, normalize=normalize, precompute=precompute, copy_X=copy_X, max_iter=max_iter, tol=tol, warm_start=warm_start, positive=positive, random_state=random_state, selection=selection)",
    ".sklearn.linear_model.coordinate_descent.py@@ElasticNet.__init__": "def __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'):\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n    self.fit_intercept = fit_intercept\n    self.normalize = normalize\n    self.precompute = precompute\n    self.max_iter = max_iter\n    self.copy_X = copy_X\n    self.tol = tol\n    self.warm_start = warm_start\n    self.positive = positive\n    self.random_state = random_state\n    self.selection = selection",
    ".sklearn.linear_model.coordinate_descent.py@@ElasticNet.fit": "def fit(self, X, y, check_input=True):\n    if self.alpha == 0:\n        warnings.warn('With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator', stacklevel=2)\n    if isinstance(self.precompute, str):\n        raise ValueError('precompute should be one of True, False or array-like. Got %r' % self.precompute)\n    X_copied = False\n    if check_input:\n        X_copied = self.copy_X and self.fit_intercept\n        X, y = check_X_y(X, y, accept_sparse='csc', order='F', dtype=[np.float64, np.float32], copy=X_copied, multi_output=True, y_numeric=True)\n        y = check_array(y, order='F', copy=False, dtype=X.dtype.type, ensure_2d=False)\n    should_copy = self.copy_X and (not X_copied)\n    X, y, X_offset, y_offset, X_scale, precompute, Xy = _pre_fit(X, y, None, self.precompute, self.normalize, self.fit_intercept, copy=should_copy, check_input=check_input)\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n    if Xy is not None and Xy.ndim == 1:\n        Xy = Xy[:, np.newaxis]\n    n_samples, n_features = X.shape\n    n_targets = y.shape[1]\n    if self.selection not in ['cyclic', 'random']:\n        raise ValueError('selection should be either random or cyclic.')\n    if not self.warm_start or not hasattr(self, 'coef_'):\n        coef_ = np.zeros((n_targets, n_features), dtype=X.dtype, order='F')\n    else:\n        coef_ = self.coef_\n        if coef_.ndim == 1:\n            coef_ = coef_[np.newaxis, :]\n    dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)\n    self.n_iter_ = []\n    for k in range(n_targets):\n        if Xy is not None:\n            this_Xy = Xy[:, k]\n        else:\n            this_Xy = None\n        _, this_coef, this_dual_gap, this_iter = self.path(X, y[:, k], l1_ratio=self.l1_ratio, eps=None, n_alphas=None, alphas=[self.alpha], precompute=precompute, Xy=this_Xy, fit_intercept=False, normalize=False, copy_X=True, verbose=False, tol=self.tol, positive=self.positive, X_offset=X_offset, X_scale=X_scale, return_n_iter=True, coef_init=coef_[k], max_iter=self.max_iter, random_state=self.random_state, selection=self.selection, check_input=False)\n        coef_[k] = this_coef[:, 0]\n        dual_gaps_[k] = this_dual_gap[0]\n        self.n_iter_.append(this_iter[0])\n    if n_targets == 1:\n        self.n_iter_ = self.n_iter_[0]\n        self.coef_ = coef_[0]\n        self.dual_gap_ = dual_gaps_[0]\n    else:\n        self.coef_ = coef_\n        self.dual_gap_ = dual_gaps_\n    self._set_intercept(X_offset, y_offset, X_scale)\n    self.coef_ = np.asarray(self.coef_, dtype=X.dtype)\n    return self",
    ".sklearn.linear_model.base.py@@_pre_fit": "def _pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy, check_input=True):\n    n_samples, n_features = X.shape\n    if sparse.isspmatrix(X):\n        precompute = False\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, copy=False, return_mean=True, check_input=check_input)\n    else:\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy, check_input=check_input)\n    if hasattr(precompute, '__array__') and (fit_intercept and (not np.allclose(X_offset, np.zeros(n_features))) or (normalize and (not np.allclose(X_scale, np.ones(n_features))))):\n        warnings.warn('Gram matrix was provided but X was centered to fit intercept, or X was normalized : recomputing Gram matrix.', UserWarning)\n        precompute = 'auto'\n        Xy = None\n    if isinstance(precompute, str) and precompute == 'auto':\n        precompute = n_samples > n_features\n    if precompute is True:\n        precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype, order='C')\n        np.dot(X.T, X, out=precompute)\n    if not hasattr(precompute, '__array__'):\n        Xy = None\n    if hasattr(precompute, '__array__') and Xy is None:\n        common_dtype = np.find_common_type([X.dtype, y.dtype], [])\n        if y.ndim == 1:\n            Xy = np.empty(shape=n_features, dtype=common_dtype, order='C')\n            np.dot(X.T, y, out=Xy)\n        else:\n            n_targets = y.shape[1]\n            Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F')\n            np.dot(y.T, X, out=Xy.T)\n    return (X, y, X_offset, y_offset, X_scale, precompute, Xy)",
    ".sklearn.linear_model.base.py@@_preprocess_data": "def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order='K')\n    y = np.asarray(y, dtype=X.dtype)\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n            if normalize:\n                X_var *= X.shape[0]\n                X_scale = np.sqrt(X_var, X_var)\n                del X_var\n                X_scale[X_scale == 0] = 1\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        else:\n            X_offset = np.average(X, axis=0, weights=sample_weight)\n            X -= X_offset\n            if normalize:\n                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)\n            else:\n                X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n    return (X, y, X_offset, y_offset, X_scale)",
    ".sklearn.linear_model.coordinate_descent.py@@enet_path": "def enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params):\n    if check_input:\n        X = check_array(X, 'csc', dtype=[np.float64, np.float32], order='F', copy=copy_X)\n        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False, ensure_2d=False)\n        if Xy is not None:\n            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False, ensure_2d=False)\n    n_samples, n_features = X.shape\n    multi_output = False\n    if y.ndim != 1:\n        multi_output = True\n        _, n_outputs = y.shape\n    if multi_output and positive:\n        raise ValueError('positive=True is not allowed for multi-output (y.ndim != 1)')\n    if not multi_output and sparse.isspmatrix(X):\n        if 'X_offset' in params:\n            X_sparse_scaling = params['X_offset'] / params['X_scale']\n            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)\n        else:\n            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)\n    if check_input:\n        X, y, X_offset, y_offset, X_scale, precompute, Xy = _pre_fit(X, y, Xy, precompute, normalize=False, fit_intercept=False, copy=False, check_input=check_input)\n    if alphas is None:\n        alphas = _alpha_grid(X, y, Xy=Xy, l1_ratio=l1_ratio, fit_intercept=False, eps=eps, n_alphas=n_alphas, normalize=False, copy_X=False)\n    else:\n        alphas = np.sort(alphas)[::-1]\n    n_alphas = len(alphas)\n    tol = params.get('tol', 0.0001)\n    max_iter = params.get('max_iter', 1000)\n    dual_gaps = np.empty(n_alphas)\n    n_iters = []\n    rng = check_random_state(params.get('random_state', None))\n    selection = params.get('selection', 'cyclic')\n    if selection not in ['random', 'cyclic']:\n        raise ValueError('selection should be either random or cyclic.')\n    random = selection == 'random'\n    if not multi_output:\n        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)\n    else:\n        coefs = np.empty((n_outputs, n_features, n_alphas), dtype=X.dtype)\n    if coef_init is None:\n        coef_ = np.zeros(coefs.shape[:-1], dtype=X.dtype, order='F')\n    else:\n        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)\n    for i, alpha in enumerate(alphas):\n        l1_reg = alpha * l1_ratio * n_samples\n        l2_reg = alpha * (1.0 - l1_ratio) * n_samples\n        if not multi_output and sparse.isspmatrix(X):\n            model = cd_fast.sparse_enet_coordinate_descent(coef_, l1_reg, l2_reg, X.data, X.indices, X.indptr, y, X_sparse_scaling, max_iter, tol, rng, random, positive)\n        elif multi_output:\n            model = cd_fast.enet_coordinate_descent_multi_task(coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random)\n        elif isinstance(precompute, np.ndarray):\n            if check_input:\n                precompute = check_array(precompute, dtype=X.dtype.type, order='C')\n            model = cd_fast.enet_coordinate_descent_gram(coef_, l1_reg, l2_reg, precompute, Xy, y, max_iter, tol, rng, random, positive)\n        elif precompute is False:\n            model = cd_fast.enet_coordinate_descent(coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive)\n        else:\n            raise ValueError(\"Precompute should be one of True, False, 'auto' or array-like. Got %r\" % precompute)\n        coef_, dual_gap_, eps_, n_iter_ = model\n        coefs[..., i] = coef_\n        dual_gaps[i] = dual_gap_\n        n_iters.append(n_iter_)\n        if verbose:\n            if verbose > 2:\n                print(model)\n            elif verbose > 1:\n                print('Path: %03i out of %03i' % (i, n_alphas))\n            else:\n                sys.stderr.write('.')\n    if return_n_iter:\n        return (alphas, coefs, dual_gaps, n_iters)\n    return (alphas, coefs, dual_gaps)",
    ".sklearn.linear_model.base.py@@LinearModel._set_intercept": "def _set_intercept(self, X_offset, y_offset, X_scale):\n    if self.fit_intercept:\n        self.coef_ = self.coef_ / X_scale\n        self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n    else:\n        self.intercept_ = 0.0",
    ".sklearn.decomposition.dict_learning.py@@_update_dict": "def _update_dict(dictionary, Y, code, verbose=False, return_r2=False, random_state=None, positive=False):\n    n_components = len(code)\n    n_features = Y.shape[0]\n    random_state = check_random_state(random_state)\n    gemm, = linalg.get_blas_funcs(('gemm',), (dictionary, code, Y))\n    ger, = linalg.get_blas_funcs(('ger',), (dictionary, code))\n    nrm2, = linalg.get_blas_funcs(('nrm2',), (dictionary,))\n    R = gemm(-1.0, dictionary, code, 1.0, Y)\n    for k in range(n_components):\n        R = ger(1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)\n        dictionary[:, k] = np.dot(R, code[k, :])\n        if positive:\n            np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])\n        atom_norm = nrm2(dictionary[:, k])\n        if atom_norm < 1e-10:\n            if verbose == 1:\n                sys.stdout.write('+')\n                sys.stdout.flush()\n            elif verbose:\n                print('Adding new random atom')\n            dictionary[:, k] = random_state.randn(n_features)\n            if positive:\n                np.clip(dictionary[:, k], 0, None, out=dictionary[:, k])\n            code[k, :] = 0.0\n            atom_norm = nrm2(dictionary[:, k])\n            dictionary[:, k] /= atom_norm\n        else:\n            dictionary[:, k] /= atom_norm\n            R = ger(-1.0, dictionary[:, k], code[k, :], a=R, overwrite_a=True)\n    if return_r2:\n        R = nrm2(R) ** 2.0\n        return (dictionary, R)\n    return dictionary",
    ".sklearn.linear_model.least_angle.py@@LassoLars.__init__": "def __init__(self, alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=np.finfo(np.float).eps, copy_X=True, fit_path=True, positive=False):\n    self.alpha = alpha\n    self.fit_intercept = fit_intercept\n    self.max_iter = max_iter\n    self.verbose = verbose\n    self.normalize = normalize\n    self.positive = positive\n    self.precompute = precompute\n    self.copy_X = copy_X\n    self.eps = eps\n    self.fit_path = fit_path",
    ".sklearn.linear_model.least_angle.py@@Lars.fit": "def fit(self, X, y, Xy=None):\n    X, y = check_X_y(X, y, y_numeric=True, multi_output=True)\n    alpha = getattr(self, 'alpha', 0.0)\n    if hasattr(self, 'n_nonzero_coefs'):\n        alpha = 0.0\n        max_iter = self.n_nonzero_coefs\n    else:\n        max_iter = self.max_iter\n    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)\n    return self",
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):\n    if y is None:\n        raise ValueError('y cannot be None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)\n    if multi_output:\n        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.linear_model.least_angle.py@@Lars._fit": "def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):\n    n_features = X.shape[1]\n    X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n    n_targets = y.shape[1]\n    Gram = self._get_gram(self.precompute, X, y)\n    self.alphas_ = []\n    self.n_iter_ = []\n    self.coef_ = np.empty((n_targets, n_features))\n    if fit_path:\n        self.active_ = []\n        self.coef_path_ = []\n        for k in range(n_targets):\n            this_Xy = None if Xy is None else Xy[:, k]\n            alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)\n            self.alphas_.append(alphas)\n            self.active_.append(active)\n            self.n_iter_.append(n_iter_)\n            self.coef_path_.append(coef_path)\n            self.coef_[k] = coef_path[:, -1]\n        if n_targets == 1:\n            self.alphas_, self.active_, self.coef_path_, self.coef_ = [a[0] for a in (self.alphas_, self.active_, self.coef_path_, self.coef_)]\n            self.n_iter_ = self.n_iter_[0]\n    else:\n        for k in range(n_targets):\n            this_Xy = None if Xy is None else Xy[:, k]\n            alphas, _, self.coef_[k], n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=False, return_n_iter=True, positive=self.positive)\n            self.alphas_.append(alphas)\n            self.n_iter_.append(n_iter_)\n        if n_targets == 1:\n            self.alphas_ = self.alphas_[0]\n            self.n_iter_ = self.n_iter_[0]\n    self._set_intercept(X_offset, y_offset, X_scale)\n    return self",
    ".sklearn.linear_model.least_angle.py@@Lars._get_gram": "def _get_gram(precompute, X, y):\n    if not hasattr(precompute, '__array__') and (precompute is True or (precompute == 'auto' and X.shape[0] > X.shape[1]) or (precompute == 'auto' and y.shape[1] > 1)):\n        precompute = np.dot(X.T, X)\n    return precompute",
    ".sklearn.linear_model.least_angle.py@@lars_path": "def lars_path(X, y, Xy=None, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False):\n    if X is None and Gram is not None:\n        warnings.warn('Use lars_path_gram to avoid passing X and y. The current option will be removed in v0.23.', DeprecationWarning)\n    return _lars_path_solver(X=X, y=y, Xy=Xy, Gram=Gram, n_samples=None, max_iter=max_iter, alpha_min=alpha_min, method=method, copy_X=copy_X, eps=eps, copy_Gram=copy_Gram, verbose=verbose, return_path=return_path, return_n_iter=return_n_iter, positive=positive)",
    ".sklearn.linear_model.least_angle.py@@_lars_path_solver": "def _lars_path_solver(X, y, Xy=None, Gram=None, n_samples=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False):\n    if method == 'lar' and positive:\n        raise ValueError(\"Positive constraint not supported for 'lar' coding method.\")\n    n_samples = n_samples if n_samples is not None else y.size\n    if Xy is None:\n        Cov = np.dot(X.T, y)\n    else:\n        Cov = Xy.copy()\n    if Gram is None or Gram is False:\n        Gram = None\n        if X is None:\n            raise ValueError('X and Gram cannot both be unspecified.')\n        if copy_X:\n            X = X.copy('F')\n    elif isinstance(Gram, str) and Gram == 'auto' or Gram is True:\n        if Gram is True or X.shape[0] > X.shape[1]:\n            Gram = np.dot(X.T, X)\n        else:\n            Gram = None\n    elif copy_Gram:\n        Gram = Gram.copy()\n    if Gram is None:\n        n_features = X.shape[1]\n    else:\n        n_features = Cov.shape[0]\n        if Gram.shape != (n_features, n_features):\n            raise ValueError('The shapes of the inputs Gram and Xy do not match.')\n    max_features = min(max_iter, n_features)\n    if return_path:\n        coefs = np.zeros((max_features + 1, n_features))\n        alphas = np.zeros(max_features + 1)\n    else:\n        coef, prev_coef = (np.zeros(n_features), np.zeros(n_features))\n        alpha, prev_alpha = (np.array([0.0]), np.array([0.0]))\n    n_iter, n_active = (0, 0)\n    active, indices = (list(), np.arange(n_features))\n    sign_active = np.empty(max_features, dtype=np.int8)\n    drop = False\n    if Gram is None:\n        L = np.empty((max_features, max_features), dtype=X.dtype)\n        swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\n    else:\n        L = np.empty((max_features, max_features), dtype=Gram.dtype)\n        swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (Cov,))\n    solve_cholesky, = get_lapack_funcs(('potrs',), (L,))\n    if verbose:\n        if verbose > 1:\n            print('Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC')\n        else:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n    tiny32 = np.finfo(np.float32).tiny\n    equality_tolerance = np.finfo(np.float32).eps\n    if Gram is not None:\n        Gram_copy = Gram.copy()\n        Cov_copy = Cov.copy()\n    while True:\n        if Cov.size:\n            if positive:\n                C_idx = np.argmax(Cov)\n            else:\n                C_idx = np.argmax(np.abs(Cov))\n            C_ = Cov[C_idx]\n            if positive:\n                C = C_\n            else:\n                C = np.fabs(C_)\n        else:\n            C = 0.0\n        if return_path:\n            alpha = alphas[n_iter, np.newaxis]\n            coef = coefs[n_iter]\n            prev_alpha = alphas[n_iter - 1, np.newaxis]\n            prev_coef = coefs[n_iter - 1]\n        alpha[0] = C / n_samples\n        if alpha[0] <= alpha_min + equality_tolerance:\n            if abs(alpha[0] - alpha_min) > equality_tolerance:\n                if n_iter > 0:\n                    ss = (prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])\n                    coef[:] = prev_coef + ss * (coef - prev_coef)\n                alpha[0] = alpha_min\n            if return_path:\n                coefs[n_iter] = coef\n            break\n        if n_iter >= max_iter or n_active >= n_features:\n            break\n        if not drop:\n            if positive:\n                sign_active[n_active] = np.ones_like(C_)\n            else:\n                sign_active[n_active] = np.sign(C_)\n            m, n = (n_active, C_idx + n_active)\n            Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n            indices[n], indices[m] = (indices[m], indices[n])\n            Cov_not_shortened = Cov\n            Cov = Cov[1:]\n            if Gram is None:\n                X.T[n], X.T[m] = swap(X.T[n], X.T[m])\n                c = nrm2(X.T[n_active]) ** 2\n                L[n_active, :n_active] = np.dot(X.T[n_active], X.T[:n_active].T)\n            else:\n                Gram[m], Gram[n] = swap(Gram[m], Gram[n])\n                Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\n                c = Gram[n_active, n_active]\n                L[n_active, :n_active] = Gram[n_active, :n_active]\n            if n_active:\n                linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, **SOLVE_TRIANGULAR_ARGS)\n            v = np.dot(L[n_active, :n_active], L[n_active, :n_active])\n            diag = max(np.sqrt(np.abs(c - v)), eps)\n            L[n_active, n_active] = diag\n            if diag < 1e-07:\n                warnings.warn('Regressors in active set degenerate. Dropping a regressor, after %i iterations, i.e. alpha=%.3e, with an active set of %i regressors, and the smallest cholesky pivot element being %.3e. Reduce max_iter or increase eps parameters.' % (n_iter, alpha, n_active, diag), ConvergenceWarning)\n                Cov = Cov_not_shortened\n                Cov[0] = 0\n                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n                continue\n            active.append(indices[n_active])\n            n_active += 1\n            if verbose > 1:\n                print('%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s' % (n_iter, active[-1], '', n_active, C))\n        if method == 'lasso' and n_iter > 0 and (prev_alpha[0] < alpha[0]):\n            warnings.warn('Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. %i iterations, alpha=%.3e, previous alpha=%.3e, with an active set of %i regressors.' % (n_iter, alpha, prev_alpha, n_active), ConvergenceWarning)\n            break\n        least_squares, _ = solve_cholesky(L[:n_active, :n_active], sign_active[:n_active], lower=True)\n        if least_squares.size == 1 and least_squares == 0:\n            least_squares[...] = 1\n            AA = 1.0\n        else:\n            AA = 1.0 / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\n            if not np.isfinite(AA):\n                i = 0\n                L_ = L[:n_active, :n_active].copy()\n                while not np.isfinite(AA):\n                    L_.flat[::n_active + 1] += 2 ** i * eps\n                    least_squares, _ = solve_cholesky(L_, sign_active[:n_active], lower=True)\n                    tmp = max(np.sum(least_squares * sign_active[:n_active]), eps)\n                    AA = 1.0 / np.sqrt(tmp)\n                    i += 1\n            least_squares *= AA\n        if Gram is None:\n            eq_dir = np.dot(X.T[:n_active].T, least_squares)\n            corr_eq_dir = np.dot(X.T[n_active:], eq_dir)\n        else:\n            corr_eq_dir = np.dot(Gram[:n_active, n_active:].T, least_squares)\n        g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))\n        if positive:\n            gamma_ = min(g1, C / AA)\n        else:\n            g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))\n            gamma_ = min(g1, g2, C / AA)\n        drop = False\n        z = -coef[active] / (least_squares + tiny32)\n        z_pos = arrayfuncs.min_pos(z)\n        if z_pos < gamma_:\n            idx = np.where(z == z_pos)[0][::-1]\n            sign_active[idx] = -sign_active[idx]\n            if method == 'lasso':\n                gamma_ = z_pos\n            drop = True\n        n_iter += 1\n        if return_path:\n            if n_iter >= coefs.shape[0]:\n                del coef, alpha, prev_alpha, prev_coef\n                add_features = 2 * max(1, max_features - n_active)\n                coefs = np.resize(coefs, (n_iter + add_features, n_features))\n                coefs[-add_features:] = 0\n                alphas = np.resize(alphas, n_iter + add_features)\n                alphas[-add_features:] = 0\n            coef = coefs[n_iter]\n            prev_coef = coefs[n_iter - 1]\n        else:\n            prev_coef = coef\n            prev_alpha[0] = alpha[0]\n            coef = np.zeros_like(coef)\n        coef[active] = prev_coef[active] + gamma_ * least_squares\n        Cov -= gamma_ * corr_eq_dir\n        if drop and method == 'lasso':\n            for ii in idx:\n                arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii)\n            n_active -= 1\n            drop_idx = [active.pop(ii) for ii in idx]\n            if Gram is None:\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\n                        indices[i], indices[i + 1] = (indices[i + 1], indices[i])\n                residual = y - np.dot(X[:, :n_active], coef[active])\n                temp = np.dot(X.T[n_active], residual)\n                Cov = np.r_[temp, Cov]\n            else:\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        indices[i], indices[i + 1] = (indices[i + 1], indices[i])\n                        Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])\n                        Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i], Gram[:, i + 1])\n                temp = Cov_copy[drop_idx] - np.dot(Gram_copy[drop_idx], coef)\n                Cov = np.r_[temp, Cov]\n            sign_active = np.delete(sign_active, idx)\n            sign_active = np.append(sign_active, 0.0)\n            if verbose > 1:\n                print('%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s' % (n_iter, '', drop_idx, n_active, abs(temp)))\n    if return_path:\n        alphas = alphas[:n_iter + 1]\n        coefs = coefs[:n_iter + 1]\n        if return_n_iter:\n            return (alphas, active, coefs.T, n_iter)\n        else:\n            return (alphas, active, coefs.T)\n    elif return_n_iter:\n        return (alpha, active, coef, n_iter)\n    else:\n        return (alpha, active, coef)"
}