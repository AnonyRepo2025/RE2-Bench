{
    ".sklearn.utils.validation.py@@_get_feature_names": "def _get_feature_names(X):\n    feature_names = None\n    if hasattr(X, 'columns'):\n        feature_names = np.asarray(X.columns, dtype=object)\n    if feature_names is None or len(feature_names) == 0:\n        return\n    types = sorted((t.__qualname__ for t in set((type(v) for v in feature_names))))\n    if len(types) > 1 and 'str' in types:\n        raise TypeError(f'Feature names are only supported if all input features have string names, but your input has {types} as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.')\n    if len(types) == 1 and types[0] == 'str':\n        return feature_names",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=''):\n    if isinstance(array, np.matrix):\n        raise TypeError('np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html')\n    xp, is_array_api = get_namespace(array)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    pandas_requires_conversion = False\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if not hasattr(array, 'sparse') and array.dtypes.apply(is_sparse).any():\n                warnings.warn('pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.')\n        dtypes_orig = list(array.dtypes)\n        pandas_requires_conversion = any((_pandas_dtype_needs_early_conversion(i) for i in dtypes_orig))\n        if all((isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig)):\n            dtype_orig = np.result_type(*dtypes_orig)\n    elif hasattr(array, 'iloc') and hasattr(array, 'dtype'):\n        pandas_requires_conversion = _pandas_dtype_needs_early_conversion(array.dtype)\n        if isinstance(array.dtype, np.dtype):\n            dtype_orig = array.dtype\n        else:\n            dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = xp.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if pandas_requires_conversion:\n        new_dtype = dtype_orig if dtype is None else dtype\n        array = array.astype(new_dtype)\n        dtype = None\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    estimator_name = _check_estimator_name(estimator)\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if hasattr(array, 'sparse') and array.ndim > 1:\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if array.dtypes.apply(is_sparse).all():\n                array = array.sparse.to_coo()\n                if array.dtype == np.dtype('object'):\n                    unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])\n                    if len(unique_dtypes) > 1:\n                        raise ValueError('Pandas DataFrame with mixed sparse extension arrays generated a sparse matrix with object dtype which can not be converted to a scipy sparse matrix.Sparse extension arrays should all have the same numeric type.')\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse, estimator_name=estimator_name, input_name=input_name)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = _asarray_with_order(array, order=order, xp=xp)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype, estimator_name=estimator_name, input_name=input_name)\n                    array = xp.astype(array, dtype, copy=False)\n                else:\n                    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            except ComplexWarning as complex_warning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array)) from complex_warning\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and array.dtype.kind in 'USV':\n            raise ValueError(\"dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.\")\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, input_name=input_name, estimator_name=estimator_name, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if copy:\n        if xp.__name__ in {'numpy', 'numpy.array_api'}:\n            if np.may_share_memory(array, array_orig):\n                array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n        else:\n            array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)\n    return array",
    ".sklearn.utils._array_api.py@@get_namespace": "def get_namespace(*arrays):\n    if not get_config()['array_api_dispatch']:\n        return (_NumPyApiWrapper(), False)\n    namespaces = {x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in arrays if not isinstance(x, (bool, int, float, complex))}\n    if not namespaces:\n        raise ValueError('Unrecognized array input')\n    if len(namespaces) != 1:\n        raise ValueError(f'Multiple namespaces for array inputs: {namespaces}')\n    xp, = namespaces\n    if xp is None:\n        return (_NumPyApiWrapper(), False)\n    return (_ArrayAPIWrapper(xp), True)",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _get_threadlocal_config().copy()",
    ".sklearn._config.py@@_get_threadlocal_config": "def _get_threadlocal_config():\n    if not hasattr(_threadlocal, 'global_config'):\n        _threadlocal.global_config = _global_config.copy()\n    return _threadlocal.global_config",
    ".sklearn.utils.validation.py@@_check_estimator_name": "def _check_estimator_name(estimator):\n    if estimator is not None:\n        if isinstance(estimator, str):\n            return estimator\n        else:\n            return estimator.__class__.__name__\n    return None",
    ".sklearn.utils._array_api.py@@_asarray_with_order": "def _asarray_with_order(array, dtype=None, order=None, copy=None, xp=None):\n    if xp is None:\n        xp, _ = get_namespace(array)\n    if xp.__name__ in {'numpy', 'numpy.array_api'}:\n        array = numpy.asarray(array, order=order, dtype=dtype)\n        return xp.asarray(array, copy=copy)\n    else:\n        return xp.asarray(array, dtype=dtype, copy=copy)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.__getattr__": "def __getattr__(self, name):\n    return getattr(numpy, name)",
    ".sklearn.utils._array_api.py@@_NumPyApiWrapper.asarray": "def asarray(self, x, *, dtype=None, device=None, copy=None):\n    if copy is True:\n        return numpy.array(x, copy=True, dtype=dtype)\n    else:\n        return numpy.asarray(x, dtype=dtype)",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None, estimator_name=None, input_name=''):\n    xp, _ = get_namespace(X)\n    if _get_config()['assume_finite']:\n        return\n    X = xp.asarray(X)\n    if X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')\n    if X.dtype.kind not in 'fc':\n        return\n    with np.errstate(over='ignore'):\n        first_pass_isfinite = xp.isfinite(xp.sum(X))\n    if first_pass_isfinite:\n        return\n    use_cython = xp is np and X.data.contiguous and (X.dtype.type in {np.float32, np.float64})\n    if use_cython:\n        out = cy_isfinite(X.reshape(-1), allow_nan=allow_nan)\n        has_nan_error = False if allow_nan else out == FiniteStatus.has_nan\n        has_inf = out == FiniteStatus.has_infinite\n    else:\n        has_inf = np.isinf(X).any()\n        has_nan_error = False if allow_nan else xp.isnan(X).any()\n    if has_inf or has_nan_error:\n        if has_nan_error:\n            type_err = 'NaN'\n        else:\n            msg_dtype = msg_dtype if msg_dtype is not None else X.dtype\n            type_err = f'infinity or a value too large for {msg_dtype!r}'\n        padded_input_name = input_name + ' ' if input_name else ''\n        msg_err = f'Input {padded_input_name}contains {type_err}.'\n        if estimator_name and input_name == 'X' and has_nan_error:\n            msg_err += f'\\n{estimator_name} does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values'\n        raise ValueError(msg_err)",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error",
    ".sklearn.utils.validation.py@@_num_features": "def _num_features(X):\n    type_ = type(X)\n    if type_.__module__ == 'builtins':\n        type_name = type_.__qualname__\n    else:\n        type_name = f'{type_.__module__}.{type_.__qualname__}'\n    message = f'Unable to find the number of features from X of type {type_name}'\n    if not hasattr(X, '__len__') and (not hasattr(X, 'shape')):\n        if not hasattr(X, '__array__'):\n            raise TypeError(message)\n        X = np.asarray(X)\n    if hasattr(X, 'shape'):\n        if not hasattr(X.shape, '__len__') or len(X.shape) <= 1:\n            message += f' with shape {X.shape}'\n            raise TypeError(message)\n        return X.shape[1]\n    first_sample = X[0]\n    if isinstance(first_sample, (str, bytes, dict)):\n        message += f' where the samples are of type {type(first_sample).__qualname__}'\n        raise TypeError(message)\n    try:\n        return len(first_sample)\n    except Exception as err:\n        raise TypeError(message) from err",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name=None, input_name=''):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan', estimator_name=estimator_name, input_name=input_name)\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.utils.validation.py@@_pandas_dtype_needs_early_conversion": "def _pandas_dtype_needs_early_conversion(pd_dtype):\n    from pandas.api.types import is_bool_dtype, is_sparse, is_float_dtype, is_integer_dtype\n    if is_bool_dtype(pd_dtype):\n        return True\n    if is_sparse(pd_dtype):\n        return False\n    try:\n        from pandas.api.types import is_extension_array_dtype\n    except ImportError:\n        return False\n    if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n        return False\n    elif is_float_dtype(pd_dtype):\n        return True\n    elif is_integer_dtype(pd_dtype):\n        return True\n    return False",
    ".sklearn.utils.fixes.py@@_object_dtype_isnan": "def _object_dtype_isnan(X):\n    return X != X",
    ".sklearn.utils.validation.py@@check_X_y": "def check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, estimator=None):\n    if y is None:\n        if estimator is None:\n            estimator_name = 'estimator'\n        else:\n            estimator_name = _check_estimator_name(estimator)\n        raise ValueError(f'{estimator_name} requires y to be passed, but the target y is None')\n    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, estimator=estimator, input_name='X')\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n    check_consistent_length(X, y)\n    return (X, y)",
    ".sklearn.utils.validation.py@@_check_y": "def _check_y(y, multi_output=False, y_numeric=False, estimator=None):\n    if multi_output:\n        y = check_array(y, accept_sparse='csr', force_all_finite=True, ensure_2d=False, dtype=None, input_name='y', estimator=estimator)\n    else:\n        estimator_name = _check_estimator_name(estimator)\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y, input_name='y', estimator_name=estimator_name)\n        _ensure_no_complex_data(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n    return y",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])"
}