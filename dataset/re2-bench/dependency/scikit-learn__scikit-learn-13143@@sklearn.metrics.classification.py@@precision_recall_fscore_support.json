{
    ".sklearn.metrics.classification.py@@_check_targets": "def _check_targets(y_true, y_pred):\n    check_consistent_length(y_true, y_pred)\n    type_true = type_of_target(y_true)\n    type_pred = type_of_target(y_pred)\n    y_type = {type_true, type_pred}\n    if y_type == {'binary', 'multiclass'}:\n        y_type = {'multiclass'}\n    if len(y_type) > 1:\n        raise ValueError(\"Classification metrics can't handle a mix of {0} and {1} targets\".format(type_true, type_pred))\n    y_type = y_type.pop()\n    if y_type not in ['binary', 'multiclass', 'multilabel-indicator']:\n        raise ValueError('{0} is not supported'.format(y_type))\n    if y_type in ['binary', 'multiclass']:\n        y_true = column_or_1d(y_true)\n        y_pred = column_or_1d(y_pred)\n        if y_type == 'binary':\n            unique_values = np.union1d(y_true, y_pred)\n            if len(unique_values) > 2:\n                y_type = 'multiclass'\n    if y_type.startswith('multilabel'):\n        y_true = csr_matrix(y_true)\n        y_pred = csr_matrix(y_pred)\n        y_type = 'multilabel-indicator'\n    return (y_type, y_true, y_pred)",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n        else:\n            return len(x)\n    else:\n        return len(x)",
    ".sklearn.utils.multiclass.py@@type_of_target": "def type_of_target(y):\n    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, str))\n    if not valid:\n        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)\n    sparseseries = y.__class__.__name__ == 'SparseSeries'\n    if sparseseries:\n        raise ValueError(\"y cannot be class 'SparseSeries'.\")\n    if is_multilabel(y):\n        return 'multilabel-indicator'\n    try:\n        y = np.asarray(y)\n    except ValueError:\n        return 'unknown'\n    try:\n        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):\n            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.')\n    except IndexError:\n        pass\n    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], str))):\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] == 0:\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = '-multioutput'\n    else:\n        suffix = ''\n    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):\n        return 'continuous' + suffix\n    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):\n        return 'multiclass' + suffix\n    else:\n        return 'binary'",
    ".sklearn.utils.multiclass.py@@is_multilabel": "def is_multilabel(y):\n    if hasattr(y, '__array__'):\n        y = np.asarray(y)\n    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):\n        return False\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))\n    else:\n        labels = np.unique(y)\n        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.utils.multiclass.py@@unique_labels": "def unique_labels(*ys):\n    if not ys:\n        raise ValueError('No argument has been passed.')\n    ys_types = set((type_of_target(x) for x in ys))\n    if ys_types == {'binary', 'multiclass'}:\n        ys_types = {'multiclass'}\n    if len(ys_types) > 1:\n        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)\n    label_type = ys_types.pop()\n    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:\n        raise ValueError('Multi-label binary indicator input with different numbers of labels')\n    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n    if not _unique_labels:\n        raise ValueError('Unknown label type: %s' % repr(ys))\n    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))\n    if len(set((isinstance(label, str) for label in ys_labels))) > 1:\n        raise ValueError('Mix of label input types (string and number)')\n    return np.array(sorted(ys_labels))",
    ".sklearn.utils.multiclass.py@@_unique_multiclass": "def _unique_multiclass(y):\n    if hasattr(y, '__array__'):\n        return np.unique(np.asarray(y))\n    else:\n        return set(y)",
    ".sklearn.metrics.classification.py@@multilabel_confusion_matrix": "def multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False):\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    if sample_weight is not None:\n        sample_weight = column_or_1d(sample_weight)\n    check_consistent_length(y_true, y_pred, sample_weight)\n    if y_type not in ('binary', 'multiclass', 'multilabel-indicator'):\n        raise ValueError('%s is not supported' % y_type)\n    present_labels = unique_labels(y_true, y_pred)\n    if labels is None:\n        labels = present_labels\n        n_labels = None\n    else:\n        n_labels = len(labels)\n        labels = np.hstack([labels, np.setdiff1d(present_labels, labels, assume_unique=True)])\n    if y_true.ndim == 1:\n        if samplewise:\n            raise ValueError('Samplewise metrics are not available outside of multilabel classification.')\n        le = LabelEncoder()\n        le.fit(labels)\n        y_true = le.transform(y_true)\n        y_pred = le.transform(y_pred)\n        sorted_labels = le.classes_\n        tp = y_true == y_pred\n        tp_bins = y_true[tp]\n        if sample_weight is not None:\n            tp_bins_weights = np.asarray(sample_weight)[tp]\n        else:\n            tp_bins_weights = None\n        if len(tp_bins):\n            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights, minlength=len(labels))\n        else:\n            true_sum = pred_sum = tp_sum = np.zeros(len(labels))\n        if len(y_pred):\n            pred_sum = np.bincount(y_pred, weights=sample_weight, minlength=len(labels))\n        if len(y_true):\n            true_sum = np.bincount(y_true, weights=sample_weight, minlength=len(labels))\n        indices = np.searchsorted(sorted_labels, labels[:n_labels])\n        tp_sum = tp_sum[indices]\n        true_sum = true_sum[indices]\n        pred_sum = pred_sum[indices]\n    else:\n        sum_axis = 1 if samplewise else 0\n        if not np.array_equal(labels, present_labels):\n            if np.max(labels) > np.max(present_labels):\n                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d > %d' % (np.max(labels), np.max(present_labels)))\n            if np.min(labels) < 0:\n                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d < 0' % np.min(labels))\n        if n_labels is not None:\n            y_true = y_true[:, labels[:n_labels]]\n            y_pred = y_pred[:, labels[:n_labels]]\n        true_and_pred = y_true.multiply(y_pred)\n        tp_sum = count_nonzero(true_and_pred, axis=sum_axis, sample_weight=sample_weight)\n        pred_sum = count_nonzero(y_pred, axis=sum_axis, sample_weight=sample_weight)\n        true_sum = count_nonzero(y_true, axis=sum_axis, sample_weight=sample_weight)\n    fp = pred_sum - tp_sum\n    fn = true_sum - tp_sum\n    tp = tp_sum\n    if sample_weight is not None and samplewise:\n        sample_weight = np.array(sample_weight)\n        tp = np.array(tp)\n        fp = np.array(fp)\n        fn = np.array(fn)\n        tn = sample_weight * y_true.shape[1] - tp - fp - fn\n    elif sample_weight is not None:\n        tn = sum(sample_weight) - tp - fp - fn\n    elif samplewise:\n        tn = y_true.shape[1] - tp - fp - fn\n    else:\n        tn = y_true.shape[0] - tp - fp - fn\n    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)",
    ".sklearn.preprocessing.label.py@@LabelEncoder.fit": "def fit(self, y):\n    y = column_or_1d(y, warn=True)\n    self.classes_ = _encode(y)\n    return self",
    ".sklearn.preprocessing.label.py@@_encode": "def _encode(values, uniques=None, encode=False):\n    if values.dtype == object:\n        return _encode_python(values, uniques, encode)\n    else:\n        return _encode_numpy(values, uniques, encode)",
    ".sklearn.preprocessing.label.py@@_encode_numpy": "def _encode_numpy(values, uniques=None, encode=False):\n    if uniques is None:\n        if encode:\n            uniques, encoded = np.unique(values, return_inverse=True)\n            return (uniques, encoded)\n        else:\n            return np.unique(values)\n    if encode:\n        diff = _encode_check_unknown(values, uniques)\n        if diff:\n            raise ValueError('y contains previously unseen labels: %s' % str(diff))\n        encoded = np.searchsorted(uniques, values)\n        return (uniques, encoded)\n    else:\n        return uniques",
    ".sklearn.preprocessing.label.py@@LabelEncoder.transform": "def transform(self, y):\n    check_is_fitted(self, 'classes_')\n    y = column_or_1d(y, warn=True)\n    if _num_samples(y) == 0:\n        return np.array([])\n    _, y = _encode(y, uniques=self.classes_, encode=True)\n    return y",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.preprocessing.label.py@@_encode_check_unknown": "def _encode_check_unknown(values, uniques, return_mask=False):\n    if values.dtype == object:\n        uniques_set = set(uniques)\n        diff = list(set(values) - uniques_set)\n        if return_mask:\n            if diff:\n                valid_mask = np.array([val in uniques_set for val in values])\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return (diff, valid_mask)\n        else:\n            return diff\n    else:\n        unique_values = np.unique(values)\n        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))\n        if return_mask:\n            if diff:\n                valid_mask = np.in1d(values, uniques)\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return (diff, valid_mask)\n        else:\n            return diff",
    ".sklearn.metrics.classification.py@@_prf_divide": "def _prf_divide(numerator, denominator, metric, modifier, average, warn_for):\n    result = numerator / denominator\n    mask = denominator == 0.0\n    if not np.any(mask):\n        return result\n    result[mask] = 0.0\n    axis0 = 'sample'\n    axis1 = 'label'\n    if average == 'samples':\n        axis0, axis1 = (axis1, axis0)\n    if metric in warn_for and 'f-score' in warn_for:\n        msg_start = '{0} and F-score are'.format(metric.title())\n    elif metric in warn_for:\n        msg_start = '{0} is'.format(metric.title())\n    elif 'f-score' in warn_for:\n        msg_start = 'F-score is'\n    else:\n        return result\n    msg = '{0} ill-defined and being set to 0.0 {{0}} no {1} {2}s.'.format(msg_start, modifier, axis0)\n    if len(mask) == 1:\n        msg = msg.format('due to')\n    else:\n        msg = msg.format('in {0}s with'.format(axis1))\n    warnings.warn(msg, UndefinedMetricWarning, stacklevel=2)\n    return result",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_ensure_sparse_format": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):\n    if dtype is None:\n        dtype = spmatrix.dtype\n    changed_format = False\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n    _check_large_sparse(spmatrix, accept_large_sparse)\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.\")\n        if spmatrix.format not in accept_sparse:\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\".format(accept_sparse))\n    if dtype != spmatrix.dtype:\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and (not changed_format):\n        spmatrix = spmatrix.copy()\n    if force_all_finite:\n        if not hasattr(spmatrix, 'data'):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')\n    return spmatrix",
    ".sklearn.utils.validation.py@@_check_large_sparse": "def _check_large_sparse(X, accept_large_sparse=False):\n    if not accept_large_sparse:\n        supported_indices = ['int32']\n        if X.getformat() == 'coo':\n            index_keys = ['col', 'row']\n        elif X.getformat() in ['csr', 'csc', 'bsr']:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.multiclass.py@@_unique_indicator": "def _unique_indicator(y):\n    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])",
    ".sklearn.utils.sparsefuncs.py@@count_nonzero": "def count_nonzero(X, axis=None, sample_weight=None):\n    if axis == -1:\n        axis = 1\n    elif axis == -2:\n        axis = 0\n    elif X.format != 'csr':\n        raise TypeError('Expected CSR sparse format, got {0}'.format(X.format))\n    if axis is None:\n        if sample_weight is None:\n            return X.nnz\n        else:\n            return np.dot(np.diff(X.indptr), sample_weight)\n    elif axis == 1:\n        out = np.diff(X.indptr)\n        if sample_weight is None:\n            return out.astype('intp')\n        return out * sample_weight\n    elif axis == 0:\n        if sample_weight is None:\n            return np.bincount(X.indices, minlength=X.shape[1])\n        else:\n            weights = np.repeat(sample_weight, np.diff(X.indptr))\n            return np.bincount(X.indices, minlength=X.shape[1], weights=weights)\n    else:\n        raise ValueError('Unsupported axis: {0}'.format(axis))",
    ".sklearn.utils.multiclass.py@@_is_integral_float": "def _is_integral_float(y):\n    return y.dtype.kind == 'f' and np.all(y.astype(int) == y)",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result"
}