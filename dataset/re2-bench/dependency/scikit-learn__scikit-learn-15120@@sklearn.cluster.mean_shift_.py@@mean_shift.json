{
    ".sklearn.cluster.mean_shift_.py@@MeanShift.__init__": "def __init__(self, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):\n    self.bandwidth = bandwidth\n    self.seeds = seeds\n    self.bin_seeding = bin_seeding\n    self.cluster_all = cluster_all\n    self.min_bin_freq = min_bin_freq\n    self.n_jobs = n_jobs\n    self.max_iter = max_iter",
    ".sklearn.cluster.mean_shift_.py@@MeanShift.fit": "def fit(self, X, y=None):\n    X = check_array(X)\n    bandwidth = self.bandwidth\n    if bandwidth is None:\n        bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)\n    elif bandwidth <= 0:\n        raise ValueError('bandwidth needs to be greater than zero or None, got %f' % bandwidth)\n    seeds = self.seeds\n    if seeds is None:\n        if self.bin_seeding:\n            seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)\n        else:\n            seeds = X\n    n_samples, n_features = X.shape\n    center_intensity_dict = {}\n    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n    all_res = Parallel(n_jobs=self.n_jobs)((delayed(_mean_shift_single_seed)(seed, X, nbrs, self.max_iter) for seed in seeds))\n    for i in range(len(seeds)):\n        if all_res[i][1]:\n            center_intensity_dict[all_res[i][0]] = all_res[i][1]\n    self.n_iter_ = max([x[2] for x in all_res])\n    if not center_intensity_dict:\n        raise ValueError('No point was within bandwidth=%f of any seed. Try a different seeding strategy                          or increase the bandwidth.' % bandwidth)\n    sorted_by_intensity = sorted(center_intensity_dict.items(), key=lambda tup: (tup[1], tup[0]), reverse=True)\n    sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\n    unique = np.ones(len(sorted_centers), dtype=np.bool)\n    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=self.n_jobs).fit(sorted_centers)\n    for i, center in enumerate(sorted_centers):\n        if unique[i]:\n            neighbor_idxs = nbrs.radius_neighbors([center], return_distance=False)[0]\n            unique[neighbor_idxs] = 0\n            unique[i] = 1\n    cluster_centers = sorted_centers[unique]\n    nbrs = NearestNeighbors(n_neighbors=1, n_jobs=self.n_jobs).fit(cluster_centers)\n    labels = np.zeros(n_samples, dtype=np.int)\n    distances, idxs = nbrs.kneighbors(X)\n    if self.cluster_all:\n        labels = idxs.flatten()\n    else:\n        labels.fill(-1)\n        bool_selector = distances.flatten() <= bandwidth\n        labels[bool_selector] = idxs.flatten()[bool_selector]\n    self.cluster_centers_, self.labels_ = (cluster_centers, labels)\n    return self",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):\n    if warn_on_dtype is not None:\n        warnings.warn(\"'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\", DeprecationWarning, stacklevel=2)\n    array_orig = array\n    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    dtypes_orig = None\n    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):\n        dtypes_orig = np.array(array.dtypes)\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)\n                    array = array.astype(dtype, casting='unsafe', copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\"Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\", FutureWarning, stacklevel=2)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):\n        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)\n        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    from .extmath import _safe_accumulator_op\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, msg_dtype if msg_dtype is not None else X.dtype))\n    elif X.dtype == np.dtype('object') and (not allow_nan):\n        if _object_dtype_isnan(X).any():\n            raise ValueError('Input contains NaN')",
    ".sklearn._config.py@@get_config": "def get_config():\n    return _global_config.copy()",
    ".sklearn.utils.extmath.py@@_safe_accumulator_op": "def _safe_accumulator_op(op, x, *args, **kwargs):\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError(message)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n    try:\n        return len(x)\n    except TypeError:\n        raise TypeError(message)",
    ".sklearn.cluster.mean_shift_.py@@estimate_bandwidth": "def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0, n_jobs=None):\n    X = check_array(X)\n    random_state = check_random_state(random_state)\n    if n_samples is not None:\n        idx = random_state.permutation(X.shape[0])[:n_samples]\n        X = X[idx]\n    n_neighbors = int(X.shape[0] * quantile)\n    if n_neighbors < 1:\n        n_neighbors = 1\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=n_jobs)\n    nbrs.fit(X)\n    bandwidth = 0.0\n    for batch in gen_batches(len(X), 500):\n        d, _ = nbrs.kneighbors(X[batch, :], return_distance=True)\n        bandwidth += np.max(d, axis=1).sum()\n    return bandwidth / X.shape[0]",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.neighbors.unsupervised.py@@NearestNeighbors.__init__": "def __init__(self, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None, **kwargs):\n    super().__init__(n_neighbors=n_neighbors, radius=radius, algorithm=algorithm, leaf_size=leaf_size, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs, **kwargs)",
    ".sklearn.neighbors.base.py@@NeighborsBase.__init__": "def __init__(self, n_neighbors=None, radius=None, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None):\n    self.n_neighbors = n_neighbors\n    self.radius = radius\n    self.algorithm = algorithm\n    self.leaf_size = leaf_size\n    self.metric = metric\n    self.metric_params = metric_params\n    self.p = p\n    self.n_jobs = n_jobs\n    self._check_algorithm_metric()",
    ".sklearn.neighbors.base.py@@NeighborsBase._check_algorithm_metric": "def _check_algorithm_metric(self):\n    if self.algorithm not in ['auto', 'brute', 'kd_tree', 'ball_tree']:\n        raise ValueError(\"unrecognized algorithm: '%s'\" % self.algorithm)\n    if self.algorithm == 'auto':\n        if self.metric == 'precomputed':\n            alg_check = 'brute'\n        elif callable(self.metric) or self.metric in VALID_METRICS['ball_tree']:\n            alg_check = 'ball_tree'\n        else:\n            alg_check = 'brute'\n    else:\n        alg_check = self.algorithm\n    if callable(self.metric):\n        if self.algorithm == 'kd_tree':\n            raise ValueError(\"kd_tree algorithm does not support callable metric '%s'\" % self.metric)\n    elif self.metric not in VALID_METRICS[alg_check]:\n        raise ValueError(\"Metric '%s' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['%s']) to get valid options. Metric can also be a callable function.\" % (self.metric, alg_check))\n    if self.metric_params is not None and 'p' in self.metric_params:\n        warnings.warn('Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.', SyntaxWarning, stacklevel=3)\n        effective_p = self.metric_params['p']\n    else:\n        effective_p = self.p\n    if self.metric in ['wminkowski', 'minkowski'] and effective_p < 1:\n        raise ValueError('p must be greater than one for minkowski metric')",
    ".sklearn.neighbors.base.py@@UnsupervisedMixin.fit": "def fit(self, X, y=None):\n    return self._fit(X)",
    ".sklearn.neighbors.base.py@@NeighborsBase._fit": "def _fit(self, X):\n    self._check_algorithm_metric()\n    if self.metric_params is None:\n        self.effective_metric_params_ = {}\n    else:\n        self.effective_metric_params_ = self.metric_params.copy()\n    effective_p = self.effective_metric_params_.get('p', self.p)\n    if self.metric in ['wminkowski', 'minkowski']:\n        self.effective_metric_params_['p'] = effective_p\n    self.effective_metric_ = self.metric\n    if self.metric == 'minkowski':\n        p = self.effective_metric_params_.pop('p', 2)\n        if p < 1:\n            raise ValueError('p must be greater than one for minkowski metric')\n        elif p == 1:\n            self.effective_metric_ = 'manhattan'\n        elif p == 2:\n            self.effective_metric_ = 'euclidean'\n        elif p == np.inf:\n            self.effective_metric_ = 'chebyshev'\n        else:\n            self.effective_metric_params_['p'] = p\n    if isinstance(X, NeighborsBase):\n        self._fit_X = X._fit_X\n        self._tree = X._tree\n        self._fit_method = X._fit_method\n        self.n_samples_fit_ = X.n_samples_fit_\n        return self\n    elif isinstance(X, BallTree):\n        self._fit_X = X.data\n        self._tree = X\n        self._fit_method = 'ball_tree'\n        self.n_samples_fit_ = X.data.shape[0]\n        return self\n    elif isinstance(X, KDTree):\n        self._fit_X = X.data\n        self._tree = X\n        self._fit_method = 'kd_tree'\n        self.n_samples_fit_ = X.data.shape[0]\n        return self\n    if self.effective_metric_ == 'precomputed':\n        X = _check_precomputed(X)\n    else:\n        X = check_array(X, accept_sparse='csr')\n    n_samples = X.shape[0]\n    if n_samples == 0:\n        raise ValueError('n_samples must be greater than 0')\n    if self.metric == 'precomputed' and X.shape[0] != X.shape[1]:\n        raise ValueError('Precomputed matrix must be a square matrix. Input is a {}x{} matrix.'.format(X.shape[0], X.shape[1]))\n    if issparse(X):\n        if self.algorithm not in ('auto', 'brute'):\n            warnings.warn('cannot use tree with sparse input: using brute force')\n        if self.effective_metric_ not in VALID_METRICS_SPARSE['brute'] and (not callable(self.effective_metric_)):\n            raise ValueError(\"Metric '%s' not valid for sparse input. Use sorted(sklearn.neighbors.VALID_METRICS_SPARSE['brute']) to get valid options. Metric can also be a callable function.\" % self.effective_metric_)\n        self._fit_X = X.copy()\n        self._tree = None\n        self._fit_method = 'brute'\n        self.n_samples_fit_ = X.shape[0]\n        return self\n    self._fit_method = self.algorithm\n    self._fit_X = X\n    self.n_samples_fit_ = X.shape[0]\n    if self._fit_method == 'auto':\n        if (self.n_neighbors is None or self.n_neighbors < self._fit_X.shape[0] // 2) and self.metric != 'precomputed':\n            if self.effective_metric_ in VALID_METRICS['kd_tree']:\n                self._fit_method = 'kd_tree'\n            elif callable(self.effective_metric_) or self.effective_metric_ in VALID_METRICS['ball_tree']:\n                self._fit_method = 'ball_tree'\n            else:\n                self._fit_method = 'brute'\n        else:\n            self._fit_method = 'brute'\n    if self._fit_method == 'ball_tree':\n        self._tree = BallTree(X, self.leaf_size, metric=self.effective_metric_, **self.effective_metric_params_)\n    elif self._fit_method == 'kd_tree':\n        self._tree = KDTree(X, self.leaf_size, metric=self.effective_metric_, **self.effective_metric_params_)\n    elif self._fit_method == 'brute':\n        self._tree = None\n    else:\n        raise ValueError(\"algorithm = '%s' not recognized\" % self.algorithm)\n    if self.n_neighbors is not None:\n        if self.n_neighbors <= 0:\n            raise ValueError('Expected n_neighbors > 0. Got %d' % self.n_neighbors)\n        elif not isinstance(self.n_neighbors, numbers.Integral):\n            raise TypeError('n_neighbors does not take %s value, enter integer value' % type(self.n_neighbors))\n    return self",
    ".sklearn.utils.__init__.py@@gen_batches": "def gen_batches(n, batch_size, min_batch_size=0):\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    ".sklearn.neighbors.base.py@@KNeighborsMixin.kneighbors": "def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n    check_is_fitted(self)\n    if n_neighbors is None:\n        n_neighbors = self.n_neighbors\n    elif n_neighbors <= 0:\n        raise ValueError('Expected n_neighbors > 0. Got %d' % n_neighbors)\n    elif not isinstance(n_neighbors, numbers.Integral):\n        raise TypeError('n_neighbors does not take %s value, enter integer value' % type(n_neighbors))\n    if X is not None:\n        query_is_train = False\n        if self.effective_metric_ == 'precomputed':\n            X = _check_precomputed(X)\n        else:\n            X = check_array(X, accept_sparse='csr')\n    else:\n        query_is_train = True\n        X = self._fit_X\n        n_neighbors += 1\n    n_samples_fit = self.n_samples_fit_\n    if n_neighbors > n_samples_fit:\n        raise ValueError('Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d' % (n_samples_fit, n_neighbors))\n    n_jobs = effective_n_jobs(self.n_jobs)\n    chunked_results = None\n    if self._fit_method == 'brute' and self.effective_metric_ == 'precomputed' and issparse(X):\n        results = _kneighbors_from_graph(X, n_neighbors=n_neighbors, return_distance=return_distance)\n    elif self._fit_method == 'brute':\n        reduce_func = partial(self._kneighbors_reduce_func, n_neighbors=n_neighbors, return_distance=return_distance)\n        if self.effective_metric_ == 'euclidean':\n            kwds = {'squared': True}\n        else:\n            kwds = self.effective_metric_params_\n        chunked_results = list(pairwise_distances_chunked(X, self._fit_X, reduce_func=reduce_func, metric=self.effective_metric_, n_jobs=n_jobs, **kwds))\n    elif self._fit_method in ['ball_tree', 'kd_tree']:\n        if issparse(X):\n            raise ValueError(\"%s does not work with sparse matrices. Densify the data, or set algorithm='brute'\" % self._fit_method)\n        old_joblib = LooseVersion(joblib.__version__) < LooseVersion('0.12')\n        if old_joblib:\n            check_pickle = False if old_joblib else None\n            delayed_query = delayed(_tree_query_parallel_helper, check_pickle=check_pickle)\n            parallel_kwargs = {'backend': 'threading'}\n        else:\n            delayed_query = delayed(_tree_query_parallel_helper)\n            parallel_kwargs = {'prefer': 'threads'}\n        chunked_results = Parallel(n_jobs, **parallel_kwargs)((delayed_query(self._tree, X[s], n_neighbors, return_distance) for s in gen_even_slices(X.shape[0], n_jobs)))\n    else:\n        raise ValueError('internal: _fit_method not recognized')\n    if chunked_results is not None:\n        if return_distance:\n            neigh_dist, neigh_ind = zip(*chunked_results)\n            results = (np.vstack(neigh_dist), np.vstack(neigh_ind))\n        else:\n            results = np.vstack(chunked_results)\n    if not query_is_train:\n        return results\n    else:\n        if return_distance:\n            neigh_dist, neigh_ind = results\n        else:\n            neigh_ind = results\n        n_queries, _ = X.shape\n        sample_range = np.arange(n_queries)[:, None]\n        sample_mask = neigh_ind != sample_range\n        dup_gr_nbrs = np.all(sample_mask, axis=1)\n        sample_mask[:, 0][dup_gr_nbrs] = False\n        neigh_ind = np.reshape(neigh_ind[sample_mask], (n_queries, n_neighbors - 1))\n        if return_distance:\n            neigh_dist = np.reshape(neigh_dist[sample_mask], (n_queries, n_neighbors - 1))\n            return (neigh_dist, neigh_ind)\n        return neigh_ind",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes='deprecated', msg=None, all_or_any='deprecated'):\n    if attributes != 'deprecated':\n        warnings.warn('Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.', DeprecationWarning)\n    if all_or_any != 'deprecated':\n        warnings.warn('Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.', DeprecationWarning)\n    if isclass(estimator):\n        raise TypeError('{} is a class, not an instance.'.format(estimator))\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    attrs = [v for v in vars(estimator) if (v.endswith('_') or v.startswith('_')) and (not v.startswith('__'))]\n    if not attrs:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.utils.__init__.py@@gen_even_slices": "def gen_even_slices(n, n_packs, n_samples=None):\n    start = 0\n    if n_packs < 1:\n        raise ValueError('gen_even_slices got n_packs=%s, must be >=1' % n_packs)\n    for pack_num in range(n_packs):\n        this_n = n // n_packs\n        if pack_num < n % n_packs:\n            this_n += 1\n        if this_n > 0:\n            end = start + this_n\n            if n_samples is not None:\n                end = min(n_samples, end)\n            yield slice(start, end, None)\n            start = end",
    ".sklearn.neighbors.base.py@@_tree_query_parallel_helper": "def _tree_query_parallel_helper(tree, *args, **kwargs):\n    return tree.query(*args, **kwargs)",
    ".sklearn.cluster.mean_shift_.py@@_mean_shift_single_seed": "def _mean_shift_single_seed(my_mean, X, nbrs, max_iter):\n    bandwidth = nbrs.get_params()['radius']\n    stop_thresh = 0.001 * bandwidth\n    completed_iterations = 0\n    while True:\n        i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth, return_distance=False)[0]\n        points_within = X[i_nbrs]\n        if len(points_within) == 0:\n            break\n        my_old_mean = my_mean\n        my_mean = np.mean(points_within, axis=0)\n        if np.linalg.norm(my_mean - my_old_mean) < stop_thresh or completed_iterations == max_iter:\n            break\n        completed_iterations += 1\n    return (tuple(my_mean), len(points_within), completed_iterations)",
    ".sklearn.base.py@@BaseEstimator.get_params": "def get_params(self, deep=True):\n    out = dict()\n    for key in self._get_param_names():\n        try:\n            value = getattr(self, key)\n        except AttributeError:\n            warnings.warn('From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.', FutureWarning)\n            value = None\n        if deep and hasattr(value, 'get_params'):\n            deep_items = value.get_params().items()\n            out.update(((key + '__' + k, val) for k, val in deep_items))\n        out[key] = value\n    return out",
    ".sklearn.base.py@@BaseEstimator._get_param_names": "def _get_param_names(cls):\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    if init is object.__init__:\n        return []\n    init_signature = inspect.signature(init)\n    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n    for p in parameters:\n        if p.kind == p.VAR_POSITIONAL:\n            raise RuntimeError(\"scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention.\" % (cls, init_signature))\n    return sorted([p.name for p in parameters])",
    ".sklearn.neighbors.base.py@@RadiusNeighborsMixin.radius_neighbors": "def radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False):\n    check_is_fitted(self)\n    if X is not None:\n        query_is_train = False\n        if self.effective_metric_ == 'precomputed':\n            X = _check_precomputed(X)\n        else:\n            X = check_array(X, accept_sparse='csr')\n    else:\n        query_is_train = True\n        X = self._fit_X\n    if radius is None:\n        radius = self.radius\n    if self._fit_method == 'brute' and self.effective_metric_ == 'precomputed' and issparse(X):\n        results = _radius_neighbors_from_graph(X, radius=radius, return_distance=return_distance)\n    elif self._fit_method == 'brute':\n        if self.effective_metric_ == 'euclidean':\n            radius *= radius\n            kwds = {'squared': True}\n        else:\n            kwds = self.effective_metric_params_\n        reduce_func = partial(self._radius_neighbors_reduce_func, radius=radius, return_distance=return_distance)\n        chunked_results = pairwise_distances_chunked(X, self._fit_X, reduce_func=reduce_func, metric=self.effective_metric_, n_jobs=self.n_jobs, **kwds)\n        if return_distance:\n            neigh_dist_chunks, neigh_ind_chunks = zip(*chunked_results)\n            neigh_dist_list = sum(neigh_dist_chunks, [])\n            neigh_ind_list = sum(neigh_ind_chunks, [])\n            neigh_dist = np.empty(len(neigh_dist_list), dtype='object')\n            neigh_dist[:] = neigh_dist_list\n            neigh_ind = np.empty(len(neigh_ind_list), dtype='object')\n            neigh_ind[:] = neigh_ind_list\n            results = (neigh_dist, neigh_ind)\n        else:\n            neigh_ind_list = sum(chunked_results, [])\n            results = np.empty(len(neigh_ind_list), dtype='object')\n            results[:] = neigh_ind_list\n    elif self._fit_method in ['ball_tree', 'kd_tree']:\n        if issparse(X):\n            raise ValueError(\"%s does not work with sparse matrices. Densify the data, or set algorithm='brute'\" % self._fit_method)\n        n_jobs = effective_n_jobs(self.n_jobs)\n        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):\n            delayed_query = delayed(_tree_query_radius_parallel_helper, check_pickle=False)\n            parallel_kwargs = {'backend': 'threading'}\n        else:\n            delayed_query = delayed(_tree_query_radius_parallel_helper)\n            parallel_kwargs = {'prefer': 'threads'}\n        chunked_results = Parallel(n_jobs, **parallel_kwargs)((delayed_query(self._tree, X[s], radius, return_distance, sort_results=sort_results) for s in gen_even_slices(X.shape[0], n_jobs)))\n        if return_distance:\n            neigh_ind, neigh_dist = tuple(zip(*chunked_results))\n            results = (np.hstack(neigh_dist), np.hstack(neigh_ind))\n        else:\n            results = np.hstack(chunked_results)\n    else:\n        raise ValueError('internal: _fit_method not recognized')\n    if not query_is_train:\n        return results\n    else:\n        if return_distance:\n            neigh_dist, neigh_ind = results\n        else:\n            neigh_ind = results\n        for ind, ind_neighbor in enumerate(neigh_ind):\n            mask = ind_neighbor != ind\n            neigh_ind[ind] = ind_neighbor[mask]\n            if return_distance:\n                neigh_dist[ind] = neigh_dist[ind][mask]\n        if return_distance:\n            return (neigh_dist, neigh_ind)\n        return neigh_ind",
    ".sklearn.neighbors.base.py@@_tree_query_radius_parallel_helper": "def _tree_query_radius_parallel_helper(tree, *args, **kwargs):\n    return tree.query_radius(*args, **kwargs)",
    ".sklearn.metrics.pairwise.py@@pairwise_distances_chunked": "def pairwise_distances_chunked(X, Y=None, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds):\n    n_samples_X = _num_samples(X)\n    if metric == 'precomputed':\n        slices = (slice(0, n_samples_X),)\n    else:\n        if Y is None:\n            Y = X\n        chunk_n_rows = get_chunk_n_rows(row_bytes=8 * _num_samples(Y), max_n_rows=n_samples_X, working_memory=working_memory)\n        slices = gen_batches(n_samples_X, chunk_n_rows)\n    params = _precompute_metric_params(X, Y, metric=metric, **kwds)\n    kwds.update(**params)\n    for sl in slices:\n        if sl.start == 0 and sl.stop == n_samples_X:\n            X_chunk = X\n        else:\n            X_chunk = X[sl]\n        D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)\n        if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(metric, None) is euclidean_distances:\n            D_chunk.flat[sl.start::_num_samples(X) + 1] = 0\n        if reduce_func is not None:\n            chunk_size = D_chunk.shape[0]\n            D_chunk = reduce_func(D_chunk, sl.start)\n            _check_chunk_size(D_chunk, chunk_size)\n        yield D_chunk",
    ".sklearn.utils.__init__.py@@get_chunk_n_rows": "def get_chunk_n_rows(row_bytes, max_n_rows=None, working_memory=None):\n    if working_memory is None:\n        working_memory = get_config()['working_memory']\n    chunk_n_rows = int(working_memory * 2 ** 20 // row_bytes)\n    if max_n_rows is not None:\n        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n    if chunk_n_rows < 1:\n        warnings.warn('Could not adhere to working_memory config. Currently %.0fMiB, %.0fMiB required.' % (working_memory, np.ceil(row_bytes * 2 ** (-20))))\n        chunk_n_rows = 1\n    return chunk_n_rows",
    ".sklearn.metrics.pairwise.py@@_precompute_metric_params": "def _precompute_metric_params(X, Y, metric=None, **kwds):\n    if metric == 'seuclidean' and 'V' not in kwds:\n        if X is Y:\n            V = np.var(X, axis=0, ddof=1)\n        else:\n            V = np.var(np.vstack([X, Y]), axis=0, ddof=1)\n        return {'V': V}\n    if metric == 'mahalanobis' and 'VI' not in kwds:\n        if X is Y:\n            VI = np.linalg.inv(np.cov(X.T)).T\n        else:\n            VI = np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T\n        return {'VI': VI}\n    return {}",
    ".sklearn.metrics.pairwise.py@@pairwise_distances": "def pairwise_distances(X, Y=None, metric='euclidean', n_jobs=None, force_all_finite=True, **kwds):\n    if metric not in _VALID_METRICS and (not callable(metric)) and (metric != 'precomputed'):\n        raise ValueError(\"Unknown metric %s. Valid metrics are %s, or 'precomputed', or a callable\" % (metric, _VALID_METRICS))\n    if metric == 'precomputed':\n        X, _ = check_pairwise_arrays(X, Y, precomputed=True, force_all_finite=force_all_finite)\n        whom = '`pairwise_distances`. Precomputed distance  need to have non-negative values.'\n        check_non_negative(X, whom=whom)\n        return X\n    elif metric in PAIRWISE_DISTANCE_FUNCTIONS:\n        func = PAIRWISE_DISTANCE_FUNCTIONS[metric]\n    elif callable(metric):\n        func = partial(_pairwise_callable, metric=metric, force_all_finite=force_all_finite, **kwds)\n    else:\n        if issparse(X) or issparse(Y):\n            raise TypeError('scipy distance metrics do not support sparse matrices.')\n        dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None\n        if dtype == bool and (X.dtype != bool or (Y is not None and Y.dtype != bool)):\n            msg = 'Data was converted to boolean for metric %s' % metric\n            warnings.warn(msg, DataConversionWarning)\n        X, Y = check_pairwise_arrays(X, Y, dtype=dtype, force_all_finite=force_all_finite)\n        params = _precompute_metric_params(X, Y, metric=metric, **kwds)\n        kwds.update(**params)\n        if effective_n_jobs(n_jobs) == 1 and X is Y:\n            return distance.squareform(distance.pdist(X, metric=metric, **kwds))\n        func = partial(distance.cdist, metric=metric, **kwds)\n    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)",
    ".sklearn.metrics.pairwise.py@@_parallel_pairwise": "def _parallel_pairwise(X, Y, func, n_jobs, **kwds):\n    if Y is None:\n        Y = X\n    X, Y, dtype = _return_float_dtype(X, Y)\n    if effective_n_jobs(n_jobs) == 1:\n        return func(X, Y, **kwds)\n    fd = delayed(_dist_wrapper)\n    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order='F')\n    Parallel(backend='threading', n_jobs=n_jobs)((fd(func, ret, s, X, Y[s], **kwds) for s in gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs))))\n    if (X is Y or Y is None) and func is euclidean_distances:\n        np.fill_diagonal(ret, 0)\n    return ret",
    ".sklearn.metrics.pairwise.py@@_return_float_dtype": "def _return_float_dtype(X, Y):\n    if not issparse(X) and (not isinstance(X, np.ndarray)):\n        X = np.asarray(X)\n    if Y is None:\n        Y_dtype = X.dtype\n    elif not issparse(Y) and (not isinstance(Y, np.ndarray)):\n        Y = np.asarray(Y)\n        Y_dtype = Y.dtype\n    else:\n        Y_dtype = Y.dtype\n    if X.dtype == Y_dtype == np.float32:\n        dtype = np.float32\n    else:\n        dtype = np.float\n    return (X, Y, dtype)",
    ".sklearn.metrics.pairwise.py@@euclidean_distances": "def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False, X_norm_squared=None):\n    X, Y = check_pairwise_arrays(X, Y)\n    if X_norm_squared is not None:\n        XX = check_array(X_norm_squared)\n        if XX.shape == (1, X.shape[0]):\n            XX = XX.T\n        elif XX.shape != (X.shape[0], 1):\n            raise ValueError('Incompatible dimensions for X and X_norm_squared')\n        if XX.dtype == np.float32:\n            XX = None\n    elif X.dtype == np.float32:\n        XX = None\n    else:\n        XX = row_norms(X, squared=True)[:, np.newaxis]\n    if X is Y and XX is not None:\n        YY = XX.T\n    elif Y_norm_squared is not None:\n        YY = np.atleast_2d(Y_norm_squared)\n        if YY.shape != (1, Y.shape[0]):\n            raise ValueError('Incompatible dimensions for Y and Y_norm_squared')\n        if YY.dtype == np.float32:\n            YY = None\n    elif Y.dtype == np.float32:\n        YY = None\n    else:\n        YY = row_norms(Y, squared=True)[np.newaxis, :]\n    if X.dtype == np.float32:\n        distances = _euclidean_distances_upcast(X, XX, Y, YY)\n    else:\n        distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n        distances += XX\n        distances += YY\n    np.maximum(distances, 0, out=distances)\n    if X is Y:\n        np.fill_diagonal(distances, 0)\n    return distances if squared else np.sqrt(distances, out=distances)",
    ".sklearn.metrics.pairwise.py@@check_pairwise_arrays": "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None, accept_sparse='csr', force_all_finite=True, copy=False):\n    X, Y, dtype_float = _return_float_dtype(X, Y)\n    estimator = 'check_pairwise_arrays'\n    if dtype is None:\n        dtype = dtype_float\n    if Y is X or Y is None:\n        X = Y = check_array(X, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, estimator=estimator)\n    else:\n        X = check_array(X, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, estimator=estimator)\n        Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, estimator=estimator)\n    if precomputed:\n        if X.shape[1] != Y.shape[0]:\n            raise ValueError('Precomputed metric requires shape (n_queries, n_indexed). Got (%d, %d) for %d indexed.' % (X.shape[0], X.shape[1], Y.shape[0]))\n    elif X.shape[1] != Y.shape[1]:\n        raise ValueError('Incompatible dimension for X and Y matrices: X.shape[1] == %d while Y.shape[1] == %d' % (X.shape[1], Y.shape[1]))\n    return (X, Y)",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n    if sparse.issparse(a) and sparse.issparse(b) and dense_output and hasattr(ret, 'toarray'):\n        return ret.toarray()\n    return ret",
    ".sklearn.neighbors.base.py@@RadiusNeighborsMixin._radius_neighbors_reduce_func": "def _radius_neighbors_reduce_func(self, dist, start, radius, return_distance):\n    neigh_ind = [np.where(d <= radius)[0] for d in dist]\n    if return_distance:\n        if self.effective_metric_ == 'euclidean':\n            dist = [np.sqrt(d[neigh_ind[i]]) for i, d in enumerate(dist)]\n        else:\n            dist = [d[neigh_ind[i]] for i, d in enumerate(dist)]\n        results = (dist, neigh_ind)\n    else:\n        results = neigh_ind\n    return results",
    ".sklearn.metrics.pairwise.py@@_check_chunk_size": "def _check_chunk_size(reduced, chunk_size):\n    is_tuple = isinstance(reduced, tuple)\n    if not is_tuple:\n        reduced = (reduced,)\n    if any((isinstance(r, tuple) or not hasattr(r, '__iter__') for r in reduced)):\n        raise TypeError('reduce_func returned %r. Expected sequence(s) of length %d.' % (reduced if is_tuple else reduced[0], chunk_size))\n    if any((_num_samples(r) != chunk_size for r in reduced)):\n        actual_size = tuple((_num_samples(r) for r in reduced))\n        raise ValueError('reduce_func returned object of length %s. Expected same length as input: %d.' % (actual_size if is_tuple else actual_size[0], chunk_size))",
    ".sklearn.neighbors.base.py@@KNeighborsMixin._kneighbors_reduce_func": "def _kneighbors_reduce_func(self, dist, start, n_neighbors, return_distance):\n    sample_range = np.arange(dist.shape[0])[:, None]\n    neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)\n    neigh_ind = neigh_ind[:, :n_neighbors]\n    neigh_ind = neigh_ind[sample_range, np.argsort(dist[sample_range, neigh_ind])]\n    if return_distance:\n        if self.effective_metric_ == 'euclidean':\n            result = (np.sqrt(dist[sample_range, neigh_ind]), neigh_ind)\n        else:\n            result = (dist[sample_range, neigh_ind], neigh_ind)\n    else:\n        result = neigh_ind\n    return result"
}