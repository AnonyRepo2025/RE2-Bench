{
    ".sklearn.linear_model.logistic.py@@_check_solver_option": "def _check_solver_option(solver, multi_class, penalty, dual):\n    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:\n        raise ValueError('Logistic Regression supports only liblinear, newton-cg, lbfgs, sag and saga solvers, got %s' % solver)\n    if multi_class not in ['multinomial', 'ovr']:\n        raise ValueError('multi_class should be either multinomial or ovr, got %s' % multi_class)\n    if multi_class == 'multinomial' and solver == 'liblinear':\n        raise ValueError('Solver %s does not support a multinomial backend.' % solver)\n    if solver not in ['liblinear', 'saga']:\n        if penalty != 'l2':\n            raise ValueError('Solver %s supports only l2 penalties, got %s penalty.' % (solver, penalty))\n    if solver != 'liblinear':\n        if dual:\n            raise ValueError('Solver %s supports only dual=False, got dual=%s' % (solver, dual))",
    ".sklearn.utils.validation.py@@check_random_state": "def check_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)",
    ".sklearn.utils.optimize.py@@newton_cg": "def newton_cg(grad_hess, func, grad, x0, args=(), tol=0.0001, maxiter=100, maxinner=200, line_search=True, warn=True):\n    x0 = np.asarray(x0).flatten()\n    xk = x0\n    k = 0\n    if line_search:\n        old_fval = func(x0, *args)\n        old_old_fval = None\n    while k < maxiter:\n        fgrad, fhess_p = grad_hess(xk, *args)\n        absgrad = np.abs(fgrad)\n        if np.max(absgrad) < tol:\n            break\n        maggrad = np.sum(absgrad)\n        eta = min([0.5, np.sqrt(maggrad)])\n        termcond = eta * maggrad\n        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)\n        alphak = 1.0\n        if line_search:\n            try:\n                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = _line_search_wolfe12(func, grad, xk, xsupi, fgrad, old_fval, old_old_fval, args=args)\n            except _LineSearchError:\n                warnings.warn('Line Search failed')\n                break\n        xk = xk + alphak * xsupi\n        k += 1\n    if warn and k >= maxiter:\n        warnings.warn('newton-cg failed to converge. Increase the number of iterations.', ConvergenceWarning)\n    return (xk, k)",
    ".sklearn.linear_model.logistic.py@@_logistic_loss": "def _logistic_loss(w, X, y, alpha, sample_weight=None):\n    w, c, yz = _intercept_dot(w, X, y)\n    if sample_weight is None:\n        sample_weight = np.ones(y.shape[0])\n    out = -np.sum(sample_weight * log_logistic(yz)) + 0.5 * alpha * np.dot(w, w)\n    return out",
    ".sklearn.linear_model.logistic.py@@_intercept_dot": "def _intercept_dot(w, X, y):\n    c = 0.0\n    if w.size == X.shape[1] + 1:\n        c = w[-1]\n        w = w[:-1]\n    z = safe_sparse_dot(X, w) + c\n    yz = y * z\n    return (w, c, yz)",
    ".sklearn.utils.extmath.py@@safe_sparse_dot": "def safe_sparse_dot(a, b, dense_output=False):\n    if issparse(a) or issparse(b):\n        ret = a * b\n        if dense_output and hasattr(ret, 'toarray'):\n            ret = ret.toarray()\n        return ret\n    else:\n        return np.dot(a, b)",
    ".sklearn.utils.extmath.py@@log_logistic": "def log_logistic(X, out=None):\n    is_1d = X.ndim == 1\n    X = np.atleast_2d(X)\n    X = check_array(X, dtype=np.float64)\n    n_samples, n_features = X.shape\n    if out is None:\n        out = np.empty_like(X)\n    _log_logistic_sigmoid(n_samples, n_features, X, out)\n    if is_1d:\n        return np.squeeze(out)\n    return out",
    ".sklearn.utils.validation.py@@check_array": "def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):\n    if accept_sparse is None:\n        warnings.warn(\"Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\", DeprecationWarning)\n        accept_sparse = False\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'\n    dtype_orig = getattr(array, 'dtype', None)\n    if not hasattr(dtype_orig, 'kind'):\n        dtype_orig = None\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == 'O':\n            dtype = np.float64\n        else:\n            dtype = None\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            dtype = None\n        else:\n            dtype = dtype[0]\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(force_all_finite))\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = 'Estimator'\n    context = ' by %s' % estimator_name if estimator is not None else ''\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)\n    else:\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.array(array, dtype=dtype, order=order, copy=copy)\n            except ComplexWarning:\n                raise ValueError('Complex data not supported\\n{}\\n'.format(array))\n        _ensure_no_complex_data(array)\n        if ensure_2d:\n            if array.ndim == 0:\n                raise ValueError('Expected 2D array, got scalar array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            if array.ndim == 1:\n                raise ValueError('Expected 2D array, got 1D array instead:\\narray={}.\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))\n            array = np.array(array, dtype=dtype, order=order, copy=copy)\n        if dtype_numeric and array.dtype.kind == 'O':\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))\n    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):\n        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)\n        warnings.warn(msg, DataConversionWarning)\n    return array",
    ".sklearn.utils.validation.py@@_ensure_no_complex_data": "def _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):\n        raise ValueError('Complex data not supported\\n{}\\n'.format(array))",
    ".sklearn.utils.validation.py@@_assert_all_finite": "def _assert_all_finite(X, allow_nan=False):\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    is_float = X.dtype.kind in 'fc'\n    if is_float and np.isfinite(X.sum()):\n        pass\n    elif is_float:\n        msg_err = 'Input contains {} or a value too large for {!r}.'\n        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(msg_err.format(type_err, X.dtype))",
    ".sklearn.__init__.py@@get_config": "def get_config():\n    return {'assume_finite': _ASSUME_FINITE}",
    ".sklearn.utils.validation.py@@_shape_repr": "def _shape_repr(shape):\n    if len(shape) == 0:\n        return '()'\n    joined = ', '.join(('%d' % e for e in shape))\n    if len(shape) == 1:\n        joined += ','\n    return '(%s)' % joined",
    ".sklearn.utils.validation.py@@_num_samples": "def _num_samples(x):\n    if hasattr(x, 'fit') and callable(x.fit):\n        raise TypeError('Expected sequence or array-like, got estimator %s' % x)\n    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError('Expected sequence or array-like, got %s' % type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)\n        return x.shape[0]\n    else:\n        return len(x)",
    ".sklearn.linear_model.logistic.py@@_logistic_grad_hess": "def _logistic_grad_hess(w, X, y, alpha, sample_weight=None):\n    n_samples, n_features = X.shape\n    grad = np.empty_like(w)\n    fit_intercept = grad.shape[0] > n_features\n    w, c, yz = _intercept_dot(w, X, y)\n    if sample_weight is None:\n        sample_weight = np.ones(y.shape[0])\n    z = expit(yz)\n    z0 = sample_weight * (z - 1) * y\n    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n    if fit_intercept:\n        grad[-1] = z0.sum()\n    d = sample_weight * z * (1 - z)\n    if sparse.issparse(X):\n        dX = safe_sparse_dot(sparse.dia_matrix((d, 0), shape=(n_samples, n_samples)), X)\n    else:\n        dX = d[:, np.newaxis] * X\n    if fit_intercept:\n        dd_intercept = np.squeeze(np.array(dX.sum(axis=0)))\n\n    def Hs(s):\n        ret = np.empty_like(s)\n        ret[:n_features] = X.T.dot(dX.dot(s[:n_features]))\n        ret[:n_features] += alpha * s[:n_features]\n        if fit_intercept:\n            ret[:n_features] += s[-1] * dd_intercept\n            ret[-1] = dd_intercept.dot(s[:n_features])\n            ret[-1] += d.sum() * s[-1]\n        return ret\n    return (grad, Hs)",
    ".sklearn.utils.optimize.py@@_cg": "def _cg(fhess_p, fgrad, maxiter, tol):\n    xsupi = np.zeros(len(fgrad), dtype=fgrad.dtype)\n    ri = fgrad\n    psupi = -ri\n    i = 0\n    dri0 = np.dot(ri, ri)\n    while i <= maxiter:\n        if np.sum(np.abs(ri)) <= tol:\n            break\n        Ap = fhess_p(psupi)\n        curv = np.dot(psupi, Ap)\n        if 0 <= curv <= 3 * np.finfo(np.float64).eps:\n            break\n        elif curv < 0:\n            if i > 0:\n                break\n            else:\n                xsupi += dri0 / curv * psupi\n                break\n        alphai = dri0 / curv\n        xsupi += alphai * psupi\n        ri = ri + alphai * Ap\n        dri1 = np.dot(ri, ri)\n        betai = dri1 / dri0\n        psupi = -ri + betai * psupi\n        i = i + 1\n        dri0 = dri1\n    return xsupi",
    ".sklearn.linear_model.logistic.py@@Hs": "def Hs(s):\n    ret = np.empty_like(s)\n    ret[:n_features] = X.T.dot(dX.dot(s[:n_features]))\n    ret[:n_features] += alpha * s[:n_features]\n    if fit_intercept:\n        ret[:n_features] += s[-1] * dd_intercept\n        ret[-1] = dd_intercept.dot(s[:n_features])\n        ret[-1] += d.sum() * s[-1]\n    return ret",
    ".sklearn.utils.optimize.py@@_line_search_wolfe12": "def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs):\n    ret = line_search_wolfe1(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\n    if ret[0] is None:\n        ret = line_search_wolfe2(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\n    if ret[0] is None:\n        raise _LineSearchError()\n    return ret",
    ".sklearn.linear_model.logistic.py@@_logistic_loss_and_grad": "def _logistic_loss_and_grad(w, X, y, alpha, sample_weight=None):\n    n_samples, n_features = X.shape\n    grad = np.empty_like(w)\n    w, c, yz = _intercept_dot(w, X, y)\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples)\n    out = -np.sum(sample_weight * log_logistic(yz)) + 0.5 * alpha * np.dot(w, w)\n    z = expit(yz)\n    z0 = sample_weight * (z - 1) * y\n    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n    if grad.shape[0] > n_features:\n        grad[-1] = z0.sum()\n    return (out, grad)",
    ".sklearn.utils.class_weight.py@@compute_class_weight": "def compute_class_weight(class_weight, classes, y):\n    from ..preprocessing import LabelEncoder\n    if set(y) - set(classes):\n        raise ValueError('classes should include all valid labels that can be in y')\n    if class_weight is None or len(class_weight) == 0:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n    elif class_weight == 'balanced':\n        le = LabelEncoder()\n        y_ind = le.fit_transform(y)\n        if not all(np.in1d(classes, le.classes_)):\n            raise ValueError('classes should have valid labels that are in y')\n        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n        weight = recip_freq[le.transform(classes)]\n    else:\n        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')\n        if not isinstance(class_weight, dict):\n            raise ValueError(\"class_weight must be dict, 'balanced', or None, got: %r\" % class_weight)\n        for c in class_weight:\n            i = np.searchsorted(classes, c)\n            if i >= len(classes) or classes[i] != c:\n                raise ValueError('Class label {} not present.'.format(c))\n            else:\n                weight[i] = class_weight[c]\n    return weight",
    ".sklearn.preprocessing.label.py@@LabelEncoder.fit_transform": "def fit_transform(self, y):\n    y = column_or_1d(y, warn=True)\n    self.classes_, y = np.unique(y, return_inverse=True)\n    return y",
    ".sklearn.utils.validation.py@@column_or_1d": "def column_or_1d(y, warn=False):\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n    raise ValueError('bad input shape {0}'.format(shape))",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.__init__": "def __init__(self, neg_label=0, pos_label=1, sparse_output=False):\n    if neg_label >= pos_label:\n        raise ValueError('neg_label={0} must be strictly less than pos_label={1}.'.format(neg_label, pos_label))\n    if sparse_output and (pos_label == 0 or neg_label != 0):\n        raise ValueError('Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={0} and neg_label={1}'.format(pos_label, neg_label))\n    self.neg_label = neg_label\n    self.pos_label = pos_label\n    self.sparse_output = sparse_output",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.fit_transform": "def fit_transform(self, y):\n    return self.fit(y).transform(y)",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.fit": "def fit(self, y):\n    self.y_type_ = type_of_target(y)\n    if 'multioutput' in self.y_type_:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    self.sparse_input_ = sp.issparse(y)\n    self.classes_ = unique_labels(y)\n    return self",
    ".sklearn.utils.multiclass.py@@type_of_target": "def type_of_target(y):\n    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, string_types))\n    if not valid:\n        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)\n    sparseseries = y.__class__.__name__ == 'SparseSeries'\n    if sparseseries:\n        raise ValueError(\"y cannot be class 'SparseSeries'.\")\n    if is_multilabel(y):\n        return 'multilabel-indicator'\n    try:\n        y = np.asarray(y)\n    except ValueError:\n        return 'unknown'\n    try:\n        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], string_types)):\n            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.')\n    except IndexError:\n        pass\n    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], string_types))):\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] == 0:\n        return 'unknown'\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = '-multioutput'\n    else:\n        suffix = ''\n    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):\n        return 'continuous' + suffix\n    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):\n        return 'multiclass' + suffix\n    else:\n        return 'binary'",
    ".sklearn.utils.multiclass.py@@is_multilabel": "def is_multilabel(y):\n    if hasattr(y, '__array__'):\n        y = np.asarray(y)\n    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):\n        return False\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))\n    else:\n        labels = np.unique(y)\n        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))",
    ".sklearn.utils.multiclass.py@@unique_labels": "def unique_labels(*ys):\n    if not ys:\n        raise ValueError('No argument has been passed.')\n    ys_types = set((type_of_target(x) for x in ys))\n    if ys_types == set(['binary', 'multiclass']):\n        ys_types = set(['multiclass'])\n    if len(ys_types) > 1:\n        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)\n    label_type = ys_types.pop()\n    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:\n        raise ValueError('Multi-label binary indicator input with different numbers of labels')\n    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n    if not _unique_labels:\n        raise ValueError('Unknown label type: %s' % repr(ys))\n    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))\n    if len(set((isinstance(label, string_types) for label in ys_labels))) > 1:\n        raise ValueError('Mix of label input types (string and number)')\n    return np.array(sorted(ys_labels))",
    ".sklearn.utils.multiclass.py@@_unique_multiclass": "def _unique_multiclass(y):\n    if hasattr(y, '__array__'):\n        return np.unique(np.asarray(y))\n    else:\n        return set(y)",
    ".sklearn.preprocessing.label.py@@LabelBinarizer.transform": "def transform(self, y):\n    check_is_fitted(self, 'classes_')\n    y_is_multilabel = type_of_target(y).startswith('multilabel')\n    if y_is_multilabel and (not self.y_type_.startswith('multilabel')):\n        raise ValueError('The object was not fitted with multilabel input.')\n    return label_binarize(y, self.classes_, pos_label=self.pos_label, neg_label=self.neg_label, sparse_output=self.sparse_output)",
    ".sklearn.utils.validation.py@@check_is_fitted": "def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    if msg is None:\n        msg = \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    if not hasattr(estimator, 'fit'):\n        raise TypeError('%s is not an estimator instance.' % estimator)\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotFittedError(msg % {'name': type(estimator).__name__})",
    ".sklearn.preprocessing.label.py@@label_binarize": "def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):\n    if not isinstance(y, list):\n        y = check_array(y, accept_sparse='csr', ensure_2d=False, dtype=None)\n    elif _num_samples(y) == 0:\n        raise ValueError('y has 0 samples: %r' % y)\n    if neg_label >= pos_label:\n        raise ValueError('neg_label={0} must be strictly less than pos_label={1}.'.format(neg_label, pos_label))\n    if sparse_output and (pos_label == 0 or neg_label != 0):\n        raise ValueError('Sparse binarization is only supported with non zero pos_label and zero neg_label, got pos_label={0} and neg_label={1}'.format(pos_label, neg_label))\n    pos_switch = pos_label == 0\n    if pos_switch:\n        pos_label = -neg_label\n    y_type = type_of_target(y)\n    if 'multioutput' in y_type:\n        raise ValueError('Multioutput target data is not supported with label binarization')\n    if y_type == 'unknown':\n        raise ValueError('The type of target data is not known')\n    n_samples = y.shape[0] if sp.issparse(y) else len(y)\n    n_classes = len(classes)\n    classes = np.asarray(classes)\n    if y_type == 'binary':\n        if n_classes == 1:\n            if sparse_output:\n                return sp.csr_matrix((n_samples, 1), dtype=int)\n            else:\n                Y = np.zeros((len(y), 1), dtype=np.int)\n                Y += neg_label\n                return Y\n        elif len(classes) >= 3:\n            y_type = 'multiclass'\n    sorted_class = np.sort(classes)\n    if y_type == 'multilabel-indicator' and classes.size != y.shape[1]:\n        raise ValueError('classes {0} missmatch with the labels {1}found in the data'.format(classes, unique_labels(y)))\n    if y_type in ('binary', 'multiclass'):\n        y = column_or_1d(y)\n        y_in_classes = np.in1d(y, classes)\n        y_seen = y[y_in_classes]\n        indices = np.searchsorted(sorted_class, y_seen)\n        indptr = np.hstack((0, np.cumsum(y_in_classes)))\n        data = np.empty_like(indices)\n        data.fill(pos_label)\n        Y = sp.csr_matrix((data, indices, indptr), shape=(n_samples, n_classes))\n    elif y_type == 'multilabel-indicator':\n        Y = sp.csr_matrix(y)\n        if pos_label != 1:\n            data = np.empty_like(Y.data)\n            data.fill(pos_label)\n            Y.data = data\n    else:\n        raise ValueError('%s target data is not supported with label binarization' % y_type)\n    if not sparse_output:\n        Y = Y.toarray()\n        Y = Y.astype(int, copy=False)\n        if neg_label != 0:\n            Y[Y == 0] = neg_label\n        if pos_switch:\n            Y[Y == pos_label] = 0\n    else:\n        Y.data = Y.data.astype(int, copy=False)\n    if np.any(classes != sorted_class):\n        indices = np.searchsorted(sorted_class, classes)\n        Y = Y[:, indices]\n    if y_type == 'binary':\n        if sparse_output:\n            Y = Y.getcol(-1)\n        else:\n            Y = Y[:, -1].reshape((-1, 1))\n    return Y",
    ".sklearn.linear_model.logistic.py@@_multinomial_loss": "def _multinomial_loss(w, X, Y, alpha, sample_weight):\n    n_classes = Y.shape[1]\n    n_features = X.shape[1]\n    fit_intercept = w.size == n_classes * (n_features + 1)\n    w = w.reshape(n_classes, -1)\n    sample_weight = sample_weight[:, np.newaxis]\n    if fit_intercept:\n        intercept = w[:, -1]\n        w = w[:, :-1]\n    else:\n        intercept = 0\n    p = safe_sparse_dot(X, w.T)\n    p += intercept\n    p -= logsumexp(p, axis=1)[:, np.newaxis]\n    loss = -(sample_weight * Y * p).sum()\n    loss += 0.5 * alpha * squared_norm(w)\n    p = np.exp(p, p)\n    return (loss, p, w)",
    ".sklearn.utils.extmath.py@@squared_norm": "def squared_norm(x):\n    x = np.ravel(x, order='K')\n    if np.issubdtype(x.dtype, np.integer):\n        warnings.warn('Array type is integer, np.dot may overflow. Data should be float type to avoid this issue', UserWarning)\n    return np.dot(x, x)",
    ".sklearn.linear_model.logistic.py@@_multinomial_grad_hess": "def _multinomial_grad_hess(w, X, Y, alpha, sample_weight):\n    n_features = X.shape[1]\n    n_classes = Y.shape[1]\n    fit_intercept = w.size == n_classes * (n_features + 1)\n    loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha, sample_weight)\n    sample_weight = sample_weight[:, np.newaxis]\n\n    def hessp(v):\n        v = v.reshape(n_classes, -1)\n        if fit_intercept:\n            inter_terms = v[:, -1]\n            v = v[:, :-1]\n        else:\n            inter_terms = 0\n        r_yhat = safe_sparse_dot(X, v.T)\n        r_yhat += inter_terms\n        r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]\n        r_yhat *= p\n        r_yhat *= sample_weight\n        hessProd = np.zeros((n_classes, n_features + bool(fit_intercept)))\n        hessProd[:, :n_features] = safe_sparse_dot(r_yhat.T, X)\n        hessProd[:, :n_features] += v * alpha\n        if fit_intercept:\n            hessProd[:, -1] = r_yhat.sum(axis=0)\n        return hessProd.ravel()\n    return (grad, hessp)",
    ".sklearn.linear_model.logistic.py@@_multinomial_loss_grad": "def _multinomial_loss_grad(w, X, Y, alpha, sample_weight):\n    n_classes = Y.shape[1]\n    n_features = X.shape[1]\n    fit_intercept = w.size == n_classes * (n_features + 1)\n    grad = np.zeros((n_classes, n_features + bool(fit_intercept)), dtype=X.dtype)\n    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)\n    sample_weight = sample_weight[:, np.newaxis]\n    diff = sample_weight * (p - Y)\n    grad[:, :n_features] = safe_sparse_dot(diff.T, X)\n    grad[:, :n_features] += alpha * w\n    if fit_intercept:\n        grad[:, -1] = diff.sum(axis=0)\n    return (loss, grad.ravel(), p)",
    ".sklearn.linear_model.logistic.py@@hessp": "def hessp(v):\n    v = v.reshape(n_classes, -1)\n    if fit_intercept:\n        inter_terms = v[:, -1]\n        v = v[:, :-1]\n    else:\n        inter_terms = 0\n    r_yhat = safe_sparse_dot(X, v.T)\n    r_yhat += inter_terms\n    r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]\n    r_yhat *= p\n    r_yhat *= sample_weight\n    hessProd = np.zeros((n_classes, n_features + bool(fit_intercept)))\n    hessProd[:, :n_features] = safe_sparse_dot(r_yhat.T, X)\n    hessProd[:, :n_features] += v * alpha\n    if fit_intercept:\n        hessProd[:, -1] = r_yhat.sum(axis=0)\n    return hessProd.ravel()",
    ".sklearn.linear_model.sag.py@@sag_solver": "def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.0, beta=0.0, max_iter=1000, tol=0.001, verbose=0, random_state=None, check_input=True, max_squared_sum=None, warm_start_mem=None, is_saga=False):\n    if warm_start_mem is None:\n        warm_start_mem = {}\n    if max_iter is None:\n        max_iter = 1000\n    if check_input:\n        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')\n        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')\n    n_samples, n_features = (X.shape[0], X.shape[1])\n    alpha_scaled = float(alpha) / n_samples\n    beta_scaled = float(beta) / n_samples\n    n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')\n    if 'coef' in warm_start_mem.keys():\n        coef_init = warm_start_mem['coef']\n    else:\n        coef_init = np.zeros((n_features, n_classes), dtype=np.float64, order='C')\n    fit_intercept = coef_init.shape[0] == n_features + 1\n    if fit_intercept:\n        intercept_init = coef_init[-1, :]\n        coef_init = coef_init[:-1, :]\n    else:\n        intercept_init = np.zeros(n_classes, dtype=np.float64)\n    if 'intercept_sum_gradient' in warm_start_mem.keys():\n        intercept_sum_gradient = warm_start_mem['intercept_sum_gradient']\n    else:\n        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)\n    if 'gradient_memory' in warm_start_mem.keys():\n        gradient_memory_init = warm_start_mem['gradient_memory']\n    else:\n        gradient_memory_init = np.zeros((n_samples, n_classes), dtype=np.float64, order='C')\n    if 'sum_gradient' in warm_start_mem.keys():\n        sum_gradient_init = warm_start_mem['sum_gradient']\n    else:\n        sum_gradient_init = np.zeros((n_features, n_classes), dtype=np.float64, order='C')\n    if 'seen' in warm_start_mem.keys():\n        seen_init = warm_start_mem['seen']\n    else:\n        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')\n    if 'num_seen' in warm_start_mem.keys():\n        num_seen_init = warm_start_mem['num_seen']\n    else:\n        num_seen_init = 0\n    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)\n    if max_squared_sum is None:\n        max_squared_sum = row_norms(X, squared=True).max()\n    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=n_samples, is_saga=is_saga)\n    if step_size * alpha_scaled == 1:\n        raise ZeroDivisionError('Current sag implementation does not handle the case step_size * alpha_scaled == 1')\n    num_seen, n_iter_ = sag(dataset, coef_init, intercept_init, n_samples, n_features, n_classes, tol, max_iter, loss, step_size, alpha_scaled, beta_scaled, sum_gradient_init, gradient_memory_init, seen_init, num_seen_init, fit_intercept, intercept_sum_gradient, intercept_decay, is_saga, verbose)\n    if n_iter_ == max_iter:\n        warnings.warn('The max_iter was reached which means the coef_ did not converge', ConvergenceWarning)\n    if fit_intercept:\n        coef_init = np.vstack((coef_init, intercept_init))\n    warm_start_mem = {'coef': coef_init, 'sum_gradient': sum_gradient_init, 'intercept_sum_gradient': intercept_sum_gradient, 'gradient_memory': gradient_memory_init, 'seen': seen_init, 'num_seen': num_seen}\n    if loss == 'multinomial':\n        coef_ = coef_init.T\n    else:\n        coef_ = coef_init[:, 0]\n    return (coef_, n_iter_, warm_start_mem)",
    ".sklearn.linear_model.base.py@@make_dataset": "def make_dataset(X, y, sample_weight, random_state=None):\n    rng = check_random_state(random_state)\n    seed = rng.randint(1, np.iinfo(np.int32).max)\n    if sp.issparse(X):\n        dataset = CSRDataset(X.data, X.indptr, X.indices, y, sample_weight, seed=seed)\n        intercept_decay = SPARSE_INTERCEPT_DECAY\n    else:\n        dataset = ArrayDataset(X, y, sample_weight, seed=seed)\n        intercept_decay = 1.0\n    return (dataset, intercept_decay)",
    ".sklearn.linear_model.sag.py@@get_auto_step_size": "def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=None, is_saga=False):\n    if loss in ('log', 'multinomial'):\n        L = 0.25 * (max_squared_sum + int(fit_intercept)) + alpha_scaled\n    elif loss == 'squared':\n        L = max_squared_sum + int(fit_intercept) + alpha_scaled\n    else:\n        raise ValueError(\"Unknown loss function for SAG solver, got %s instead of 'log' or 'squared'\" % loss)\n    if is_saga:\n        mun = min(2 * n_samples * alpha_scaled, L)\n        step = 1.0 / (2 * L + mun)\n    else:\n        step = 1.0 / L\n    return step",
    ".sklearn.utils.validation.py@@check_consistent_length": "def check_consistent_length(*arrays):\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])",
    ".sklearn.utils.extmath.py@@row_norms": "def row_norms(X, squared=False):\n    if issparse(X):\n        if not isinstance(X, csr_matrix):\n            X = csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms",
    ".sklearn.svm.base.py@@_fit_liblinear": "def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state=None, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None):\n    if loss not in ['epsilon_insensitive', 'squared_epsilon_insensitive']:\n        enc = LabelEncoder()\n        y_ind = enc.fit_transform(y)\n        classes_ = enc.classes_\n        if len(classes_) < 2:\n            raise ValueError('This solver needs samples of at least 2 classes in the data, but the data contains only one class: %r' % classes_[0])\n        class_weight_ = compute_class_weight(class_weight, classes_, y)\n    else:\n        class_weight_ = np.empty(0, dtype=np.float64)\n        y_ind = y\n    liblinear.set_verbosity_wrap(verbose)\n    rnd = check_random_state(random_state)\n    if verbose:\n        print('[LibLinear]', end='')\n    bias = -1.0\n    if fit_intercept:\n        if intercept_scaling <= 0:\n            raise ValueError('Intercept scaling is %r but needs to be greater than 0. To disable fitting an intercept, set fit_intercept=False.' % intercept_scaling)\n        else:\n            bias = intercept_scaling\n    libsvm.set_verbosity_wrap(verbose)\n    libsvm_sparse.set_verbosity_wrap(verbose)\n    liblinear.set_verbosity_wrap(verbose)\n    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()\n    if sample_weight is None:\n        sample_weight = np.ones(X.shape[0])\n    else:\n        sample_weight = np.array(sample_weight, dtype=np.float64, order='C')\n        check_consistent_length(sample_weight, X)\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n    raw_coef_, n_iter_ = liblinear.train_wrap(X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C, class_weight_, max_iter, rnd.randint(np.iinfo('i').max), epsilon, sample_weight)\n    n_iter_ = max(n_iter_)\n    if n_iter_ >= max_iter and verbose > 0:\n        warnings.warn('Liblinear failed to converge, increase the number of iterations.', ConvergenceWarning)\n    if fit_intercept:\n        coef_ = raw_coef_[:, :-1]\n        intercept_ = intercept_scaling * raw_coef_[:, -1]\n    else:\n        coef_ = raw_coef_\n        intercept_ = 0.0\n    return (coef_, intercept_, n_iter_)",
    ".sklearn.svm.base.py@@_get_liblinear_solver_type": "def _get_liblinear_solver_type(multi_class, penalty, loss, dual):\n    _solver_type_dict = {'logistic_regression': {'l1': {False: 6}, 'l2': {False: 0, True: 7}}, 'hinge': {'l2': {True: 3}}, 'squared_hinge': {'l1': {False: 5}, 'l2': {False: 2, True: 1}}, 'epsilon_insensitive': {'l2': {True: 13}}, 'squared_epsilon_insensitive': {'l2': {False: 11, True: 12}}, 'crammer_singer': 4}\n    if multi_class == 'crammer_singer':\n        return _solver_type_dict[multi_class]\n    elif multi_class != 'ovr':\n        raise ValueError('`multi_class` must be one of `ovr`, `crammer_singer`, got %r' % multi_class)\n    _solver_pen = _solver_type_dict.get(loss, None)\n    if _solver_pen is None:\n        error_string = \"loss='%s' is not supported\" % loss\n    else:\n        _solver_dual = _solver_pen.get(penalty, None)\n        if _solver_dual is None:\n            error_string = \"The combination of penalty='%s' and loss='%s' is not supported\" % (penalty, loss)\n        else:\n            solver_num = _solver_dual.get(dual, None)\n            if solver_num is None:\n                error_string = \"The combination of penalty='%s' and loss='%s' are not supported when dual=%s\" % (penalty, loss, dual)\n            else:\n                return solver_num\n    raise ValueError('Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r' % (error_string, penalty, loss, dual))",
    ".sklearn.preprocessing.label.py@@LabelEncoder.transform": "def transform(self, y):\n    check_is_fitted(self, 'classes_')\n    y = column_or_1d(y, warn=True)\n    if _num_samples(y) == 0:\n        return np.array([])\n    classes = np.unique(y)\n    if len(np.intersect1d(classes, self.classes_)) < len(classes):\n        diff = np.setdiff1d(classes, self.classes_)\n        raise ValueError('y contains previously unseen labels: %s' % str(diff))\n    return np.searchsorted(self.classes_, y)"
}