[THOUGHT]
We have three classes {blue, green, red}.  From y_true we count supports:
- blue: 24, green: 31, red: 20.

We tally true positives by scanning each index:
- tp_blue = 12, tp_green = 5, tp_red = 14.

Then  
- precision = tp / (tp+fp) = tp / pred_count  
   pred_count_blue = 23 → precision_blue = 12/23 ≈ 0.52  
   pred_count_green = 9 → precision_green = 5/9 ≈ 0.56  
   pred_count_red = 43 → precision_red = 14/43 ≈ 0.33  

- recall = tp / support  
   recall_blue = 12/24 = 0.50  
   recall_green = 5/31 ≈ 0.16  
   recall_red = 14/20 = 0.70  

- f1 = 2·p·r/(p+r)  
   f1_blue ≈ 0.51, f1_green ≈ 0.25, f1_red ≈ 0.44  

Micro‐averaged precision=recall=f1 = total_tp/total = 31/75 ≈ 0.41 → shown as “accuracy”.  
Macro averages are unweighted means of the three classes:  
- precision ≈ (0.52+0.56+0.33)/3 ≈ 0.47  
- recall    ≈ (0.50+0.16+0.70)/3 ≈ 0.45  
- f1        ≈ (0.51+0.25+0.44)/3 ≈ 0.40  

Weighted averages weight by support:  
- precision ≈ (0.52·24 + 0.56·31 + 0.33·20)/75 ≈ 0.48  
- recall    ≈ (0.50·24 + 0.16·31 + 0.70·20)/75 ≈ 0.41  
- f1        ≈ (0.51·24 + 0.25·31 + 0.44·20)/75 ≈ 0.38  

The function then formats these into a report with width=12 for names, 9 for each metric, 2 decimal places:
[/THOUGHT]
[OUTPUT]
              precision    recall  f1-score  support

        blue      0.52      0.50      0.51       24
       green      0.56      0.16      0.25       31
         red      0.33      0.70      0.44       20

     accuracy                0.41       75
    macro avg      0.47      0.45      0.40       75
 weighted avg      0.48      0.41      0.38       75

[/OUTPUT]