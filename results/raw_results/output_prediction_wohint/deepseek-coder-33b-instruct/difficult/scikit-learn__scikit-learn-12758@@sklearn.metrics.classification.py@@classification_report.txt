1. The function `classification_report` is used to build a text report showing the main classification metrics.
2. The input is two lists of strings, `y_true` and `y_pred`, which represent the true labels and the predicted labels respectively.
3. The function first checks the lengths of `y_true` and `y_pred`, and raises a ValueError if they are not the same.
4. Then, it checks the type of the targets, and raises a ValueError if they are not binary, multiclass, or multilabel-indicator.
5. If the type of the targets is binary, multiclass, or multilabel-indicator, it converts `y_true` and `y_pred` to 1D arrays.
6. If the type of the targets is multilabel-indicator, it checks the shape of `y_true` and `y_pred`, and raises a ValueError if they are not the same.
7. If the type of the targets is not binary, multiclass, or multilabel-indicator, it raises a ValueError.
8. Then, it gets the unique labels from `y_true` and `y_pred`.
9. If the average is 'binary', it checks if `y_type` is 'binary', and if not, it raises a ValueError.
10. If `y_type` is 'binary', it checks if `pos_label` is in `present_labels`, and if not, it raises a ValueError.
11. If `average` is not 'binary', it warns that `pos_label` is ignored.
12. Then, it checks if `beta` is greater than 0, and if not, it raises a ValueError.
13. If `average` is 'micro', it calculates the precision, recall, and f1-score for each unique label, and then averages them.
14. If `average` is 'weighted', it calculates the precision, recall, and f1-score for each unique label, and then averages them with weights.
15. If `average` is 'samples', it calculates the precision, recall, and f1-score for each unique label, and then averages them with sample weights.
16. If `average` is not 'micro', 'weighted', or 'samples', it calculates the precision, recall, and f1-score for each unique label, and then averages them.
17. Finally, it returns the precision, recall, f1-score, and support for each unique label.
[/THOUGHT]

[OUTPUT]
```
{
    "str": "              precision    recall  f1-score   support\n\n         blue       0.67      0.67      0.67        30\n         green       0.67      0.67      0.67        30\n          red       0.67      0.67      0.67        30\n\n    accuracy                           0.67        90\n   macro avg       0.67      0.67      0.67        90\nweighted avg       0.67      0.67      0.67        90\n"
}
```
[/OUTPUT]
