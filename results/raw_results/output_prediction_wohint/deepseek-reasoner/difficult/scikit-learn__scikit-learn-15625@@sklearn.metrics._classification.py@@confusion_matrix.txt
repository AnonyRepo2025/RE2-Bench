[THOUGHT]
The function `confusion_matrix` computes the confusion matrix to evaluate the accuracy of a classification. Given the inputs:
- `y_true`: [0]*60 + [1]*50 + [2]*4 (114 elements)
- `y_pred`: [0]*20 + [1]*20 + [0]*10 + [1]*60 + [2]*4 (114 elements)

Steps:
1. **Check targets**: `_check_targets` verifies consistent lengths and determines the target type. Both `y_true` and `y_pred` are multiclass (3 unique labels). They are converted to 1D arrays.
2. **Labels handling**: Since `labels=None`, `unique_labels` returns sorted unique labels: [0, 1, 2].
3. **Sample weight**: `sample_weight=None`, so an array of ones is used.
4. **Map labels to indices**: `label_to_ind` maps: 0→0, 1→1, 2→2. Unseen labels map to 4, but all labels are present.
5. **Filter valid labels**: All entries are valid (indices < 3), so no filtering occurs.
6. **Compute confusion matrix**: 
   - True class 0 (60 instances): 
     - Predicted as 0: 30 (first 20 zeros + next 10 zeros in true zero segment)
     - Predicted as 1: 30 (next 20 ones + 10 ones in true zero segment)
     - Predicted as 2: 0
   - True class 1 (50 instances): 
     - Predicted as 0: 0
     - Predicted as 1: 50 (all 50 ones predicted correctly)
     - Predicted as 2: 0
   - True class 2 (4 instances): 
     - Predicted as 0: 0
     - Predicted as 1: 0
     - Predicted as 2: 4
7. **Normalization**: `normalize=None`, so raw counts are returned.
8. **Result**: The confusion matrix is:
   - Row 0: [30, 30, 0]
   - Row 1: [0, 50, 0]
   - Row 2: [0, 0, 4]

The output is a 3x3 matrix represented as a list of lists.
[/THOUGHT]

[OUTPUT]
{
    "cm": [
        [30, 30, 0],
        [0, 50, 0],
        [0, 0, 4]
    ]
}
[/OUTPUT]