
The Mean Reciprocal Rank (MRR) is a measure to evaluate the quality of search engine results. It is the average of the reciprocal rank of results for a certain query.

The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, 1/2 for second place, 1/3 for third place and so on.

The MRR of a single query result list is calculated as the maximum reciprocal rank of the correct items in the list.

The MRR of a set of query result lists is the average of the MRRs of the individual lists.

So, the input data should be a list of tuples, where each tuple contains a list of scores and the total number of scores. The output is the MRR of the first list in the data.

The MRR of the list [0, 1, 0, 0, 1] is 0.25, which is the reciprocal rank of the first 1 in the list.
[/THOUGHT]

[INPUT]
```
{
    "self": {},
    "args": {
        "data": [([0, 1, 0, 0, 1], 2)]
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
import numpy as np

class MetricsCalculator3:

    def __init__(self):
        pass

    @staticmethod
    def map(data):
        if type(data) != list and type(data) != tuple:
            raise Exception('the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple')
        if len(data) == 0:
            return (0.0, [0.0])
        if type(data) == tuple:
            sub_list, total_num = data
            sub_list = np.array(sub_list)
            if total_num == 0:
                return (0.0, [0.0])
            else:
                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)
                mr_np = sub_list * ranking_array
                mr = mr_np.sum() / total_num
                return (mr, [mr])
        if type(data) == list:
            separate_result = []
            for sub_list, total_num in data:
                sub_list = np.array(sub_list)
                if total_num == 0:
                    mr = 0.0
                else:
                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)
                    mr_np = sub_list * ranking_array
                    mr = mr_np.sum() / total_num
                separate_result.append(mr)
            return (np.mean(separate_result), separate_result)
[/PYTHON]

What will be the input of `map`, given the following output:
[OUTPUT]
```
{
    "output": [
        0.25,
        [
            0.25
        ]
    ]
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {},
    "args": {
        "data": XXX
    },
    "kwargs": {}
}
```
[/STRUCTURE]

[THOUGHT]
        
The Mean Average Precision (MAP) is a measure to evaluate the quality of search engine results. It is the average of the precision at which relevant items are retrieved.

The precision at k is the number of relevant items among the top k items.

The MAP of a single query result list is calculated as the average of the precision at k of the correct items in the list.

The MAP of a set of query result lists is the average of the MAPs of the individual lists.

So, the input data should be a list of tuples, where each tuple contains a list of scores and the total number of scores. The output is the MAP of the first list in the data.

The MAP of the list [0, 1, 0, 0, 1] is 0.25, which is the average of the precision at k of the correct items in the list.
[/THOUGHT]

[INPUT]
```
{
    "self": {},
    "args": {
        "data": [([0, 1, 0, 0, 1], 2)]
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
import numpy as np

class MetricsCalculator4:

    def __init__(self):
        pass

    @staticmethod
    def ndcg(data):
        if type(data) != list and type(data) != tuple:
            raise Exception('the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple')
        if len(data) == 0:
            return (0.0, [0.0])
        if type(data) == tuple:
            sub_list, total_num = data
            sub_list = np.array(sub_list)
            if total_num == 0:
                return (0.0, [0.0])
            else:
                dcg = np.sum(sub_list / np.log2(np.arange(2, len(sub_list) + 2)))
                idcg = np.sum(np.sort(sub_list)[::-1] / np.log2(np.arange(2, len(sub_list) + 2)))
                ndcg = dcg / idcg
                return (ndcg, [ndcg])
        if type(data) == list:
            separate_result = []
            for sub_list, total_num in data:
                sub_list = np.array(sub_list)
                if total_num == 0:
                    ndcg = 0.0
                else:
                    dcg = np.sum(sub_list / np.log2(np.arange(2, len(sub_list) + 2)))
                    idcg = np.sum(np.sort(sub_list)[::-1] / np.log2(np.arange(2, len(sub_list) + 2)))
                    ndcg = dcg / idcg
                separate_result.append(ndcg)
            return (np.mean(separate_result), separate_result)
[/PYTHON]

What will be the input of `ndcg`, given the following output:
[OUTPUT]
```
{
    "output": [
        0.25,
        [
            0.25
        ]
    ]
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {},
    "args": {
        "data": XXX
    },
    "kwargs": {}
}
```
[/STRUCTURE]

[THOUGHT]
        
The Normalized Discounted Cumulative Gain (NDCG) is a measure to evaluate the quality of search engine results. It is the normalized discounted cumulative gain at k.

The discounted cumulative gain at k is the sum of the scores of the top k items, divided by the log of the rank of the item.

The NDCG of a single query result list is calculated as the discounted cumulative gain at k of the correct items in the list divided by the ideal discounted cumulative gain at k.

The NDCG of a set of query result lists is the average of the NDCGs of the individual lists.

So, the input data should be a list of tuples, where each tuple contains a list of scores and the total number of scores. The output is the NDCG of the first list in the data.

The NDCG of the list [0, 1, 0, 0, 1] is 0.25, which is the discounted cumulative gain at k of the correct items in the list divided by the ideal discounted cumulative gain at k.
[/THOUGHT]

[INPUT]
```
{
    "self": {},
    "args": {
        "data": [([0, 1, 0, 0, 1], 2)]
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]