You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [\INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. You need to predict output variable values, and print output between [OUTPUT] and [\OUTPUT]. For prediction, simulate the execution of the program step by step and print your reasoning process before arriving at an answer between [THOUGHT] and [/THOUGHT].
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]

[THOUGHT]
1. Inside `mktemp`, since `numbered`=True (default), it calls: p = make_numbered_dir(root=self.getbasetemp(), prefix="world")
2. Inside `getbasetemp`, since `_basetemp` is None, so basetemp = Path("/tmp/pytest-of-root/pytest-0/test_mktemp0"). Therefore, getbasetemp() returns "/tmp/pytest-of-root/pytest-0/test_mktemp0".
3. Inside `make_numbered_dir`:
    root = /tmp/pytest-of-root/pytest-0/test_mktemp0
    prefix = "world"
    Since max_existing = -1, new_number = 0, so `new_path` = "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
[/THOUGHT]

[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[PYTHON]
from __future__ import division
import warnings
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from ..preprocessing import LabelBinarizer, label_binarize
from ..preprocessing import LabelEncoder
from ..utils import assert_all_finite
from ..utils import check_array
from ..utils import check_consistent_length
from ..utils import column_or_1d
from ..utils.multiclass import unique_labels
from ..utils.multiclass import type_of_target
from ..utils.validation import _num_samples
from ..utils.sparsefuncs import count_nonzero
from ..exceptions import UndefinedMetricWarning

def classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False):
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    labels_given = True
    if labels is None:
        labels = unique_labels(y_true, y_pred)
        labels_given = False
    else:
        labels = np.asarray(labels)
    micro_is_accuracy = (y_type == 'multiclass' or y_type == 'binary') and (not labels_given or set(labels) == set(unique_labels(y_true, y_pred)))
    if target_names is not None and len(labels) != len(target_names):
        if labels_given:
            warnings.warn('labels size, {0}, does not match size of target_names, {1}'.format(len(labels), len(target_names)))
        else:
            raise ValueError('Number of classes, {0}, does not match size of target_names, {1}. Try specifying the labels parameter'.format(len(labels), len(target_names)))
    if target_names is None:
        target_names = [u'%s' % l for l in labels]
    headers = ['precision', 'recall', 'f1-score', 'support']
    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight)
    rows = zip(target_names, p, r, f1, s)
    if y_type.startswith('multilabel'):
        average_options = ('micro', 'macro', 'weighted', 'samples')
    else:
        average_options = ('micro', 'macro', 'weighted')
    if output_dict:
        report_dict = {label[0]: label[1:] for label in rows}
        for label, scores in report_dict.items():
            report_dict[label] = dict(zip(headers, [i.item() for i in scores]))
    else:
        longest_last_line_heading = 'weighted avg'
        name_width = max((len(cn) for cn in target_names))
        width = max(name_width, len(longest_last_line_heading), digits)
        head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)
        report = head_fmt.format(u'', *headers, width=width)
        report += u'\n\n'
        row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\n'
        for row in rows:
            report += row_fmt.format(*row, width=width, digits=digits)
        report += u'\n'
    for average in average_options:
        if average.startswith('micro') and micro_is_accuracy:
            line_heading = 'accuracy'
        else:
            line_heading = average + ' avg'
        avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average=average, sample_weight=sample_weight)
        avg = [avg_p, avg_r, avg_f1, np.sum(s)]
        if output_dict:
            report_dict[line_heading] = dict(zip(headers, [i.item() for i in avg]))
        elif line_heading == 'accuracy':
            row_fmt_accuracy = u'{:>{width}s} ' + u' {:>9.{digits}}' * 2 + u' {:>9.{digits}f}' + u' {:>9}\n'
            report += row_fmt_accuracy.format(line_heading, '', '', *avg[2:], width=width, digits=digits)
        else:
            report += row_fmt.format(line_heading, *avg, width=width, digits=digits)
    if output_dict:
        if 'accuracy' in report_dict.keys():
            report_dict['accuracy'] = report_dict['accuracy']['precision']
        return report_dict
    else:
        return report
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.metrics.classification._check_targets

def _check_targets(y_true, y_pred):
    check_consistent_length(y_true, y_pred)
    type_true = type_of_target(y_true)
    type_pred = type_of_target(y_pred)
    y_type = set([type_true, type_pred])
    if y_type == set(['binary', 'multiclass']):
        y_type = set(['multiclass'])
    if len(y_type) > 1:
        raise ValueError("Classification metrics can't handle a mix of {0} and {1} targets".format(type_true, type_pred))
    y_type = y_type.pop()
    if y_type not in ['binary', 'multiclass', 'multilabel-indicator']:
        raise ValueError('{0} is not supported'.format(y_type))
    if y_type in ['binary', 'multiclass']:
        y_true = column_or_1d(y_true)
        y_pred = column_or_1d(y_pred)
        if y_type == 'binary':
            unique_values = np.union1d(y_true, y_pred)
            if len(unique_values) > 2:
                y_type = 'multiclass'
    if y_type.startswith('multilabel'):
        y_true = csr_matrix(y_true)
        y_pred = csr_matrix(y_pred)
        y_type = 'multilabel-indicator'
    return (y_type, y_true, y_pred)

.sklearn.utils.validation.check_consistent_length

def check_consistent_length(*arrays):
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])

.sklearn.utils.validation._num_samples

def _num_samples(x):
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError('Expected sequence or array-like, got estimator %s' % x)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError('Expected sequence or array-like, got %s' % type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
        else:
            return len(x)
    else:
        return len(x)

.sklearn.utils.multiclass.type_of_target

def type_of_target(y):
    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, string_types))
    if not valid:
        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)
    sparseseries = y.__class__.__name__ == 'SparseSeries'
    if sparseseries:
        raise ValueError("y cannot be class 'SparseSeries'.")
    if is_multilabel(y):
        return 'multilabel-indicator'
    try:
        y = np.asarray(y)
    except ValueError:
        return 'unknown'
    try:
        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], string_types)):
            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.')
    except IndexError:
        pass
    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], string_types))):
        return 'unknown'
    if y.ndim == 2 and y.shape[1] == 0:
        return 'unknown'
    if y.ndim == 2 and y.shape[1] > 1:
        suffix = '-multioutput'
    else:
        suffix = ''
    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):
        return 'continuous' + suffix
    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):
        return 'multiclass' + suffix
    else:
        return 'binary'

.sklearn.utils.multiclass.is_multilabel

def is_multilabel(y):
    if hasattr(y, '__array__'):
        y = np.asarray(y)
    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):
        return False
    if issparse(y):
        if isinstance(y, (dok_matrix, lil_matrix)):
            y = y.tocsr()
        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))
    else:
        labels = np.unique(y)
        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))

.sklearn.utils.validation.column_or_1d

def column_or_1d(y, warn=False):
    shape = np.shape(y)
    if len(shape) == 1:
        return np.ravel(y)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)
        return np.ravel(y)
    raise ValueError('bad input shape {0}'.format(shape))

.sklearn.utils.multiclass.unique_labels

def unique_labels(*ys):
    if not ys:
        raise ValueError('No argument has been passed.')
    ys_types = set((type_of_target(x) for x in ys))
    if ys_types == set(['binary', 'multiclass']):
        ys_types = set(['multiclass'])
    if len(ys_types) > 1:
        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)
    label_type = ys_types.pop()
    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:
        raise ValueError('Multi-label binary indicator input with different numbers of labels')
    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    if not _unique_labels:
        raise ValueError('Unknown label type: %s' % repr(ys))
    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))
    if len(set((isinstance(label, string_types) for label in ys_labels))) > 1:
        raise ValueError('Mix of label input types (string and number)')
    return np.array(sorted(ys_labels))

.sklearn.utils.multiclass._unique_multiclass

def _unique_multiclass(y):
    if hasattr(y, '__array__'):
        return np.unique(np.asarray(y))
    else:
        return set(y)

.sklearn.metrics.classification.precision_recall_fscore_support

def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None):
    average_options = (None, 'micro', 'macro', 'weighted', 'samples')
    if average not in average_options and average != 'binary':
        raise ValueError('average has to be one of ' + str(average_options))
    if beta <= 0:
        raise ValueError('beta should be >0 in the F-beta score')
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    check_consistent_length(y_true, y_pred, sample_weight)
    present_labels = unique_labels(y_true, y_pred)
    if average == 'binary':
        if y_type == 'binary':
            if pos_label not in present_labels:
                if len(present_labels) < 2:
                    return (0.0, 0.0, 0.0, 0)
                else:
                    raise ValueError('pos_label=%r is not a valid label: %r' % (pos_label, present_labels))
            labels = [pos_label]
        else:
            raise ValueError("Target is %s but average='binary'. Please choose another average setting." % y_type)
    elif pos_label not in (None, 1):
        warnings.warn("Note that pos_label (set to %r) is ignored when average != 'binary' (got %r). You may use labels=[pos_label] to specify a single positive class." % (pos_label, average), UserWarning)
    samplewise = average == 'samples'
    MCM = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels, samplewise=samplewise)
    tp_sum = MCM[:, 1, 1]
    pred_sum = tp_sum + MCM[:, 0, 1]
    true_sum = tp_sum + MCM[:, 1, 0]
    if average == 'micro':
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])
    beta2 = beta ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        precision = _prf_divide(tp_sum, pred_sum, 'precision', 'predicted', average, warn_for)
        recall = _prf_divide(tp_sum, true_sum, 'recall', 'true', average, warn_for)
        f_score = (1 + beta2) * precision * recall / (beta2 * precision + recall)
        f_score[tp_sum == 0] = 0.0
    if average == 'weighted':
        weights = true_sum
        if weights.sum() == 0:
            return (0, 0, 0, None)
    elif average == 'samples':
        weights = sample_weight
    else:
        weights = None
    if average is not None:
        assert average != 'binary' or len(precision) == 1
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None
    return (precision, recall, f_score, true_sum)

.sklearn.metrics.classification.multilabel_confusion_matrix

def multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False):
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
    check_consistent_length(y_true, y_pred, sample_weight)
    if y_type not in ('binary', 'multiclass', 'multilabel-indicator'):
        raise ValueError('%s is not supported' % y_type)
    present_labels = unique_labels(y_true, y_pred)
    if labels is None:
        labels = present_labels
        n_labels = None
    else:
        n_labels = len(labels)
        labels = np.hstack([labels, np.setdiff1d(present_labels, labels, assume_unique=True)])
    if y_true.ndim == 1:
        if samplewise:
            raise ValueError('Samplewise metrics are not available outside of multilabel classification.')
        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        y_pred = le.transform(y_pred)
        sorted_labels = le.classes_
        tp = y_true == y_pred
        tp_bins = y_true[tp]
        if sample_weight is not None:
            tp_bins_weights = np.asarray(sample_weight)[tp]
        else:
            tp_bins_weights = None
        if len(tp_bins):
            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights, minlength=len(labels))
        else:
            true_sum = pred_sum = tp_sum = np.zeros(len(labels))
        if len(y_pred):
            pred_sum = np.bincount(y_pred, weights=sample_weight, minlength=len(labels))
        if len(y_true):
            true_sum = np.bincount(y_true, weights=sample_weight, minlength=len(labels))
        indices = np.searchsorted(sorted_labels, labels[:n_labels])
        tp_sum = tp_sum[indices]
        true_sum = true_sum[indices]
        pred_sum = pred_sum[indices]
    else:
        sum_axis = 1 if samplewise else 0
        if not np.array_equal(labels, present_labels):
            if np.max(labels) > np.max(present_labels):
                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d > %d' % (np.max(labels), np.max(present_labels)))
            if np.min(labels) < 0:
                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d < 0' % np.min(labels))
        if n_labels is not None:
            y_true = y_true[:, labels[:n_labels]]
            y_pred = y_pred[:, labels[:n_labels]]
        true_and_pred = y_true.multiply(y_pred)
        tp_sum = count_nonzero(true_and_pred, axis=sum_axis, sample_weight=sample_weight)
        pred_sum = count_nonzero(y_pred, axis=sum_axis, sample_weight=sample_weight)
        true_sum = count_nonzero(y_true, axis=sum_axis, sample_weight=sample_weight)
    fp = pred_sum - tp_sum
    fn = true_sum - tp_sum
    tp = tp_sum
    if sample_weight is not None and samplewise:
        sample_weight = np.array(sample_weight)
        tp = np.array(tp)
        fp = np.array(fp)
        fn = np.array(fn)
        tn = sample_weight * y_true.shape[1] - tp - fp - fn
    elif sample_weight is not None:
        tn = sum(sample_weight) - tp - fp - fn
    elif samplewise:
        tn = y_true.shape[1] - tp - fp - fn
    else:
        tn = y_true.shape[0] - tp - fp - fn
    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)

.sklearn.preprocessing.label.LabelEncoder.fit

def fit(self, y):
    y = column_or_1d(y, warn=True)
    self.classes_ = _encode(y)
    return self

.sklearn.preprocessing.label._encode

def _encode(values, uniques=None, encode=False):
    if values.dtype == object:
        return _encode_python(values, uniques, encode)
    else:
        return _encode_numpy(values, uniques, encode)

.sklearn.preprocessing.label._encode_numpy

def _encode_numpy(values, uniques=None, encode=False):
    if uniques is None:
        if encode:
            uniques, encoded = np.unique(values, return_inverse=True)
            return (uniques, encoded)
        else:
            return np.unique(values)
    if encode:
        diff = _encode_check_unknown(values, uniques)
        if diff:
            raise ValueError('y contains previously unseen labels: %s' % str(diff))
        encoded = np.searchsorted(uniques, values)
        return (uniques, encoded)
    else:
        return uniques

.sklearn.preprocessing.label.LabelEncoder.transform

def transform(self, y):
    check_is_fitted(self, 'classes_')
    y = column_or_1d(y, warn=True)
    if _num_samples(y) == 0:
        return np.array([])
    _, y = _encode(y, uniques=self.classes_, encode=True)
    return y

.sklearn.utils.validation.check_is_fitted

def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):
    if msg is None:
        msg = "This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
    if not hasattr(estimator, 'fit'):
        raise TypeError('%s is not an estimator instance.' % estimator)
    if not isinstance(attributes, (list, tuple)):
        attributes = [attributes]
    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
        raise NotFittedError(msg % {'name': type(estimator).__name__})


[/PYTHON]
What will be the output of `classification_report`, given the following input:
[INPUT]
```
{
    "self": {},
    "args": {
        "y_true": "['red' 'red' 'blue' 'red' 'green' 'green' 'blue' 'green' 'red' 'green'\n 'red' 'green' 'green' 'green' 'green' 'blue' 'red' 'red' 'green' 'blue'\n 'red' 'green' 'red' 'red' 'blue' 'green' 'blue' 'red' 'green' 'blue'\n 'green' 'blue' 'green' 'green' 'blue' 'blue' 'blue' 'blue' 'red' 'blue'\n 'green' 'red' 'blue' 'green' 'blue' 'green' 'green' 'blue' 'blue' 'green'\n 'green' 'green' 'green' 'red' 'green' 'green' 'blue' 'blue' 'red' 'blue'\n 'green' 'blue' 'red' 'red' 'blue' 'green' 'green' 'green' 'green' 'blue'\n 'red' 'blue' 'green' 'red' 'red']",
        "y_pred": "['red' 'red' 'green' 'red' 'red' 'red' 'blue' 'green' 'red' 'red' 'red'\n 'red' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'red' 'green' 'red'\n 'red' 'red' 'blue' 'red' 'blue' 'red' 'green' 'green' 'red' 'blue' 'red'\n 'green' 'blue' 'blue' 'blue' 'blue' 'red' 'blue' 'red' 'green' 'blue'\n 'red' 'blue' 'blue' 'blue' 'blue' 'green' 'red' 'red' 'red' 'blue' 'red'\n 'red' 'red' 'blue' 'blue' 'red' 'green' 'red' 'blue' 'red' 'red' 'blue'\n 'red' 'red' 'red' 'red' 'blue' 'red' 'blue' 'red' 'red' 'red']"
    },
    "kwargs": {}
}
```
[/INPUT]

[THOUGHT]
