You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [\INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. The output structure is defined between [STRUCTURE] and [\STRUCTURE]. You only need to predict output variable values to fill out placeholders XXX in the structure, and print output between [OUTPUT] and [\OUTPUT]. You should maintain the structure when printing output. Do not change anything else. For prediction, simulate the execution of the program step by step and print your reasoning process before arriving at an answer between [THOUGHT] and [/THOUGHT].
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
[/INPUT]

[STRUCTURE]
```
{ 
    "strpath": XXX
}
```
[/STRUCTURE]

[THOUGHT]
1. Inside `mktemp`, since `numbered`=True (default), it calls: p = make_numbered_dir(root=self.getbasetemp(), prefix="world")
2. Inside `getbasetemp`, since `_basetemp` is None, so basetemp = Path("/tmp/pytest-of-root/pytest-0/test_mktemp0"). Therefore, getbasetemp() returns "/tmp/pytest-of-root/pytest-0/test_mktemp0".
3. Inside `make_numbered_dir`:
    root = /tmp/pytest-of-root/pytest-0/test_mktemp0
    prefix = "world"
    Since max_existing = -1, new_number = 0, so `new_path` = "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]
[/EXAMPLE]
[PYTHON]
import warnings
import numpy as np
from ..base import BaseEstimator, RegressorMixin, clone
from ..utils.validation import check_is_fitted
from ..utils import check_array, safe_indexing
from ..preprocessing import FunctionTransformer
from ..linear_model import LinearRegression
__all__ = ['TransformedTargetRegressor']

class TransformedTargetRegressor(RegressorMixin, BaseEstimator):

    def __init__(self, regressor=None, transformer=None, func=None, inverse_func=None, check_inverse=True):
        self.regressor = regressor
        self.transformer = transformer
        self.func = func
        self.inverse_func = inverse_func
        self.check_inverse = check_inverse

    def _fit_transformer(self, y):
        if self.transformer is not None and (self.func is not None or self.inverse_func is not None):
            raise ValueError("'transformer' and functions 'func'/'inverse_func' cannot both be set.")
        elif self.transformer is not None:
            self.transformer_ = clone(self.transformer)
        else:
            if self.func is not None and self.inverse_func is None:
                raise ValueError("When 'func' is provided, 'inverse_func' must also be provided")
            self.transformer_ = FunctionTransformer(func=self.func, inverse_func=self.inverse_func, validate=True, check_inverse=self.check_inverse)
        self.transformer_.fit(y)
        if self.check_inverse:
            idx_selected = slice(None, None, max(1, y.shape[0] // 10))
            y_sel = safe_indexing(y, idx_selected)
            y_sel_t = self.transformer_.transform(y_sel)
            if not np.allclose(y_sel, self.transformer_.inverse_transform(y_sel_t)):
                warnings.warn("The provided functions or transformer are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'", UserWarning)

    def fit(self, X, y, **fit_params):
        y = check_array(y, accept_sparse=False, force_all_finite=True, ensure_2d=False, dtype='numeric')
        self._training_dim = y.ndim
        if y.ndim == 1:
            y_2d = y.reshape(-1, 1)
        else:
            y_2d = y
        self._fit_transformer(y_2d)
        y_trans = self.transformer_.transform(y_2d)
        if y_trans.ndim == 2 and y_trans.shape[1] == 1:
            y_trans = y_trans.squeeze(axis=1)
        if self.regressor is None:
            from ..linear_model import LinearRegression
            self.regressor_ = LinearRegression()
        else:
            self.regressor_ = clone(self.regressor)
        self.regressor_.fit(X, y_trans, **fit_params)
        return self

    def predict(self, X):
        check_is_fitted(self)
        pred = self.regressor_.predict(X)
        if pred.ndim == 1:
            pred_trans = self.transformer_.inverse_transform(pred.reshape(-1, 1))
        else:
            pred_trans = self.transformer_.inverse_transform(pred)
        if self._training_dim == 1 and pred_trans.ndim == 2 and (pred_trans.shape[1] == 1):
            pred_trans = pred_trans.squeeze(axis=1)
        return pred_trans

    def _more_tags(self):
        return {'poor_score': True, 'no_validation': True}
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):
    if warn_on_dtype is not None:
        warnings.warn("'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.", DeprecationWarning, stacklevel=2)
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        dtypes_orig = np.array(array.dtypes)
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, str):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                array = np.asarray(array, dtype=dtype, order=order)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).", FutureWarning, stacklevel=2)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=2)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):
        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=3)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False):
    from .extmath import _safe_accumulator_op
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, X.dtype))
    elif X.dtype == np.dtype('object') and (not allow_nan):
        if _object_dtype_isnan(X).any():
            raise ValueError('Input contains NaN')

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.extmath._safe_accumulator_op

def _safe_accumulator_op(op, x, *args, **kwargs):
    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:
        result = op(x, *args, **kwargs, dtype=np.float64)
    else:
        result = op(x, *args, **kwargs)
    return result

.sklearn.utils.validation._num_samples

def _num_samples(x):
    message = 'Expected sequence or array-like, got %s' % type(x)
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError(message)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError(message)
    if hasattr(x, 'shape') and x.shape is not None:
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
    try:
        return len(x)
    except TypeError:
        raise TypeError(message)

.sklearn.preprocessing._function_transformer.FunctionTransformer.__init__

def __init__(self, func=None, inverse_func=None, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None):
    self.func = func
    self.inverse_func = inverse_func
    self.validate = validate
    self.accept_sparse = accept_sparse
    self.check_inverse = check_inverse
    self.kw_args = kw_args
    self.inv_kw_args = inv_kw_args

.sklearn.preprocessing._function_transformer.FunctionTransformer.fit

def fit(self, X, y=None):
    X = self._check_input(X)
    if self.check_inverse and (not (self.func is None or self.inverse_func is None)):
        self._check_inverse_transform(X)
    return self

.sklearn.preprocessing._function_transformer.FunctionTransformer._check_input

def _check_input(self, X):
    if self.validate:
        return check_array(X, accept_sparse=self.accept_sparse)
    return X

.sklearn.preprocessing._function_transformer.FunctionTransformer._check_inverse_transform

def _check_inverse_transform(self, X):
    idx_selected = slice(None, None, max(1, X.shape[0] // 100))
    X_round_trip = self.inverse_transform(self.transform(X[idx_selected]))
    if not _allclose_dense_sparse(X[idx_selected], X_round_trip):
        warnings.warn("The provided functions are not strictly inverse of each other. If you are sure you want to proceed regardless, set 'check_inverse=False'.", UserWarning)

.sklearn.preprocessing._function_transformer.FunctionTransformer.transform

def transform(self, X):
    return self._transform(X, func=self.func, kw_args=self.kw_args)

.sklearn.preprocessing._function_transformer.FunctionTransformer._transform

def _transform(self, X, func=None, kw_args=None):
    X = self._check_input(X)
    if func is None:
        func = _identity
    return func(X, **kw_args if kw_args else {})

.sklearn.preprocessing._function_transformer.FunctionTransformer.inverse_transform

def inverse_transform(self, X):
    return self._transform(X, func=self.inverse_func, kw_args=self.inv_kw_args)

.sklearn.utils.validation._allclose_dense_sparse

def _allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-09):
    if sp.issparse(x) and sp.issparse(y):
        x = x.tocsr()
        y = y.tocsr()
        x.sum_duplicates()
        y.sum_duplicates()
        return np.array_equal(x.indices, y.indices) and np.array_equal(x.indptr, y.indptr) and np.allclose(x.data, y.data, rtol=rtol, atol=atol)
    elif not sp.issparse(x) and (not sp.issparse(y)):
        return np.allclose(x, y, rtol=rtol, atol=atol)
    raise ValueError('Can only compare two sparse matrices, not a sparse matrix and an array')

.sklearn.utils.__init__.safe_indexing

def safe_indexing(X, indices, axis=0):
    if indices is None:
        return X
    if axis not in (0, 1):
        raise ValueError("'axis' should be either 0 (to index rows) or 1 (to index  column). Got {} instead.".format(axis))
    indices_dtype = _determine_key_type(indices)
    if axis == 0 and indices_dtype == 'str':
        raise ValueError("String indexing is not supported with 'axis=0'")
    if axis == 1 and X.ndim != 2:
        raise ValueError("'X' should be a 2D NumPy array, 2D sparse matrix or pandas dataframe when indexing the columns (i.e. 'axis=1'). Got {} instead with {} dimension(s).".format(type(X), X.ndim))
    if axis == 1 and indices_dtype == 'str' and (not hasattr(X, 'loc')):
        raise ValueError('Specifying the columns using strings is only supported for pandas DataFrames')
    if hasattr(X, 'iloc'):
        return _pandas_indexing(X, indices, indices_dtype, axis=axis)
    elif hasattr(X, 'shape'):
        return _array_indexing(X, indices, indices_dtype, axis=axis)
    else:
        return _list_indexing(X, indices, indices_dtype)


[/PYTHON]
What will be the output of `fit`, given the following input:
[INPUT]
```
{
    "self": {
        "regressor": {
            "fit_intercept": true,
            "normalize": false,
            "copy_X": true,
            "n_jobs": null
        },
        "transformer": null,
        "func": "\"<function sqrt>\"",
        "inverse_func": "\"<function log>\"",
        "check_inverse": true
    },
    "args": {
        "X": "[[5.48813504e-01 7.15189366e-01 6.02763376e-01 5.44883183e-01\n  4.23654799e-01 6.45894113e-01 4.37587211e-01 8.91773001e-01\n  9.63662761e-01 3.83441519e-01]\n [7.91725038e-01 5.28894920e-01 5.68044561e-01 9.25596638e-01\n  7.10360582e-02 8.71292997e-02 2.02183974e-02 8.32619846e-01\n  7.78156751e-01 8.70012148e-01]\n [9.78618342e-01 7.99158564e-01 4.61479362e-01 7.80529176e-01\n  1.18274426e-01 6.39921021e-01 1.43353287e-01 9.44668917e-01\n  5.21848322e-01 4.14661940e-01]\n [2.64555612e-01 7.74233689e-01 4.56150332e-01 5.68433949e-01\n  1.87898004e-02 6.17635497e-01 6.12095723e-01 6.16933997e-01\n  9.43748079e-01 6.81820299e-01]\n [3.59507901e-01 4.37031954e-01 6.97631196e-01 6.02254716e-02\n  6.66766715e-01 6.70637870e-01 2.10382561e-01 1.28926298e-01\n  3.15428351e-01 3.63710771e-01]\n [5.70196770e-01 4.38601513e-01 9.88373838e-01 1.02044811e-01\n  2.08876756e-01 1.61309518e-01 6.53108325e-01 2.53291603e-01\n  4.66310773e-01 2.44425592e-01]\n [1.58969584e-01 1.10375141e-01 6.56329589e-01 1.38182951e-01\n  1.96582362e-01 3.68725171e-01 8.20993230e-01 9.71012758e-02\n  8.37944907e-01 9.60984079e-02]\n [9.76459465e-01 4.68651202e-01 9.76761088e-01 6.04845520e-01\n  7.39263579e-01 3.91877923e-02 2.82806963e-01 1.20196561e-01\n  2.96140198e-01 1.18727719e-01]\n [3.17983179e-01 4.14262995e-01 6.41474963e-02 6.92472119e-01\n  5.66601454e-01 2.65389491e-01 5.23248053e-01 9.39405108e-02\n  5.75946496e-01 9.29296198e-01]\n [3.18568952e-01 6.67410380e-01 1.31797862e-01 7.16327204e-01\n  2.89406093e-01 1.83191362e-01 5.86512935e-01 2.01075462e-02\n  8.28940029e-01 4.69547619e-03]\n [6.77816537e-01 2.70007973e-01 7.35194022e-01 9.62188545e-01\n  2.48753144e-01 5.76157334e-01 5.92041931e-01 5.72251906e-01\n  2.23081633e-01 9.52749012e-01]\n [4.47125379e-01 8.46408672e-01 6.99479275e-01 2.97436951e-01\n  8.13797820e-01 3.96505741e-01 8.81103197e-01 5.81272873e-01\n  8.81735362e-01 6.92531590e-01]\n [7.25254280e-01 5.01324382e-01 9.56083635e-01 6.43990199e-01\n  4.23855049e-01 6.06393214e-01 1.91931983e-02 3.01574817e-01\n  6.60173537e-01 2.90077607e-01]\n [6.18015429e-01 4.28768701e-01 1.35474064e-01 2.98282326e-01\n  5.69964911e-01 5.90872761e-01 5.74325249e-01 6.53200820e-01\n  6.52103270e-01 4.31418435e-01]\n [8.96546596e-01 3.67561870e-01 4.35864925e-01 8.91923355e-01\n  8.06193989e-01 7.03888584e-01 1.00226887e-01 9.19482614e-01\n  7.14241300e-01 9.98847007e-01]\n [1.49448305e-01 8.68126057e-01 1.62492935e-01 6.15559564e-01\n  1.23819983e-01 8.48008229e-01 8.07318959e-01 5.69100739e-01\n  4.07183297e-01 6.91669955e-02]\n [6.97428773e-01 4.53542683e-01 7.22055599e-01 8.66382326e-01\n  9.75521505e-01 8.55803342e-01 1.17140842e-02 3.59978064e-01\n  7.29990562e-01 1.71629677e-01]\n [5.21036606e-01 5.43379883e-02 1.99996525e-01 1.85217945e-02\n  7.93697703e-01 2.23924688e-01 3.45351681e-01 9.28081293e-01\n  7.04414402e-01 3.18389295e-02]\n [1.64694156e-01 6.21478401e-01 5.77228589e-01 2.37892821e-01\n  9.34213998e-01 6.13965956e-01 5.35632803e-01 5.89909976e-01\n  7.30122030e-01 3.11944995e-01]\n [3.98221062e-01 2.09843749e-01 1.86193006e-01 9.44372390e-01\n  7.39550795e-01 4.90458809e-01 2.27414628e-01 2.54356482e-01\n  5.80291603e-02 4.34416626e-01]\n [3.11795882e-01 6.96343489e-01 3.77751839e-01 1.79603678e-01\n  2.46787284e-02 6.72496315e-02 6.79392773e-01 4.53696845e-01\n  5.36579211e-01 8.96671293e-01]\n [9.90338947e-01 2.16896984e-01 6.63078203e-01 2.63322377e-01\n  2.06509995e-02 7.58378654e-01 3.20017151e-01 3.83463894e-01\n  5.88317114e-01 8.31048455e-01]\n [6.28981844e-01 8.72650655e-01 2.73542035e-01 7.98046834e-01\n  1.85635944e-01 9.52791657e-01 6.87488276e-01 2.15507677e-01\n  9.47370590e-01 7.30855807e-01]\n [2.53941643e-01 2.13311977e-01 5.18200714e-01 2.56627181e-02\n  2.07470075e-01 4.24685469e-01 3.74169980e-01 4.63575424e-01\n  2.77628706e-01 5.86784346e-01]\n [8.63855606e-01 1.17531856e-01 5.17379107e-01 1.32068106e-01\n  7.16859681e-01 3.96059703e-01 5.65421312e-01 1.83279836e-01\n  1.44847759e-01 4.88056281e-01]\n [3.55612738e-01 9.40431945e-01 7.65325254e-01 7.48663620e-01\n  9.03719740e-01 8.34224354e-02 5.52192470e-01 5.84476069e-01\n  9.61936379e-01 2.92147527e-01]\n [2.40828780e-01 1.00293942e-01 1.64296296e-02 9.29529317e-01\n  6.69916547e-01 7.85152912e-01 2.81730106e-01 5.86410166e-01\n  6.39552661e-02 4.85627596e-01]\n [9.77495140e-01 8.76505245e-01 3.38158952e-01 9.61570155e-01\n  2.31701626e-01 9.49318822e-01 9.41377705e-01 7.99202587e-01\n  6.30447937e-01 8.74287967e-01]\n [2.93020285e-01 8.48943555e-01 6.17876692e-01 1.32368578e-02\n  3.47233518e-01 1.48140861e-01 9.81829390e-01 4.78370307e-01\n  4.97391365e-01 6.39472516e-01]\n [3.68584606e-01 1.36900272e-01 8.22117733e-01 1.89847912e-01\n  5.11318983e-01 2.24317029e-01 9.78444845e-02 8.62191517e-01\n  9.72919489e-01 9.60834658e-01]\n [9.06555499e-01 7.74047333e-01 3.33145152e-01 8.11013900e-02\n  4.07241171e-01 2.32234142e-01 1.32487635e-01 5.34271818e-02\n  7.25594364e-01 1.14274586e-02]\n [7.70580749e-01 1.46946645e-01 7.95220826e-02 8.96030342e-02\n  6.72047807e-01 2.45367210e-01 4.20539467e-01 5.57368791e-01\n  8.60551174e-01 7.27044263e-01]\n [2.70327905e-01 1.31482799e-01 5.53743204e-02 3.01598634e-01\n  2.62118149e-01 4.56140567e-01 6.83281336e-01 6.95625446e-01\n  2.83518847e-01 3.79926956e-01]\n [1.81150962e-01 7.88545512e-01 5.68480764e-02 6.96997242e-01\n  7.78695396e-01 7.77407562e-01 2.59422564e-01 3.73813138e-01\n  5.87599635e-01 2.72821902e-01]\n [3.70852799e-01 1.97054280e-01 4.59855884e-01 4.46123013e-02\n  7.99795885e-01 7.69564470e-02 5.18835149e-01 3.06810100e-01\n  5.77542949e-01 9.59433341e-01]\n [6.45570244e-01 3.53624358e-02 4.30402440e-01 5.10016852e-01\n  5.36177495e-01 6.81392511e-01 2.77596098e-01 1.28860565e-01\n  3.92675677e-01 9.56405723e-01]\n [1.87130892e-01 9.03983955e-01 5.43805950e-01 4.56911422e-01\n  8.82041410e-01 4.58603962e-01 7.24167637e-01 3.99025322e-01\n  9.04044393e-01 6.90025020e-01]\n [6.99622054e-01 3.27720402e-01 7.56778643e-01 6.36061055e-01\n  2.40020273e-01 1.60538822e-01 7.96391475e-01 9.59166603e-01\n  4.58138827e-01 5.90984165e-01]\n [8.57722644e-01 4.57223453e-01 9.51874477e-01 5.75751162e-01\n  8.20767121e-01 9.08843718e-01 8.15523819e-01 1.59414463e-01\n  6.28898439e-01 3.98434259e-01]\n [6.27129520e-02 4.24032252e-01 2.58684067e-01 8.49038308e-01\n  3.33046265e-02 9.58982722e-01 3.55368848e-01 3.56706890e-01\n  1.63285027e-02 1.85232325e-01]\n [4.01259501e-01 9.29291417e-01 9.96149302e-02 9.45301533e-01\n  8.69488531e-01 4.54162397e-01 3.26700882e-01 2.32744129e-01\n  6.14464706e-01 3.30745915e-02]\n [1.56060644e-02 4.28795722e-01 6.80740740e-02 2.51940988e-01\n  2.21160915e-01 2.53191194e-01 1.31055231e-01 1.20362229e-02\n  1.15484297e-01 6.18480260e-01]\n [9.74256213e-01 9.90345002e-01 4.09054095e-01 1.62954426e-01\n  6.38761757e-01 4.90305347e-01 9.89409777e-01 6.53042072e-02\n  7.83234438e-01 2.88398497e-01]\n [2.41418620e-01 6.62504572e-01 2.46063185e-01 6.65859118e-01\n  5.17308517e-01 4.24088988e-01 5.54687809e-01 2.87051520e-01\n  7.06574706e-01 4.14856869e-01]\n [3.60545560e-01 8.28656915e-01 9.24966912e-01 4.60073109e-02\n  2.32626993e-01 3.48519369e-01 8.14966479e-01 9.85491428e-01\n  9.68971705e-01 9.04948346e-01]\n [2.96556265e-01 9.92011243e-01 2.49420041e-01 1.05906155e-01\n  9.50952611e-01 2.33420255e-01 6.89768265e-01 5.83563590e-02\n  7.30709099e-01 8.81720212e-01]\n [2.72436895e-01 3.79056896e-01 3.74296183e-01 7.48788258e-01\n  2.37807243e-01 1.71853099e-01 4.49291649e-01 3.04468407e-01\n  8.39189122e-01 2.37741826e-01]\n [5.02389457e-01 9.42583600e-01 6.33997698e-01 8.67289405e-01\n  9.40209689e-01 7.50764862e-01 6.99575060e-01 9.67965567e-01\n  9.94400790e-01 4.51821683e-01]\n [7.08697782e-02 2.92794031e-01 1.52354706e-01 4.17486375e-01\n  1.31289328e-01 6.04117804e-01 3.82808059e-01 8.95385884e-01\n  9.67794672e-01 5.46884902e-01]\n [2.74823570e-01 5.92230419e-01 8.96761158e-01 4.06733346e-01\n  5.52078277e-01 2.71652768e-01 4.55444149e-01 4.01713535e-01\n  2.48413465e-01 5.05866384e-01]\n [3.10380826e-01 3.73034864e-01 5.24970442e-01 7.50595023e-01\n  3.33507466e-01 9.24158767e-01 8.62318547e-01 4.86902960e-02\n  2.53642524e-01 4.46135513e-01]\n [1.04627889e-01 3.48475989e-01 7.40097526e-01 6.80514481e-01\n  6.22384429e-01 7.10528403e-01 2.04923687e-01 3.41698115e-01\n  6.76242482e-01 8.79234763e-01]\n [5.43678054e-01 2.82699651e-01 3.02352580e-02 7.10336829e-01\n  7.88410351e-03 3.72679070e-01 5.30537215e-01 9.22111462e-01\n  8.94945450e-02 4.05942322e-01]\n [2.43131997e-02 3.42610984e-01 6.22231059e-01 2.79067948e-01\n  2.09749950e-01 1.15703233e-01 5.77140244e-01 6.95270006e-01\n  6.71957141e-01 9.48861021e-01]\n [2.70321389e-03 6.47196654e-01 6.00392237e-01 5.88739610e-01\n  9.62770320e-01 1.68716734e-02 6.96482431e-01 8.13678650e-01\n  5.09807197e-01 3.33964870e-01]\n [7.90840163e-01 9.72429256e-02 4.42035638e-01 5.19952375e-01\n  6.93956411e-01 9.08857320e-02 2.27759502e-01 4.10301563e-01\n  6.23294673e-01 8.86960781e-01]\n [6.18826168e-01 1.33461471e-01 9.80580133e-01 8.71785735e-01\n  5.02720761e-01 9.22347982e-01 5.41380794e-01 9.23306068e-01\n  8.29897369e-01 9.68286410e-01]\n [9.19782811e-01 3.60338174e-02 1.74772004e-01 3.89134677e-01\n  9.52142697e-01 3.00028919e-01 1.60467644e-01 8.86304666e-01\n  4.46394415e-01 9.07875594e-01]\n [1.60230466e-01 6.61117512e-01 4.40263753e-01 7.64867690e-02\n  6.96463145e-01 2.47398756e-01 3.96155226e-02 5.99442982e-02\n  6.10785371e-02 9.07732957e-01]\n [7.39883918e-01 8.98062357e-01 6.72582311e-01 5.28939929e-01\n  3.04446364e-01 9.97962251e-01 3.62189059e-01 4.70648949e-01\n  3.78245175e-01 9.79526929e-01]\n [1.74658385e-01 3.27988001e-01 6.80348666e-01 6.32076183e-02\n  6.07249374e-01 4.77646503e-01 2.83999977e-01 2.38413281e-01\n  5.14512743e-01 3.67927581e-01]\n [4.56519891e-01 3.37477382e-01 9.70493694e-01 1.33439432e-01\n  9.68039532e-02 3.43391729e-01 5.91026901e-01 6.59176472e-01\n  3.97256747e-01 9.99277994e-01]\n [3.51892996e-01 7.21406668e-01 6.37582695e-01 8.13053863e-01\n  9.76225663e-01 8.89793656e-01 7.64561974e-01 6.98248478e-01\n  3.35498170e-01 1.47685578e-01]\n [6.26360031e-02 2.41901704e-01 4.32281481e-01 5.21996274e-01\n  7.73083554e-01 9.58740923e-01 1.17320480e-01 1.07004140e-01\n  5.89694723e-01 7.45398074e-01]\n [8.48150380e-01 9.35832080e-01 9.83426242e-01 3.99801692e-01\n  3.80335184e-01 1.47808677e-01 6.84934439e-01 6.56761958e-01\n  8.62062596e-01 9.72579948e-02]\n [4.97776908e-01 5.81081930e-01 2.41557040e-01 1.69025406e-01\n  8.59580836e-01 5.85349222e-02 4.70620904e-01 1.15834001e-01\n  4.57058761e-01 9.79962326e-01]\n [4.23706353e-01 8.57124918e-01 1.17315564e-01 2.71252077e-01\n  4.03792741e-01 3.99812140e-01 6.71383479e-01 3.44718127e-01\n  7.13766868e-01 6.39186899e-01]\n [3.99161145e-01 4.31760128e-01 6.14527700e-01 7.00421901e-02\n  8.22406738e-01 6.53421161e-01 7.26342464e-01 5.36923001e-01\n  1.10477111e-01 4.05035613e-01]\n [4.05373583e-01 3.21042990e-01 2.99503249e-02 7.37254243e-01\n  1.09784458e-01 6.06308133e-01 7.03217496e-01 6.34786323e-01\n  9.59142252e-01 1.03298155e-01]\n [8.67167159e-01 2.91902348e-02 5.34916855e-01 4.04243618e-01\n  5.24183860e-01 3.65099877e-01 1.90566915e-01 1.91228974e-02\n  5.18149814e-01 8.42776863e-01]\n [3.73215956e-01 2.22863818e-01 8.05320035e-02 8.53109231e-02\n  2.21396446e-01 1.00014061e-01 2.65039698e-01 6.61494621e-02\n  6.56048672e-02 8.56276180e-01]\n [1.62120261e-01 5.59682406e-01 7.73455544e-01 4.56409565e-01\n  1.53368878e-01 1.99596142e-01 4.32984206e-01 5.28234089e-01\n  3.49440292e-01 7.81479600e-01]\n [7.51021649e-01 9.27211807e-01 2.89525490e-02 8.95691291e-01\n  3.92568788e-01 8.78372495e-01 6.90784776e-01 9.87348757e-01\n  7.59282452e-01 3.64544626e-01]\n [5.01063173e-01 3.76389155e-01 3.64911836e-01 2.60904499e-01\n  4.95970295e-01 6.81739945e-01 2.77340271e-01 5.24379811e-01\n  1.17380294e-01 1.59845287e-01]\n [4.68063547e-02 9.70731443e-01 3.86035151e-03 1.78579968e-01\n  6.12866753e-01 8.13695989e-02 8.81896503e-01 7.19620158e-01\n  9.66389971e-01 5.07635547e-01]\n [3.00403683e-01 5.49500573e-01 9.30818717e-01 5.20761437e-01\n  2.67207032e-01 8.77398789e-01 3.71918749e-01 1.38335000e-03\n  2.47685022e-01 3.18233509e-01]\n [8.58777468e-01 4.58503167e-01 4.44587288e-01 3.36102266e-01\n  8.80678123e-01 9.45026777e-01 9.91890329e-01 3.76741267e-01\n  9.66147446e-01 7.91879570e-01]\n [6.75689148e-01 2.44889479e-01 2.16457261e-01 1.66047825e-01\n  9.22756610e-01 2.94076662e-01 4.53094245e-01 4.93957834e-01\n  7.78171595e-01 8.44234962e-01]\n [1.39072701e-01 4.26904360e-01 8.42854888e-01 8.18033306e-01\n  1.02413758e-01 1.56383349e-01 3.04198692e-01 7.53590691e-02\n  4.24663003e-01 1.07617705e-01]\n [5.68217594e-01 2.46556940e-01 5.96433065e-01 1.17525643e-01\n  9.75883868e-01 9.32561204e-01 3.91796939e-01 2.42178594e-01\n  2.50398213e-01 4.83393535e-01]\n [3.99928019e-02 6.39705106e-01 4.08302908e-01 3.77406573e-01\n  8.09364971e-01 7.09035460e-01 9.54333815e-01 3.51936240e-01\n  8.97542765e-01 7.69967186e-01]\n [3.57424652e-01 6.21665436e-01 2.88569958e-01 8.74399917e-01\n  1.12427317e-01 2.12434361e-01 1.83033292e-01 4.03026002e-01\n  7.45232960e-01 5.26907449e-01]\n [4.87676324e-01 5.45964897e-04 4.25401725e-01 6.35537748e-02\n  2.08253252e-01 9.32393939e-01 2.15398204e-01 8.58337639e-01\n  8.02893372e-01 1.59146237e-01]\n [6.05711957e-01 1.15661872e-01 7.27888158e-01 6.37462277e-01\n  8.11938562e-01 4.79384549e-01 9.14863088e-01 4.93489468e-02\n  2.92888565e-01 7.15052597e-01]\n [4.18109212e-01 1.72951354e-01 1.07210745e-01 8.17339111e-01\n  4.73142978e-01 8.82283672e-01 7.33289134e-01 4.09726206e-01\n  3.73511014e-01 5.15638347e-01]\n [8.89059953e-01 7.37278580e-01 5.15296427e-03 6.94157851e-01\n  9.19507407e-01 7.10455760e-01 1.77005782e-01 4.83518127e-01\n  1.40316018e-01 3.58995278e-01]\n [9.37117042e-01 9.23305308e-01 2.82836852e-01 3.39631044e-01\n  6.00212868e-01 9.63197295e-01 1.47801334e-01 2.56916644e-01\n  8.73556827e-01 4.91892232e-01]\n [8.98961092e-01 1.85517898e-01 5.32668587e-01 3.26269633e-01\n  3.16542560e-01 4.46876964e-01 4.33077449e-01 3.57346880e-01\n  9.14970770e-01 7.31744185e-01]\n [7.27546991e-01 2.89913450e-01 5.77709424e-01 7.79179433e-01\n  7.95590369e-01 3.44530461e-01 7.70872757e-01 7.35893897e-01\n  1.41506486e-01 8.65945469e-01]\n [4.41321470e-01 4.86410449e-01 4.48369179e-01 5.67846001e-01\n  6.21169247e-01 4.98179566e-01 8.66788543e-01 6.27734756e-01\n  4.01427949e-01 4.16691757e-01]\n [8.10838615e-01 3.48191943e-01 2.11454796e-01 5.93831880e-02\n  8.76026848e-01 9.18546451e-01 1.20120182e-01 3.34473741e-01\n  1.75372070e-01 1.15898469e-01]\n [8.99866743e-01 5.68772591e-02 9.80485663e-01 9.64508607e-02\n  8.63470649e-01 5.66506107e-01 3.67917488e-01 3.42342377e-01\n  7.57364143e-01 3.14573295e-01]\n [6.57318917e-01 5.17326084e-01 4.84965645e-01 9.01162171e-01\n  5.54645059e-01 8.26861603e-01 7.25573534e-01 3.85572461e-02\n  7.73110053e-01 2.16870250e-01]\n [9.03149647e-01 4.29241906e-02 3.33072034e-01 9.97329472e-02\n  4.75589117e-01 8.20022436e-01 2.98187360e-01 1.50934897e-01\n  3.30267036e-01 8.13880142e-01]\n [1.40383958e-01 2.27362449e-01 6.88519645e-02 7.05710044e-01\n  3.95233244e-01 3.10839977e-01 7.18626390e-01 3.35977542e-01\n  7.27771273e-01 8.15199395e-01]\n [2.17662843e-01 9.73818697e-01 1.62357948e-01 2.90840907e-01\n  1.79795291e-01 3.45505656e-01 4.80060888e-01 5.22175869e-01\n  8.53606042e-01 8.89447909e-01]\n [2.20103861e-01 6.22894032e-01 1.11496057e-01 4.58969860e-01\n  3.22333538e-01 3.16500745e-01 4.82584242e-01 7.29827636e-01\n  6.91826588e-02 8.79173338e-01]\n [7.34813775e-01 1.76499389e-01 9.39160909e-01 5.06312224e-01\n  9.99808578e-01 1.97259474e-01 5.34908198e-01 2.90248043e-01\n  3.04173557e-01 5.91065381e-01]\n [9.21719067e-01 8.05263856e-01 7.23941399e-01 5.59173782e-01\n  9.22298504e-01 4.92361407e-01 8.73832178e-01 8.33981644e-01\n  2.13835347e-01 7.71225463e-01]\n [1.21711569e-02 3.22829538e-01 2.29567445e-01 5.06862958e-01\n  7.36853162e-01 9.76763674e-02 5.14922202e-01 9.38412022e-01\n  2.28646551e-01 6.77141144e-01]]",
        "y": "[17.2134918  19.37965436 14.75030901 11.81662128  9.45520588 13.90806032\n  3.40447398 24.20228984 17.5783377  17.51556532 17.40999895 17.11893656\n 21.81510702 15.88623898 21.63246599 13.01689624 22.908266    6.84202611\n 10.32969753 17.70617001  8.52251585  9.51592807 19.81663198  2.99416378\n  8.04688453 22.09060713 18.07976383 15.64717616  9.18985112  8.10886643\n 11.46237964 11.27510587  9.39472899 19.12963765  6.75303583  8.59450992\n 14.08547252 15.47556192 23.37684908 10.65602456 26.21981092  7.56662861\n  6.09081028 15.3507226  13.30251573 15.05111884 12.18062778 23.69853054\n  7.89988206 14.86941989 12.74370258 12.21293464 16.19946578  4.3999017\n 10.95778196 11.12906602 18.41621638 11.80687188  7.58538837 16.10200268\n  6.10882485 10.89902258 20.54649545  9.65292317 16.60986366 15.21108433\n 16.75064635 10.22838974 16.31598133  7.48212681  8.06258535  9.63860205\n 23.51439611 11.03813837 11.1958086  15.21229612 17.27390765 12.84957523\n 12.89778228 10.5012407   8.79192515 16.62749545  1.79646668 13.65617967\n 15.87704956 25.26710572 11.44869485  9.8696567  18.04254388 15.08179079\n 14.39066711 11.50020011 20.55316414  5.1474656  13.75209451 12.26513907\n 13.395294   17.88207091 18.44783937 10.33900734]"
    },
    "kwargs": {}
}
```
[/INPUT]

[STRUCTURE]
```
{
    "regressor": {
        "fit_intercept": XXX,
        "normalize": XXX,
        "copy_X": XXX,
        "n_jobs": XXX
    },
    "transformer": XXX,
    "func": XXX,
    "inverse_func": XXX,
    "check_inverse": XXX,
    "_training_dim": XXX,
    "transformer_": {
        "func": XXX,
        "inverse_func": XXX,
        "validate": XXX,
        "accept_sparse": XXX,
        "check_inverse": XXX,
        "kw_args": XXX,
        "inv_kw_args": XXX
    },
    "regressor_": {
        "fit_intercept": XXX,
        "normalize": XXX,
        "copy_X": XXX,
        "n_jobs": XXX,
        "coef_": XXX,
        "_residues": XXX,
        "rank_": XXX,
        "singular_": XXX,
        "intercept_": XXX
    }
}
```
[/STRUCTURE]

[THOUGHT]
