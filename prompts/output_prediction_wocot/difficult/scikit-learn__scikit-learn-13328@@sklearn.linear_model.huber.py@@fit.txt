You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [\INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. The output structure is defined between [STRUCTURE] and [\STRUCTURE]. You only need to predict output variable values to fill out placeholders XXX in the structure, and print output between [OUTPUT] and [\OUTPUT]. You should maintain the structure when printing output. Do not change anything else. ONLY print the output, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
[/INPUT]

[STRUCTURE]
```
{ 
    "strpath": XXX
}
```
[/STRUCTURE]

[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]
[/EXAMPLE]
[PYTHON]
import numpy as np
from scipy import optimize
from ..base import BaseEstimator, RegressorMixin
from .base import LinearModel
from ..utils import check_X_y
from ..utils import check_consistent_length
from ..utils import axis0_safe_slice
from ..utils.extmath import safe_sparse_dot

class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):

    def __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05):
        self.epsilon = epsilon
        self.max_iter = max_iter
        self.alpha = alpha
        self.warm_start = warm_start
        self.fit_intercept = fit_intercept
        self.tol = tol

    def fit(self, X, y, sample_weight=None):
        X, y = check_X_y(X, y, copy=False, accept_sparse=['csr'], y_numeric=True, dtype=[np.float64, np.float32])
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
        if self.epsilon < 1.0:
            raise ValueError('epsilon should be greater than or equal to 1.0, got %f' % self.epsilon)
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate((self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            parameters[-1] = 1
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(_huber_loss_and_gradient, parameters, args=(X, y, self.epsilon, self.alpha, sample_weight), maxiter=self.max_iter, pgtol=self.tol, bounds=bounds, iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError('HuberRegressor convergence failed: l-BFGS-b solver terminated with %s' % dict_['task'].decode('ascii'))
        self.n_iter_ = min(dict_['nit'], self.max_iter)
        self.scale_ = parameters[-1]
        if self.fit_intercept:
            self.intercept_ = parameters[-2]
        else:
            self.intercept_ = 0.0
        self.coef_ = parameters[:X.shape[1]]
        residual = np.abs(y - safe_sparse_dot(X, self.coef_) - self.intercept_)
        self.outliers_ = residual > self.scale_ * self.epsilon
        return self
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.utils.validation.check_X_y

def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None):
    if y is None:
        raise ValueError('y cannot be None')
    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)
    if multi_output:
        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)
    else:
        y = column_or_1d(y, warn=True)
        _assert_all_finite(y)
    if y_numeric and y.dtype.kind == 'O':
        y = y.astype(np.float64)
    check_consistent_length(X, y)
    return (X, y)

.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):
    if accept_sparse is None:
        warnings.warn("Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.", DeprecationWarning)
        accept_sparse = False
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        dtypes_orig = np.array(array.dtypes)
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, str):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                array = np.asarray(array, dtype=dtype, order=order)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).", FutureWarning)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):
        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=3)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False):
    from .extmath import _safe_accumulator_op
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, X.dtype))
    elif X.dtype == np.dtype('object') and (not allow_nan):
        if _object_dtype_isnan(X).any():
            raise ValueError('Input contains NaN')

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.extmath._safe_accumulator_op

def _safe_accumulator_op(op, x, *args, **kwargs):
    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:
        result = op(x, *args, **kwargs, dtype=np.float64)
    else:
        result = op(x, *args, **kwargs)
    return result

.sklearn.utils.validation._num_samples

def _num_samples(x):
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError('Expected sequence or array-like, got estimator %s' % x)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError('Expected sequence or array-like, got %s' % type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
        else:
            return len(x)
    else:
        return len(x)

.sklearn.utils.validation.column_or_1d

def column_or_1d(y, warn=False):
    shape = np.shape(y)
    if len(shape) == 1:
        return np.ravel(y)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)
        return np.ravel(y)
    raise ValueError('bad input shape {0}'.format(shape))

.sklearn.utils.validation.check_consistent_length

def check_consistent_length(*arrays):
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])

.sklearn.linear_model.huber._huber_loss_and_gradient

def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):
    _, n_features = X.shape
    fit_intercept = n_features + 2 == w.shape[0]
    if fit_intercept:
        intercept = w[-2]
    sigma = w[-1]
    w = w[:n_features]
    n_samples = np.sum(sample_weight)
    linear_loss = y - safe_sparse_dot(X, w)
    if fit_intercept:
        linear_loss -= intercept
    abs_linear_loss = np.abs(linear_loss)
    outliers_mask = abs_linear_loss > epsilon * sigma
    outliers = abs_linear_loss[outliers_mask]
    num_outliers = np.count_nonzero(outliers_mask)
    n_non_outliers = X.shape[0] - num_outliers
    outliers_sw = sample_weight[outliers_mask]
    n_sw_outliers = np.sum(outliers_sw)
    outlier_loss = 2.0 * epsilon * np.sum(outliers_sw * outliers) - sigma * n_sw_outliers * epsilon ** 2
    non_outliers = linear_loss[~outliers_mask]
    weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers
    weighted_loss = np.dot(weighted_non_outliers.T, non_outliers)
    squared_loss = weighted_loss / sigma
    if fit_intercept:
        grad = np.zeros(n_features + 2)
    else:
        grad = np.zeros(n_features + 1)
    X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)
    grad[:n_features] = 2.0 / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers)
    signed_outliers = np.ones_like(outliers)
    signed_outliers_mask = linear_loss[outliers_mask] < 0
    signed_outliers[signed_outliers_mask] = -1.0
    X_outliers = axis0_safe_slice(X, outliers_mask, num_outliers)
    sw_outliers = sample_weight[outliers_mask] * signed_outliers
    grad[:n_features] -= 2.0 * epsilon * safe_sparse_dot(sw_outliers, X_outliers)
    grad[:n_features] += alpha * 2.0 * w
    grad[-1] = n_samples
    grad[-1] -= n_sw_outliers * epsilon ** 2
    grad[-1] -= squared_loss / sigma
    if fit_intercept:
        grad[-2] = -2.0 * np.sum(weighted_non_outliers) / sigma
        grad[-2] -= 2.0 * epsilon * np.sum(sw_outliers)
    loss = n_samples * sigma + squared_loss + outlier_loss
    loss += alpha * np.dot(w, w)
    return (loss, grad)

.sklearn.utils.extmath.safe_sparse_dot

def safe_sparse_dot(a, b, dense_output=False):
    if sparse.issparse(a) or sparse.issparse(b):
        ret = a * b
        if dense_output and hasattr(ret, 'toarray'):
            ret = ret.toarray()
        return ret
    else:
        return np.dot(a, b)

.sklearn.utils.__init__.axis0_safe_slice

def axis0_safe_slice(X, mask, len_mask):
    if len_mask != 0:
        return X[safe_mask(X, mask), :]
    return np.zeros(shape=(0, X.shape[1]))

.sklearn.utils.__init__.safe_mask

def safe_mask(X, mask):
    mask = np.asarray(mask)
    if np.issubdtype(mask.dtype, np.signedinteger):
        return mask
    if hasattr(X, 'toarray'):
        ind = np.arange(mask.shape[0])
        mask = ind[mask]
    return mask

.sklearn.utils.validation._ensure_sparse_format

def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):
    if dtype is None:
        dtype = spmatrix.dtype
    changed_format = False
    if isinstance(accept_sparse, str):
        accept_sparse = [accept_sparse]
    _check_large_sparse(spmatrix, accept_large_sparse)
    if accept_sparse is False:
        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError("When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.")
        if spmatrix.format not in accept_sparse:
            spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
        raise ValueError("Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.".format(accept_sparse))
    if dtype != spmatrix.dtype:
        spmatrix = spmatrix.astype(dtype)
    elif copy and (not changed_format):
        spmatrix = spmatrix.copy()
    if force_all_finite:
        if not hasattr(spmatrix, 'data'):
            warnings.warn("Can't check %s sparse matrix for nan or inf." % spmatrix.format)
        else:
            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')
    return spmatrix

.sklearn.utils.validation._check_large_sparse

def _check_large_sparse(X, accept_large_sparse=False):
    if not accept_large_sparse:
        supported_indices = ['int32']
        if X.getformat() == 'coo':
            index_keys = ['col', 'row']
        elif X.getformat() in ['csr', 'csc', 'bsr']:
            index_keys = ['indices', 'indptr']
        else:
            return
        for key in index_keys:
            indices_datatype = getattr(X, key).dtype
            if indices_datatype not in supported_indices:
                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)


[/PYTHON]
What will be the output of `fit`, given the following input:
[INPUT]
```
{
    "self": {
        "epsilon": 1.35,
        "max_iter": 100,
        "alpha": 0.0001,
        "warm_start": false,
        "fit_intercept": true,
        "tol": 1e-05,
        "n_iter_": 58,
        "scale_": 0.029961783359883615,
        "intercept_": 0.010763322132748835,
        "coef_": "[ 1.54977025e-02 -1.32398131e-02  8.22074812e+01 -2.45727407e-04\n -2.26475828e-02  9.66624010e+01  1.41986418e+00  3.74932146e+00\n  8.83203717e+01 -4.58982512e-02  2.99176896e-02 -5.74773489e-03\n  1.44533947e-02  3.41917964e+01  7.00870246e+01  8.79389962e-03\n  6.14821566e+01  9.94097735e+01  7.74533534e+01 -8.91951416e-03]",
        "outliers_": "[ True False False  True False False False False False False False False\n False False False False False  True False False False  True  True False\n  True  True False False False False False False  True False False  True\n False False False  True  True False False False  True  True  True  True\n False  True]"
    },
    "args": {
        "X": "[[ 1.22991463e+00  2.85932153e+00 -4.23904513e-01 -1.60674512e-01\n   8.10795568e-01  2.37213170e-01  2.50882814e+00  2.83820408e+00\n  -1.48771217e+00 -5.03487421e+00 -3.01419205e+00  2.29815225e+00\n  -2.38715650e+00  2.28208489e+00  3.01889016e+00  2.13555026e+00\n  -1.37317895e+00  2.97466321e-02 -7.51331792e-01 -7.64472749e-02]\n [-4.24663302e-01 -7.57870860e-01  1.48935596e+00  1.31247037e+00\n  -5.00840943e-02 -1.34149673e+00 -1.35978073e+00  4.22628622e-01\n   6.11927193e-01 -8.97400927e-01 -1.07709907e+00 -8.58972388e-01\n   7.45864065e-02 -4.14008116e-02  5.21303748e-01 -8.98942156e-01\n   5.29045238e-01  1.48449581e-01  4.76898369e-01 -8.29964598e-01]\n [ 8.20321797e-01 -6.87299037e-01  6.76460732e-01 -3.70242441e-01\n  -1.17474546e-01 -3.02249730e-01 -5.61330204e-02  1.67094303e+00\n  -2.24258934e-01  4.66166426e-01  2.52496627e-01 -4.53804041e-01\n  -9.18004770e-01 -1.38504274e-03 -3.82008956e-01  4.03264540e-01\n   1.83339199e-01 -1.22619619e+00 -3.75147117e-01  1.35994854e+00]\n [-2.10151591e+00  2.02415811e+00  3.08743286e+00 -8.04229778e-01\n   1.72949820e+00 -4.53738454e-01  1.62232054e+00  5.75589226e-01\n  -1.07693632e+00  3.81692420e+00 -9.51552002e-01  2.88146224e+00\n  -2.29815136e+00  1.61142910e+00  3.51221406e+00  1.94595983e+00\n  -3.10223822e+00  3.82286956e-01  3.20955214e+00 -2.84005186e+00]\n [-2.03068447e+00  3.99046346e-01 -1.18885926e+00  3.90093323e-01\n  -2.77259276e+00 -5.25672963e-02 -3.10886172e-01  8.84220870e-02\n  -5.96314038e-01  1.95591231e+00 -1.16103939e-01 -6.52408582e-01\n   4.93741777e-01  9.74001663e-02 -5.06816354e-01 -3.90953375e-01\n   5.23891024e-01  1.88778597e-01 -1.93627981e+00  2.06449286e+00]\n [ 1.97967290e-01  3.30576756e-01 -5.91402668e-01 -1.77766695e+00\n   9.49246474e-01  8.67407411e-01 -3.57680719e-02 -1.61087840e+00\n   7.55395696e-01 -1.50239657e+00 -7.94636321e-01 -5.32702792e-01\n  -3.46249448e-01  2.38074535e+00  1.12441918e+00  1.09074973e+00\n   2.11679102e+00 -2.83455451e+00 -6.56463675e-01  1.08193522e+00]\n [ 1.02179059e+00 -7.04921353e-01 -1.44494020e+00 -2.90397101e-01\n   6.79974844e-01  1.09463837e+00  1.26507784e+00 -3.50951769e-02\n  -7.88669255e-01 -6.96326654e-01 -4.64337691e-01  1.32778270e+00\n  -8.03141387e-01  2.11497013e-01 -1.21054299e+00 -1.01281486e-01\n   9.36445726e-01  2.13215341e+00  2.34821526e-01 -5.52540673e-01]\n [-6.16264021e-02 -1.33701560e-01 -1.30652685e+00 -7.30677753e-01\n   1.07774381e+00 -6.80178204e-01  6.93773153e-01 -1.34671751e+00\n  -1.18164045e-01 -1.12682581e+00 -2.86887192e-01 -3.84879809e-01\n  -4.21714513e-02 -1.59573438e-01  1.65813068e+00  9.43515893e-02\n  -1.33425847e+00 -4.60719787e-01  6.66383082e-01 -1.07305276e-01]\n [-5.35270165e-01 -2.58279663e+00  2.39582760e-01 -1.35338886e+00\n  -1.15395036e+00  2.13386825e+00 -7.49690345e-01 -5.39132637e-01\n   9.72535789e-01 -3.47961856e-01 -4.06071796e-01 -1.03264310e+00\n  -1.64296529e+00  3.28087476e-02 -3.69801166e-01 -4.36748337e-01\n   7.55740289e-01 -1.93176702e-01  4.06415494e-01  2.54052084e-02]\n [ 4.70433145e-01  3.24869616e-01 -1.11456118e-02 -6.96415784e-02\n   9.97117981e-01 -5.91183104e-01  1.37098901e+00  2.22594433e+00\n  -8.37678042e-01  3.06018243e-02 -3.25669469e-01  5.15749428e-02\n  -8.48320523e-01 -5.09843242e-01  1.14988999e-02  8.67276629e-01\n   3.30035115e-01  3.26962595e-01 -6.67720286e-01  3.11447072e-01]\n [-3.70704003e-01  2.86904488e-01 -9.03820073e-02  5.20040615e-01\n  -2.32059428e+00 -9.96212640e-01 -6.00657558e-01 -7.22870076e-02\n   1.03440989e+00  3.17160626e-01 -1.31839587e+00  2.25608654e-01\n  -6.72756089e-02  1.55224318e+00  1.36759724e+00  4.49712100e-01\n   1.02893549e+00 -3.04963638e-01 -1.21793851e+00 -9.45615796e-01]\n [ 1.46657872e+00  5.67290278e-01 -1.69810582e+00 -1.61647419e+00\n  -2.22675101e-01 -1.02250684e+00  1.64813493e+00 -1.47183501e+00\n  -2.25556423e+00 -3.53431749e-01  1.14110187e+00 -2.91837363e-01\n   8.57923924e-01  1.64227755e-01  3.87280475e-01 -7.61492212e-01\n  -9.85510738e-01 -1.65671510e+00  3.86305518e-02  8.52551939e-01]\n [-1.14761094e+00  4.41032907e-01  3.59504400e-01  2.40787510e-01\n   1.78792866e-01  1.06458514e+00  1.38526155e+00  7.24368505e-01\n  -3.61599281e-01 -7.99422400e-01  9.41923003e-02  2.89120505e-01\n  -1.98398897e-01 -3.03098253e-01 -1.44566817e-01  4.12870820e-01\n  -4.05941727e-01  4.33107953e-01 -9.37880231e-01 -3.58114075e-01]\n [-3.87326817e-01  1.54947426e-01 -2.55298982e+00 -1.98079647e+00\n   3.78162520e-01 -7.42165020e-01  1.53277921e+00 -1.87183850e-01\n   8.64436199e-01 -8.87785748e-01  1.20237985e+00 -3.47912149e-01\n   1.23029068e+00  1.46935877e+00  6.53618595e-01  1.56348969e-01\n   4.57585173e-02 -1.45436567e+00  2.26975462e+00 -3.02302751e-01]\n [-1.43579512e-01 -6.57926093e-01 -3.65551090e-01  1.38914532e+00\n   9.68882639e-01  8.29986159e-01  6.90429024e-01  1.56925961e+00\n   2.96733172e-01  2.25581664e-01 -8.64044991e-01  2.01406015e+00\n  -4.06303130e-01  7.96672108e-01  9.38092541e-01 -3.06765776e-01\n   1.22319836e-02 -7.48049827e-02 -4.96102334e-01 -3.82025449e-01]\n [ 3.97667346e-02  7.72694837e-01 -1.17762896e+00  1.69618157e+00\n  -1.66159829e+00 -1.32988422e-01 -1.82425666e+00  7.20033759e-01\n   1.75498615e+00  4.48195284e-01 -7.07505698e-01 -1.48577034e-02\n   6.70570450e-01  3.03603904e-01 -1.14019630e+00  8.21405937e-01\n   1.03493146e-02  5.55786964e-01 -7.65702194e-01 -1.56699471e+00]\n [-8.52585847e-01 -5.32489919e-01  1.41117206e+00 -6.57951045e-01\n   6.45055273e-01 -3.91217052e-01 -1.68823003e+00 -2.61922373e-02\n  -5.74695185e-02  1.01184243e+00  1.68192174e+00  4.68385234e-01\n  -6.67712721e-01 -1.12465983e-01  7.85803827e-01  1.73587900e+00\n   4.98052405e-01  4.05204080e-01  9.40917615e-01  2.29597556e-02]\n [-4.79655814e-01 -7.64143924e-01  1.06850940e+00 -6.89449185e-01\n  -1.43779147e+00 -1.21407740e+00  5.78521498e-01  1.56703855e-01\n  -6.87837611e-01  1.36453185e+00 -4.77974004e-01 -6.52293600e-01\n  -1.84306955e+00  3.49654457e-01 -4.53385804e-01 -5.21189312e-01\n  -3.64693544e-01 -2.80355495e-01 -4.40922632e-01  6.20358298e-01]\n [ 1.28598401e+00 -1.74235620e+00 -1.55042935e+00  8.95555986e-01\n  -1.30324275e+00  2.38103148e-01  1.15147873e-01 -1.66069981e+00\n  -9.44368491e-01  6.05120084e-01  3.29622982e-01 -1.31908640e-01\n   2.23843563e-01 -3.79147563e-01  4.17318821e-01  4.04761812e-01\n  -1.10489405e-01 -5.90057646e-01 -1.40596292e+00 -1.50699840e+00]\n [-6.34322094e-01 -8.95466561e-01 -1.04855297e+00 -1.18063218e+00\n   3.86902498e-01  1.95077540e+00 -1.61389785e+00  7.77490356e-01\n  -1.70627019e+00 -5.10805138e-01  3.02471898e-01 -2.81822283e-02\n   6.65172224e-02 -2.12740280e-01 -1.42001794e+00  4.28331871e-01\n  -1.25279536e+00 -4.38074302e-01 -5.09652182e-01 -3.62741166e-01]\n [-1.58293840e+00  5.21064876e-01 -6.37437026e-01 -3.19328417e-01\n  -5.75787970e-01 -2.97790879e-01 -8.13364259e-01  1.07961859e+00\n  -1.32880578e-01  1.41953163e-01 -1.38336396e+00  6.91538751e-01\n  -7.25597378e-01 -1.46642433e+00 -3.97271814e-01  6.94749144e-01\n   1.15233156e+00 -1.67600381e+00 -3.09012969e-01  6.10379379e-01]\n [ 1.99300197e-01 -1.05462846e+00 -3.95228983e-01  2.79095764e-01\n   8.20247837e-01  1.94292938e-01 -7.82629156e-01 -9.64612014e-01\n  -8.59307670e-02  4.63130329e-01 -2.20144129e+00  3.38904125e-01\n  -4.68864188e-01 -1.10389299e-01 -1.15942052e+00  2.02104356e+00\n   4.57415606e-01 -1.15107468e-01  8.75832762e-01 -5.06035410e-02]\n [ 4.26258731e-01 -1.42406091e+00 -9.55945000e-01  4.16050046e-01\n  -4.93319883e-01  4.81481474e-01 -5.97316069e-01  2.32181036e-01\n  -4.63595975e-01 -5.42861476e-01 -2.06998503e+00 -1.15618243e+00\n   1.49448454e+00 -2.37921730e-01 -3.45981776e-01  7.81198102e-01\n   1.56506538e-01  6.32619942e-02 -1.54079701e+00  6.76908035e-01]\n [-6.84010898e-01  7.47188334e-01 -7.19604389e-01 -1.18388064e+00\n  -1.18894496e+00 -8.90915083e-01 -7.04700276e-01  2.25672350e+00\n   2.74516358e-01  7.73252977e-01  4.50934462e-01 -2.65917224e+00\n  -1.75589058e+00  9.43260725e-01 -8.12992989e-01  6.06319524e-01\n  -1.57667016e-01 -3.12292251e-01 -1.15735526e+00  1.65955080e+00]\n [-1.14746865e+00 -6.82416053e-02 -1.49125759e+00 -8.26438539e-01\n   1.71334272e+00  6.35031437e-01 -1.31590741e+00  1.11701629e+00\n   1.66673495e-01 -7.44754822e-01 -1.07993151e+00 -9.84525244e-02\n   1.12663592e+00 -4.61584605e-01  4.39391701e-01 -6.63478286e-01\n  -9.12822225e-01  9.44479487e-01  2.38314477e+00 -4.37820045e-01]\n [ 6.14079370e-01  1.86755896e+00  1.88315070e+00  1.91006495e+00\n   9.06044658e-01  9.69396708e-01  1.92294203e+00 -7.47454811e-01\n  -1.27048500e+00 -8.61225685e-01 -1.55010093e-01 -2.68003371e-01\n   9.47251968e-01  1.48051479e+00 -1.34775906e+00  8.02456396e-01\n  -4.13618981e-01  1.94362119e+00 -1.17312341e+00  9.22206672e-01]\n [ 2.59442459e+00 -6.47181432e-01  1.32646164e+00 -1.75316402e-01\n   4.72247150e-01 -2.12523045e-01  2.71170185e-01 -5.25640593e-01\n   5.98946831e-02  9.30408496e-01 -1.54158740e+00 -1.42191987e+00\n  -8.56549308e-01 -8.01496885e-01 -9.64606424e-01  1.99795608e+00\n   9.36398544e-01 -8.87780137e-01 -7.62114512e-01 -4.04032294e-01]\n [-1.63242330e+00 -4.99016638e-01 -4.51303037e-01  1.92753849e-01\n   2.13512238e-02  2.46121252e-02  1.84959125e+00  1.92793845e-02\n   7.23100494e-01 -9.19113445e-01 -3.17543094e-01 -3.65055217e-01\n  -5.85865511e-02 -2.14166656e-01  2.65687975e-01 -1.79132755e+00\n  -1.01697275e-01 -1.10290621e+00  7.19983730e-01 -6.71341546e-02]\n [-3.99449029e-01 -6.28087560e-01 -1.10540657e-01 -1.06001582e+00\n  -4.81027118e-01  1.53637705e+00  6.89818165e-01  1.21114529e+00\n  -6.92049848e-01  2.30391670e+00  5.82953680e-01 -1.35949701e-01\n   9.77249677e-02  1.30184623e+00  1.02017271e+00  1.13689136e+00\n  -1.04525337e+00  6.08843834e-01  2.86343689e-01  3.70055888e-01]\n [ 3.13067702e-01  1.44043571e-01  1.76405235e+00  1.21675016e-01\n   1.45427351e+00  2.24089320e+00 -1.03218852e-01 -1.51357208e-01\n   9.78737984e-01  7.61037725e-01 -2.05158264e-01  4.43863233e-01\n   1.49407907e+00  4.10598502e-01  4.00157208e-01  3.33674327e-01\n   9.50088418e-01 -9.77277880e-01  1.86755799e+00 -8.54095739e-01]\n [ 2.84279671e-01  8.95260273e-01  3.82732430e-01 -1.96862469e+00\n   1.37496407e+00 -2.34215801e-01 -1.17915793e+00 -1.56776772e+00\n   1.09634685e+00 -1.33221165e+00  1.04797216e+00 -6.60056320e-01\n   4.98690275e-01  1.30142807e+00 -3.42422805e-02  1.75818953e-01\n  -1.63263453e+00 -5.81268477e-01 -3.47450652e-01  1.74266878e+00]\n [-2.02896841e-01 -2.23960406e+00 -2.22605681e-01  6.48561063e-02\n   4.01499055e-01 -8.88971358e-01 -2.36958691e+00  1.41232771e+00\n  -1.68121822e+00  1.22487056e+00 -1.82244784e-01 -1.27968917e+00\n  -2.61645446e-01  8.64052300e-01 -9.13079218e-01 -5.85431204e-01\n   9.36742464e-01 -8.88720257e-01  2.42117961e-01 -1.09882779e-01]\n [-1.34792542e+00  1.63928572e-01  2.13480049e-01 -2.67594746e-01\n   9.63213559e-02  1.51826117e+00  1.18137860e+00 -2.55918467e+00\n  -2.42019830e-01  9.42468119e-01  2.03341817e-02 -6.78025782e-01\n  -2.36417382e+00 -6.31903758e-01 -1.20857365e+00  1.29784579e+00\n   1.07819730e+00 -4.43836093e-01 -3.84645423e-01 -7.61573388e-01]\n [-1.04343491e-01 -5.05358317e-01 -3.86870847e-01 -1.05188010e+00\n  -8.15791542e-01 -3.85489760e-01  8.12674042e-01  1.24331938e+00\n   1.83925494e-01 -5.07517602e-01 -1.28455230e+00  2.49720039e+00\n   5.64008535e-01  5.87259379e-01 -5.10292740e-01 -2.24532165e+00\n  -9.32789042e-01 -8.87180942e-01 -1.60183605e+00 -9.88001942e-01]\n [ 1.51999486e+00 -1.44653470e+00 -5.98653937e-01 -2.33466662e-01\n   8.00297949e-01  3.56292817e-01 -1.85053671e-01  5.89255892e-02\n   7.66663182e-01 -3.09114445e-01  1.42061805e-01  1.73272119e+00\n   3.70825001e-01 -8.07648488e-01 -1.11589699e+00  6.84501107e-01\n   8.14519822e-01  3.55481793e-01 -1.76853845e+00  1.71958931e+00]\n [ 3.52816606e-01 -8.17493098e-01 -1.46173269e+00 -2.04732361e+00\n  -1.40134729e+00  1.90311558e-01  9.60693398e-01 -1.18468659e+00\n   3.67544896e-01  1.03043827e+00 -2.63937349e-01 -1.22662166e+00\n  -5.53525480e-02  1.32906285e+00 -6.83439767e-01  9.67446150e-01\n  -5.21579678e-01  1.82272360e+00 -8.51729197e-01 -1.52774424e-01]\n [ 8.00564803e-01 -4.66845546e-01  9.29505111e-01  2.76871906e-01\n  -1.41690611e+00  1.23721914e-01 -5.69312053e-01 -2.73967717e+00\n  -2.09460307e+00  8.68963487e-01  5.29264630e-03 -9.71104570e-01\n   8.21585712e-01  2.69904355e-01  5.82224591e-01  3.14817205e-01\n   9.43046087e-01  9.39532294e-02 -1.30106954e-01  7.82601752e-02]\n [-2.06903676e-01  2.80441705e-01  6.98457149e-01 -2.49458580e-01\n  -9.93123611e-01  3.39964984e-01 -2.67733537e-01 -3.94849514e-01\n   9.31848374e-01  8.41631264e-01 -1.57062341e+00  4.94949817e-02\n   6.43314465e-01 -1.12801133e+00  3.77088909e-03  4.93836776e-01\n  -1.90653494e-01  1.60928168e-01 -1.56821116e-02  8.80178912e-01]\n [-3.11552532e-01  7.29090562e-01 -6.72460448e-01 -1.23482582e+00\n   1.28982911e-01 -1.72628260e+00 -9.07298364e-01  4.62782256e-01\n  -8.13146282e-01  1.13940068e+00 -5.78849665e-01  4.02341641e-01\n  -8.70797149e-01  5.19453958e-02 -3.59553162e-01 -6.84810091e-01\n  -1.63019835e+00 -4.01780936e-01  1.77426142e-01  5.61653422e-02]\n [ 5.09542770e-02  1.08137603e+00  1.15418403e+00 -8.78190343e-01\n  -6.31375988e-01  9.94544570e-02 -1.37075998e+00  3.08751242e-01\n   2.10620213e-02 -2.41337791e-01 -8.58919908e-01  6.99380484e-01\n  -2.22477010e-01  8.65652923e-01  1.72504416e-01 -1.06122229e+00\n  -1.14775325e-01 -1.01673865e+00  2.27392775e-01 -1.79422927e+00]\n [-1.31054012e-01  4.53781913e-01  2.01125668e+00  7.67902408e-01\n  -1.82974041e+00 -1.78156286e+00  8.62789892e-03  6.16886554e-01\n   1.95069697e-01  3.70057219e-02 -1.11831192e+00  5.89879821e-01\n  -8.05626508e-01  5.27004208e-01 -4.45954265e-02 -3.63858810e-01\n   3.54757693e-01  1.96557401e-01 -7.29044659e-01  1.13307988e+00]\n [-9.30156503e-01  5.43311891e-01 -1.95180410e+00 -1.08403662e+00\n   4.39042958e-01  7.84957521e-01 -3.92388998e-01  4.45393251e-01\n  -1.13980246e+00 -2.19541028e-01 -2.16731471e-01  3.51780111e-01\n  -4.70032883e-01 -3.04614305e+00 -6.59891730e-01  3.79235534e-01\n  -2.16949570e-01 -4.70637658e-01 -5.54309627e-01 -1.78589092e-01]\n [ 2.25930895e+00  9.10178908e-01 -3.69181838e-01 -4.66419097e-01\n   3.17218215e-01  6.55263731e-01  2.79924599e-01 -7.38030909e-01\n   1.09965960e+00  7.86327962e-01  3.79151736e-01 -9.44446256e-01\n  -1.70204139e-02 -9.81503896e-02 -2.39379178e-01 -4.10049693e-01\n  -2.43261244e-02 -1.61695604e+00  6.40131526e-01 -4.22571517e-02]\n [-7.92286662e-01  1.59277075e+00 -1.29868672e+00 -1.37808347e+00\n  -2.58572632e-01  2.05332564e-01  3.64481249e-01 -2.59576982e-01\n   1.32501405e+00  3.08331246e-01  1.68157672e+00 -3.11976108e-01\n  -1.00683175e+00  1.47132196e+00  1.27607535e+00 -8.40290395e-01\n  -2.76432845e-01  2.33962481e+00  4.51340154e-02 -5.31605908e-01]\n [ 7.40510770e-01  2.08106150e+00 -3.03396547e+00 -1.73255242e+00\n  -1.10070246e-01 -2.14620904e-01  2.73093436e+00 -1.95391443e-01\n  -4.85190914e+00 -9.06111604e-01 -9.41542009e-01  1.94603199e+00\n  -2.55629824e+00  2.87474136e+00 -1.55409133e-01  2.17926033e+00\n   1.93085335e-01  2.83733422e+00  2.33654627e+00  1.89437189e+00]\n [ 3.96006713e-01 -7.69916074e-01  3.76425531e-01  3.18305583e-02\n   5.39249191e-01  1.32638590e+00  6.72294757e-01  1.84926373e+00\n   2.98238174e-01 -6.74332661e-01 -2.08298756e-01 -6.35846078e-01\n   5.76590817e-01  4.07461836e-01 -1.09940079e+00  6.76433295e-01\n  -4.35153552e-01 -1.49634540e-01 -6.94567860e-01 -1.09306151e+00]\n [-1.01804188e+00 -1.02993528e+00  7.71405949e-01  1.29802197e+00\n  -3.49943365e-01 -4.24317621e-01 -4.57039607e-02  5.53132064e-01\n  -9.08763246e-01  1.10028434e+00 -5.14233966e-01  2.69622405e+00\n  -6.58552967e-01  2.20507656e-01  1.02943883e+00 -7.39246663e-02\n   1.51332808e+00 -2.65561909e+00  8.62596011e-01 -7.78547559e-02]\n [ 2.17097407e+00  4.76444890e+00 -8.12047473e-01  5.32890682e-01\n  -2.71142745e+00 -2.28205064e-01 -1.68846173e+00  1.41128162e+00\n  -7.97572347e-01 -1.65439307e+00 -8.31489403e-01 -1.04902439e+00\n   1.62620254e+00 -4.58501258e-01  4.32343475e+00 -1.91386286e+00\n   1.34621664e-01  4.12997674e-01 -9.13762652e-01 -2.11995153e+00]\n [ 1.12859406e+00 -2.01640663e+00 -5.17519043e-01 -7.09727966e-01\n  -5.39454633e-01  1.81338429e-01 -2.28862004e+00 -7.93117363e-01\n  -4.39189522e-01 -2.75670535e-01 -8.82418819e-01  1.73887268e+00\n   1.31913688e+00  2.51484415e-01 -9.78829859e-01  9.94394391e-01\n  -9.60504382e-01  2.41245368e+00 -5.02816701e-01  4.96000946e-01]\n [ 2.16323595e+00 -1.29285691e+00 -3.53993911e-01 -1.16809350e+00\n   2.67050869e-01 -2.22340315e+00 -7.39562996e-01  5.21650793e-02\n  -6.43618403e-01 -3.92828182e-02  8.23504154e-01  5.23276661e-01\n   7.71790551e-01  1.54301460e+00 -1.37495129e+00 -1.71546331e-01\n  -1.10438334e+00 -1.60205766e+00  6.25231451e-01  1.33652795e+00]]",
        "y": "[ 1.20866536e+02  1.65803518e+02 -1.53756503e+02  4.69095117e+02\n -2.86860094e+02  5.35979427e+01  1.29164776e+02 -1.53076680e+02\n  3.42737911e+02 -1.37362797e+02  7.40341003e+01 -6.30316704e+02\n  3.01786231e+01 -7.36119863e+01  1.31035960e+02 -2.74922835e+01\n  2.65667308e+02 -1.93082191e+02 -3.51896400e+02 -3.14499314e+02\n -2.87778330e+02 -2.65077974e+01 -2.08842388e+02 -2.68681688e+02\n  1.93296790e+02  1.69420194e+02 -9.25306276e+01 -1.69610825e+01\n  2.18255739e+02  5.95331662e+02 -4.48494943e+01 -2.97274829e+02\n  2.11881438e+01 -3.32267944e+02 -1.04423680e+02  8.39884740e+00\n -3.81137659e-01  1.35426279e+02 -4.43431425e+02  5.66710656e+01\n  1.23973757e+01 -4.37652098e+02 -5.07097220e+00  3.88478984e+02\n -2.02361875e+02  3.47374988e+01 -8.03250608e+01  2.70115543e+02\n  1.18312506e+01 -5.23890666e+02]"
    },
    "kwargs": {
        "sample_weight": "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1.]"
    }
}
```
[/INPUT]

[STRUCTURE]
```
{
    "epsilon": XXX,
    "max_iter": XXX,
    "alpha": XXX,
    "warm_start": XXX,
    "fit_intercept": XXX,
    "tol": XXX,
    "n_iter_": XXX,
    "scale_": XXX,
    "intercept_": XXX,
    "coef_": XXX,
    "outliers_": XXX
}
```
[/STRUCTURE]
